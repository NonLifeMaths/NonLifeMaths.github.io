<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 10 Prior Ratemaking | Non Life Insurance Mathematics</title>
  <meta name="description" content="Chapter 10 Prior Ratemaking | Non Life Insurance Mathematics" />
  <meta name="generator" content="bookdown 0.35 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 10 Prior Ratemaking | Non Life Insurance Mathematics" />
  <meta property="og:type" content="book" />
  
  
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 10 Prior Ratemaking | Non Life Insurance Mathematics" />
  
  
  

<meta name="author" content="Arthur Charpentier and Michel Denuit" />


<meta name="date" content="2023-09-05" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="chap8.html"/>
<link rel="next" href="chap10.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Non Life Insurance Mathematics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#general-context"><i class="fa fa-check"></i><b>1.1</b> General Context</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#the-risk-and-its-contractual-coverage"><i class="fa fa-check"></i><b>1.2</b> The risk and its contractual coverage</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#actuarial-risk-modeling"><i class="fa fa-check"></i><b>1.3</b> Actuarial risk modeling</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#pure-premium"><i class="fa fa-check"></i><b>1.4</b> Pure premium</a></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#from-pure-to-net-premium"><i class="fa fa-check"></i><b>1.5</b> From pure to net Premium</a></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#measurement-and-comparison-of-risks"><i class="fa fa-check"></i><b>1.6</b> Measurement and comparison of risks</a></li>
<li class="chapter" data-level="1.7" data-path="intro.html"><a href="intro.html#collective-model"><i class="fa fa-check"></i><b>1.7</b> Collective model</a></li>
<li class="chapter" data-level="1.8" data-path="intro.html"><a href="intro.html#solvency"><i class="fa fa-check"></i><b>1.8</b> Solvency</a></li>
<li class="chapter" data-level="1.9" data-path="intro.html"><a href="intro.html#multiple-risks"><i class="fa fa-check"></i><b>1.9</b> Multiple risks</a></li>
<li class="chapter" data-level="1.10" data-path="intro.html"><a href="intro.html#prior-ratemaking"><i class="fa fa-check"></i><b>1.10</b> Prior ratemaking</a></li>
<li class="chapter" data-level="1.11" data-path="intro.html"><a href="intro.html#credibility"><i class="fa fa-check"></i><b>1.11</b> Credibility</a></li>
<li class="chapter" data-level="1.12" data-path="intro.html"><a href="intro.html#bonus-malus"><i class="fa fa-check"></i><b>1.12</b> Bonus-Malus</a></li>
<li class="chapter" data-level="1.13" data-path="intro.html"><a href="intro.html#economics-of-insurance"><i class="fa fa-check"></i><b>1.13</b> Economics of insurance</a></li>
<li class="chapter" data-level="1.14" data-path="intro.html"><a href="intro.html#claims-reserving"><i class="fa fa-check"></i><b>1.14</b> Claims reserving</a></li>
<li class="chapter" data-level="1.15" data-path="intro.html"><a href="intro.html#large-risks-and-extreme-value"><i class="fa fa-check"></i><b>1.15</b> Large risks and extreme value</a></li>
<li class="chapter" data-level="1.16" data-path="intro.html"><a href="intro.html#monte-carlo"><i class="fa fa-check"></i><b>1.16</b> Monte Carlo</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chap1.html"><a href="chap1.html"><i class="fa fa-check"></i><b>2</b> The risk and its contractual coverage</a>
<ul>
<li class="chapter" data-level="2.1" data-path="chap1.html"><a href="chap1.html#risk"><i class="fa fa-check"></i><b>2.1</b> Risk</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="chap1.html"><a href="chap1.html#did-you-say-risk"><i class="fa fa-check"></i><b>2.1.1</b> Did you say risk?</a></li>
<li class="chapter" data-level="2.1.2" data-path="chap1.html"><a href="chap1.html#the-reason-for-insurance-risquophobia"><i class="fa fa-check"></i><b>2.1.2</b> The reason for insurance: risquophobia</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="chap1.html"><a href="chap1.html#risk-management-methods"><i class="fa fa-check"></i><b>2.2</b> Risk management methods</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="chap1.html"><a href="chap1.html#caution-and-self-insurance"><i class="fa fa-check"></i><b>2.2.1</b> Caution and self-insurance</a></li>
<li class="chapter" data-level="2.2.2" data-path="chap1.html"><a href="chap1.html#risk-management-vehicles"><i class="fa fa-check"></i><b>2.2.2</b> Risk management vehicles</a></li>
<li class="chapter" data-level="2.2.3" data-path="chap1.html"><a href="chap1.html#risks-assumed-by-insurers"><i class="fa fa-check"></i><b>2.2.3</b> Risks assumed by insurers</a></li>
<li class="chapter" data-level="2.2.4" data-path="chap1.html"><a href="chap1.html#risk-management-by-the-insurer"><i class="fa fa-check"></i><b>2.2.4</b> Risk management by the insurer</a></li>
<li class="chapter" data-level="2.2.5" data-path="chap1.html"><a href="chap1.html#transfer-of-risks"><i class="fa fa-check"></i><b>2.2.5</b> Transfer of risks</a></li>
<li class="chapter" data-level="2.2.6" data-path="chap1.html"><a href="chap1.html#insurance-and-financial-risks"><i class="fa fa-check"></i><b>2.2.6</b> Insurance and financial risks</a></li>
<li class="chapter" data-level="2.2.7" data-path="chap1.html"><a href="chap1.html#common-point-risk"><i class="fa fa-check"></i><b>2.2.7</b> Common point: risk</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="chap1.html"><a href="chap1.html#alea-iacta-est"><i class="fa fa-check"></i><b>2.3</b> <em>Alea iacta est…</em></a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="chap1.html"><a href="chap1.html#insolvency-of-the-insurer"><i class="fa fa-check"></i><b>2.3.1</b> Insolvency of the insurer</a></li>
<li class="chapter" data-level="2.3.2" data-path="chap1.html"><a href="chap1.html#settlements"><i class="fa fa-check"></i><b>2.3.2</b> Settlements</a></li>
<li class="chapter" data-level="2.3.3" data-path="chap1.html"><a href="chap1.html#necessity-of-reserving"><i class="fa fa-check"></i><b>2.3.3</b> Necessity of reserving</a></li>
<li class="chapter" data-level="2.3.4" data-path="chap1.html"><a href="chap1.html#inversion-of-production-cycle"><i class="fa fa-check"></i><b>2.3.4</b> Inversion of production cycle</a></li>
<li class="chapter" data-level="2.3.5" data-path="chap1.html"><a href="chap1.html#liabilities-a-reflection-of-the-insurers-business"><i class="fa fa-check"></i><b>2.3.5</b> Liabilities: a reflection of the insurer’s business</a></li>
<li class="chapter" data-level="2.3.6" data-path="chap1.html"><a href="chap1.html#liabilities-of-an-insurance-company"><i class="fa fa-check"></i><b>2.3.6</b> Liabilities of an insurance company</a></li>
<li class="chapter" data-level="2.3.7" data-path="chap1.html"><a href="chap1.html#the-assets-of-an-insurance-company"><i class="fa fa-check"></i><b>2.3.7</b> The assets of an insurance company</a></li>
<li class="chapter" data-level="2.3.8" data-path="chap1.html"><a href="chap1.html#premiums-written-premiums-earned"><i class="fa fa-check"></i><b>2.3.8</b> Premiums written, premiums earned</a></li>
<li class="chapter" data-level="2.3.9" data-path="chap1.html"><a href="chap1.html#insurers-institutional-investors"><i class="fa fa-check"></i><b>2.3.9</b> Insurers, institutional investors</a></li>
<li class="chapter" data-level="2.3.10" data-path="chap1.html"><a href="chap1.html#asset-and-liability-management"><i class="fa fa-check"></i><b>2.3.10</b> Asset and liability management</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="chap1.html"><a href="chap1.html#the-insurance-contract"><i class="fa fa-check"></i><b>2.4</b> The insurance contract</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="chap1.html"><a href="chap1.html#the-origins-the-marine-insurance-contract"><i class="fa fa-check"></i><b>2.4.1</b> The origins: the marine insurance contract</a></li>
<li class="chapter" data-level="2.4.2" data-path="chap1.html"><a href="chap1.html#the-birth-of-terrestrial-insurance-the-great-fire-of-london"><i class="fa fa-check"></i><b>2.4.2</b> The birth of terrestrial insurance: the great fire of London</a></li>
<li class="chapter" data-level="2.4.3" data-path="chap1.html"><a href="chap1.html#from-informal-solidarity-to-insurance"><i class="fa fa-check"></i><b>2.4.3</b> From informal solidarity to insurance</a></li>
<li class="chapter" data-level="2.4.4" data-path="chap1.html"><a href="chap1.html#contract-and-policy"><i class="fa fa-check"></i><b>2.4.4</b> Contract and policy</a></li>
<li class="chapter" data-level="2.4.5" data-path="chap1.html"><a href="chap1.html#insured-policyholder-and-beneficiary"><i class="fa fa-check"></i><b>2.4.5</b> Insured, policyholder and beneficiary</a></li>
<li class="chapter" data-level="2.4.6" data-path="chap1.html"><a href="chap1.html#what-about-technology"><i class="fa fa-check"></i><b>2.4.6</b> What about technology?</a></li>
<li class="chapter" data-level="2.4.7" data-path="chap1.html"><a href="chap1.html#party-representations"><i class="fa fa-check"></i><b>2.4.7</b> Party representations</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="chap1.html"><a href="chap1.html#bibliographical-notes"><i class="fa fa-check"></i><b>2.5</b> Bibliographical notes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chap2.html"><a href="chap2.html"><i class="fa fa-check"></i><b>3</b> Actuarial risk modeling</a>
<ul>
<li class="chapter" data-level="3.1" data-path="chap2.html"><a href="chap2.html#introduction"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="chap2.html"><a href="chap2.html#probabilistic-description-of-risk"><i class="fa fa-check"></i><b>3.2</b> Probabilistic description of risk</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="chap2.html"><a href="chap2.html#events"><i class="fa fa-check"></i><b>3.2.1</b> Events</a></li>
<li class="chapter" data-level="3.2.2" data-path="chap2.html"><a href="chap2.html#elementary-events"><i class="fa fa-check"></i><b>3.2.2</b> Elementary events</a></li>
<li class="chapter" data-level="3.2.3" data-path="chap2.html"><a href="chap2.html#set-formalism"><i class="fa fa-check"></i><b>3.2.3</b> Set formalism</a></li>
<li class="chapter" data-level="3.2.4" data-path="chap2.html"><a href="chap2.html#properties-satisfied-by-the-set-of-events"><i class="fa fa-check"></i><b>3.2.4</b> Properties satisfied by the set of events</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="chap2.html"><a href="chap2.html#probability-calculation-and-lack-of-arbitrage-opportunity"><i class="fa fa-check"></i><b>3.3</b> Probability calculation and lack of arbitrage opportunity</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="chap2.html"><a href="chap2.html#the-notion-of-probability"><i class="fa fa-check"></i><b>3.3.1</b> The notion of probability</a></li>
<li class="chapter" data-level="3.3.2" data-path="chap2.html"><a href="chap2.html#risk-and-uncertainty"><i class="fa fa-check"></i><b>3.3.2</b> Risk and uncertainty</a></li>
<li class="chapter" data-level="3.3.3" data-path="chap2.html"><a href="chap2.html#probability-and-insurance-premium"><i class="fa fa-check"></i><b>3.3.3</b> Probability and insurance premium</a></li>
<li class="chapter" data-level="3.3.4" data-path="chap2.html"><a href="chap2.html#absence-of-arbitrage-opportunity"><i class="fa fa-check"></i><b>3.3.4</b> Absence of arbitrage opportunity</a></li>
<li class="chapter" data-level="3.3.5" data-path="chap2.html"><a href="chap2.html#property-of-additivity-for-incompatible-events"><i class="fa fa-check"></i><b>3.3.5</b> Property of additivity for incompatible events</a></li>
<li class="chapter" data-level="3.3.6" data-path="chap2.html"><a href="chap2.html#premiums-grow-with-risk"><i class="fa fa-check"></i><b>3.3.6</b> Premiums grow with risk</a></li>
<li class="chapter" data-level="3.3.7" data-path="chap2.html"><a href="chap2.html#fairness-property"><i class="fa fa-check"></i><b>3.3.7</b> Fairness property</a></li>
<li class="chapter" data-level="3.3.8" data-path="chap2.html"><a href="chap2.html#subadditivity-property"><i class="fa fa-check"></i><b>3.3.8</b> Subadditivity property</a></li>
<li class="chapter" data-level="3.3.9" data-path="chap2.html"><a href="chap2.html#poincaré-equality"><i class="fa fa-check"></i><b>3.3.9</b> Poincaré equality</a></li>
<li class="chapter" data-level="3.3.10" data-path="chap2.html"><a href="chap2.html#conditional-probability"><i class="fa fa-check"></i><b>3.3.10</b> Conditional probability</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="chap2.html"><a href="chap2.html#independent-events"><i class="fa fa-check"></i><b>3.4</b> Independent events</a></li>
<li class="chapter" data-level="3.5" data-path="chap2.html"><a href="chap2.html#multiplication-rule-bayes"><i class="fa fa-check"></i><b>3.5</b> Multiplication rule (Bayes)</a></li>
<li class="chapter" data-level="3.6" data-path="chap2.html"><a href="chap2.html#conditionally-independent-events"><i class="fa fa-check"></i><b>3.6</b> Conditionally independent events</a></li>
<li class="chapter" data-level="3.7" data-path="chap2.html"><a href="chap2.html#total-probability-theorem"><i class="fa fa-check"></i><b>3.7</b> Total probability theorem</a></li>
<li class="chapter" data-level="3.8" data-path="chap2.html"><a href="chap2.html#bayes-theorem"><i class="fa fa-check"></i><b>3.8</b> Bayes’ theorem</a></li>
<li class="chapter" data-level="3.9" data-path="chap2.html"><a href="chap2.html#random-variables"><i class="fa fa-check"></i><b>3.9</b> Random variables</a>
<ul>
<li class="chapter" data-level="3.9.1" data-path="chap2.html"><a href="chap2.html#definition"><i class="fa fa-check"></i><b>3.9.1</b> Definition</a></li>
<li class="chapter" data-level="3.9.2" data-path="chap2.html"><a href="chap2.html#distribution-function"><i class="fa fa-check"></i><b>3.9.2</b> Distribution function</a></li>
<li class="chapter" data-level="3.9.3" data-path="chap2.html"><a href="chap2.html#support-of-a-random-variable"><i class="fa fa-check"></i><b>3.9.3</b> Support of a random variable</a></li>
<li class="chapter" data-level="3.9.4" data-path="chap2.html"><a href="chap2.html#tail-or-survival-function"><i class="fa fa-check"></i><b>3.9.4</b> Tail (or Survival) function</a></li>
<li class="chapter" data-level="3.9.5" data-path="chap2.html"><a href="chap2.html#equality-in-distribution"><i class="fa fa-check"></i><b>3.9.5</b> Equality in distribution</a></li>
<li class="chapter" data-level="3.9.6" data-path="chap2.html"><a href="chap2.html#quantiles-and-generalized-inverses"><i class="fa fa-check"></i><b>3.9.6</b> Quantiles and generalized inverses</a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="chap2.html"><a href="chap2.html#discrete-random-variables-and-counts"><i class="fa fa-check"></i><b>3.10</b> Discrete random variables and counts</a>
<ul>
<li class="chapter" data-level="3.10.1" data-path="chap2.html"><a href="chap2.html#notion"><i class="fa fa-check"></i><b>3.10.1</b> Notion</a></li>
<li class="chapter" data-level="3.10.2" data-path="chap2.html"><a href="chap2.html#uniform-discrete-variable"><i class="fa fa-check"></i><b>3.10.2</b> Uniform discrete variable</a></li>
<li class="chapter" data-level="3.10.3" data-path="chap2.html"><a href="chap2.html#bernoulli-variables"><i class="fa fa-check"></i><b>3.10.3</b> Bernoulli variables</a></li>
<li class="chapter" data-level="3.10.4" data-path="chap2.html"><a href="chap2.html#binomial-variable"><i class="fa fa-check"></i><b>3.10.4</b> Binomial variable</a></li>
<li class="chapter" data-level="3.10.5" data-path="chap2.html"><a href="chap2.html#geometric-variable"><i class="fa fa-check"></i><b>3.10.5</b> Geometric variable</a></li>
<li class="chapter" data-level="3.10.6" data-path="chap2.html"><a href="chap2.html#negative-binomial-variable"><i class="fa fa-check"></i><b>3.10.6</b> Negative binomial variable</a></li>
<li class="chapter" data-level="3.10.7" data-path="chap2.html"><a href="chap2.html#poissons-distribution"><i class="fa fa-check"></i><b>3.10.7</b> Poisson’s distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.11" data-path="chap2.html"><a href="chap2.html#continuous-random-variables"><i class="fa fa-check"></i><b>3.11</b> Continuous random variables</a>
<ul>
<li class="chapter" data-level="3.11.1" data-path="chap2.html"><a href="chap2.html#notion-1"><i class="fa fa-check"></i><b>3.11.1</b> Notion</a></li>
<li class="chapter" data-level="3.11.2" data-path="chap2.html"><a href="chap2.html#continuous-uniform-distribution"><i class="fa fa-check"></i><b>3.11.2</b> Continuous uniform distribution</a></li>
<li class="chapter" data-level="3.11.3" data-path="chap2.html"><a href="chap2.html#beta-distribution"><i class="fa fa-check"></i><b>3.11.3</b> Beta Distribution</a></li>
<li class="chapter" data-level="3.11.4" data-path="chap2.html"><a href="chap2.html#normal-or-gaussian-distribution"><i class="fa fa-check"></i><b>3.11.4</b> Normal (or Gaussian) distribution</a></li>
<li class="chapter" data-level="3.11.5" data-path="chap2.html"><a href="chap2.html#log-normal-variable"><i class="fa fa-check"></i><b>3.11.5</b> Log-normal variable</a></li>
<li class="chapter" data-level="3.11.6" data-path="chap2.html"><a href="chap2.html#negative-exponential-distribution"><i class="fa fa-check"></i><b>3.11.6</b> (Negative) exponential distribution</a></li>
<li class="chapter" data-level="3.11.7" data-path="chap2.html"><a href="chap2.html#gamma-distribution"><i class="fa fa-check"></i><b>3.11.7</b> Gamma distribution</a></li>
<li class="chapter" data-level="3.11.8" data-path="chap2.html"><a href="chap2.html#pareto-distribution"><i class="fa fa-check"></i><b>3.11.8</b> Pareto distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.12" data-path="chap2.html"><a href="chap2.html#random-vector"><i class="fa fa-check"></i><b>3.12</b> Random vector</a>
<ul>
<li class="chapter" data-level="3.12.1" data-path="chap2.html"><a href="chap2.html#definition-1"><i class="fa fa-check"></i><b>3.12.1</b> Definition</a></li>
<li class="chapter" data-level="3.12.2" data-path="chap2.html"><a href="chap2.html#distribution-function-1"><i class="fa fa-check"></i><b>3.12.2</b> Distribution function</a></li>
<li class="chapter" data-level="3.12.3" data-path="chap2.html"><a href="chap2.html#support-of-vector-xvec"><i class="fa fa-check"></i><b>3.12.3</b> Support of vector <span class="math inline">\(\Xvec\)</span></a></li>
<li class="chapter" data-level="3.12.4" data-path="chap2.html"><a href="chap2.html#independence"><i class="fa fa-check"></i><b>3.12.4</b> Independence</a></li>
<li class="chapter" data-level="3.12.5" data-path="chap2.html"><a href="chap2.html#gaussian-vector"><i class="fa fa-check"></i><b>3.12.5</b> Gaussian vector</a></li>
<li class="chapter" data-level="3.12.6" data-path="chap2.html"><a href="chap2.html#ellipticpart"><i class="fa fa-check"></i><b>3.12.6</b> Elliptical vectors</a></li>
<li class="chapter" data-level="3.12.7" data-path="chap2.html"><a href="chap2.html#definition-2"><i class="fa fa-check"></i><b>3.12.7</b> Definition</a></li>
<li class="chapter" data-level="3.12.8" data-path="chap2.html"><a href="chap2.html#multinomial-vector"><i class="fa fa-check"></i><b>3.12.8</b> Multinomial vector</a></li>
</ul></li>
<li class="chapter" data-level="3.13" data-path="chap2.html"><a href="chap2.html#conditional-variables"><i class="fa fa-check"></i><b>3.13</b> Conditional Variables</a>
<ul>
<li class="chapter" data-level="3.13.1" data-path="chap2.html"><a href="chap2.html#the-case-of-counting-variables"><i class="fa fa-check"></i><b>3.13.1</b> The case of counting variables</a></li>
<li class="chapter" data-level="3.13.2" data-path="chap2.html"><a href="chap2.html#the-case-of-continuous-variables"><i class="fa fa-check"></i><b>3.13.2</b> The case of continuous variables</a></li>
<li class="chapter" data-level="3.13.3" data-path="chap2.html"><a href="chap2.html#the-mixed-case-one-counting-variable-and-another-continuous"><i class="fa fa-check"></i><b>3.13.3</b> The mixed case: one counting variable and another continuous</a></li>
<li class="chapter" data-level="3.13.4" data-path="chap2.html"><a href="chap2.html#conditional-independence"><i class="fa fa-check"></i><b>3.13.4</b> Conditional independence</a></li>
</ul></li>
<li class="chapter" data-level="3.14" data-path="chap2.html"><a href="chap2.html#compound-distributions"><i class="fa fa-check"></i><b>3.14</b> Compound distributions</a>
<ul>
<li class="chapter" data-level="3.14.1" data-path="chap2.html"><a href="chap2.html#definition-4"><i class="fa fa-check"></i><b>3.14.1</b> Definition</a></li>
<li class="chapter" data-level="3.14.2" data-path="chap2.html"><a href="chap2.html#SecProdConv"><i class="fa fa-check"></i><b>3.14.2</b> Convolution product</a></li>
<li class="chapter" data-level="3.14.3" data-path="chap2.html"><a href="chap2.html#distribution-function-associated-with-a-compound-distribution"><i class="fa fa-check"></i><b>3.14.3</b> Distribution function associated with a compound distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.15" data-path="chap2.html"><a href="chap2.html#RiskTransformations"><i class="fa fa-check"></i><b>3.15</b> Transformations of risks and conventional damage clauses</a>
<ul>
<li class="chapter" data-level="3.15.1" data-path="chap2.html"><a href="chap2.html#concept"><i class="fa fa-check"></i><b>3.15.1</b> Concept</a></li>
<li class="chapter" data-level="3.15.2" data-path="chap2.html"><a href="chap2.html#DecOblig"><i class="fa fa-check"></i><b>3.15.2</b> The compulsory overdraft</a></li>
<li class="chapter" data-level="3.15.3" data-path="chap2.html"><a href="chap2.html#the-deductible"><i class="fa fa-check"></i><b>3.15.3</b> The deductible</a></li>
<li class="chapter" data-level="3.15.4" data-path="chap2.html"><a href="chap2.html#upper-limit-of-indemnity"><i class="fa fa-check"></i><b>3.15.4</b> (Upper) limit of indemnity</a></li>
<li class="chapter" data-level="3.15.5" data-path="chap2.html"><a href="chap2.html#technical-consequence-censored-data"><i class="fa fa-check"></i><b>3.15.5</b> Technical consequence: censored data</a></li>
<li class="chapter" data-level="3.15.6" data-path="chap2.html"><a href="chap2.html#poissons-distribution-and-damage-clauses"><i class="fa fa-check"></i><b>3.15.6</b> Poisson’s distribution and damage clauses</a></li>
<li class="chapter" data-level="3.15.7" data-path="chap2.html"><a href="chap2.html#perverse-effects-of-contractual-damage-clauses"><i class="fa fa-check"></i><b>3.15.7</b> Perverse effects of contractual damage clauses</a></li>
</ul></li>
<li class="chapter" data-level="3.16" data-path="chap2.html"><a href="chap2.html#exercises"><i class="fa fa-check"></i><b>3.16</b> Exercises</a></li>
<li class="chapter" data-level="3.17" data-path="chap2.html"><a href="chap2.html#bibliographical-notes-1"><i class="fa fa-check"></i><b>3.17</b> Bibliographical notes</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="chap3.html"><a href="chap3.html"><i class="fa fa-check"></i><b>4</b> Pure Premium</a>
<ul>
<li class="chapter" data-level="4.1" data-path="chap3.html"><a href="chap3.html#introduction-1"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="chap3.html"><a href="chap3.html#pure-premium-and-mathematical-expectation"><i class="fa fa-check"></i><b>4.2</b> Pure Premium and Mathematical Expectation</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="chap3.html"><a href="chap3.html#mathematical-expectation"><i class="fa fa-check"></i><b>4.2.1</b> Mathematical Expectation</a></li>
<li class="chapter" data-level="4.2.2" data-path="chap3.html"><a href="chap3.html#probabilities-and-expectations-of-indicators"><i class="fa fa-check"></i><b>4.2.2</b> Probabilities and Expectations of Indicators</a></li>
<li class="chapter" data-level="4.2.3" data-path="chap3.html"><a href="chap3.html#determination-of-the-pure-premium"><i class="fa fa-check"></i><b>4.2.3</b> Determination of the Pure Premium</a></li>
<li class="chapter" data-level="4.2.4" data-path="chap3.html"><a href="chap3.html#mean-squared-error-is-it-a-must"><i class="fa fa-check"></i><b>4.2.4</b> Mean Squared Error, Is It a Must?</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="chap3.html"><a href="chap3.html#variance"><i class="fa fa-check"></i><b>4.3</b> Variance</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="chap3.html"><a href="chap3.html#definition-5"><i class="fa fa-check"></i><b>4.3.1</b> Definition</a></li>
<li class="chapter" data-level="4.3.2" data-path="chap3.html"><a href="chap3.html#actuarial-interpretation"><i class="fa fa-check"></i><b>4.3.2</b> Actuarial Interpretation</a></li>
<li class="chapter" data-level="4.3.3" data-path="chap3.html"><a href="chap3.html#some-examples"><i class="fa fa-check"></i><b>4.3.3</b> Some Examples</a></li>
<li class="chapter" data-level="4.3.4" data-path="chap3.html"><a href="chap3.html#properties"><i class="fa fa-check"></i><b>4.3.4</b> Properties</a></li>
<li class="chapter" data-level="4.3.5" data-path="chap3.html"><a href="chap3.html#variance-of-common-distributions"><i class="fa fa-check"></i><b>4.3.5</b> Variance of Common Distributions</a></li>
<li class="chapter" data-level="4.3.6" data-path="chap3.html"><a href="chap3.html#variance-of-composite-distributions"><i class="fa fa-check"></i><b>4.3.6</b> Variance of Composite Distributions</a></li>
<li class="chapter" data-level="4.3.7" data-path="chap3.html"><a href="chap3.html#coefficient-of-variation-and-risk-pooling"><i class="fa fa-check"></i><b>4.3.7</b> Coefficient of Variation and Risk Pooling</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="chap3.html"><a href="chap3.html#insurance-and-bienaymé-chebyshev-inequality"><i class="fa fa-check"></i><b>4.4</b> Insurance and Bienaymé-Chebyshev Inequality</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="chap3.html"><a href="chap3.html#markovs-inequality"><i class="fa fa-check"></i><b>4.4.1</b> Markov’s Inequality</a></li>
<li class="chapter" data-level="4.4.2" data-path="chap3.html"><a href="chap3.html#bienaymé-chebyshev-inequality"><i class="fa fa-check"></i><b>4.4.2</b> Bienaymé-Chebyshev Inequality</a></li>
<li class="chapter" data-level="4.4.3" data-path="chap3.html"><a href="chap3.html#actuarial-interpretation-of-the-bienaymé-chebyshev-inequality"><i class="fa fa-check"></i><b>4.4.3</b> Actuarial Interpretation of the Bienaymé-Chebyshev Inequality</a></li>
<li class="chapter" data-level="4.4.4" data-path="chap3.html"><a href="chap3.html#conservative-nature-of-the-bienaymé-chebyshev-inequality"><i class="fa fa-check"></i><b>4.4.4</b> Conservative Nature of the Bienaymé-Chebyshev Inequality</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="chap3.html"><a href="chap3.html#insurance-and-law-of-large-numbers"><i class="fa fa-check"></i><b>4.5</b> Insurance and Law of Large Numbers</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="chap3.html"><a href="chap3.html#convergence-in-probability"><i class="fa fa-check"></i><b>4.5.1</b> Convergence in Probability</a></li>
<li class="chapter" data-level="4.5.2" data-path="chap3.html"><a href="chap3.html#convergence-of-average-claim-amount-per-policy-to-the-pure-premium"><i class="fa fa-check"></i><b>4.5.2</b> Convergence of Average Claim Amount per Policy to the Pure Premium</a></li>
<li class="chapter" data-level="4.5.3" data-path="chap3.html"><a href="chap3.html#case-of-flat-indemnity"><i class="fa fa-check"></i><b>4.5.3</b> Case of Flat Indemnity</a></li>
<li class="chapter" data-level="4.5.4" data-path="chap3.html"><a href="chap3.html#case-of-indemnity-compensation"><i class="fa fa-check"></i><b>4.5.4</b> Case of Indemnity Compensation</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="chap3.html"><a href="chap3.html#characteristic-functions"><i class="fa fa-check"></i><b>4.6</b> Characteristic Functions</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="chap3.html"><a href="chap3.html#probability-generating-function"><i class="fa fa-check"></i><b>4.6.1</b> Probability Generating Function</a></li>
<li class="chapter" data-level="4.6.2" data-path="chap3.html"><a href="chap3.html#laplace-transform"><i class="fa fa-check"></i><b>4.6.2</b> Laplace Transform</a></li>
<li class="chapter" data-level="4.6.3" data-path="chap3.html"><a href="chap3.html#moment-generating-function"><i class="fa fa-check"></i><b>4.6.3</b> Moment Generating Function</a></li>
<li class="chapter" data-level="4.6.4" data-path="chap3.html"><a href="chap3.html#hazard-rate"><i class="fa fa-check"></i><b>4.6.4</b> Hazard Rate</a></li>
<li class="chapter" data-level="4.6.5" data-path="chap3.html"><a href="chap3.html#stop-loss-premiums"><i class="fa fa-check"></i><b>4.6.5</b> Stop-Loss Premiums</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="chap3.html"><a href="chap3.html#Hetero"><i class="fa fa-check"></i><b>4.7</b> Heterogeneity and Mixtures</a></li>
<li class="chapter" data-level="4.8" data-path="chap3.html"><a href="chap3.html#context"><i class="fa fa-check"></i><b>4.8</b> Context</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="chap3.html"><a href="chap3.html#a-simple-example"><i class="fa fa-check"></i><b>4.8.1</b> A Simple Example…</a></li>
<li class="chapter" data-level="4.8.2" data-path="chap3.html"><a href="chap3.html#poisson-mixtures"><i class="fa fa-check"></i><b>4.8.2</b> Poisson Mixtures</a></li>
<li class="chapter" data-level="4.8.3" data-path="chap3.html"><a href="chap3.html#shakeds-theorem"><i class="fa fa-check"></i><b>4.8.3</b> Shaked’s Theorem</a></li>
<li class="chapter" data-level="4.8.4" data-path="chap3.html"><a href="chap3.html#composite-mixed-poisson-distributions"><i class="fa fa-check"></i><b>4.8.4</b> Composite Mixed Poisson Distributions</a></li>
<li class="chapter" data-level="4.8.5" data-path="chap3.html"><a href="chap3.html#exponential-mixtures"><i class="fa fa-check"></i><b>4.8.5</b> Exponential Mixtures</a></li>
<li class="chapter" data-level="4.8.6" data-path="chap3.html"><a href="chap3.html#identifiability-of-exponential-mixtures"><i class="fa fa-check"></i><b>4.8.6</b> Identifiability of Exponential Mixtures</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="chap3.html"><a href="chap3.html#pure-premium-in-segmented-universe"><i class="fa fa-check"></i><b>4.9</b> Pure Premium in Segmented Universe</a>
<ul>
<li class="chapter" data-level="4.9.1" data-path="chap3.html"><a href="chap3.html#segmentation-techniques"><i class="fa fa-check"></i><b>4.9.1</b> Segmentation Techniques</a></li>
<li class="chapter" data-level="4.9.2" data-path="chap3.html"><a href="chap3.html#conditional-expectation"><i class="fa fa-check"></i><b>4.9.2</b> Conditional Expectation</a></li>
<li class="chapter" data-level="4.9.3" data-path="chap3.html"><a href="chap3.html#customization-of-premiums"><i class="fa fa-check"></i><b>4.9.3</b> Customization of Premiums</a></li>
<li class="chapter" data-level="4.9.4" data-path="chap3.html"><a href="chap3.html#segmentation-pooling-and-solidarity"><i class="fa fa-check"></i><b>4.9.4</b> Segmentation, Pooling, and Solidarity</a></li>
<li class="chapter" data-level="4.9.5" data-path="chap3.html"><a href="chap3.html#DeWit"><i class="fa fa-check"></i><b>4.9.5</b> Formalization of the Segmentation Concept</a></li>
<li class="chapter" data-level="4.9.6" data-path="chap3.html"><a href="chap3.html#drawbacks-resulting-from-extensive-segmentation"><i class="fa fa-check"></i><b>4.9.6</b> Drawbacks Resulting from Extensive Segmentation</a></li>
<li class="chapter" data-level="4.9.7" data-path="chap3.html"><a href="chap3.html#segmentation-and-information-asymmetry"><i class="fa fa-check"></i><b>4.9.7</b> Segmentation and Information Asymmetry</a></li>
</ul></li>
<li class="chapter" data-level="4.10" data-path="chap3.html"><a href="chap3.html#exercises-1"><i class="fa fa-check"></i><b>4.10</b> Exercises</a></li>
<li class="chapter" data-level="4.11" data-path="chap3.html"><a href="chap3.html#bibliographical-notes-2"><i class="fa fa-check"></i><b>4.11</b> Bibliographical notes</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="chap4.html"><a href="chap4.html"><i class="fa fa-check"></i><b>5</b> From Pure to Net Premium</a>
<ul>
<li class="chapter" data-level="5.1" data-path="chap4.html"><a href="chap4.html#introduction-2"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="chap4.html"><a href="chap4.html#insurance-and-the-central-limit-theorem"><i class="fa fa-check"></i><b>5.2</b> Insurance and the Central Limit Theorem</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="chap4.html"><a href="chap4.html#central-limit-theorem"><i class="fa fa-check"></i><b>5.2.1</b> Central Limit Theorem</a></li>
<li class="chapter" data-level="5.2.2" data-path="chap4.html"><a href="chap4.html#quality-of-the-approximation-based-on-the-central-limit-theorem"><i class="fa fa-check"></i><b>5.2.2</b> Quality of the Approximation Based on the Central Limit Theorem</a></li>
<li class="chapter" data-level="5.2.3" data-path="chap4.html"><a href="chap4.html#central-limit-theorem-and-law-of-large-numbers"><i class="fa fa-check"></i><b>5.2.3</b> Central Limit Theorem and Law of Large Numbers</a></li>
<li class="chapter" data-level="5.2.4" data-path="chap4.html"><a href="chap4.html#central-limit-theorem-for-the-compound-poisson-distribution"><i class="fa fa-check"></i><b>5.2.4</b> Central Limit Theorem for the Compound Poisson Distribution</a></li>
<li class="chapter" data-level="5.2.5" data-path="chap4.html"><a href="chap4.html#tail-function-approximation-in-the-case-of-fixed-forfaiture-policy"><i class="fa fa-check"></i><b>5.2.5</b> Tail Function Approximation in the Case of Fixed-Forfaiture Policy</a></li>
<li class="chapter" data-level="5.2.6" data-path="chap4.html"><a href="chap4.html#tail-function-approximation-in-the-case-of-indemnity-policy"><i class="fa fa-check"></i><b>5.2.6</b> Tail Function Approximation in the Case of Indemnity Policy</a></li>
<li class="chapter" data-level="5.2.7" data-path="chap4.html"><a href="chap4.html#pure-premium-as-the-minimum-price-for-risk"><i class="fa fa-check"></i><b>5.2.7</b> Pure Premium as the Minimum Price for Risk</a></li>
<li class="chapter" data-level="5.2.8" data-path="chap4.html"><a href="chap4.html#sensitivity-of-results-to-possible-dependence"><i class="fa fa-check"></i><b>5.2.8</b> Sensitivity of Results to Possible Dependence</a></li>
<li class="chapter" data-level="5.2.9" data-path="chap4.html"><a href="chap4.html#stable-distributions"><i class="fa fa-check"></i><b>5.2.9</b> Stable Distributions</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="chap4.html"><a href="chap4.html#probability-of-ruin-over-a-period"><i class="fa fa-check"></i><b>5.3</b> Probability of Ruin over a Period</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="chap4.html"><a href="chap4.html#definition-14"><i class="fa fa-check"></i><b>5.3.1</b> Definition</a></li>
<li class="chapter" data-level="5.3.2" data-path="chap4.html"><a href="chap4.html#approximation-based-on-the-central-limit-theorem"><i class="fa fa-check"></i><b>5.3.2</b> Approximation based on the central limit theorem</a></li>
<li class="chapter" data-level="5.3.3" data-path="chap4.html"><a href="chap4.html#the-case-of-the-flat-rate-deductible"><i class="fa fa-check"></i><b>5.3.3</b> The case of the flat-rate deductible</a></li>
<li class="chapter" data-level="5.3.4" data-path="chap4.html"><a href="chap4.html#the-case-of-indemnity-based-claims"><i class="fa fa-check"></i><b>5.3.4</b> The case of indemnity-based claims</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="chap4.html"><a href="chap4.html#security-loading"><i class="fa fa-check"></i><b>5.4</b> Security Loading</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="chap4.html"><a href="chap4.html#concept-1"><i class="fa fa-check"></i><b>5.4.1</b> Concept</a></li>
<li class="chapter" data-level="5.4.2" data-path="chap4.html"><a href="chap4.html#determining-the-security-loading-based-on-the-central-limit-theorem"><i class="fa fa-check"></i><b>5.4.2</b> Determining the security loading Based on the Central Limit Theorem</a></li>
<li class="chapter" data-level="5.4.3" data-path="chap4.html"><a href="chap4.html#the-absolute-necessity-of-security-load"><i class="fa fa-check"></i><b>5.4.3</b> The Absolute Necessity of Security Load</a></li>
<li class="chapter" data-level="5.4.4" data-path="chap4.html"><a href="chap4.html#PrincCalculPrim"><i class="fa fa-check"></i><b>5.4.4</b> Premium Calculation Principle</a></li>
<li class="chapter" data-level="5.4.5" data-path="chap4.html"><a href="chap4.html#comments"><i class="fa fa-check"></i><b>5.4.5</b> Comments</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="chap4.html"><a href="chap4.html#security-coefficient"><i class="fa fa-check"></i><b>5.5</b> Security Coefficient</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="chap4.html"><a href="chap4.html#companys-technical-result"><i class="fa fa-check"></i><b>5.5.1</b> Company’s Technical Result</a></li>
<li class="chapter" data-level="5.5.2" data-path="chap4.html"><a href="chap4.html#determining-the-security-coefficient"><i class="fa fa-check"></i><b>5.5.2</b> Determining the Security Coefficient</a></li>
<li class="chapter" data-level="5.5.3" data-path="chap4.html"><a href="chap4.html#determining-the-safety-loading-based-on-the-bienaymé-chebyshev-inequality"><i class="fa fa-check"></i><b>5.5.3</b> Determining the Safety Loading Based on the Bienaymé-Chebyshev Inequality</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="chap4.html"><a href="chap4.html#the-normal-power-np-approximation"><i class="fa fa-check"></i><b>5.6</b> The Normal-Power (NP) Approximation</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="chap4.html"><a href="chap4.html#skewness-coefficient"><i class="fa fa-check"></i><b>5.6.1</b> Skewness Coefficient</a></li>
<li class="chapter" data-level="5.6.2" data-path="chap4.html"><a href="chap4.html#edgeworth-expansion"><i class="fa fa-check"></i><b>5.6.2</b> Edgeworth Expansion</a></li>
<li class="chapter" data-level="5.6.3" data-path="chap4.html"><a href="chap4.html#esscher-approximation"><i class="fa fa-check"></i><b>5.6.3</b> Esscher Approximation</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="chap4.html"><a href="chap4.html#using-the-np-approximation-to-determine-the-safety-loading-rate-rho-in-the-expected-value-principle"><i class="fa fa-check"></i><b>5.7</b> Using the NP Approximation to Determine the Safety Loading Rate <span class="math inline">\(\rho\)</span> in the Expected Value Principle</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="chap4.html"><a href="chap4.html#concluding-remarks-on-esscher-and-np-approximations"><i class="fa fa-check"></i><b>5.7.1</b> Concluding Remarks on Esscher and NP Approximations</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="chap4.html"><a href="chap4.html#the-premium"><i class="fa fa-check"></i><b>5.8</b> The Premium</a></li>
<li class="chapter" data-level="5.9" data-path="chap4.html"><a href="chap4.html#calculation-of-the-commercial-premium"><i class="fa fa-check"></i><b>5.9</b> Calculation of the Commercial Premium</a></li>
<li class="chapter" data-level="5.10" data-path="chap4.html"><a href="chap4.html#exercises-2"><i class="fa fa-check"></i><b>5.10</b> Exercises</a></li>
<li class="chapter" data-level="5.11" data-path="chap4.html"><a href="chap4.html#bibliographical-notes-3"><i class="fa fa-check"></i><b>5.11</b> Bibliographical Notes</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="chap5.html"><a href="chap5.html"><i class="fa fa-check"></i><b>6</b> Measurement and Comparison of Risks</a>
<ul>
<li class="chapter" data-level="6.1" data-path="chap5.html"><a href="chap5.html#introduction-3"><i class="fa fa-check"></i><b>6.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="chap5.html"><a href="chap5.html#measuring-risk-an-essential-task-for-actuaries"><i class="fa fa-check"></i><b>6.1.1</b> Measuring Risk: An Essential Task for Actuaries</a></li>
<li class="chapter" data-level="6.1.2" data-path="chap5.html"><a href="chap5.html#comparing-risks-another-specialty-of-actuaries"><i class="fa fa-check"></i><b>6.1.2</b> Comparing Risks: Another Specialty of Actuaries</a></li>
<li class="chapter" data-level="6.1.3" data-path="chap5.html"><a href="chap5.html#measuring-and-then-comparing-risks-two-related-tasks"><i class="fa fa-check"></i><b>6.1.3</b> Measuring and then Comparing Risks, Two Related Tasks</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="chap5.html"><a href="chap5.html#risk-measures"><i class="fa fa-check"></i><b>6.2</b> Risk Measures</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="chap5.html"><a href="chap5.html#definition-17"><i class="fa fa-check"></i><b>6.2.1</b> Definition</a></li>
<li class="chapter" data-level="6.2.2" data-path="chap5.html"><a href="chap5.html#coherence"><i class="fa fa-check"></i><b>6.2.2</b> Coherence</a></li>
<li class="chapter" data-level="6.2.3" data-path="chap5.html"><a href="chap5.html#value-at-risk"><i class="fa fa-check"></i><b>6.2.3</b> Value-at-Risk</a></li>
<li class="chapter" data-level="6.2.4" data-path="chap5.html"><a href="chap5.html#tail-var-and-related-measures"><i class="fa fa-check"></i><b>6.2.4</b> Tail-VaR and Related Measures</a></li>
<li class="chapter" data-level="6.2.5" data-path="chap5.html"><a href="chap5.html#esscher-risk-measure"><i class="fa fa-check"></i><b>6.2.5</b> Esscher Risk Measure</a></li>
<li class="chapter" data-level="6.2.6" data-path="chap5.html"><a href="chap5.html#wang-risk-measures"><i class="fa fa-check"></i><b>6.2.6</b> Wang Risk Measures</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="chap5.html"><a href="chap5.html#main-properties-of-wangs-risk-measures"><i class="fa fa-check"></i><b>6.3</b> Main Properties of Wang’s Risk Measures</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="chap5.html"><a href="chap5.html#concavity-of-the-distortion-function"><i class="fa fa-check"></i><b>6.3.1</b> Concavity of the Distortion Function</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="chap5.html"><a href="chap5.html#uniform-comparison-of-var"><i class="fa fa-check"></i><b>6.4</b> Uniform Comparison of VaR</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="chap5.html"><a href="chap5.html#definition-22"><i class="fa fa-check"></i><b>6.4.1</b> Definition</a></li>
<li class="chapter" data-level="6.4.2" data-path="chap5.html"><a href="chap5.html#equivalent-conditions"><i class="fa fa-check"></i><b>6.4.2</b> Equivalent Conditions</a></li>
<li class="chapter" data-level="6.4.3" data-path="chap5.html"><a href="chap5.html#properties-3"><i class="fa fa-check"></i><b>6.4.3</b> Properties</a></li>
<li class="chapter" data-level="6.4.4" data-path="chap5.html"><a href="chap5.html#hazard-rate-and-ph-risk-measure"><i class="fa fa-check"></i><b>6.4.4</b> Hazard Rate and PH Risk Measure</a></li>
<li class="chapter" data-level="6.4.5" data-path="chap5.html"><a href="chap5.html#likelihood-ratio-and-esscher-principle"><i class="fa fa-check"></i><b>6.4.5</b> Likelihood Ratio and Esscher Principle</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="chap5.html"><a href="chap5.html#uniform-comparison-of-tvars"><i class="fa fa-check"></i><b>6.5</b> Uniform Comparison of TVaRs</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="chap5.html"><a href="chap5.html#definition-24"><i class="fa fa-check"></i><b>6.5.1</b> Definition</a></li>
<li class="chapter" data-level="6.5.2" data-path="chap5.html"><a href="chap5.html#equivalent-conditions-1"><i class="fa fa-check"></i><b>6.5.2</b> Equivalent Conditions</a></li>
<li class="chapter" data-level="6.5.3" data-path="chap5.html"><a href="chap5.html#sufficient-condition"><i class="fa fa-check"></i><b>6.5.3</b> Sufficient Condition</a></li>
<li class="chapter" data-level="6.5.4" data-path="chap5.html"><a href="chap5.html#properties-4"><i class="fa fa-check"></i><b>6.5.4</b> Properties</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="chap5.html"><a href="chap5.html#optimal-form-of-risk-transfer"><i class="fa fa-check"></i><b>6.6</b> Optimal Form of Risk Transfer</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="chap5.html"><a href="chap5.html#the-problem"><i class="fa fa-check"></i><b>6.6.1</b> The Problem</a></li>
<li class="chapter" data-level="6.6.2" data-path="chap5.html"><a href="chap5.html#admissible-indemnity-functions"><i class="fa fa-check"></i><b>6.6.2</b> Admissible Indemnity Functions</a></li>
<li class="chapter" data-level="6.6.3" data-path="chap5.html"><a href="chap5.html#ordering-of-contracts"><i class="fa fa-check"></i><b>6.6.3</b> Ordering of Contracts</a></li>
<li class="chapter" data-level="6.6.4" data-path="chap5.html"><a href="chap5.html#optimality-of-the-stop-loss-contract"><i class="fa fa-check"></i><b>6.6.4</b> Optimality of the Stop-Loss Contract</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="chap5.html"><a href="chap5.html#incomplete-information"><i class="fa fa-check"></i><b>6.7</b> Incomplete Information</a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="chap5.html"><a href="chap5.html#context-2"><i class="fa fa-check"></i><b>6.7.1</b> Context</a></li>
<li class="chapter" data-level="6.7.2" data-path="chap5.html"><a href="chap5.html#known-mean-and-support"><i class="fa fa-check"></i><b>6.7.2</b> Known Mean and Support</a></li>
<li class="chapter" data-level="6.7.3" data-path="chap5.html"><a href="chap5.html#application-to-calculating-a-grouped-data-stop-loss-premium"><i class="fa fa-check"></i><b>6.7.3</b> Application to Calculating a Grouped Data Stop-Loss Premium</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="chap5.html"><a href="chap5.html#exercises-3"><i class="fa fa-check"></i><b>6.8</b> Exercises</a></li>
<li class="chapter" data-level="6.9" data-path="chap5.html"><a href="chap5.html#bibliographical-notes-4"><i class="fa fa-check"></i><b>6.9</b> Bibliographical notes</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="chap6.html"><a href="chap6.html"><i class="fa fa-check"></i><b>7</b> Collective Model</a>
<ul>
<li class="chapter" data-level="7.1" data-path="chap6.html"><a href="chap6.html#introduction-4"><i class="fa fa-check"></i><b>7.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="chap6.html"><a href="chap6.html#different-levels-of-analysis"><i class="fa fa-check"></i><b>7.1.1</b> Different Levels of Analysis</a></li>
<li class="chapter" data-level="7.1.2" data-path="chap6.html"><a href="chap6.html#the-individual-model"><i class="fa fa-check"></i><b>7.1.2</b> The Individual Model</a></li>
<li class="chapter" data-level="7.1.3" data-path="chap6.html"><a href="chap6.html#total-claims-in-the-individual-model"><i class="fa fa-check"></i><b>7.1.3</b> Total Claims in the Individual Model</a></li>
<li class="chapter" data-level="7.1.4" data-path="chap6.html"><a href="chap6.html#difficulty-of-calculations-in-the-individual-model"><i class="fa fa-check"></i><b>7.1.4</b> Difficulty of Calculations in the Individual Model</a></li>
<li class="chapter" data-level="7.1.5" data-path="chap6.html"><a href="chap6.html#the-collective-model"><i class="fa fa-check"></i><b>7.1.5</b> The Collective Model</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="chap6.html"><a href="chap6.html#approximation-of-the-individual-model"><i class="fa fa-check"></i><b>7.2</b> Approximation of the Individual Model</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="chap6.html"><a href="chap6.html#formalization-of-the-individual-model"><i class="fa fa-check"></i><b>7.2.1</b> Formalization of the Individual Model</a></li>
<li class="chapter" data-level="7.2.2" data-path="chap6.html"><a href="chap6.html#representation-of-the-total-claims-amount-in-the-individual-model"><i class="fa fa-check"></i><b>7.2.2</b> Representation of the Total Claims Amount in the Individual Model</a></li>
<li class="chapter" data-level="7.2.3" data-path="chap6.html"><a href="chap6.html#justification-of-approximating-the-individual-model-with-the"><i class="fa fa-check"></i><b>7.2.3</b> Justification of Approximating the Individual Model with the</a></li>
<li class="chapter" data-level="7.2.4" data-path="chap6.html"><a href="chap6.html#transition-from-the-individual-model-to-the-collective-model"><i class="fa fa-check"></i><b>7.2.4</b> Transition from the Individual Model to the Collective Model</a></li>
<li class="chapter" data-level="7.2.5" data-path="chap6.html"><a href="chap6.html#choice-of-parameters-for-the-collective-model"><i class="fa fa-check"></i><b>7.2.5</b> Choice of Parameters for the Collective Model</a></li>
<li class="chapter" data-level="7.2.6" data-path="chap6.html"><a href="chap6.html#bounds-on-the-approximation-error-cumulative-distribution-function"><i class="fa fa-check"></i><b>7.2.6</b> Bounds on the Approximation Error: Cumulative Distribution Function</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="chap6.html"><a href="chap6.html#ApproxPoisson"><i class="fa fa-check"></i><b>7.3</b> Analysis of Proposition</a></li>
<li class="chapter" data-level="7.4" data-path="chap6.html"><a href="chap6.html#numerical-analysis"><i class="fa fa-check"></i><b>7.4</b> Numerical Analysis</a></li>
<li class="chapter" data-level="7.5" data-path="chap6.html"><a href="chap6.html#approximating-the-individual-model-by-a-collective-model"><i class="fa fa-check"></i><b>7.5</b> Approximating the Individual Model by a Collective Model</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="chap6.html"><a href="chap6.html#bounds-on-approximation-error-stop-loss-premiums"><i class="fa fa-check"></i><b>7.5.1</b> Bounds on Approximation Error: Stop-Loss Premiums</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="chap6.html"><a href="chap6.html#discretization-of-claim-amounts"><i class="fa fa-check"></i><b>7.6</b> Discretization of Claim Amounts</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="chap6.html"><a href="chap6.html#necessity-of-discretization"><i class="fa fa-check"></i><b>7.6.1</b> Necessity of Discretization</a></li>
<li class="chapter" data-level="7.6.2" data-path="chap6.html"><a href="chap6.html#discretization-in-accordance-with-var"><i class="fa fa-check"></i><b>7.6.2</b> Discretization in Accordance with VaR</a></li>
<li class="chapter" data-level="7.6.3" data-path="chap6.html"><a href="chap6.html#discretization-in-accordance-with-tvar"><i class="fa fa-check"></i><b>7.6.3</b> Discretization in Accordance with TVaR</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="chap6.html"><a href="chap6.html#panjers-algorithm"><i class="fa fa-check"></i><b>7.7</b> Panjer’s Algorithm</a>
<ul>
<li class="chapter" data-level="7.7.1" data-path="chap6.html"><a href="chap6.html#the-challenge-of-a-direct-approach"><i class="fa fa-check"></i><b>7.7.1</b> The Challenge of a Direct Approach</a></li>
<li class="chapter" data-level="7.7.2" data-path="chap6.html"><a href="chap6.html#panjer-family"><i class="fa fa-check"></i><b>7.7.2</b> Panjer Family</a></li>
<li class="chapter" data-level="7.7.3" data-path="chap6.html"><a href="chap6.html#panjers-algorithm-for-positive-claim-costs"><i class="fa fa-check"></i><b>7.7.3</b> Panjer’s Algorithm for Positive Claim Costs</a></li>
<li class="chapter" data-level="7.7.4" data-path="chap6.html"><a href="chap6.html#panjers-algorithm-for-non-negative-claim-costs"><i class="fa fa-check"></i><b>7.7.4</b> Panjer’s Algorithm for Non-Negative Claim Costs</a></li>
<li class="chapter" data-level="7.7.5" data-path="chap6.html"><a href="chap6.html#evaluating-ruin-probabilities-over-a-period"><i class="fa fa-check"></i><b>7.7.5</b> Evaluating Ruin Probabilities over a Period</a></li>
<li class="chapter" data-level="7.7.6" data-path="chap6.html"><a href="chap6.html#evaluation-of-var-and-solvency-margin"><i class="fa fa-check"></i><b>7.7.6</b> Evaluation of VaR and Solvency Margin</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="chap6.html"><a href="chap6.html#evaluation-of-stop-loss-premiums"><i class="fa fa-check"></i><b>7.8</b> Evaluation of Stop-Loss Premiums</a>
<ul>
<li class="chapter" data-level="7.8.1" data-path="chap6.html"><a href="chap6.html#iterative-scheme-for-calculating-stop-loss-premiums"><i class="fa fa-check"></i><b>7.8.1</b> Iterative Scheme for Calculating Stop-Loss Premiums</a></li>
<li class="chapter" data-level="7.8.2" data-path="chap6.html"><a href="chap6.html#error-due-to-discretization"><i class="fa fa-check"></i><b>7.8.2</b> Error Due to Discretization</a></li>
</ul></li>
<li class="chapter" data-level="7.9" data-path="chap6.html"><a href="chap6.html#exercises-4"><i class="fa fa-check"></i><b>7.9</b> Exercises</a></li>
<li class="chapter" data-level="7.10" data-path="chap6.html"><a href="chap6.html#bibliographical-notes-5"><i class="fa fa-check"></i><b>7.10</b> Bibliographical Notes</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="chap7.html"><a href="chap7.html"><i class="fa fa-check"></i><b>8</b> Solvency</a></li>
<li class="chapter" data-level="9" data-path="chap8.html"><a href="chap8.html"><i class="fa fa-check"></i><b>9</b> Multiple Risks</a>
<ul>
<li class="chapter" data-level="9.1" data-path="chap8.html"><a href="chap8.html#introduction-5"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="chap8.html"><a href="chap8.html#comonotonicity-and-antimonotonicity"><i class="fa fa-check"></i><b>9.2</b> Comonotonicity and Antimonotonicity</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="chap8.html"><a href="chap8.html#fréchet-classes"><i class="fa fa-check"></i><b>9.2.1</b> Fréchet Classes</a></li>
<li class="chapter" data-level="9.2.2" data-path="chap8.html"><a href="chap8.html#fréchet-bounds"><i class="fa fa-check"></i><b>9.2.2</b> Fréchet Bounds</a></li>
<li class="chapter" data-level="9.2.3" data-path="chap8.html"><a href="chap8.html#perfect-dependence-comonotonicity-and-antimonotonicity"><i class="fa fa-check"></i><b>9.2.3</b> Perfect Dependence: Comonotonicity and Antimonotonicity</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="chap8.html"><a href="chap8.html#measures-of-dependence"><i class="fa fa-check"></i><b>9.3</b> Measures of Dependence</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="chap8.html"><a href="chap8.html#concept-2"><i class="fa fa-check"></i><b>9.3.1</b> Concept</a></li>
<li class="chapter" data-level="9.3.2" data-path="chap8.html"><a href="chap8.html#linear-correlation-or-pearson-correlation"><i class="fa fa-check"></i><b>9.3.2</b> Linear Correlation or Pearson Correlation</a></li>
<li class="chapter" data-level="9.3.3" data-path="chap8.html"><a href="chap8.html#values-of-the-linear-correlation-coefficient"><i class="fa fa-check"></i><b>9.3.3</b> Values of the Linear Correlation Coefficient</a></li>
<li class="chapter" data-level="9.3.4" data-path="chap8.html"><a href="chap8.html#kendalls-rank-correlation-coefficient"><i class="fa fa-check"></i><b>9.3.4</b> Kendall’s Rank Correlation Coefficient</a></li>
<li class="chapter" data-level="9.3.5" data-path="chap8.html"><a href="chap8.html#spearmans-rank-correlation-coefficient"><i class="fa fa-check"></i><b>9.3.5</b> Spearman’s Rank Correlation Coefficient</a></li>
<li class="chapter" data-level="9.3.6" data-path="chap8.html"><a href="chap8.html#relationship-between-kendalls-tau-and-spearmans-rho"><i class="fa fa-check"></i><b>9.3.6</b> Relationship Between Kendall’s Tau and Spearman’s Rho</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="chap8.html"><a href="chap8.html#comparison-of-dependence"><i class="fa fa-check"></i><b>9.4</b> Comparison of Dependence</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="chap8.html"><a href="chap8.html#order-of-dependence"><i class="fa fa-check"></i><b>9.4.1</b> Order of Dependence</a></li>
<li class="chapter" data-level="9.4.2" data-path="chap8.html"><a href="chap8.html#supermodular-comparison"><i class="fa fa-check"></i><b>9.4.2</b> Supermodular Comparison</a></li>
<li class="chapter" data-level="9.4.3" data-path="chap8.html"><a href="chap8.html#functional-stability-of-supermodular-comparisons"><i class="fa fa-check"></i><b>9.4.3</b> Functional Stability of Supermodular Comparisons</a></li>
<li class="chapter" data-level="9.4.4" data-path="chap8.html"><a href="chap8.html#supermodular-comparison-and-fréchet-space"><i class="fa fa-check"></i><b>9.4.4</b> Supermodular Comparison and Fréchet Space</a></li>
<li class="chapter" data-level="9.4.5" data-path="chap8.html"><a href="chap8.html#supermodular-comparison-and-joint-distributiontail-functions"><i class="fa fa-check"></i><b>9.4.5</b> Supermodular Comparison and Joint Distribution/Tail Functions</a></li>
<li class="chapter" data-level="9.4.6" data-path="chap8.html"><a href="chap8.html#extreme-structures-of-supermodular-dependence"><i class="fa fa-check"></i><b>9.4.6</b> Extreme Structures of Supermodular Dependence</a></li>
<li class="chapter" data-level="9.4.7" data-path="chap8.html"><a href="chap8.html#supermodular-comparison-and-correlation-coefficients"><i class="fa fa-check"></i><b>9.4.7</b> Supermodular Comparison and Correlation Coefficients</a></li>
<li class="chapter" data-level="9.4.8" data-path="chap8.html"><a href="chap8.html#lcx-order-and-supermodular-comparison"><i class="fa fa-check"></i><b>9.4.8</b> <span class="math inline">\(\lcx\)</span> Order and Supermodular Comparison</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="chap8.html"><a href="chap8.html#notions-of-positive-dependence"><i class="fa fa-check"></i><b>9.5</b> Notions of Positive Dependence</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="chap8.html"><a href="chap8.html#concept-3"><i class="fa fa-check"></i><b>9.5.1</b> Concept</a></li>
<li class="chapter" data-level="9.5.2" data-path="chap8.html"><a href="chap8.html#positive-quadrant-dependence"><i class="fa fa-check"></i><b>9.5.2</b> Positive Quadrant Dependence</a></li>
<li class="chapter" data-level="9.5.3" data-path="chap8.html"><a href="chap8.html#association"><i class="fa fa-check"></i><b>9.5.3</b> Association</a></li>
<li class="chapter" data-level="9.5.4" data-path="chap8.html"><a href="chap8.html#conditional-growth"><i class="fa fa-check"></i><b>9.5.4</b> Conditional Growth</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="chap8.html"><a href="chap8.html#introduction-to-copula-theory"><i class="fa fa-check"></i><b>9.6</b> Introduction to Copula Theory</a>
<ul>
<li class="chapter" data-level="9.6.1" data-path="chap8.html"><a href="chap8.html#principle-2"><i class="fa fa-check"></i><b>9.6.1</b> Principle</a></li>
<li class="chapter" data-level="9.6.2" data-path="chap8.html"><a href="chap8.html#definition-32"><i class="fa fa-check"></i><b>9.6.2</b> Definition</a></li>
<li class="chapter" data-level="9.6.3" data-path="chap8.html"><a href="chap8.html#sklars-theorem"><i class="fa fa-check"></i><b>9.6.3</b> Sklar’s Theorem</a></li>
<li class="chapter" data-level="9.6.4" data-path="chap8.html"><a href="chap8.html#properties-of-copulas"><i class="fa fa-check"></i><b>9.6.4</b> Properties of Copulas</a></li>
<li class="chapter" data-level="9.6.5" data-path="chap8.html"><a href="chap8.html#dependencemeasures"><i class="fa fa-check"></i><b>9.6.5</b> Dependence Measures and Copulas</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="chap8.html"><a href="chap8.html#archimedean-copulas"><i class="fa fa-check"></i><b>9.7</b> Archimedean Copulas</a>
<ul>
<li class="chapter" data-level="9.7.1" data-path="chap8.html"><a href="chap8.html#definition-33"><i class="fa fa-check"></i><b>9.7.1</b> Definition</a></li>
<li class="chapter" data-level="9.7.2" data-path="chap8.html"><a href="chap8.html#frailty-models-and-archimedean-copulas"><i class="fa fa-check"></i><b>9.7.2</b> Frailty Models and Archimedean Copulas</a></li>
<li class="chapter" data-level="9.7.3" data-path="chap8.html"><a href="chap8.html#survival-function"><i class="fa fa-check"></i><b>9.7.3</b> Survival Function</a></li>
<li class="chapter" data-level="9.7.4" data-path="chap8.html"><a href="chap8.html#regression-function"><i class="fa fa-check"></i><b>9.7.4</b> Regression Function</a></li>
<li class="chapter" data-level="9.7.5" data-path="chap8.html"><a href="chap8.html#bivariate-integral-transformation"><i class="fa fa-check"></i><b>9.7.5</b> Bivariate Integral Transformation</a></li>
<li class="chapter" data-level="9.7.6" data-path="chap8.html"><a href="chap8.html#order-relations-for-archimedean-copulas"><i class="fa fa-check"></i><b>9.7.6</b> Order Relations for Archimedean Copulas</a></li>
<li class="chapter" data-level="9.7.7" data-path="chap8.html"><a href="chap8.html#study-of-a-function-of-two-correlated-risks"><i class="fa fa-check"></i><b>9.7.7</b> Study of a Function of Two Correlated Risks</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="chap8.html"><a href="chap8.html#multivariate-discrete-distributions"><i class="fa fa-check"></i><b>9.8</b> Multivariate Discrete Distributions</a>
<ul>
<li class="chapter" data-level="9.8.1" data-path="chap8.html"><a href="chap8.html#two-classes-of-correlated-risks-model"><i class="fa fa-check"></i><b>9.8.1</b> Two Classes of Correlated Risks Model</a></li>
<li class="chapter" data-level="9.8.2" data-path="chap8.html"><a href="chap8.html#multivariate-bernoulli-distribution"><i class="fa fa-check"></i><b>9.8.2</b> Multivariate Bernoulli Distribution</a></li>
<li class="chapter" data-level="9.8.3" data-path="chap8.html"><a href="chap8.html#common-shock-poisson-model-bivariate-poisson-distribution"><i class="fa fa-check"></i><b>9.8.3</b> Common Shock Poisson Model: Bivariate Poisson Distribution</a></li>
<li class="chapter" data-level="9.8.4" data-path="chap8.html"><a href="chap8.html#common-shock-bernoulli-model-the-marceau-model"><i class="fa fa-check"></i><b>9.8.4</b> Common Shock Bernoulli Model: The Marceau Model</a></li>
</ul></li>
<li class="chapter" data-level="9.9" data-path="chap8.html"><a href="chap8.html#exercices"><i class="fa fa-check"></i><b>9.9</b> Exercices</a></li>
<li class="chapter" data-level="9.10" data-path="chap8.html"><a href="chap8.html#bibliographical-notes-6"><i class="fa fa-check"></i><b>9.10</b> Bibliographical Notes</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="chap9.html"><a href="chap9.html"><i class="fa fa-check"></i><b>10</b> Prior Ratemaking</a>
<ul>
<li class="chapter" data-level="10.1" data-path="chap9.html"><a href="chap9.html#introduction-6"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="chap9.html"><a href="chap9.html#Sec965New"><i class="fa fa-check"></i><b>10.2</b> Rating Variables</a></li>
<li class="chapter" data-level="10.3" data-path="chap9.html"><a href="chap9.html#basic-principles-of-statistics"><i class="fa fa-check"></i><b>10.3</b> Basic Principles of Statistics</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="chap9.html"><a href="chap9.html#empirical-cumulative-distribution-function"><i class="fa fa-check"></i><b>10.3.1</b> Empirical Cumulative Distribution Function</a></li>
<li class="chapter" data-level="10.3.2" data-path="chap9.html"><a href="chap9.html#the-parametric-approach"><i class="fa fa-check"></i><b>10.3.2</b> The Parametric Approach</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="chap9.html"><a href="chap9.html#fishers-information"><i class="fa fa-check"></i><b>10.4</b> Fisher’s Information</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="chap9.html"><a href="chap9.html#conditions-on-the-statistical-model"><i class="fa fa-check"></i><b>10.4.1</b> Conditions on the Statistical Model</a></li>
<li class="chapter" data-level="10.4.2" data-path="chap9.html"><a href="chap9.html#definition-35"><i class="fa fa-check"></i><b>10.4.2</b> Definition</a></li>
<li class="chapter" data-level="10.4.3" data-path="chap9.html"><a href="chap9.html#parameter-estimation-using-maximum-likelihood-method"><i class="fa fa-check"></i><b>10.4.3</b> Parameter Estimation Using Maximum Likelihood Method</a></li>
<li class="chapter" data-level="10.4.4" data-path="chap9.html"><a href="chap9.html#likelihood-ratio-test"><i class="fa fa-check"></i><b>10.4.4</b> Likelihood Ratio Test</a></li>
<li class="chapter" data-level="10.4.5" data-path="chap9.html"><a href="chap9.html#other-estimation-methods"><i class="fa fa-check"></i><b>10.4.5</b> Other Estimation Methods</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="chap9.html"><a href="chap9.html#SecPCA"><i class="fa fa-check"></i><b>10.5</b> Data Analysis</a>
<ul>
<li class="chapter" data-level="10.5.1" data-path="chap9.html"><a href="chap9.html#principle-3"><i class="fa fa-check"></i><b>10.5.1</b> Principle</a></li>
<li class="chapter" data-level="10.5.2" data-path="chap9.html"><a href="chap9.html#principal-component-analysis-pca"><i class="fa fa-check"></i><b>10.5.2</b> Principal Component Analysis (PCA)</a></li>
<li class="chapter" data-level="10.5.3" data-path="chap9.html"><a href="chap9.html#multiple-correspondence-analysis-mca"><i class="fa fa-check"></i><b>10.5.3</b> Multiple Correspondence Analysis (MCA)</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="chap9.html"><a href="chap9.html#Sec923MC"><i class="fa fa-check"></i><b>10.6</b> Scoring Methods</a>
<ul>
<li class="chapter" data-level="10.6.1" data-path="chap9.html"><a href="chap9.html#classification-methods"><i class="fa fa-check"></i><b>10.6.1</b> Classification Methods</a></li>
<li class="chapter" data-level="10.6.2" data-path="chap9.html"><a href="chap9.html#definition-of-a-score"><i class="fa fa-check"></i><b>10.6.2</b> Definition of a Score</a></li>
<li class="chapter" data-level="10.6.3" data-path="chap9.html"><a href="chap9.html#principle-of-scoring"><i class="fa fa-check"></i><b>10.6.3</b> Principle of Scoring</a></li>
<li class="chapter" data-level="10.6.4" data-path="chap9.html"><a href="chap9.html#optimal-classification-and-threshold-selection"><i class="fa fa-check"></i><b>10.6.4</b> Optimal Classification and Threshold Selection</a></li>
<li class="chapter" data-level="10.6.5" data-path="chap9.html"><a href="chap9.html#practical-construction-of-a-score"><i class="fa fa-check"></i><b>10.6.5</b> Practical Construction of a Score</a></li>
<li class="chapter" data-level="10.6.6" data-path="chap9.html"><a href="chap9.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>10.6.6</b> (Linear) Discriminant Analysis</a></li>
<li class="chapter" data-level="10.6.7" data-path="chap9.html"><a href="chap9.html#discriminant-analysis"><i class="fa fa-check"></i><b>10.6.7</b> Discriminant Analysis</a></li>
<li class="chapter" data-level="10.6.8" data-path="chap9.html"><a href="chap9.html#the-disqual-method"><i class="fa fa-check"></i><b>10.6.8</b> The DISQUAL Method</a></li>
<li class="chapter" data-level="10.6.9" data-path="chap9.html"><a href="chap9.html#the-probit-model"><i class="fa fa-check"></i><b>10.6.9</b> The Probit Model</a></li>
<li class="chapter" data-level="10.6.10" data-path="chap9.html"><a href="chap9.html#the-logit-model"><i class="fa fa-check"></i><b>10.6.10</b> The Logit Model</a></li>
<li class="chapter" data-level="10.6.11" data-path="chap9.html"><a href="chap9.html#duality-of-approaches"><i class="fa fa-check"></i><b>10.6.11</b> Duality of Approaches</a></li>
<li class="chapter" data-level="10.6.12" data-path="chap9.html"><a href="chap9.html#performance-and-selection-curves"><i class="fa fa-check"></i><b>10.6.12</b> Performance and Selection Curves</a></li>
<li class="chapter" data-level="10.6.13" data-path="chap9.html"><a href="chap9.html#desirable-properties-of-a-score"><i class="fa fa-check"></i><b>10.6.13</b> Desirable Properties of a Score</a></li>
<li class="chapter" data-level="10.6.14" data-path="chap9.html"><a href="chap9.html#score-comparison"><i class="fa fa-check"></i><b>10.6.14</b> Score Comparison</a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="chap9.html"><a href="chap9.html#Sec93MLMC"><i class="fa fa-check"></i><b>10.7</b> Linear Model</a>
<ul>
<li class="chapter" data-level="10.7.1" data-path="chap9.html"><a href="chap9.html#definition-36"><i class="fa fa-check"></i><b>10.7.1</b> Definition</a></li>
<li class="chapter" data-level="10.7.2" data-path="chap9.html"><a href="chap9.html#matrix-formalism"><i class="fa fa-check"></i><b>10.7.2</b> Matrix Formalism</a></li>
<li class="chapter" data-level="10.7.3" data-path="chap9.html"><a href="chap9.html#parameter-estimation"><i class="fa fa-check"></i><b>10.7.3</b> Parameter Estimation</a></li>
<li class="chapter" data-level="10.7.4" data-path="chap9.html"><a href="chap9.html#prediction-matrix"><i class="fa fa-check"></i><b>10.7.4</b> Prediction Matrix</a></li>
<li class="chapter" data-level="10.7.5" data-path="chap9.html"><a href="chap9.html#estimation-of-means-and-variance"><i class="fa fa-check"></i><b>10.7.5</b> Estimation of Means and Variance</a></li>
<li class="chapter" data-level="10.7.6" data-path="chap9.html"><a href="chap9.html#measurement-of-fit-quality-the-coefficient-of-determination"><i class="fa fa-check"></i><b>10.7.6</b> Measurement of Fit Quality: The Coefficient of Determination</a></li>
<li class="chapter" data-level="10.7.7" data-path="chap9.html"><a href="chap9.html#standardized-residuals"><i class="fa fa-check"></i><b>10.7.7</b> Standardized Residuals</a></li>
<li class="chapter" data-level="10.7.8" data-path="chap9.html"><a href="chap9.html#inferential-results-for-parameters"><i class="fa fa-check"></i><b>10.7.8</b> Inferential Results for Parameters</a></li>
<li class="chapter" data-level="10.7.9" data-path="chap9.html"><a href="chap9.html#testing-a-simple-hypothesis"><i class="fa fa-check"></i><b>10.7.9</b> Testing a Simple Hypothesis</a></li>
<li class="chapter" data-level="10.7.10" data-path="chap9.html"><a href="chap9.html#comparison-of-nested-models"><i class="fa fa-check"></i><b>10.7.10</b> Comparison of Nested Models</a></li>
<li class="chapter" data-level="10.7.11" data-path="chap9.html"><a href="chap9.html#confidence-regions"><i class="fa fa-check"></i><b>10.7.11</b> Confidence Regions</a></li>
<li class="chapter" data-level="10.7.12" data-path="chap9.html"><a href="chap9.html#confidence-intervals"><i class="fa fa-check"></i><b>10.7.12</b> Confidence Intervals</a></li>
<li class="chapter" data-level="10.7.13" data-path="chap9.html"><a href="chap9.html#measures-of-influence"><i class="fa fa-check"></i><b>10.7.13</b> Measures of Influence</a></li>
<li class="chapter" data-level="10.7.14" data-path="chap9.html"><a href="chap9.html#weighted-least-squares"><i class="fa fa-check"></i><b>10.7.14</b> Weighted Least Squares</a></li>
</ul></li>
<li class="chapter" data-level="10.8" data-path="chap9.html"><a href="chap9.html#additive-models"><i class="fa fa-check"></i><b>10.8</b> Additive Models</a>
<ul>
<li class="chapter" data-level="10.8.1" data-path="chap9.html"><a href="chap9.html#principle-5"><i class="fa fa-check"></i><b>10.8.1</b> Principle</a></li>
<li class="chapter" data-level="10.8.2" data-path="chap9.html"><a href="chap9.html#single-regressor-case"><i class="fa fa-check"></i><b>10.8.2</b> Single Regressor Case</a></li>
<li class="chapter" data-level="10.8.3" data-path="chap9.html"><a href="chap9.html#estimation-with-multiple-regressors-backfitting"><i class="fa fa-check"></i><b>10.8.3</b> Estimation with Multiple Regressors: Backfitting</a></li>
<li class="chapter" data-level="10.8.4" data-path="chap9.html"><a href="chap9.html#comparison-of-different-approaches"><i class="fa fa-check"></i><b>10.8.4</b> Comparison of Different Approaches</a></li>
</ul></li>
<li class="chapter" data-level="10.9" data-path="chap9.html"><a href="chap9.html#Sec96GLM"><i class="fa fa-check"></i><b>10.9</b> Generalized Linear Models (GLM)</a>
<ul>
<li class="chapter" data-level="10.9.1" data-path="chap9.html"><a href="chap9.html#a-brief-history-of-actuarial-applications-of-regression-models"><i class="fa fa-check"></i><b>10.9.1</b> A Brief History of Actuarial Applications of Regression Models</a></li>
<li class="chapter" data-level="10.9.2" data-path="chap9.html"><a href="chap9.html#definition-38"><i class="fa fa-check"></i><b>10.9.2</b> Definition</a></li>
<li class="chapter" data-level="10.9.3" data-path="chap9.html"><a href="chap9.html#mean-and-variance"><i class="fa fa-check"></i><b>10.9.3</b> Mean and Variance</a></li>
<li class="chapter" data-level="10.9.4" data-path="chap9.html"><a href="chap9.html#regression-model"><i class="fa fa-check"></i><b>10.9.4</b> Regression Model</a></li>
<li class="chapter" data-level="10.9.5" data-path="chap9.html"><a href="chap9.html#canonical-link-function"><i class="fa fa-check"></i><b>10.9.5</b> Canonical Link Function</a></li>
<li class="chapter" data-level="10.9.6" data-path="chap9.html"><a href="chap9.html#likelihood-equations"><i class="fa fa-check"></i><b>10.9.6</b> Likelihood Equations</a></li>
<li class="chapter" data-level="10.9.7" data-path="chap9.html"><a href="chap9.html#solving-likelihood-equations"><i class="fa fa-check"></i><b>10.9.7</b> Solving Likelihood Equations</a></li>
<li class="chapter" data-level="10.9.9" data-path="chap9.html"><a href="chap9.html#confidence-interval-for-parameters"><i class="fa fa-check"></i><b>10.9.9</b> Confidence Interval for Parameters</a></li>
<li class="chapter" data-level="10.9.10" data-path="chap9.html"><a href="chap9.html#model-comparison"><i class="fa fa-check"></i><b>10.9.10</b> Model Comparison</a></li>
<li class="chapter" data-level="10.9.11" data-path="chap9.html"><a href="chap9.html#hypothesis-tests-on-parameters"><i class="fa fa-check"></i><b>10.9.11</b> Hypothesis Tests on Parameters</a></li>
<li class="chapter" data-level="10.9.12" data-path="chap9.html"><a href="chap9.html#estimation-of-the-dispersion-parameter"><i class="fa fa-check"></i><b>10.9.12</b> Estimation of the Dispersion Parameter</a></li>
<li class="chapter" data-level="10.9.13" data-path="chap9.html"><a href="chap9.html#residual-analysis"><i class="fa fa-check"></i><b>10.9.13</b> Residual Analysis</a></li>
<li class="chapter" data-level="10.9.14" data-path="chap9.html"><a href="chap9.html#the-practice-of-generalized-linear-models"><i class="fa fa-check"></i><b>10.9.14</b> The Practice of Generalized Linear Models</a></li>
</ul></li>
<li class="chapter" data-level="10.10" data-path="chap9.html"><a href="chap9.html#generalized-additive-models-gams"><i class="fa fa-check"></i><b>10.10</b> Generalized Additive Models (GAMs)</a>
<ul>
<li class="chapter" data-level="10.10.1" data-path="chap9.html"><a href="chap9.html#principle-6"><i class="fa fa-check"></i><b>10.10.1</b> Principle</a></li>
<li class="chapter" data-level="10.10.2" data-path="chap9.html"><a href="chap9.html#in-practice"><i class="fa fa-check"></i><b>10.10.2</b> In Practice…</a></li>
</ul></li>
<li class="chapter" data-level="10.11" data-path="chap9.html"><a href="chap9.html#practical-case-of-auto-insurance-pricing"><i class="fa fa-check"></i><b>10.11</b> Practical Case of Auto Insurance Pricing</a>
<ul>
<li class="chapter" data-level="10.11.1" data-path="chap9.html"><a href="chap9.html#portfolio-description"><i class="fa fa-check"></i><b>10.11.1</b> Portfolio Description</a></li>
<li class="chapter" data-level="10.11.2" data-path="chap9.html"><a href="chap9.html#variables-describing-claims"><i class="fa fa-check"></i><b>10.11.2</b> Variables Describing Claims</a></li>
<li class="chapter" data-level="10.11.3" data-path="chap9.html"><a href="chap9.html#measuring-exposure-to-risk-the-variable"><i class="fa fa-check"></i><b>10.11.3</b> Measuring Exposure to Risk: the Variable</a></li>
<li class="chapter" data-level="10.11.4" data-path="chap9.html"><a href="chap9.html#characteristics-of-the-policyholder"><i class="fa fa-check"></i><b>10.11.4</b> Characteristics of the Policyholder</a></li>
<li class="chapter" data-level="10.11.5" data-path="chap9.html"><a href="chap9.html#vehicle-characteristics"><i class="fa fa-check"></i><b>10.11.5</b> Vehicle Characteristics</a></li>
<li class="chapter" data-level="10.11.6" data-path="chap9.html"><a href="chap9.html#interaction-between-rating-variables-1"><i class="fa fa-check"></i><b>10.11.6</b> Interaction Between Rating Variables</a></li>
<li class="chapter" data-level="10.11.7" data-path="chap9.html"><a href="chap9.html#initial-screening-of-rating-variables"><i class="fa fa-check"></i><b>10.11.7</b> Initial Screening of Rating Variables</a></li>
<li class="chapter" data-level="10.11.8" data-path="chap9.html"><a href="chap9.html#analysis-of-claim-frequencies"><i class="fa fa-check"></i><b>10.11.8</b> Analysis of Claim Frequencies</a></li>
<li class="chapter" data-level="10.11.9" data-path="chap9.html"><a href="chap9.html#analysis-of-claim-costs"><i class="fa fa-check"></i><b>10.11.9</b> Analysis of Claim Costs</a></li>
</ul></li>
<li class="chapter" data-level="10.12" data-path="chap9.html"><a href="chap9.html#panel-data-rating"><i class="fa fa-check"></i><b>10.12</b> Panel Data Rating</a>
<ul>
<li class="chapter" data-level="10.12.1" data-path="chap9.html"><a href="chap9.html#rating-based-on-panel-data"><i class="fa fa-check"></i><b>10.12.1</b> Rating Based on Panel Data</a></li>
<li class="chapter" data-level="10.12.2" data-path="chap9.html"><a href="chap9.html#notation"><i class="fa fa-check"></i><b>10.12.2</b> Notation</a></li>
<li class="chapter" data-level="10.12.3" data-path="chap9.html"><a href="chap9.html#presentation-of-the-dataset"><i class="fa fa-check"></i><b>10.12.3</b> Presentation of the Dataset</a></li>
<li class="chapter" data-level="10.12.4" data-path="chap9.html"><a href="chap9.html#poisson-regression-assuming-temporal-independence"><i class="fa fa-check"></i><b>10.12.4</b> Poisson Regression Assuming Temporal Independence</a></li>
<li class="chapter" data-level="10.12.5" data-path="chap9.html"><a href="chap9.html#accounting-for-temporal-dependence"><i class="fa fa-check"></i><b>10.12.5</b> Accounting for Temporal Dependence</a></li>
<li class="chapter" data-level="10.12.6" data-path="chap9.html"><a href="chap9.html#modeling-dependence-using-the-working-correlation-matrix"><i class="fa fa-check"></i><b>10.12.6</b> Modeling Dependence Using the “Working Correlation Matrix”</a></li>
<li class="chapter" data-level="10.12.7" data-path="chap9.html"><a href="chap9.html#obtaining-estimates"><i class="fa fa-check"></i><b>10.12.7</b> Obtaining Estimates</a></li>
<li class="chapter" data-level="10.12.8" data-path="chap9.html"><a href="chap9.html#numerical-illustration-1"><i class="fa fa-check"></i><b>10.12.8</b> Numerical Illustration</a></li>
</ul></li>
<li class="chapter" data-level="10.13" data-path="chap9.html"><a href="chap9.html#technical-justifications-for-segmentation"><i class="fa fa-check"></i><b>10.13</b> Technical Justifications for Segmentation</a>
<ul>
<li class="chapter" data-level="10.13.1" data-path="chap9.html"><a href="chap9.html#technical-rate-and-commercial-rate"><i class="fa fa-check"></i><b>10.13.1</b> Technical Rate and Commercial Rate</a></li>
<li class="chapter" data-level="10.13.2" data-path="chap9.html"><a href="chap9.html#segmentation-of-technical-and-commercial-rates"><i class="fa fa-check"></i><b>10.13.2</b> Segmentation of Technical and Commercial Rates</a></li>
<li class="chapter" data-level="10.13.3" data-path="chap9.html"><a href="chap9.html#adverse-selection-and-segmentation"><i class="fa fa-check"></i><b>10.13.3</b> Adverse Selection and Segmentation</a></li>
<li class="chapter" data-level="10.13.4" data-path="chap9.html"><a href="chap9.html#inequity-of-prior-pricing"><i class="fa fa-check"></i><b>10.13.4</b> Inequity of Prior Pricing</a></li>
</ul></li>
<li class="chapter" data-level="10.14" data-path="chap9.html"><a href="chap9.html#bibliographical-notes-7"><i class="fa fa-check"></i><b>10.14</b> Bibliographical Notes</a></li>
<li class="chapter" data-level="10.15" data-path="chap9.html"><a href="chap9.html#exercises-5"><i class="fa fa-check"></i><b>10.15</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="chap10.html"><a href="chap10.html"><i class="fa fa-check"></i><b>11</b> Credibility</a>
<ul>
<li class="chapter" data-level="11.1" data-path="chap10.html"><a href="chap10.html#introduction-7"><i class="fa fa-check"></i><b>11.1</b> Introduction</a></li>
<li class="chapter" data-level="11.2" data-path="chap10.html"><a href="chap10.html#bayesian-credibility"><i class="fa fa-check"></i><b>11.2</b> Bayesian Credibility</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="chap10.html"><a href="chap10.html#introductory-example"><i class="fa fa-check"></i><b>11.2.1</b> Introductory Example</a></li>
<li class="chapter" data-level="11.2.2" data-path="chap10.html"><a href="chap10.html#bayesian-posterior-pricing-model"><i class="fa fa-check"></i><b>11.2.2</b> Bayesian Posterior Pricing Model</a></li>
<li class="chapter" data-level="11.2.3" data-path="chap10.html"><a href="chap10.html#frequentist-bayesian-credibility-without-a-priori-pricing"><i class="fa fa-check"></i><b>11.2.3</b> Frequentist Bayesian Credibility without A Priori Pricing</a></li>
<li class="chapter" data-level="11.2.4" data-path="chap10.html"><a href="chap10.html#bayesian-frequentist-credibility-with-prior-rate-making"><i class="fa fa-check"></i><b>11.2.4</b> Bayesian-Frequentist Credibility with Prior Rate Making</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="chap10.html"><a href="chap10.html#linear-credibility"><i class="fa fa-check"></i><b>11.3</b> Linear Credibility</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="chap10.html"><a href="chap10.html#bühlmann-model"><i class="fa fa-check"></i><b>11.3.1</b> Bühlmann Model</a></li>
<li class="chapter" data-level="11.3.2" data-path="chap10.html"><a href="chap10.html#bühlmann-straub-model"><i class="fa fa-check"></i><b>11.3.2</b> Bühlmann-Straub Model</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="chap10.html"><a href="chap10.html#SecCredTot"><i class="fa fa-check"></i><b>11.4</b> Total Credibility</a></li>
<li class="chapter" data-level="11.5" data-path="chap10.html"><a href="chap10.html#multivariate-credibility"><i class="fa fa-check"></i><b>11.5</b> Multivariate Credibility</a></li>
<li class="chapter" data-level="11.6" data-path="chap10.html"><a href="chap10.html#sec:matcorp"><i class="fa fa-check"></i><b>11.6</b> Modeling</a>
<ul>
<li class="chapter" data-level="11.6.1" data-path="chap10.html"><a href="chap10.html#linear-credibility-premium"><i class="fa fa-check"></i><b>11.6.1</b> Linear Credibility Premium</a></li>
<li class="chapter" data-level="11.6.2" data-path="chap10.html"><a href="chap10.html#an-approach-on-disaggregated-data"><i class="fa fa-check"></i><b>11.6.2</b> An Approach on Disaggregated Data</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="chap10.html"><a href="chap10.html#hierarchical-credibility"><i class="fa fa-check"></i><b>11.7</b> Hierarchical Credibility</a>
<ul>
<li class="chapter" data-level="11.7.1" data-path="chap10.html"><a href="chap10.html#motivation-fleet-insurance"><i class="fa fa-check"></i><b>11.7.1</b> Motivation: Fleet Insurance</a></li>
<li class="chapter" data-level="11.7.2" data-path="chap10.html"><a href="chap10.html#linear-credibility-1"><i class="fa fa-check"></i><b>11.7.2</b> Linear Credibility</a></li>
<li class="chapter" data-level="11.7.3" data-path="chap10.html"><a href="chap10.html#the-case-of-new-vehicles"><i class="fa fa-check"></i><b>11.7.3</b> The Case of New Vehicles</a></li>
<li class="chapter" data-level="11.7.4" data-path="chap10.html"><a href="chap10.html#the-case-of-existing-vehicles"><i class="fa fa-check"></i><b>11.7.4</b> The Case of Existing Vehicles</a></li>
<li class="chapter" data-level="11.7.5" data-path="chap10.html"><a href="chap10.html#open-fleets"><i class="fa fa-check"></i><b>11.7.5</b> Open Fleets</a></li>
</ul></li>
<li class="chapter" data-level="11.8" data-path="chap10.html"><a href="chap10.html#bibliographical-notes-8"><i class="fa fa-check"></i><b>11.8</b> Bibliographical Notes</a></li>
<li class="chapter" data-level="11.9" data-path="chap10.html"><a href="chap10.html#exercises-6"><i class="fa fa-check"></i><b>11.9</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="chap11.html"><a href="chap11.html"><i class="fa fa-check"></i><b>12</b> Bonus-Malus</a>
<ul>
<li class="chapter" data-level="12.1" data-path="chap11.html"><a href="chap11.html#introduction-8"><i class="fa fa-check"></i><b>12.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="chap11.html"><a href="chap11.html#residual-heterogeneity-1"><i class="fa fa-check"></i><b>12.1.1</b> Residual Heterogeneity</a></li>
<li class="chapter" data-level="12.1.2" data-path="chap11.html"><a href="chap11.html#nature-of-dependency"><i class="fa fa-check"></i><b>12.1.2</b> Nature of Dependency</a></li>
<li class="chapter" data-level="12.1.3" data-path="chap11.html"><a href="chap11.html#objectives-of-bonus-malus-systems"><i class="fa fa-check"></i><b>12.1.3</b> Objectives of Bonus-Malus Systems</a></li>
<li class="chapter" data-level="12.1.4" data-path="chap11.html"><a href="chap11.html#nature-of-dependency-1"><i class="fa fa-check"></i><b>12.1.4</b> Nature of Dependency</a></li>
<li class="chapter" data-level="12.1.5" data-path="chap11.html"><a href="chap11.html#objectives-of-bonus-malus-systems-1"><i class="fa fa-check"></i><b>12.1.5</b> Objectives of Bonus-Malus Systems</a></li>
<li class="chapter" data-level="12.1.6" data-path="chap11.html"><a href="chap11.html#nature-of-dependency-2"><i class="fa fa-check"></i><b>12.1.6</b> Nature of Dependency</a></li>
<li class="chapter" data-level="12.1.7" data-path="chap11.html"><a href="chap11.html#bjectives-of-bonus-malus-systems"><i class="fa fa-check"></i><b>12.1.7</b> bjectives of Bonus-Malus Systems</a></li>
<li class="chapter" data-level="12.1.8" data-path="chap11.html"><a href="chap11.html#nature-of-dependency-3"><i class="fa fa-check"></i><b>12.1.8</b> Nature of Dependency</a></li>
<li class="chapter" data-level="12.1.9" data-path="chap11.html"><a href="chap11.html#objectives-of-bonus-malus-systems-2"><i class="fa fa-check"></i><b>12.1.9</b> Objectives of Bonus-Malus Systems</a></li>
<li class="chapter" data-level="12.1.10" data-path="chap11.html"><a href="chap11.html#bonus-thirst"><i class="fa fa-check"></i><b>12.1.10</b> Bonus Thirst</a></li>
<li class="chapter" data-level="12.1.11" data-path="chap11.html"><a href="chap11.html#class-based-systems-and-french-style-systems"><i class="fa fa-check"></i><b>12.1.11</b> Class-Based Systems and “<em>French-Style</em>” Systems</a></li>
<li class="chapter" data-level="12.1.12" data-path="chap11.html"><a href="chap11.html#a-brief-history-of-the-bonus-malus-system-in-france"><i class="fa fa-check"></i><b>12.1.12</b> A Brief History of the Bonus-Malus System in France</a></li>
<li class="chapter" data-level="12.1.13" data-path="chap11.html"><a href="chap11.html#a-brief-history-of-the-bonus-malus-system-in-belgium"><i class="fa fa-check"></i><b>12.1.13</b> A Brief History of the Bonus-Malus System in Belgium</a></li>
<li class="chapter" data-level="12.1.14" data-path="chap11.html"><a href="chap11.html#chapter-outline"><i class="fa fa-check"></i><b>12.1.14</b> Chapter Outline</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="chap11.html"><a href="chap11.html#scales-in-unsegmented-universes"><i class="fa fa-check"></i><b>12.2</b> Scales in Unsegmented Universes}</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="chap11.html"><a href="chap11.html#SecExIntro"><i class="fa fa-check"></i><b>12.2.1</b> Introductory Example: the Good/Bad Driver Model</a></li>
<li class="chapter" data-level="12.2.2" data-path="chap11.html"><a href="chap11.html#scales-and-markov-chains"><i class="fa fa-check"></i><b>12.2.2</b> Scales and Markov Chains</a></li>
<li class="chapter" data-level="12.2.3" data-path="chap11.html"><a href="chap11.html#norbergs-method"><i class="fa fa-check"></i><b>12.2.3</b> Norberg’s Method</a></li>
<li class="chapter" data-level="12.2.4" data-path="chap11.html"><a href="chap11.html#gilde-and-sundts-method"><i class="fa fa-check"></i><b>12.2.4</b> Gilde and Sundt’s Method</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="chap11.html"><a href="chap11.html#segmented-universe-scales"><i class="fa fa-check"></i><b>12.3</b> Segmented Universe Scales</a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="chap11.html"><a href="chap11.html#introductory-example-1"><i class="fa fa-check"></i><b>12.3.1</b> Introductory Example</a></li>
<li class="chapter" data-level="12.3.2" data-path="chap11.html"><a href="chap11.html#modeling-claims-frequency-in-segmented-universe"><i class="fa fa-check"></i><b>12.3.2</b> Modeling Claims Frequency in Segmented Universe</a></li>
<li class="chapter" data-level="12.3.3" data-path="chap11.html"><a href="chap11.html#severity-of-posterior-adjustments-depending-on-the-degree-of-a-priori-differentiation"><i class="fa fa-check"></i><b>12.3.3</b> Severity of Posterior Adjustments Depending on the Degree of A priori Differentiation</a></li>
<li class="chapter" data-level="12.3.4" data-path="chap11.html"><a href="chap11.html#norberg-method-in-a-segmented-universe"><i class="fa fa-check"></i><b>12.3.4</b> Norberg Method in a Segmented Universe</a></li>
<li class="chapter" data-level="12.3.5" data-path="chap11.html"><a href="chap11.html#interaction-between-posterior-adjustments-induced-by-the-scale-and-prior-pricing"><i class="fa fa-check"></i><b>12.3.5</b> Interaction between Posterior Adjustments Induced by the Scale and Prior Pricing</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="chap11.html"><a href="chap11.html#numerical-illustrations"><i class="fa fa-check"></i><b>12.4</b> Numerical Illustrations</a>
<ul>
<li class="chapter" data-level="12.4.1" data-path="chap11.html"><a href="chap11.html#prior-pricing"><i class="fa fa-check"></i><b>12.4.1</b> Prior Pricing</a></li>
<li class="chapter" data-level="12.4.2" data-path="chap11.html"><a href="chap11.html#scale--1top"><i class="fa fa-check"></i><b>12.4.2</b> Scale “-1/top”</a></li>
<li class="chapter" data-level="12.4.3" data-path="chap11.html"><a href="chap11.html#scale"><i class="fa fa-check"></i><b>12.4.3</b> “-1/+2” Scale</a></li>
<li class="chapter" data-level="12.4.4" data-path="chap11.html"><a href="chap11.html#scale-1"><i class="fa fa-check"></i><b>12.4.4</b> “-1/+4” Scale</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="chap11.html"><a href="chap11.html#performance-of-bonus-malus-scales"><i class="fa fa-check"></i><b>12.5</b> Performance of Bonus-Malus Scales</a>
<ul>
<li class="chapter" data-level="12.5.1" data-path="chap11.html"><a href="chap11.html#the-relative-stationary-average-level-rsal"><i class="fa fa-check"></i><b>12.5.1</b> The Relative Stationary Average Level (RSAL)</a></li>
<li class="chapter" data-level="12.5.2" data-path="chap11.html"><a href="chap11.html#the-relative-stationary-average-premium-rsap"><i class="fa fa-check"></i><b>12.5.2</b> The Relative Stationary Average Premium (RSAP)</a></li>
<li class="chapter" data-level="12.5.3" data-path="chap11.html"><a href="chap11.html#the-coefficient-of-variation-of-premiums"><i class="fa fa-check"></i><b>12.5.3</b> The Coefficient of Variation of Premiums</a></li>
<li class="chapter" data-level="12.5.4" data-path="chap11.html"><a href="chap11.html#loimarantas-efficiency"><i class="fa fa-check"></i><b>12.5.4</b> Loimaranta’s Efficiency</a></li>
<li class="chapter" data-level="12.5.5" data-path="chap11.html"><a href="chap11.html#lemaires-efficiency"><i class="fa fa-check"></i><b>12.5.5</b> Lemaire’s Efficiency</a></li>
<li class="chapter" data-level="12.5.6" data-path="chap11.html"><a href="chap11.html#lemaires-efficiency-1"><i class="fa fa-check"></i><b>12.5.6</b> Lemaire’s Efficiency</a></li>
<li class="chapter" data-level="12.5.7" data-path="chap11.html"><a href="chap11.html#optimal-average-retention"><i class="fa fa-check"></i><b>12.5.7</b> Optimal Average Retention</a></li>
</ul></li>
<li class="chapter" data-level="12.6" data-path="chap11.html"><a href="chap11.html#bibliographical-notes-9"><i class="fa fa-check"></i><b>12.6</b> Bibliographical Notes</a></li>
<li class="chapter" data-level="12.7" data-path="chap11.html"><a href="chap11.html#exercises-7"><i class="fa fa-check"></i><b>12.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="chap12.html"><a href="chap12.html"><i class="fa fa-check"></i><b>13</b> Economics of Insurance</a>
<ul>
<li class="chapter" data-level="13.1" data-path="chap12.html"><a href="chap12.html#introduction-9"><i class="fa fa-check"></i><b>13.1</b> Introduction</a></li>
<li class="chapter" data-level="13.2" data-path="chap12.html"><a href="chap12.html#decision-under-uncertainty"><i class="fa fa-check"></i><b>13.2</b> Decision under Uncertainty</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="chap12.html"><a href="chap12.html#the-von-neumann-and-morgenstern-expected-utility-model"><i class="fa fa-check"></i><b>13.2.1</b> The von Neumann and Morgenstern Expected Utility Model</a></li>
<li class="chapter" data-level="13.2.2" data-path="chap12.html"><a href="chap12.html#on-the-notion-of-subjectivity-in-probabilities"><i class="fa fa-check"></i><b>13.2.2</b> On the Notion of Subjectivity in Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="chap12.html"><a href="chap12.html#the-role-of-time-and-prudence"><i class="fa fa-check"></i><b>13.3</b> The Role of Time and Prudence</a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="chap12.html"><a href="chap12.html#limits-of-the-expected-utility-model-in-risk-allais-paradox-and-the-certainty-effect"><i class="fa fa-check"></i><b>13.3.1</b> Limits of the Expected Utility Model in Risk: Allais’ Paradox and the Certainty Effect</a></li>
<li class="chapter" data-level="13.3.2" data-path="chap12.html"><a href="chap12.html#the-certainty-effect-and-non-linear-probability-processing"><i class="fa fa-check"></i><b>13.3.2</b> The Certainty Effect and Non-Linear Probability Processing</a></li>
<li class="chapter" data-level="13.3.3" data-path="chap12.html"><a href="chap12.html#limits-of-the-expected-utility-model-in-non-probabilistic-uncertainty-the-ellsberg-paradox-and-the-concept-of-ambiguity"><i class="fa fa-check"></i><b>13.3.3</b> Limits of the Expected Utility Model in Non-Probabilistic Uncertainty: The Ellsberg Paradox and the Concept of Ambiguity</a></li>
<li class="chapter" data-level="13.3.4" data-path="chap12.html"><a href="chap12.html#section-choquet"><i class="fa fa-check"></i><b>13.3.4</b> Extension of the Notion of Expectation: Choquet Integral</a></li>
<li class="chapter" data-level="13.3.5" data-path="chap12.html"><a href="chap12.html#generalization-of-utility-expectation-models-rank-dependent-models"><i class="fa fa-check"></i><b>13.3.5</b> Generalization of Utility Expectation Models: Rank-Dependent Models</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="chap12.html"><a href="chap12.html#risk-measure-and-risk-aversion"><i class="fa fa-check"></i><b>13.4</b> Risk Measure and Risk Aversion</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="chap12.html"><a href="chap12.html#risk-aversion-and-risk-price"><i class="fa fa-check"></i><b>13.4.1</b> Risk Aversion and Risk Price</a></li>
<li class="chapter" data-level="13.4.2" data-path="chap12.html"><a href="chap12.html#measures-of-prudence-and-aversion"><i class="fa fa-check"></i><b>13.4.2</b> Measures of Prudence and Aversion</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="chap12.html"><a href="chap12.html#supply-demand-section"><i class="fa fa-check"></i><b>13.5</b> Insurance Supply and Demand</a>
<ul>
<li class="chapter" data-level="13.5.1" data-path="chap12.html"><a href="chap12.html#normalization-of-behaviors"><i class="fa fa-check"></i><b>13.5.1</b> Normalization of Behaviors</a></li>
<li class="chapter" data-level="13.5.2" data-path="chap12.html"><a href="chap12.html#insurance-demand"><i class="fa fa-check"></i><b>13.5.2</b> Insurance Demand</a></li>
<li class="chapter" data-level="13.5.3" data-path="chap12.html"><a href="chap12.html#the-mossin-model"><i class="fa fa-check"></i><b>13.5.3</b> The Mossin Model</a></li>
<li class="chapter" data-level="13.5.4" data-path="chap12.html"><a href="chap12.html#the-general-model-of-insurance-demand"><i class="fa fa-check"></i><b>13.5.4</b> The General Model of Insurance Demand</a></li>
<li class="chapter" data-level="13.5.5" data-path="chap12.html"><a href="chap12.html#special-case-of-proportional-insurance"><i class="fa fa-check"></i><b>13.5.5</b> Special Case of Proportional Insurance</a></li>
<li class="chapter" data-level="13.5.6" data-path="chap12.html"><a href="chap12.html#special-case-of-insurance-with-mandatory-deductible"><i class="fa fa-check"></i><b>13.5.6</b> Special Case of Insurance with Mandatory Deductible</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="chap12.html"><a href="chap12.html#information-asymmetry-and-adverse-selection"><i class="fa fa-check"></i><b>13.6</b> Information Asymmetry and Adverse Selection</a>
<ul>
<li class="chapter" data-level="13.6.1" data-path="chap12.html"><a href="chap12.html#incomplete-information-1"><i class="fa fa-check"></i><b>13.6.1</b> Incomplete Information</a></li>
<li class="chapter" data-level="13.6.2" data-path="chap12.html"><a href="chap12.html#adverse-selection-moral-hazard-and-signals"><i class="fa fa-check"></i><b>13.6.2</b> Adverse Selection, Moral Hazard, and Signals</a></li>
<li class="chapter" data-level="13.6.3" data-path="chap12.html"><a href="chap12.html#the-rothschild-stiglitz-equilibrium-model"><i class="fa fa-check"></i><b>13.6.3</b> The Rothschild &amp; Stiglitz Equilibrium Model</a></li>
<li class="chapter" data-level="13.6.4" data-path="chap12.html"><a href="chap12.html#study-of-adverse-selection-mechanisms"><i class="fa fa-check"></i><b>13.6.4</b> Study of Adverse Selection Mechanisms</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="chap12.html"><a href="chap12.html#coverage-of-multiple-risks"><i class="fa fa-check"></i><b>13.7</b> Coverage of Multiple Risks</a>
<ul>
<li class="chapter" data-level="13.7.1" data-path="chap12.html"><a href="chap12.html#context-3"><i class="fa fa-check"></i><b>13.7.1</b> Context</a></li>
<li class="chapter" data-level="13.7.2" data-path="chap12.html"><a href="chap12.html#insurable-risk-and-non-insurable-risk"><i class="fa fa-check"></i><b>13.7.2</b> Insurable Risk and Non-Insurable Risk</a></li>
<li class="chapter" data-level="13.7.3" data-path="chap12.html"><a href="chap12.html#presence-of-multiple-insurable-risks"><i class="fa fa-check"></i><b>13.7.3</b> Presence of Multiple Insurable Risks</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="chap13.html"><a href="chap13.html"><i class="fa fa-check"></i><b>14</b> Claims Reserving</a>
<ul>
<li class="chapter" data-level="14.1" data-path="chap13.html"><a href="chap13.html#introduction-10"><i class="fa fa-check"></i><b>14.1</b> Introduction</a></li>
<li class="chapter" data-level="14.2" data-path="chap13.html"><a href="chap13.html#notation-and-motivation"><i class="fa fa-check"></i><b>14.2</b> Notation and Motivation</a>
<ul>
<li class="chapter" data-level="14.2.1" data-path="chap13.html"><a href="chap13.html#the-dynamics-of-claim-life"><i class="fa fa-check"></i><b>14.2.1</b> The Dynamics of Claim Life</a></li>
<li class="chapter" data-level="14.2.2" data-path="chap13.html"><a href="chap13.html#the-dynamics-of-claims"><i class="fa fa-check"></i><b>14.2.2</b> The Dynamics of Claims</a></li>
<li class="chapter" data-level="14.2.3" data-path="chap13.html"><a href="chap13.html#time-lags-before-reporting"><i class="fa fa-check"></i><b>14.2.3</b> Time Lags Before Reporting</a></li>
<li class="chapter" data-level="14.2.4" data-path="chap13.html"><a href="chap13.html#run-off-triangles"><i class="fa fa-check"></i><b>14.2.4</b> Run-off Triangles</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="chap13.html"><a href="chap13.html#section-chain-ladder"><i class="fa fa-check"></i><b>14.3</b> Deterministic Methods</a>
<ul>
<li class="chapter" data-level="14.3.1" data-path="chap13.html"><a href="chap13.html#the-chain-ladder-method"><i class="fa fa-check"></i><b>14.3.1</b> The Chain Ladder Method</a></li>
<li class="chapter" data-level="14.3.2" data-path="chap13.html"><a href="chap13.html#link-ratios"><i class="fa fa-check"></i><b>14.3.2</b> Link Ratios</a></li>
<li class="chapter" data-level="14.3.3" data-path="chap13.html"><a href="chap13.html#boni-mali"><i class="fa fa-check"></i><b>14.3.3</b> Bonuses and Penalties, or Updating Estimates</a></li>
<li class="chapter" data-level="14.3.4" data-path="chap13.html"><a href="chap13.html#critiques-of-the-chain-ladder-method"><i class="fa fa-check"></i><b>14.3.4</b> Critiques of the Chain Ladder Method</a></li>
<li class="chapter" data-level="14.3.5" data-path="chap13.html"><a href="chap13.html#variations-on-the-chain-ladder-method"><i class="fa fa-check"></i><b>14.3.5</b> Variations on the Chain-Ladder Method</a></li>
<li class="chapter" data-level="14.3.6" data-path="chap13.html"><a href="chap13.html#projected-case-estimate-method"><i class="fa fa-check"></i><b>14.3.6</b> Projected Case Estimate Method</a></li>
<li class="chapter" data-level="14.3.7" data-path="chap13.html"><a href="chap13.html#de-vylders-least-squares-method"><i class="fa fa-check"></i><b>14.3.7</b> De Vylder’s Least Squares Method</a></li>
<li class="chapter" data-level="14.3.8" data-path="chap13.html"><a href="chap13.html#taylors-separation-method"><i class="fa fa-check"></i><b>14.3.8</b> Taylor’s Separation Method</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="chap13.html"><a href="chap13.html#stochastic-methods"><i class="fa fa-check"></i><b>14.4</b> Stochastic Methods</a>
<ul>
<li class="chapter" data-level="14.4.1" data-path="chap13.html"><a href="chap13.html#Section-Mack"><i class="fa fa-check"></i><b>14.4.1</b> The Mack Model</a></li>
<li class="chapter" data-level="14.4.2" data-path="chap13.html"><a href="chap13.html#the-christophides-log-linear-model"><i class="fa fa-check"></i><b>14.4.2</b> The Christophides Log-Linear Model</a></li>
<li class="chapter" data-level="14.4.3" data-path="chap13.html"><a href="chap13.html#the-renshaw-and-verrall-poisson-model"><i class="fa fa-check"></i><b>14.4.3</b> The Renshaw and Verrall Poisson Model</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="chap13.html"><a href="chap13.html#glm-and-reserving"><i class="fa fa-check"></i><b>14.5</b> GLM and Reserving</a>
<ul>
<li class="chapter" data-level="14.5.1" data-path="chap13.html"><a href="chap13.html#principle-10"><i class="fa fa-check"></i><b>14.5.1</b> Principle</a></li>
<li class="chapter" data-level="14.5.2" data-path="chap13.html"><a href="chap13.html#tweedie-models"><i class="fa fa-check"></i><b>14.5.2</b> Tweedie Models</a></li>
<li class="chapter" data-level="14.5.3" data-path="chap13.html"><a href="chap13.html#model-factors"><i class="fa fa-check"></i><b>14.5.3</b> Which Factorial Model to Choose?</a></li>
</ul></li>
<li class="chapter" data-level="14.6" data-path="chap13.html"><a href="chap13.html#choosing-the-reserving-method"><i class="fa fa-check"></i><b>14.6</b> Choosing the Reserving Method</a></li>
<li class="chapter" data-level="14.7" data-path="chap13.html"><a href="chap13.html#practical-case-studies"><i class="fa fa-check"></i><b>14.7</b> Practical Case Studies</a>
<ul>
<li class="chapter" data-level="14.7.1" data-path="chap13.html"><a href="chap13.html#automobile-insurance"><i class="fa fa-check"></i><b>14.7.1</b> Automobile Insurance</a></li>
<li class="chapter" data-level="14.7.2" data-path="chap13.html"><a href="chap13.html#medical-malpractice"><i class="fa fa-check"></i><b>14.7.2</b> Medical Malpractice</a></li>
</ul></li>
<li class="chapter" data-level="14.8" data-path="chap13.html"><a href="chap13.html#bibliographical-notes-10"><i class="fa fa-check"></i><b>14.8</b> Bibliographical Notes</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="chap14.html"><a href="chap14.html"><i class="fa fa-check"></i><b>15</b> Large Risks</a>
<ul>
<li class="chapter" data-level="15.1" data-path="chap14.html"><a href="chap14.html#introduction-11"><i class="fa fa-check"></i><b>15.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="15.1.1" data-path="chap14.html"><a href="chap14.html#the-concept-of-catastrophe"><i class="fa fa-check"></i><b>15.1.1</b> The Concept of Catastrophe</a></li>
<li class="chapter" data-level="15.1.2" data-path="chap14.html"><a href="chap14.html#why-manage-extreme-events-ex-ante"><i class="fa fa-check"></i><b>15.1.2</b> Why Manage Extreme Events <em>Ex Ante</em>?</a></li>
<li class="chapter" data-level="15.1.3" data-path="chap14.html"><a href="chap14.html#what-types-of-catastrophes"><i class="fa fa-check"></i><b>15.1.3</b> What Types of Catastrophes?</a></li>
<li class="chapter" data-level="15.1.4" data-path="chap14.html"><a href="chap14.html#data-presentation"><i class="fa fa-check"></i><b>15.1.4</b> Data Presentation</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="chap14.html"><a href="chap14.html#limit-law-of-maxima"><i class="fa fa-check"></i><b>15.2</b> Limit Law of Maxima</a>
<ul>
<li class="chapter" data-level="15.2.1" data-path="chap14.html"><a href="chap14.html#behavior-of-the-maximum-in-large-samples"><i class="fa fa-check"></i><b>15.2.1</b> Behavior of the Maximum in Large Samples</a></li>
<li class="chapter" data-level="15.2.2" data-path="chap14.html"><a href="chap14.html#large-deviations"><i class="fa fa-check"></i><b>15.2.2</b> Large Deviations</a></li>
<li class="chapter" data-level="15.2.3" data-path="chap14.html"><a href="chap14.html#estimation-of-the-maximum-distribution"><i class="fa fa-check"></i><b>15.2.3</b> Estimation of the Maximum Distribution</a></li>
<li class="chapter" data-level="15.2.4" data-path="chap14.html"><a href="chap14.html#the-n-th-largest-value"><i class="fa fa-check"></i><b>15.2.4</b> The <span class="math inline">\(n\)</span>-th Largest Value</a></li>
<li class="chapter" data-level="15.2.5" data-path="chap14.html"><a href="chap14.html#random-frequency-of-claims"><i class="fa fa-check"></i><b>15.2.5</b> Random Frequency of Claims</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="chap14.html"><a href="chap14.html#tail-thickness-of-distributions"><i class="fa fa-check"></i><b>15.3</b> Tail Thickness of Distributions</a>
<ul>
<li class="chapter" data-level="15.3.1" data-path="chap14.html"><a href="chap14.html#the-concept-of-regular-variation"><i class="fa fa-check"></i><b>15.3.1</b> The Concept of Regular Variation</a></li>
<li class="chapter" data-level="15.3.2" data-path="chap14.html"><a href="chap14.html#regular-variation-and-the-max-domain-of-attraction-of-the-fréchet-distribution"><i class="fa fa-check"></i><b>15.3.2</b> Regular Variation and the Max-Domain of Attraction of the Fréchet Distribution</a></li>
<li class="chapter" data-level="15.3.3" data-path="chap14.html"><a href="chap14.html#sum-maximum-and-subexponential-distribution"><i class="fa fa-check"></i><b>15.3.3</b> Sum, Maximum, and Subexponential Distribution</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="chap14.html"><a href="chap14.html#study-of-the-excess-distribution"><i class="fa fa-check"></i><b>15.4</b> Study of the Excess Distribution</a>
<ul>
<li class="chapter" data-level="15.4.1" data-path="chap14.html"><a href="chap14.html#generalized-pareto-distribution"><i class="fa fa-check"></i><b>15.4.1</b> Generalized Pareto Distribution</a></li>
<li class="chapter" data-level="15.4.2" data-path="chap14.html"><a href="chap14.html#pickands-balkema-de-haan-theorem"><i class="fa fa-check"></i><b>15.4.2</b> Pickands-Balkema-de Haan Theorem</a></li>
<li class="chapter" data-level="15.4.3" data-path="chap14.html"><a href="chap14.html#number-of-exceedances"><i class="fa fa-check"></i><b>15.4.3</b> Number of Exceedances</a></li>
<li class="chapter" data-level="15.4.4" data-path="chap14.html"><a href="chap14.html#sample-size-with-a-poisson-distribution"><i class="fa fa-check"></i><b>15.4.4</b> Sample Size with a Poisson Distribution</a></li>
<li class="chapter" data-level="15.4.5" data-path="chap14.html"><a href="chap14.html#sec-comportement-ex"><i class="fa fa-check"></i><b>15.4.5</b> Examples of Tail Behavior</a></li>
<li class="chapter" data-level="15.4.6" data-path="chap14.html"><a href="chap14.html#heavy-tailed-distributions-the-max-domain-of-attraction-of-the-fréchet-distribution"><i class="fa fa-check"></i><b>15.4.6</b> Heavy-Tailed Distributions: the Max-Domain of Attraction of the Fréchet Distribution</a></li>
<li class="chapter" data-level="15.4.7" data-path="chap14.html"><a href="chap14.html#thin-tailed-distributions-the-max-domain-of-attraction-of-the-gumbel-distribution"><i class="fa fa-check"></i><b>15.4.7</b> Thin-Tailed Distributions: the Max-Domain of Attraction of the Gumbel Distribution</a></li>
</ul></li>
<li class="chapter" data-level="15.5" data-path="chap14.html"><a href="chap14.html#estimation-of-extreme-quantiles"><i class="fa fa-check"></i><b>15.5</b> Estimation of Extreme Quantiles</a>
<ul>
<li class="chapter" data-level="15.5.1" data-path="chap14.html"><a href="chap14.html#estimation-of-the-tail-index"><i class="fa fa-check"></i><b>15.5.1</b> Estimation of the Tail Index</a></li>
<li class="chapter" data-level="15.5.2" data-path="chap14.html"><a href="chap14.html#time-and-return-period"><i class="fa fa-check"></i><b>15.5.2</b> Time and Return Period</a></li>
<li class="chapter" data-level="15.5.3" data-path="chap14.html"><a href="chap14.html#gpd-approximation-for-var"><i class="fa fa-check"></i><b>15.5.3</b> GPD Approximation for VaR</a></li>
<li class="chapter" data-level="15.5.4" data-path="chap14.html"><a href="chap14.html#hill-estimator-for-var"><i class="fa fa-check"></i><b>15.5.4</b> Hill Estimator for VaR</a></li>
<li class="chapter" data-level="15.5.5" data-path="chap14.html"><a href="chap14.html#cumulative-risk-extremes-and-compound-distributions"><i class="fa fa-check"></i><b>15.5.5</b> Cumulative Risk, Extremes, and Compound Distributions</a></li>
</ul></li>
<li class="chapter" data-level="15.6" data-path="chap14.html"><a href="chap14.html#multivariate-extreme-value-theory"><i class="fa fa-check"></i><b>15.6</b> Multivariate Extreme Value Theory</a>
<ul>
<li class="chapter" data-level="15.6.1" data-path="chap14.html"><a href="chap14.html#componentwise-maxima"><i class="fa fa-check"></i><b>15.6.1</b> Componentwise Maxima</a></li>
<li class="chapter" data-level="15.6.2" data-path="chap14.html"><a href="chap14.html#expression-of-limit-distributions"><i class="fa fa-check"></i><b>15.6.2</b> Expression of Limit Distributions</a></li>
<li class="chapter" data-level="15.6.3" data-path="chap14.html"><a href="chap14.html#representation-of-the-dependence-function-a"><i class="fa fa-check"></i><b>15.6.3</b> Representation of the Dependence Function <span class="math inline">\(A\)</span></a></li>
<li class="chapter" data-level="15.6.4" data-path="chap14.html"><a href="chap14.html#estimation-of-the-dependence-function"><i class="fa fa-check"></i><b>15.6.4</b> Estimation of the Dependence Function</a></li>
<li class="chapter" data-level="15.6.5" data-path="chap14.html"><a href="chap14.html#copulas-of-multivariate-extreme-value-distributions"><i class="fa fa-check"></i><b>15.6.5</b> Copulas of Multivariate Extreme Value Distributions</a></li>
<li class="chapter" data-level="15.6.6" data-path="chap14.html"><a href="chap14.html#correlation-coefficient"><i class="fa fa-check"></i><b>15.6.6</b> Correlation Coefficient</a></li>
<li class="chapter" data-level="15.6.7" data-path="chap14.html"><a href="chap14.html#comparison-of-dependence-1"><i class="fa fa-check"></i><b>15.6.7</b> Comparison of Dependence</a></li>
<li class="chapter" data-level="15.6.8" data-path="chap14.html"><a href="chap14.html#tail-dependence-coefficient"><i class="fa fa-check"></i><b>15.6.8</b> Tail Dependence Coefficient</a></li>
<li class="chapter" data-level="15.6.9" data-path="chap14.html"><a href="chap14.html#application-in-reinsurance-cost-vs.-expenses"><i class="fa fa-check"></i><b>15.6.9</b> Application in Reinsurance, Cost vs. Expenses</a></li>
</ul></li>
<li class="chapter" data-level="15.7" data-path="chap14.html"><a href="chap14.html#standard-reinsurance-treaties"><i class="fa fa-check"></i><b>15.7</b> Standard Reinsurance Treaties</a>
<ul>
<li class="chapter" data-level="15.7.1" data-path="chap14.html"><a href="chap14.html#reinsurance"><i class="fa fa-check"></i><b>15.7.1</b> Reinsurance</a></li>
<li class="chapter" data-level="15.7.2" data-path="chap14.html"><a href="chap14.html#proportional-reinsurance"><i class="fa fa-check"></i><b>15.7.2</b> Proportional Reinsurance</a></li>
<li class="chapter" data-level="15.7.3" data-path="chap14.html"><a href="chap14.html#Reass-NP"><i class="fa fa-check"></i><b>15.7.3</b> Non-Proportional Reinsurance</a></li>
<li class="chapter" data-level="15.7.4" data-path="chap14.html"><a href="chap14.html#pricing-of-non-proportional-treaties"><i class="fa fa-check"></i><b>15.7.4</b> Pricing of Non-Proportional Treaties</a></li>
<li class="chapter" data-level="15.7.5" data-path="chap14.html"><a href="chap14.html#quantile-quantile-q-q-plots"><i class="fa fa-check"></i><b>15.7.5</b> Quantile-Quantile (Q-Q) Plots</a></li>
<li class="chapter" data-level="15.7.6" data-path="chap14.html"><a href="chap14.html#mean-excess-function"><i class="fa fa-check"></i><b>15.7.6</b> Mean Excess Function</a></li>
<li class="chapter" data-level="15.7.7" data-path="chap14.html"><a href="chap14.html#the-lorenz-curve"><i class="fa fa-check"></i><b>15.7.7</b> The Lorenz Curve</a></li>
<li class="chapter" data-level="15.7.8" data-path="chap14.html"><a href="chap14.html#approximation-of-pure-premium"><i class="fa fa-check"></i><b>15.7.8</b> Approximation of Pure Premium</a></li>
<li class="chapter" data-level="15.7.9" data-path="chap14.html"><a href="chap14.html#approximation-of-a-wang-premium"><i class="fa fa-check"></i><b>15.7.9</b> Approximation of a Wang Premium</a></li>
<li class="chapter" data-level="15.7.10" data-path="chap14.html"><a href="chap14.html#estimation-of-tvar"><i class="fa fa-check"></i><b>15.7.10</b> Estimation of TVaR</a></li>
<li class="chapter" data-level="15.7.11" data-path="chap14.html"><a href="chap14.html#index-based-coverage-and-securitization"><i class="fa fa-check"></i><b>15.7.11</b> Index-Based Coverage and Securitization</a></li>
</ul></li>
<li class="chapter" data-level="15.8" data-path="chap14.html"><a href="chap14.html#insolvency-and-large-risks"><i class="fa fa-check"></i><b>15.8</b> Insolvency and Large Risks</a>
<ul>
<li class="chapter" data-level="15.8.1" data-path="chap14.html"><a href="chap14.html#probability-of-ruin-in-the-presence-of-large-losses-von-bahrs-approximation"><i class="fa fa-check"></i><b>15.8.1</b> Probability of Ruin in the Presence of Large Losses: von Bahr’s Approximation</a></li>
<li class="chapter" data-level="15.8.2" data-path="chap14.html"><a href="chap14.html#using-the-pollaczeck-khinchine-beekman-formula"><i class="fa fa-check"></i><b>15.8.2</b> Using the Pollaczeck-Khinchine-Beekman Formula</a></li>
</ul></li>
<li class="chapter" data-level="15.9" data-path="chap14.html"><a href="chap14.html#bibliographical-notes-11"><i class="fa fa-check"></i><b>15.9</b> Bibliographical Notes</a></li>
<li class="chapter" data-level="15.10" data-path="chap14.html"><a href="chap14.html#exercises-8"><i class="fa fa-check"></i><b>15.10</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="chap15.html"><a href="chap15.html"><i class="fa fa-check"></i><b>16</b> Monte Carlo</a>
<ul>
<li class="chapter" data-level="16.1" data-path="chap15.html"><a href="chap15.html#introduction-12"><i class="fa fa-check"></i><b>16.1</b> Introduction</a></li>
<li class="chapter" data-level="16.2" data-path="chap15.html"><a href="chap15.html#general-principles"><i class="fa fa-check"></i><b>16.2</b> General Principles</a>
<ul>
<li class="chapter" data-level="16.2.1" data-path="chap15.html"><a href="chap15.html#pseudo-random-numbers"><i class="fa fa-check"></i><b>16.2.1</b> Pseudo-Random Numbers</a></li>
<li class="chapter" data-level="16.2.2" data-path="chap15.html"><a href="chap15.html#Section-inversion"><i class="fa fa-check"></i><b>16.2.2</b> The Inversion Method</a></li>
<li class="chapter" data-level="16.2.3" data-path="chap15.html"><a href="chap15.html#Section-rejet"><i class="fa fa-check"></i><b>16.2.3</b> Rejection Method</a></li>
<li class="chapter" data-level="16.2.4" data-path="chap15.html"><a href="chap15.html#using-mixture-distributions"><i class="fa fa-check"></i><b>16.2.4</b> Using Mixture Distributions</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="chap15.html"><a href="chap15.html#bootstrap-resampling"><i class="fa fa-check"></i><b>16.3</b> Bootstrap Resampling</a>
<ul>
<li class="chapter" data-level="16.3.1" data-path="chap15.html"><a href="chap15.html#principles"><i class="fa fa-check"></i><b>16.3.1</b> Principles</a></li>
<li class="chapter" data-level="16.3.2" data-path="chap15.html"><a href="chap15.html#bootstrap-confidence-intervals"><i class="fa fa-check"></i><b>16.3.2</b> Bootstrap Confidence Intervals</a></li>
<li class="chapter" data-level="16.3.3" data-path="chap15.html"><a href="chap15.html#bootstrap-and-quantiles"><i class="fa fa-check"></i><b>16.3.3</b> Bootstrap and Quantiles</a></li>
<li class="chapter" data-level="16.3.4" data-path="chap15.html"><a href="chap15.html#bootstrap-and-correlated-samples"><i class="fa fa-check"></i><b>16.3.4</b> Bootstrap and Correlated Samples</a></li>
<li class="chapter" data-level="16.3.5" data-path="chap15.html"><a href="chap15.html#application-to-loss-reserving"><i class="fa fa-check"></i><b>16.3.5</b> Application to Loss Reserving</a></li>
<li class="chapter" data-level="16.3.6" data-path="chap15.html"><a href="chap15.html#bootstrap-and-correlated-samples-1"><i class="fa fa-check"></i><b>16.3.6</b> Bootstrap and Correlated Samples</a></li>
<li class="chapter" data-level="16.3.7" data-path="chap15.html"><a href="chap15.html#application-to-loss-reserving-1"><i class="fa fa-check"></i><b>16.3.7</b> Application to Loss Reserving</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="chap15.html"><a href="chap15.html#simulation-of-univariate-common-probability-distributions"><i class="fa fa-check"></i><b>16.4</b> Simulation of Univariate Common Probability Distributions</a>
<ul>
<li class="chapter" data-level="16.4.1" data-path="chap15.html"><a href="chap15.html#uniform-distribution"><i class="fa fa-check"></i><b>16.4.1</b> Uniform Distribution</a></li>
<li class="chapter" data-level="16.4.2" data-path="chap15.html"><a href="chap15.html#normal-gaussian-distribution"><i class="fa fa-check"></i><b>16.4.2</b> Normal (Gaussian) Distribution</a></li>
<li class="chapter" data-level="16.4.3" data-path="chap15.html"><a href="chap15.html#lognormal-distribution"><i class="fa fa-check"></i><b>16.4.3</b> Lognormal Distribution</a></li>
<li class="chapter" data-level="16.4.4" data-path="chap15.html"><a href="chap15.html#gamma-distribution-1"><i class="fa fa-check"></i><b>16.4.4</b> Gamma Distribution</a></li>
<li class="chapter" data-level="16.4.5" data-path="chap15.html"><a href="chap15.html#beta-distribution-1"><i class="fa fa-check"></i><b>16.4.5</b> Beta Distribution</a></li>
<li class="chapter" data-level="16.4.6" data-path="chap15.html"><a href="chap15.html#poisson-distribution"><i class="fa fa-check"></i><b>16.4.6</b> Poisson Distribution</a></li>
<li class="chapter" data-level="16.4.7" data-path="chap15.html"><a href="chap15.html#poisson-distribution-1"><i class="fa fa-check"></i><b>16.4.7</b> Poisson Distribution</a></li>
<li class="chapter" data-level="16.4.8" data-path="chap15.html"><a href="chap15.html#geometric-distribution"><i class="fa fa-check"></i><b>16.4.8</b> Geometric Distribution</a></li>
<li class="chapter" data-level="16.4.9" data-path="chap15.html"><a href="chap15.html#binomial-distribution"><i class="fa fa-check"></i><b>16.4.9</b> Binomial Distribution</a></li>
<li class="chapter" data-level="16.4.10" data-path="chap15.html"><a href="chap15.html#negative-binomial-distribution"><i class="fa fa-check"></i><b>16.4.10</b> Negative Binomial Distribution</a></li>
<li class="chapter" data-level="16.4.11" data-path="chap15.html"><a href="chap15.html#negative-binomial-distribution-1"><i class="fa fa-check"></i><b>16.4.11</b> Negative Binomial Distribution</a></li>
<li class="chapter" data-level="16.4.12" data-path="chap15.html"><a href="chap15.html#elliptical-distributions"><i class="fa fa-check"></i><b>16.4.12</b> Elliptical Distributions</a></li>
<li class="chapter" data-level="16.4.13" data-path="chap15.html"><a href="chap15.html#using-copulas"><i class="fa fa-check"></i><b>16.4.13</b> Using Copulas</a></li>
</ul></li>
<li class="chapter" data-level="16.5" data-path="chap15.html"><a href="chap15.html#simulation-of-stochastic-processes"><i class="fa fa-check"></i><b>16.5</b> Simulation of Stochastic Processes</a>
<ul>
<li class="chapter" data-level="16.5.1" data-path="chap15.html"><a href="chap15.html#simulation-of-markov-chains"><i class="fa fa-check"></i><b>16.5.1</b> Simulation of Markov Chains</a></li>
<li class="chapter" data-level="16.5.2" data-path="chap15.html"><a href="chap15.html#simulation-of-a-poisson-process"><i class="fa fa-check"></i><b>16.5.2</b> Simulation of a Poisson Process</a></li>
<li class="chapter" data-level="16.5.3" data-path="chap15.html"><a href="chap15.html#calculating-ruin-probability-through-simulation"><i class="fa fa-check"></i><b>16.5.3</b> Calculating Ruin Probability through Simulation</a></li>
</ul></li>
<li class="chapter" data-level="16.6" data-path="chap15.html"><a href="chap15.html#monte-carlo-via-markov-chains"><i class="fa fa-check"></i><b>16.6</b> Monte Carlo via Markov Chains</a>
<ul>
<li class="chapter" data-level="16.6.1" data-path="chap15.html"><a href="chap15.html#principle-11"><i class="fa fa-check"></i><b>16.6.1</b> Principle</a></li>
<li class="chapter" data-level="16.6.2" data-path="chap15.html"><a href="chap15.html#Ergodicity"><i class="fa fa-check"></i><b>16.6.2</b> Some Notions of Ergodic Theory</a></li>
<li class="chapter" data-level="16.6.3" data-path="chap15.html"><a href="chap15.html#simulation-of-an-invariant-measure-hastings-metropolis-algorithm"><i class="fa fa-check"></i><b>16.6.3</b> Simulation of an Invariant Measure: Hastings-Metropolis Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="16.7" data-path="chap15.html"><a href="chap15.html#variance-reduction"><i class="fa fa-check"></i><b>16.7</b> Variance Reduction</a>
<ul>
<li class="chapter" data-level="16.7.1" data-path="chap15.html"><a href="chap15.html#use-of-antithetic-variables"><i class="fa fa-check"></i><b>16.7.1</b> Use of Antithetic Variables</a></li>
<li class="chapter" data-level="16.7.2" data-path="chap15.html"><a href="chap15.html#use-of-control-variates"><i class="fa fa-check"></i><b>16.7.2</b> Use of Control Variates</a></li>
<li class="chapter" data-level="16.7.3" data-path="chap15.html"><a href="chap15.html#use-of-conditioning"><i class="fa fa-check"></i><b>16.7.3</b> Use of Conditioning</a></li>
<li class="chapter" data-level="16.7.4" data-path="chap15.html"><a href="chap15.html#stratified-sampling"><i class="fa fa-check"></i><b>16.7.4</b> Stratified Sampling</a></li>
<li class="chapter" data-level="16.7.5" data-path="chap15.html"><a href="chap15.html#importance-sampling"><i class="fa fa-check"></i><b>16.7.5</b> Importance Sampling</a></li>
</ul></li>
<li class="chapter" data-level="16.8" data-path="chap15.html"><a href="chap15.html#convergence-control-and-stopping-criteria"><i class="fa fa-check"></i><b>16.8</b> Convergence Control and Stopping Criteria</a>
<ul>
<li class="chapter" data-level="16.8.1" data-path="chap15.html"><a href="chap15.html#two-step-estimation"><i class="fa fa-check"></i><b>16.8.1</b> Two-Step Estimation</a></li>
<li class="chapter" data-level="16.8.2" data-path="chap15.html"><a href="chap15.html#sequential-approach"><i class="fa fa-check"></i><b>16.8.2</b> Sequential Approach</a></li>
</ul></li>
<li class="chapter" data-level="16.9" data-path="chap15.html"><a href="chap15.html#bibliographical-notes-12"><i class="fa fa-check"></i><b>16.9</b> Bibliographical Notes</a></li>
<li class="chapter" data-level="16.10" data-path="chap15.html"><a href="chap15.html#exercises-9"><i class="fa fa-check"></i><b>16.10</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="chap16.html"><a href="chap16.html"><i class="fa fa-check"></i><b>17</b> Conclusion</a></li>
<li class="chapter" data-level="" data-path="postface.html"><a href="postface.html"><i class="fa fa-check"></i>Postface</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Non Life Insurance Mathematics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chap9" class="section level1 hasAnchor" number="10">
<h1><span class="header-section-number">Chapter 10</span> Prior Ratemaking<a href="chap9.html#chap9" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="introduction-6" class="section level2 hasAnchor" number="10.1">
<h2><span class="header-section-number">10.1</span> Introduction<a href="chap9.html#introduction-6" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In so-called pricing, the idea is to divide contracts (and policyholders) into several categories so that within each category, risks can be considered “equivalent.” The foundations of segmented universe pricing were laid out in Chapter 3 of Volume 1 (see Section 3.8).</p>
<p>As we saw in Section 3.7, heterogeneity within a portfolio poses numerous problems, particularly adverse selection. If the same premium is applied to the entire portfolio, “bad” risks will insure themselves (at a lower price than they should), while “good” risks might be discouraged by the higher premium, which tends to deteriorate the result. The natural idea developed in the early sections of this chapter is to partition the portfolio to create sub-portfolios where risks can be considered independent and identically distributed (i.i.d.). These are referred to as risk classes. Classes are considered when risk classification is based on information available prior to the insurance contract (information about the insured, the insured property, etc.) and when information about the policyholder’s claims history is considered (as will be done in the following two chapters).</p>
<p>Chapters 3 and 4 of Volume 1 presented the general principles of premium calculation, primarily based on the calculation of the pure premium, which corresponds to the mathematical expectation of the annual cost of reported claims. However, this pure premium can be decomposed into two components: frequency and average cost. It should be noted that
<span class="math display">\[
\mathbb{E}\left[\sum_{i=1}^N X_i
\right]=\mathbb{E}\left[N\right]\times\mathbb{E}\left[X_i\right],
\]</span>
for independent claim amounts <span class="math inline">\(X_1,X_2,\ldots\)</span> and independent claim counts <span class="math inline">\(N\)</span> (see Property 3.2.11). It is essential to separate these two notions for several reasons: the explanatory factors are not always the same (in motor insurance, frequency is primarily related to the driver, while average cost is related to the vehicle); average cost is subject to inflation, while frequency follows more complex cycles; and while frequency can be known quickly, estimating cost can be time-consuming, especially for bodily injury claims (this will be discussed in detail in the Chapter on provisioning).</p>
<div class="example">
<p><span id="exm:unlabeled-div-1" class="example"><strong>Example 10.1  </strong></span>Figure <span class="math inline">\(\ref{FreqCoutM}\)</span> compares the average cost of claims and the claim frequency for different population segments based on data from France. The segmentation takes into account gender, age (18-25, 25-40, 40-65, over 65), vehicle usage (commercial or personal), and vehicle power (graded from A to K, with K representing high-powered vehicles), as well as some socio-professional categories (liberal professions, public service, or agriculture). The two axes represent the average behavior across the entire population (frequency around 13.5% and average cost around €1,100, corresponding to the vertical and horizontal axes of the figure, respectively).
It can be observed that the segment that stands out the most is young male drivers (aged 18 to 25, category H-18-25 in the top right), who have both more claims and more costly claims. Men aged 25 to 40 also stand out, especially when they have a powerful car or belong to a liberal profession and use their vehicle for work. Female liberal professionals are also highlighted, with a very high claim frequency but an average cost in the normal range. Conversely, elderly individuals or inactive men have a relatively low frequency for a standard average cost.</p>
</div>
<p>Furthermore, while direct modeling of the pure premium may appear more robust and faster, it is relatively complex to implement because it is difficult to find simple laws that correctly model the pure premium. In contrast, simple laws can be used to model frequencies and average costs (see Chapter 2).</p>
</div>
<div id="Sec965New" class="section level2 hasAnchor" number="10.2">
<h2><span class="header-section-number">10.2</span> Rating Variables<a href="chap9.html#Sec965New" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In insurance rating, for historical reasons and technical considerations, rating variables are generally qualitative variables. Continuous variables are typically grouped into classes.</p>
<p>In motor insurance, the premium can depend on the specific characteristics of the vehicle (power and top speed, vehicle age), its usage, the geographical area (traffic density), or certain specific traits of the regular driver.</p>
<p>The information that insurers try to obtain about the insured, in order to counter adverse selection, must, from a practical point of view, be verifiable. Some information that could reveal risky behavior (i.e., strongly correlated with claims) cannot be used because it would lead to significant fraud (a topic we will not address in this chapter). For example, the annual mileage of a vehicle is difficult to verify.</p>
<p>The explanatory variables comprising <span class="math inline">\(\mathbf{X}\)</span> can be of different types. Some of them can be quantitative and continuous (such as the car’s power or the age of the insured, for example). Other explanatory variables that the insurer has about policyholders can be discrete quantitative variables (the number of children of the insured, for example). Others are qualitative or categorical (such as the gender of the insured). One can also use indices summarizing the characteristics of the insured’s residential area based on public data, such as those from a census. By condensing relevant information about claims available from a national statistical institute, one can obtain new explanatory variables whose predictive power (recognized by the model) is often very significant. We will revisit this point in Section <span class="math inline">\(\ref{SecACP}\)</span>.</p>
<p>Quantitative variables require no particular comments. Concerning categorical variables, it is customary to code any factor that partitions the population into <span class="math inline">\(k\)</span> categories by integers <span class="math inline">\(0,1,\ldots,k-1\)</span>. Some factors may be ordinal and result from a quantitative variable (such as the vehicle’s power, which is coded into different classes), either ordinal but without a quantitative scale (such as the level of education), or purely qualitative without inducing any order (such as gender). A categorical variable with <span class="math inline">\(k\)</span> levels is generally encoded by <span class="math inline">\(k-1\)</span> binary variables, all of which are zero for the reference level.</p>
<div class="example">
<p><span id="exm:unlabeled-div-2" class="example"><strong>Example 10.2  </strong></span>Most of the time, explanatory variables are all categorical in a commercial tariff. Consider, for example, an insurance company segmenting based on gender, the sportiness of the vehicle, and the age of the insured (3 age classes: under 30, 30-65, and over 65). A policyholder is represented by a binary vector indicating the values of the variables:
<span class="math display">\[\begin{eqnarray*}
X_1&amp;=&amp;\left\{
\begin{array}{l}
0,\text{ if the insured is male},\\
1,\text{ if the insured is female},
\end{array}
\right.
\\
X_2&amp;=&amp;\left\{
\begin{array}{l}
0,\text{ if the vehicle is not sporty},\\
1,\text{ if the vehicle is sporty},
\end{array}
\right.
\\
X_3&amp;=&amp;\left\{
\begin{array}{l}
1,\text{ if the insured is under 30 years old},\\
0,\text{ otherwise},
\end{array}
\right.
\\
X_4&amp;=&amp;\left\{
\begin{array}{l}
1,\text{ if the insured is over 65 years old},\\
0,\text{ otherwise.}
\end{array}
\right.
\end{eqnarray*}\]</span>
For each variable, the most represented category in the portfolio is chosen as the reference level (i.e., the one for which all binary variables used to code it simultaneously have a value of 0). The results are then interpreted as over- or under-representation relative to this reference class. Thus, the vector (0,1,1,0) represents a male policyholder under 30 years old driving a sporty vehicle.</p>
</div>
</div>
<div id="basic-principles-of-statistics" class="section level2 hasAnchor" number="10.3">
<h2><span class="header-section-number">10.3</span> Basic Principles of Statistics<a href="chap9.html#basic-principles-of-statistics" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This section briefly recalls elementary statistical methods. Our presentation is intuitive and informal. For more details, we refer the reader, for example, to <span class="citation">(<a href="#ref-monfort1982cours" role="doc-biblioref">Monfort 1982</a>)</span>.</p>
<div id="empirical-cumulative-distribution-function" class="section level3 hasAnchor" number="10.3.1">
<h3><span class="header-section-number">10.3.1</span> Empirical Cumulative Distribution Function<a href="chap9.html#empirical-cumulative-distribution-function" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="ungrouped-data" class="section level4 hasAnchor" number="10.3.1.1">
<h4><span class="header-section-number">10.3.1.1</span> Ungrouped Data<a href="chap9.html#ungrouped-data" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Suppose we have observations <span class="math inline">\(x_1,x_2,\ldots, x_n\)</span> and consider them as realizations of random variables <span class="math inline">\(X_1,X_2,\ldots, X_n\)</span>, which are independent and have the same cumulative distribution function <span class="math inline">\(F\)</span>. These observations could represent the claims cost reported to the company.</p>
<p>The empirical cumulative distribution function, denoted as <span class="math inline">\({\hat F}_n\)</span>, provides an idea of the shape of <span class="math inline">\(F\)</span> based on the observations <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(x_n\)</span>. It is obtained by assigning a probability mass of <span class="math inline">\(1/n\)</span> to each of the <span class="math inline">\(x_i\)</span>, i.e.,
<span class="math display">\[
{\hat F}_n(x)=\frac{\#\{x_i \text{ such that }x_i\leq x\}}{n},\hspace{2mm}
x\in \mathbb{R}.
\]</span>
In other words, <span class="math inline">\({\hat F}_n(x)\)</span> is the proportion of observations in the sample that are less than or equal to <span class="math inline">\(x\in \mathbb{R}\)</span>. The function <span class="math inline">\(x\mapsto{\hat F}_n(x)\)</span> is a “staircase” function, with a jump of size <span class="math inline">\(k/n\)</span> at each value that appears <span class="math inline">\(k\)</span> times in the sample.</p>
<p>The empirical approach involves using <span class="math inline">\({\hat F}_n\)</span> instead of <span class="math inline">\(F\)</span> for all actuarial calculations. This approach is justified by the Glivenko-Cantelli theorem, which ensures that
<span class="math display">\[
\Pr\Big[\sup_{x\in \mathbb{R}}|{\hat F}_n(x)-F(x)|\to 0
\text{ as }n\text{ tends to }+\infty\Big]=1.
\]</span>
In other words, the graph of <span class="math inline">\({\hat F}_n\)</span> fits the graph of <span class="math inline">\(F\)</span> better as the number of observations <span class="math inline">\(n\)</span>, i.e., our information, increases; therefore, the graph of <span class="math inline">\({\hat F}_n\)</span> should provide a good approximation of that of <span class="math inline">\(F\)</span> for a “large” <span class="math inline">\(n\)</span>.</p>
<p>The distribution of <span class="math inline">\(n\widehat{F}_n(x)\)</span> is <span class="math inline">\(\text{Binomial}(n, F(x))\)</span>. Thus, according to the central limit theorem, for any <span class="math inline">\(x\)</span>,
<span class="math display">\[
\sqrt{n}\Big(\widehat{F}_n(x)-F(x)\Big)\xrightarrow{\text{d}}\mathcal{N}(0,\sigma_x^2)
\text{ as }n\to +\infty,
\]</span>
where <span class="math inline">\(\sigma_x^2=F(x)(1-F(x))\)</span>. For a sufficiently large <span class="math inline">\(n\)</span>, this allows us to obtain a confidence interval for the unknown value <span class="math inline">\(F(x)\)</span>.</p>
<p><strong>Remark:</strong></p>
<p>One can also “smooth” the empirical cumulative distribution function and derive an empirical density function <span class="math inline">\(\widehat{f}_n\)</span>. This leads to a kernel density estimator defined as follows:
<span class="math display">\[
\widehat{f}_K(x)=\frac{1}{nh}\sum_{i=1}^nK\left(\frac{x_i-x}{h}\right),
\]</span>
where <span class="math inline">\(K\)</span> is called the kernel, and <span class="math inline">\(h\)</span> is a positive parameter (called the bandwidth) that determines the level of smoothing. In a first approach, one can choose a Gaussian kernel, i.e., <span class="math inline">\(K(x)=\frac{1}{\sqrt{2\pi}}\exp(-x^2/2)\)</span>. Note that <span class="math inline">\(\widehat{f}_K\)</span> is a convergent estimator of the density as <span class="math inline">\(n\to +\infty\)</span> and <span class="math inline">\(h\to 0\)</span> at an adequate rate.</p>
<p><strong>Ungrouped Data:</strong></p>
<p>In many cases, the data available to the actuary is grouped into classes, which may be more or less wide. This implies that the actuary does not have individual observations <span class="math inline">\(x_1, x_2, \ldots, x_n\)</span>. Instead, if we denote <span class="math inline">\(c_0&lt;c_1&lt;c_2&lt;\ldots&lt;c_r\)</span> as the boundaries of the <span class="math inline">\(r\)</span> classes, it is known that there are <span class="math inline">\(n_j\)</span> claims with amounts between <span class="math inline">\(c_{j-1}\)</span> and <span class="math inline">\(c_j\)</span>, for <span class="math inline">\(j=1,2,\ldots,r\)</span>. Here, <span class="math inline">\(n_j\)</span> is the frequency of class <span class="math inline">\(C_j=]c_{j-1},c_j]\)</span>. Often, <span class="math inline">\(c_0=0\)</span>, so the first class is something like “claims with amounts <span class="math inline">\(\leq c_1\)</span>.” Sometimes, the upper limit <span class="math inline">\(c_r\)</span> of <span class="math inline">\(C_r\)</span> is not specified, and the last class is “claims with amounts <span class="math inline">\(&gt;c_{r-1}\)</span>,” assuming that <span class="math inline">\(c_r=+\infty\)</span>. Sometimes, the average <span class="math inline">\(m_j\)</span> of the amounts of claims falling into class <span class="math inline">\(C_j\)</span> is also provided.</p>
<p>Grouping data means that the empirical cumulative distribution function <span class="math inline">\({\hat F}_n\)</span> is only precisely known at the class boundaries <span class="math inline">\(c_j\)</span>, where it takes the values <span class="math inline">\({\hat F}_n(c_0)=0\)</span> and
<span class="math display">\[
{\hat F}_n(c_j)=\left\{
\begin{array}{l}
0,\text{ if }j=0\\
\frac{1}{n}\sum_{i=1}^jn_i,\hspace{2mm}
j=1,2,\ldots,r.
\end{array}
\right.
\]</span>
To approximate <span class="math inline">\({\hat F}_n\)</span>, linear interpolation is used on the segments <span class="math inline">\(]c_{j-1},c_j]\)</span> to obtain
<span class="math display">\[
{\hat F}_n(x)=
\left\{
\begin{array}{l}
0,\text{ if }x&lt;c_0,
\\
\frac{(c_j-x){\hat F}_n(c_{j-1})+(x-c_{j-1}){\hat F}_n(c_j)}
{c_j-c_{j-1}},\text{ if }c_{j-1}\leq x&lt;c_j,
\\
1,\text{ if }x\geq c_r.
\end{array}
\right.
\]</span>
It is noteworthy that in the grouped case, <span class="math inline">\({\hat F}_n\)</span> is not defined on <span class="math inline">\(]c_{r-1},+\infty[\)</span> when <span class="math inline">\(c_r=+\infty\)</span>, unless <span class="math inline">\(n_r=0\)</span>.</p>
<p><strong>Remark:</strong></p>
<p>Since <span class="math inline">\({\hat F}_n\)</span> is now a piecewise linear function, <span class="math inline">\({\hat F}_n\)</span> is differentiable everywhere, except at the ends of the classes <span class="math inline">\(c_0,c_1,\ldots,c_r\)</span>, where the right and left derivatives still exist. This derivative estimates the density function <span class="math inline">\(f\)</span> associated with <span class="math inline">\(F\)</span>. We will denote this estimator as <span class="math inline">\({\hat f}_n\)</span>, also called the histogram. The estimator <span class="math inline">\({\hat f}_n\)</span> is given by
<span class="math display">\[
{\hat f}_n(x)=
\left\{
\begin{array}{l}
0,\text{ if }x&lt;c_0,
\\
\frac{{\hat F}_n(c_{j})-{\hat F}_n(c_{j-1})}
{c_j-c_{j-1}}=\frac{n_j}{n(c_j-c_{j-1})},\text{ if }c_{j-1}\leq x&lt;c_j,
\\
0,\text{ if }x\geq c_r.
\end{array}
\right.
\]</span>
The graph of the function <span class="math inline">\(x\mapsto{\hat f}_n(x)\)</span> appears as a sequence of blocks this time. The area under the graph of <span class="math inline">\({\hat f}_n\)</span> is 1 by construction, except when <span class="math inline">\(c_r=+\infty\)</span>, in which case we cannot represent the probability of class <span class="math inline">\(C_r\)</span>. It is important to note that the area, not the height, of the blocks is proportional to the number of observations in each class. This accounts for the length of the classes in the calculation of relative frequencies.</p>
<p>In general, for the sake of precision, the actuary will keep the raw data <span class="math inline">\(x_1, x_2, \ldots, x_n\)</span> for calculations but will often use grouping for presenting results.</p>
</div>
<div id="estimation-of-main-parameters" class="section level4 hasAnchor" number="10.3.1.2">
<h4><span class="header-section-number">10.3.1.2</span> Estimation of Main Parameters<a href="chap9.html#estimation-of-main-parameters" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Once we have <span class="math inline">\({\hat F}_n\)</span>, we can easily estimate the main parameters of <span class="math inline">\(F\)</span>, namely, the mean, variance, coefficient of variation, and various quantiles. In each case, it is sufficient to replace the unknown cumulative distribution function <span class="math inline">\(F\)</span> with its empirical counterpart <span class="math inline">\({\hat F}_n\)</span>.</p>
<p><strong>The Mean</strong></p>
<p>To estimate the mean, we naturally use the sample mean:
<span class="math display">\[
{\hat\mu}_1=\int_{x\in \mathbb{R}}xd{\hat F}_n(x)
=\left\{
\begin{array}{l}
\frac{1}{n}\sum_{i=1}^nx_i\text{ (ungrouped case)}\\
\sum_{j=1}^r\frac{n_j(c_j+c_{j-1})}{2n}\text{ (grouped case)}.
\end{array}
\right.
\]</span>
Often, we use the notation <span class="math inline">\(\overline{x}\)</span> instead of <span class="math inline">\({\hat\mu}_1\)</span>. The sample mean <span class="math inline">\(\overline{x}\)</span> represents the center of gravity of the data and is highly sensitive to extreme values.</p>
<p><strong>The Variance</strong></p>
<p>Denoting
<span class="math display">\[
{\hat\mu}_2=\int_{x\in \mathbb{R}}x^2d{\hat F}_n(x)
=\left\{
\begin{array}{l}
\frac{1}{n}\sum_{i=1}^nx_i^2\text{ (ungrouped case)}\\
\sum_{j=1}^r\frac{n_j(c_j^3-c_{j-1}^3)}{3n(c_j-c_{j-1})}
\text{ (grouped case)}
\end{array}
\right.
\]</span>
the natural estimator for the variance is then given by
<span class="math inline">\(s^2={\hat\mu}_2-{\hat\mu}_1^2\)</span>.
The observed standard deviation is the positive square root of the sample variance <span class="math inline">\(s^2\)</span>, denoted as <span class="math inline">\(s\)</span>.</p>
<p><strong>The Coefficient of Variation</strong></p>
<p>Often, actuaries use the coefficient of variation <span class="math inline">\(cv\)</span>, defined as the ratio of the sample standard deviation to the sample mean, i.e.,
<span class="math inline">\(cv=\frac{s}{\overline{x}}\)</span>.
The coefficient of variation has the advantage of being dimensionless, making comparisons easier (e.g., when dealing with different currency units). It can be seen as a normalization of the standard deviation.</p>
<p><strong>Quantiles</strong></p>
<p>The empirical quantiles, denoted as <span class="math inline">\({\hat q}_p\)</span>, are simply obtained using the relation:
<span class="math display">\[
{\hat q}_p={\hat F}_n^{-1}(p)=\inf\{x\in\mathbb{R}|{\hat F}_n(x)\geq p\},\hspace{2mm}0&lt;p&lt;1.
\]</span>
In the ungrouped case, <span class="math inline">\({\hat F}_n^{-1}\)</span> maps <span class="math inline">\(p\)</span>, <span class="math inline">\(0&lt;p&lt;1\)</span>, to the smallest observation that leaves at least <span class="math inline">\(100p\%\)</span> of the data to its left. If we denote <span class="math inline">\(x_{(1)},x_{(2)},\ldots,x_{(n)}\)</span> as the observations arranged in ascending order, i.e.,
<span class="math inline">\(x_{(1)}\leq x_{(2)}\leq\ldots\leq x_{(n)}\)</span>,
then
<span class="math display">\[
{\hat q}_p=x_{(i)}\text{ for }p\in\left]\frac{i-1}{n},\frac{i}{n}
\right].
\]</span>
In the grouped case, the estimation of the <span class="math inline">\(p\)</span>th quantile is done as follows. First, we determine <span class="math inline">\(j_0\)</span> such that
<span class="math display">\[
{\hat F}_n(c_{j_0-1})\leq p&lt;{\hat F}_n(c_{j_0}).
\]</span>
Then, we have
<span class="math display">\[
{\hat F}_n({\hat q}_p)=\frac{(c_{j_0}-{\hat q}_p)
{\hat F}_n(c_{j_0-1})+({\hat q}_p-c_{j_0-1})
{\hat F}_n(c_{j_0})}{c_{j_0}-c_{j_0-1}}=p,
\]</span>
from which we obtain
<span class="math display">\[
{\hat q}_p=\frac{c_{j_0}-c_{j_0-1}}{{\hat F}_n(c_{j_0})-
{\hat F}_n(c_{j_0-1})}p-
\frac{c_{j_0}{\hat F}_n(c_{j_0-1})-c_{j_0-1}
{\hat F}_n(c_{j_0})}{{\hat F}_n(c_{j_0})-
{\hat F}_n(c_{j_0-1})}.
\]</span></p>
</div>
</div>
<div id="the-parametric-approach" class="section level3 hasAnchor" number="10.3.2">
<h3><span class="header-section-number">10.3.2</span> The Parametric Approach<a href="chap9.html#the-parametric-approach" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The empirical approach may not provide answers to all the questions actuaries have. To illustrate this point, consider the following example.</p>
<div class="example">
<p><span id="exm:unlabeled-div-3" class="example"><strong>Example 10.3  </strong></span>
An insurer offers policies with a mandatory deductible of
50 Euros per claim. The insurer has recorded the following claim costs (in Euros):
141, 16, 46, 40, 351, 259, 317, 1,511, 107, and 567. The average amount
<span class="math inline">\(\overline{x}\)</span> paid by the insurer per claim is 335.5 Euros.
Now, suppose the insurer increases the mandatory deductible to 100 Euros. We can then estimate the average claim cost as
<span class="math display">\[
\frac{91+301+209+267+1461+57+517}{7}=414.71.
\]</span>
The insurer, who previously paid 3,355 Euros to compensate policyholders, now only needs to pay 2,903 Euros.</p>
<p>The change from a mandatory deductible of 50 to 100 Euros would reduce costs by
<span class="math display">\[
\frac{3,355 - 2,903}{3,355} = 13.47\%.
\]</span>
Now, let’s assume the insurer instead wants to remove the mandatory deductible clause, for example, to gain new market share. The empirical approach does not allow us to evaluate the additional cost incurred by this contractual change. The reason is that the claim amounts recorded will be increased by 50 Euros each, bringing the total cost to 3,855 Euros. However, it’s important to note that the claims with costs less than 50 Euros, for which we have no information (as they were not reported to the company), will now become eligible for indemnification. All the empirical approach can tell us is that the additional cost incurred by removing the mandatory deductible clause is at least
<span class="math display">\[
\frac{500}{3,355} = 14.9\%.
\]</span></p>
<p>Now, suppose the company introduces an upper limit on indemnification, set at 1,500 Euros per claim, for example, in the policy’s terms and conditions. This would not lead to any change in the insurer’s average cost because no claims exceeding 1,500 Euros have been observed. However, any rational policyholder would most likely expect a premium reduction to offset the increased risk on their part or, at the very least, some form of benefit.</p>
<p>Next, suppose that claim amounts are subject to a 10% inflation. What can be said about the average claim cost? We can easily obtain inflation-adjusted claim amounts as follows:
<span class="math display">\[\begin{eqnarray*}
1.1(141+50)-50 &amp; = &amp; 160.1\\
1.1(16+50)-50 &amp; = &amp; 22.6\\
\text{etc.} &amp; &amp;,
\end{eqnarray*}\]</span>
which provides the new series 160.1, 22.6, 55.6, 391.1, 289.9, 353.7, 1,667.1, 122.7, and 628.7. One might be tempted to claim that the average amount paid by the company per claim now amounts to 374.05 Euros. However, this would ignore the claims falling within the range of 45.45-50, which were not reported to the insurer due to the mandatory deductible of 50 Euros but are now covered due to inflation. Inflation not only leads to a modification of claim amounts but also to a change in the number of claims reported to the company.</p>
</div>
<p>The alternative to address the unanswered questions in the above example is the parametric approach. In this approach, it is assumed that the unknown cumulative distribution function <span class="math inline">\(F\)</span> of the amount paid by the insurer following a claim is part of a family <span class="math inline">\(\mathcal{F}=\{F_{\boldsymbol{\theta}}, \boldsymbol{\theta}\in\Theta\}\)</span> of cumulative distribution functions with known analytical forms but depending on one or more unknown parameters <span class="math inline">\(\boldsymbol{\theta}\)</span>. Here, <span class="math inline">\(\Theta\)</span> represents the parameter space, i.e., the set of all permissible values for the parameter. Note that <span class="math inline">\(\boldsymbol{\theta}\)</span> can be either one-dimensional or multidimensional, depending on the parametric family under consideration. Therefore, identifying <span class="math inline">\(F\)</span> in <span class="math inline">\(\mathcal{F}\)</span> means determining the value of <span class="math inline">\(\boldsymbol{\theta}\)</span> such that <span class="math inline">\(F\equiv F_{\boldsymbol{\theta}}\)</span>.</p>
<p>To fully understand the complementarity of parametric and empirical approaches, let’s continue the study of the scenario presented above from a parametric perspective.</p>
<p>Example (Continuation of Example <span class="math inline">\(\ref{ExHOSPI}\)</span>):</p>
<p>Suppose we can validly consider that the amount of a claim follows a negative exponential distribution with mean <span class="math inline">\(\theta\)</span>. Let <span class="math inline">\(X\)</span> be the claim amount, and <span class="math inline">\(Y\)</span> be the amount paid by the insurer. A convenient way to determine the value of <span class="math inline">\(\theta\)</span> is to equate <span class="math inline">\(\mathbb{E}[Y]\)</span> and <span class="math inline">\(\overline{x}\)</span> because
<span class="math display">\[\begin{eqnarray*}
\mathbb{E}[Y] &amp; = &amp; \mathbb{E}[X-50|X&gt;50] \\
&amp; = &amp; \int_{x=50}^{+\infty}(x-50)\frac{\exp(-x/\theta)}
{\theta\exp(-50/\theta)}dx=\theta.
\end{eqnarray*}\]</span>
Thus, we choose <span class="math inline">\(\theta=335.5\)</span>. Following inflation, the average claim cost for the insurer becomes
<span class="math display">\[
\mathbb{E}[1.1X-50|1.1X&gt;50]=369.05.
\]</span>
Furthermore, the probability that a claim falls under the coverage was
<span class="math display">\[
\Pr[X&gt;50]=0.86154
\]</span>
without inflation, whereas it becomes
<span class="math display">\[
\Pr[1.1X&gt;50]=0.87329
\]</span>
after the effect of inflation.</p>
<p>As demonstrated by this very simple example, the parametric approach allows answering all the questions posed by the insurer.</p>
</div>
</div>
<div id="fishers-information" class="section level2 hasAnchor" number="10.4">
<h2><span class="header-section-number">10.4</span> Fisher’s Information<a href="chap9.html#fishers-information" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="conditions-on-the-statistical-model" class="section level3 hasAnchor" number="10.4.1">
<h3><span class="header-section-number">10.4.1</span> Conditions on the Statistical Model<a href="chap9.html#conditions-on-the-statistical-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider a parametric statistical model <span class="math inline">\(\mathcal{F}=\{F_{\boldsymbol{\theta}},\hspace{2mm}\boldsymbol{\theta}\in \Theta\}\)</span> with a common support <span class="math inline">\(\mathcal{S}\)</span>, where the parameter space <span class="math inline">\(\Theta\)</span> is an open subset of <span class="math inline">\({\mathbb{R}}^p\)</span>. Let <span class="math inline">\(f_{\boldsymbol{\theta}}\)</span> be the probability density (discrete or continuous) associated with <span class="math inline">\(F_{\boldsymbol{\theta}}\)</span>. We will now assume that the following three conditions are satisfied:</p>
<p>\begin{description}
1. <span class="math inline">\(f_{\boldsymbol{\theta}}(x)&gt;0\)</span> for all <span class="math inline">\(x\in\mathcal{S}\)</span> and <span class="math inline">\({\boldsymbol{\theta}}\in\Theta\)</span>;
2. <span class="math inline">\(\frac{\partial}{\partial{\boldsymbol{\theta}}}f_{\boldsymbol{\theta}}(x)\)</span>
exists for all <span class="math inline">\(x\in\mathcal{S}\)</span> and <span class="math inline">\({\boldsymbol{\theta}}\in\Theta\)</span>;
3, For any <span class="math inline">\({\boldsymbol{\theta}}\)</span> in <span class="math inline">\(\Theta\)</span>, we can differentiate
<span class="math inline">\(\int_{x\in A}dF_{\boldsymbol{\theta}}(x)\)</span> under the integral sign with respect to the components of <span class="math inline">\({\boldsymbol{\theta}}\)</span>.</p>
</div>
<div id="definition-35" class="section level3 hasAnchor" number="10.4.2">
<h3><span class="header-section-number">10.4.2</span> Definition<a href="chap9.html#definition-35" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider a random variable <span class="math inline">\(X\)</span> with distribution function <span class="math inline">\(F_{\boldsymbol{\theta}}\)</span>, for <span class="math inline">\(\boldsymbol{\theta}\in\Theta\)</span>. The Fisher information of the model is defined as the variance-covariance matrix, if it exists, of the random vector
<span class="math display">\[
\frac{\partial}{\partial\boldsymbol{\theta}}\ln f_{\boldsymbol{\theta}}(X)=\left(
\frac{\partial}{\partial\theta_1}\ln f_{\boldsymbol{\theta}}(X),\ldots,
\frac{\partial}{\partial\theta_p}\ln f_{\boldsymbol{\theta}}(X)\right)^\top;
\]</span>
this matrix will be denoted as <span class="math inline">\(\mathcal{I}({\boldsymbol{\theta}})\)</span>.</p>
<div class="proof">
<p><span id="unlabeled-div-4" class="proof"><em>Proof</em>. </span>Consider the continuous case. Starting from
<span class="math display">\[
\int_{x\in{\mathbb{R}}}f_{\boldsymbol{\theta}}(x)dx=1
\]</span>
we can differentiate both sides with respect to <span class="math inline">\(\theta_i\)</span>, <span class="math inline">\(i=1,2,\ldots,p\)</span>, and obtain
<span class="math display">\[\begin{eqnarray*}
0&amp;=&amp;\frac{\partial}{\partial\theta_i}\int_{x\in{\mathbb{R}}}f_{\boldsymbol{\theta}}(x)dx\\
&amp;=&amp;\int_{x\in{\mathbb{R}}}\frac{\partial}{\partial\theta_i}f_{\boldsymbol{\theta}}(x)dx\\
&amp;=&amp;\int_{x\in{\mathbb{R}}}\left\{\frac{\partial}{\partial\theta_i}\ln f_{\boldsymbol{\theta}}(x)\right\}
f_{\boldsymbol{\theta}}(x)dx\\
&amp;=&amp; \mathbb{E}\left[\frac{\partial}{\partial\theta_i}\ln f_{\boldsymbol{\theta}}(X)\right].
\end{eqnarray*}\]</span></p>
</div>
<p>Therefore, Property <span class="math inline">\(\ref{PropInfoFisher}\)</span> allows us to obtain the expression for <span class="math inline">\(\mathcal{I}({\boldsymbol{\theta}})\)</span> since
<span class="math display">\[
\mathbb{C}\left[\frac{\partial}{\partial\theta_i}\ln f_{\boldsymbol{\theta}}(X),
\frac{\partial}{\partial\theta_j}\ln f_{\boldsymbol{\theta}}(X)\right]
=\mathbb{E}\left[\frac{\partial}{\partial\theta_i}\ln f_{\boldsymbol{\theta}}(X)
\frac{\partial}{\partial\theta_j}\ln f_{\boldsymbol{\theta}}(X)\right].
\]</span></p>
<p>This can be expressed in matrix form as</p>
<p><span class="math display">\[
\mathcal{I}({\boldsymbol{\theta}})=\mathbb{E}\left[\frac{\partial}{\partial\boldsymbol{\theta}}
\ln f_{\boldsymbol{\theta}}(X)\left(\frac{\partial}{\partial\boldsymbol{\theta}}
\ln f_{\boldsymbol{\theta}}(X)\right)^\top\right].
\]</span></p>
<div id="alternative-expression" class="section level4 hasAnchor" number="10.4.2.1">
<h4><span class="header-section-number">10.4.2.1</span> Alternative Expression<a href="chap9.html#alternative-expression" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In general, as long as <span class="math inline">\(\int_{x\in{\mathbb{R}}}dF_{\boldsymbol{\theta}}(x)\)</span>
is twice differentiable under the integral sign, we obtain two equivalent expressions for <span class="math inline">\(\mathcal{I}({\boldsymbol{\theta}})\)</span>. Specifically, in the continuous case, starting with</p>
<p><span class="math display">\[
0=\frac{\partial^2}{\partial\theta_i\partial\theta_j}\int_{x\in{\mathbb{R}}}f_{\boldsymbol{\theta}}(x)dx,
\]</span></p>
<p>we have</p>
<p><span class="math display">\[
=\int_{x\in{\mathbb{R}}}\frac{\partial^2}{\partial\theta_i\partial\theta_j}f_{\boldsymbol{\theta}}(x)dx,
\]</span></p>
<p>which leads to</p>
<p><span class="math display">\[
=\int_{x\in{\mathbb{R}}}\frac{\frac{\partial^2}{\partial\theta_i\partial\theta_j}f_{\boldsymbol{\theta}}(x)}
{f_{\boldsymbol{\theta}}(x)}f_{\boldsymbol{\theta}}(x)dx,
\]</span></p>
<p>and finally</p>
<p><span class="math display">\[
=\mathbb{E}\left[\frac{\frac{\partial^2}{\partial\theta_i\partial\theta_j}f_{\boldsymbol{\theta}}(X)}
{f_{\boldsymbol{\theta}}(X)}\right].
\]</span></p>
<p>This leads to</p>
<p><span class="math display">\[\begin{eqnarray*}
\mathbb{E}\left[\frac{\partial^2}{\partial\theta_i\partial\theta_j}\ln f_{\boldsymbol{\theta}}(X)\right]
  &amp;=&amp;\mathbb{E}\left[\frac{\partial}{\partial\theta_i}\frac{\frac{\partial}{\partial\theta_j}f_{\boldsymbol{\theta}}(X)}
   {f_{\boldsymbol{\theta}}(X)}\right]
   \\
&amp;=&amp;
\mathbb{E}\left[\frac{\frac{\partial^2}{\partial\theta_i\partial\theta_j}f_{\boldsymbol{\theta}}(X)}
{f_{\boldsymbol{\theta}}(X)}\right]-\mathbb{E}\left[\frac{\frac{\partial}{\partial\theta_i}f_{\boldsymbol{\theta}}(X)
\frac{\partial}{\partial\theta_j}f_{\boldsymbol{\theta}}(X)}
{\{f_{\boldsymbol{\theta}}(X)\}^2}\right]\\
  &amp;=&amp;-\mathbb{E}\left[\frac{\frac{\partial}{\partial\theta_i}f_{\boldsymbol{\theta}}(X)
\frac{\partial}{\partial\theta_j}f_{\boldsymbol{\theta}}(X)}
{\{f_{\boldsymbol{\theta}}(X)\}^2}\right]\\
  &amp;=&amp;-\mathbb{E}\left[\frac{\partial}{\partial\theta_i}\ln f_{\boldsymbol{\theta}}(X)
\frac{\partial}{\partial\theta_j}\ln f_{\boldsymbol{\theta}}(X)\right].
\end{eqnarray*}\]</span></p>
<p>We can then write this in matrix form as</p>
<p><span class="math display">\[\begin{eqnarray}
\mathcal{I}(\boldsymbol{\theta})&amp;=&amp;\mathbb{E}\left[\frac{\partial}{\partial\boldsymbol{\theta}}
\ln f_{\boldsymbol{\theta}}(X)\left(\frac{\partial}{\partial\boldsymbol{\theta}}
\ln f_{\boldsymbol{\theta}}(X)\right)^\top\right]\nonumber\\
&amp;=&amp;-\mathbb{E}\left[\frac{\partial^2}{\partial\boldsymbol{\theta}\partial\boldsymbol{\theta}^\top}
\ln f_{\boldsymbol{\theta}}(X)\right].\label{ExpressFisher}
\end{eqnarray}\]</span></p>
<p>So, Fisher’s information <span class="math inline">\(\mathcal{I}(\boldsymbol{\theta})\)</span> can be obtained either as the expectation of the product of the
gradient vectors of <span class="math inline">\(\ln f_{\boldsymbol{\theta}}(X)\)</span> or as the negative expectation of the Hessian matrix
of <span class="math inline">\(\ln f_{\boldsymbol{\theta}}(X)\)</span>.</p>
</div>
<div id="kullback-information" class="section level4 hasAnchor" number="10.4.2.2">
<h4><span class="header-section-number">10.4.2.2</span> Kullback Information<a href="chap9.html#kullback-information" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Let <span class="math inline">\(\widetilde{\boldsymbol{\theta}}\)</span> be the true parameter value, and define, for outcome <span class="math inline">\(x\)</span>, the discriminative power of <span class="math inline">\(x\)</span>
between the true value <span class="math inline">\(\widetilde{\boldsymbol{\theta}}\)</span> of the parameter and another possible value <span class="math inline">\({\boldsymbol{\theta}}\)</span> for the parameter as</p>
<p><span class="math display">\[\begin{equation}
\label{Kullback94}
\ln\frac{f_{\widetilde{\boldsymbol{\theta}}}(x)}{f_{\boldsymbol{\theta}}(x)}.
\end{equation}\]</span></p>
<p>We should interpret <span class="math inline">\(\eqref{Kullback94}\)</span> as the logarithm of the ratio between the “likelihood” of observing
<span class="math inline">\(x\)</span> for the true parameter value <span class="math inline">\(\widetilde{\boldsymbol{\theta}}\)</span>
and the “likelihood” of observing <span class="math inline">\(x\)</span> if the parameter is <span class="math inline">\({\boldsymbol{\theta}}\)</span>.
For values of <span class="math inline">\(x\)</span> such that <span class="math inline">\(f_{\boldsymbol{\theta}}(x)&gt;f_{\widetilde{\boldsymbol{\theta}}}(x)\)</span>,
the quantity <span class="math inline">\(\eqref{Kullback94}\)</span> is negative, which is natural since, in such cases, the actuary would be more inclined to favor <span class="math inline">\({\boldsymbol{\theta}}\)</span>.</p>
<p>Kullback proposed defining an “average discriminative power” or “average discriminative information” as follows: the Kullback information from <span class="math inline">\(\widetilde{\boldsymbol{\theta}}\)</span>
to <span class="math inline">\({\boldsymbol{\theta}}\)</span> is defined as</p>
<p><span class="math display">\[
\mathbb{E}_{\widetilde{\boldsymbol{\theta}}}
\left[\ln\frac{f_{\widetilde{\boldsymbol{\theta}}}(X)}{f_{\boldsymbol{\theta}}(X)}\right]
=\int_{x\in{\mathbb{R}}}\ln\frac{f_{\widetilde{\boldsymbol{\theta}}}(x)}
{f_{\boldsymbol{\theta}}(x)}f_{\widetilde{\boldsymbol{\theta}}}(x)dx.
\]</span></p>
<p>This is denoted as <span class="math inline">\(\mathcal{I}(\widetilde{\boldsymbol{\theta}}|{\boldsymbol{\theta}})\)</span>.
We can easily see that <span class="math inline">\(\mathcal{I}(\widetilde{\boldsymbol{\theta}}|{\boldsymbol{\theta}})\geq 0\)</span>; indeed,</p>
<p><span class="math display">\[
\mathcal{I}(\widetilde{\boldsymbol{\theta}}|{\boldsymbol{\theta}})=-
\mathbb{E}_{\widetilde{\boldsymbol{\theta}}}
\left[\ln\frac{f_{\boldsymbol{\theta}}(X)}{f_{\widetilde{\boldsymbol{\theta}}}(X)}\right]\geq -\ln
\mathbb{E}_{\widetilde{\boldsymbol{\theta}}}
\left[\frac{f_{\boldsymbol{\theta}}(X)}{f_{\widetilde{\boldsymbol{\theta}}}(X)}\right]
\]</span>
due to the Jensen’s inequality applied to the function $-$. Furthermore,
<span class="math display">\[
\mathbb{E}_{\widetilde{\boldsymbol{\theta}}}
\left[\frac{f_{\boldsymbol{\theta}}(X)}{f_{\widetilde{\boldsymbol{\theta}}}(X)}\right]
=\int_{x\in{\mathbb{R}}}f_{\boldsymbol{\theta}}(x)dx=1,
\]</span>
which completes the justification. Moreover,
<span class="math display">\[
\mathcal{I}(\widetilde{\boldsymbol{\theta}}|{\boldsymbol{\theta}})=0
\Leftrightarrow f_{\widetilde{\boldsymbol{\theta}}}\equiv f_{\boldsymbol{\theta}}.
\]</span>
Note, however, that <span class="math inline">\(\mathcal{I}(\widetilde{\boldsymbol{\theta}}|{\boldsymbol{\theta}})\)</span> is generally not
a metric because it is not necessarily symmetric and does not always satisfy the triangle inequality.</p>
</div>
<div id="link-between-fisher-and-kullback-information" class="section level4 hasAnchor" number="10.4.2.3">
<h4><span class="header-section-number">10.4.2.3</span> Link between Fisher and Kullback Information<a href="chap9.html#link-between-fisher-and-kullback-information" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Let’s now establish the link between the two types of information. To do this, let’s again assume that the three technical assumptions listed at the beginning of this section are satisfied, and that <span class="math inline">\(\Theta\)</span> is an open subset of <span class="math inline">\({\mathbb{R}}^p\)</span>. We are considering the continuous case and starting with
<span class="math display">\[
\mathcal{I}(\widetilde{\boldsymbol{\theta}}|{\boldsymbol{\theta}})=
\int_{x\in{\mathbb{R}}}\ln\frac{f_{\widetilde{\boldsymbol{\theta}}}(x)}
{f_{\boldsymbol{\theta}}(x)}f_{\widetilde{\boldsymbol{\theta}}}(x)
dx
\]</span>
Let’s differentiate with respect to <span class="math inline">\(\theta_i\)</span> to obtain
<span class="math display">\[
\frac{\partial}{\partial\theta_i}\mathcal{I}(\widetilde{\boldsymbol{\theta}}|{\boldsymbol{\theta}})
=\int_{x\in{\mathbb{R}}}\left\{-\frac{\partial}{\partial\theta_i}\ln
f_{\boldsymbol{\theta}}(x)\right\}f_{\widetilde{\boldsymbol{\theta}}}(x)
dx.
\]</span></p>
<p>In passing, it can be verified that for <span class="math inline">\(i=1,2,\ldots,p\)</span>,</p>
<p><span class="math display">\[
\left.\frac{\partial}{\partial\theta_i}\mathcal{I}(\widetilde{\boldsymbol{\theta}}|{\boldsymbol{\theta}})
\right|_{\boldsymbol{\theta}=\widetilde{\boldsymbol{\theta}}}
=0,
\]</span></p>
<p>which is natural since <span class="math inline">\(\mathcal{I}(\widetilde{\boldsymbol{\theta}}|{\boldsymbol{\theta}})\)</span> reaches its minimum of 0 at <span class="math inline">\({\boldsymbol{\theta}}=\widetilde{\boldsymbol{\theta}}\)</span>. Now, consider the second derivatives; we have</p>
<p><span class="math display">\[
\frac{\partial^2}{\partial\theta_i\partial\theta_j}\mathcal{I}(\widetilde{\boldsymbol{\theta}}|{\boldsymbol{\theta}})
=\int_{x\in{\mathbb{R}}}\left\{\frac{\frac{\partial}{\partial\theta_i}
f_{\boldsymbol{\theta}}(x)\frac{\partial}{\partial\theta_j}
f_{\boldsymbol{\theta}}(x)}{\{f_{\boldsymbol{\theta}}(x)\}^2}-\frac{\frac{\partial^2}{\partial\theta_i\partial\theta_j}
f_{\boldsymbol{\theta}}(x)}{f_{\boldsymbol{\theta}}(x)}\right\}f_{\widetilde{\boldsymbol{\theta}}}(x)
dx
\]</span></p>
<p>From this, we obtain</p>
<p><span class="math display">\[\begin{equation}
\label{LienFisherKullback}
\left.\frac{\partial^2}{\partial\boldsymbol{\theta}\partial\boldsymbol{\theta}^\top}
\mathcal{I}(\widetilde{\boldsymbol{\theta}}|{\boldsymbol{\theta}})\right|_{\boldsymbol{\theta}=\widetilde{\boldsymbol{\theta}}}
=\mathbb{V}\left[\frac{\partial}{\partial{\boldsymbol{\theta}}}\ln f_{\widetilde{\boldsymbol{\theta}}}(X)\right]
=\mathcal{I}(\widetilde{\boldsymbol{\theta}})
\end{equation}\]</span></p>
<p>since <span class="math inline">\(\int_{x\in{\mathbb{R}}}\frac{\partial^2}{\partial\theta_i\theta_j} f_{\boldsymbol{\theta}}(x)dx=0\)</span>. It can also be verified that the matrix of second derivatives
of <span class="math inline">\(\mathcal{I}(\widetilde{\boldsymbol{\theta}}|{\boldsymbol{\theta}})\)</span> is positive semidefinite
at <span class="math inline">\({\boldsymbol{\theta}}=\widetilde{\boldsymbol{\theta}}\)</span>.</p>
<p>The relationship <span class="math inline">\(\eqref{LienFisherKullback}\)</span>
establishes the link between Fisher’s information and Kullback’s information.
This relationship allows us to interpret <span class="math inline">\(\mathcal{I}({\boldsymbol{\theta}})\)</span>, which
will play a very important role in the following.
Indeed, the matrix <span class="math inline">\(\mathcal{I}(\widetilde{\boldsymbol{\theta}})\)</span> describes the local behavior
of the function <span class="math inline">\({\boldsymbol{\theta}}\mapsto\mathcal{I}(\widetilde{\boldsymbol{\theta}}|{\boldsymbol{\theta}})\)</span> in the neighborhood
of the point <span class="math inline">\(\widetilde{\boldsymbol{\theta}}\)</span> where it reaches its minimum of 0. Thus, for <span class="math inline">\(p=1\)</span>
(i.e., there is only one parameter)</p>
<ol style="list-style-type: decimal">
<li>if <span class="math inline">\(\mathcal{I}(\widetilde{\theta})\)</span> is close to 0, the curve <span class="math inline">\(\mathcal{I}(\widetilde{\theta}|\theta)\)</span> is very flat in the vicinity of <span class="math inline">\(\widetilde{\theta}\)</span>, and it is difficult to distinguish <span class="math inline">\(\widetilde{\theta}\)</span> from values <span class="math inline">\(\theta\)</span> in its vicinity;</li>
<li>if <span class="math inline">\(\mathcal{I}(\widetilde{\theta})\)</span> is high, on the contrary, we can easily distinguish the true value <span class="math inline">\(\widetilde{\theta}\)</span> of the parameter from the values in the vicinity.</li>
</ol>
</div>
</div>
<div id="parameter-estimation-using-maximum-likelihood-method" class="section level3 hasAnchor" number="10.4.3">
<h3><span class="header-section-number">10.4.3</span> Parameter Estimation Using Maximum Likelihood Method<a href="chap9.html#parameter-estimation-using-maximum-likelihood-method" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="likelihood-function" class="section level4 hasAnchor" number="10.4.3.1">
<h4><span class="header-section-number">10.4.3.1</span> Likelihood Function<a href="chap9.html#likelihood-function" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>From now on, we will consider identifiable models, i.e., models for which
the mapping <span class="math inline">\({\boldsymbol{\theta}}\mapsto F_{\boldsymbol{\theta}}\)</span>
is injective.
Suppose we have selected the parametric family <span class="math inline">\(\{F_{\boldsymbol{\theta}},\hspace{2mm}\boldsymbol{\theta}\in\Theta\subseteq\mathbb{R}^p\}\)</span>.
We seek to determine the most plausible value of <span class="math inline">\({\boldsymbol{\theta}}\)</span> given the observations
<span class="math inline">\(x_1,x_2,\ldots,x_n\)</span> we have;
we will denote this value as <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span>, which is the estimation of the
parameter of interest.</p>
<p>The maximum likelihood method requires the definition of the likelihood function, denoted as <span class="math inline">\(\mathcal{L}({\boldsymbol{\theta}}|\boldsymbol{x})\)</span>, which is given by</p>
<ol style="list-style-type: decimal">
<li>In the non-grouped case:
<span class="math display">\[
\mathcal{L}({\boldsymbol{\theta}}|\boldsymbol{x})=\prod_{j=1}^nf_{\boldsymbol{\theta}}(x_j),
\]</span>
where <span class="math inline">\(f_{\boldsymbol{\theta}}\)</span> is the probability density (discrete or continuous) associated with
<span class="math inline">\(F_{\boldsymbol{\theta}}\)</span>;</li>
<li>In the grouped case:
<span class="math display">\[
\mathcal{L}({\boldsymbol{\theta}}|\boldsymbol{x})=\prod_{j=1}^r(F_{\boldsymbol{\theta}}(c_j)
-F_{\boldsymbol{\theta}}(c_{j-1}))^{n_j}.
\]</span></li>
</ol>
<p>Intuitively, <span class="math inline">\(\mathcal{L}(\boldsymbol{\theta}|\boldsymbol{x})\)</span> should be considered as the “chance” of observing the values <span class="math inline">\(x_1,x_2, \ldots,x_n\)</span> for the value <span class="math inline">\(\boldsymbol{\theta}\)</span> of the parameter. It is very important to note that <span class="math inline">\(\mathcal{L}(\boldsymbol{\theta}|\boldsymbol{x})\)</span> is a function of
<span class="math inline">\(\boldsymbol{\theta}\)</span>, given that the observations <span class="math inline">\(\boldsymbol{x}=(x_1,x_2,\ldots,x_n)^\top\)</span> are fixed, while
the joint density of observations <span class="math inline">\(f_{\boldsymbol{\theta}}(\boldsymbol{x})=\prod_{i=1}^nf_{\boldsymbol{\theta}}(x_i)\)</span>
is a function of the observations <span class="math inline">\(x_1,x_2,\ldots,x_n\)</span>,
parametrized by <span class="math inline">\(\boldsymbol{\theta}\)</span>.</p>
</div>
<div id="estimation-method" class="section level4 hasAnchor" number="10.4.3.2">
<h4><span class="header-section-number">10.4.3.2</span> Estimation Method<a href="chap9.html#estimation-method" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The maximum likelihood estimator <span class="math inline">\(\widehat{\boldsymbol{\theta}}\)</span> of <span class="math inline">\(\boldsymbol{\theta}\)</span> is obtained by maximizing the “chance” of observing <span class="math inline">\(x_1,x_2,\ldots,x_n\)</span>, i.e.</p>
<p><span class="math display">\[\begin{equation}
\label{EMV}
\widehat{\boldsymbol{\theta}}=\arg\max_{\boldsymbol{\theta}\in\Theta}
\mathcal{L}(\boldsymbol{\theta}|\boldsymbol{x}).
\end{equation}\]</span></p>
<p>In essence, this is a fairly intuitive method when you have a reliable sample: you estimate
the parameter <span class="math inline">\(\boldsymbol{\theta}\)</span> as the value <span class="math inline">\(\widehat{\boldsymbol{\theta}}\)</span> that maximizes the probability of obtaining the observations
<span class="math inline">\(x_1,x_2,\ldots,x_n\)</span> from the sample.
In practice, it is often easier to work with the logarithm
before the maximization step. Thus, we define the log-likelihood <span class="math inline">\(L(\boldsymbol{\theta}|\boldsymbol{x})\)</span>
associated with the sample, given by</p>
<p><span class="math display">\[
L(\boldsymbol{\theta}|\boldsymbol{x})=\ln\mathcal{L}(\boldsymbol{\theta}|\boldsymbol{x}).
\]</span></p>
<p>The maximum likelihood estimator <span class="math inline">\(\widehat{\boldsymbol{\theta}}\)</span> of <span class="math inline">\(\boldsymbol{\theta}\)</span>
is obtained through the maximization program</p>
<p><span class="math display">\[\begin{equation}
\label{OptBis}
\widehat{\boldsymbol{\theta}}=\arg\max_{\boldsymbol{\theta}\in\Theta}
L(\boldsymbol{\theta}|\boldsymbol{x}).
\end{equation}\]</span></p>
<div class="remark">
<p><span id="unlabeled-div-5" class="remark"><em>Remark</em> (Censoring). </span>In some cases, the likelihood function can take forms other than those mentioned above.
In most branches of insurance, the amount of compensation paid by the insurer does not exactly correspond to the loss suffered by the insured, due to the introduction
of deductible or mandatory coverage clauses in policy terms, or even the specification of
a coverage limit. This has a significant impact on estimation methods. Indeed, the actuary wishes to model
the amount of the loss, not the insurer’s payout.
For example, if the insurer has introduced
a coverage limit, set at <span class="math inline">\(\omega\)</span> for example, and the actuary has data <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>,
<span class="math inline">\(\ldots\)</span>, <span class="math inline">\(x_k\)</span>, <span class="math inline">\(\omega\)</span>, <span class="math inline">\(\omega\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(\omega\)</span>, then the likelihood <span class="math inline">\(\mathcal{L}({\boldsymbol{\theta}}|\boldsymbol{x})\)</span>
is written in the form</p>
<p><span class="math display">\[
\mathcal{L}({\boldsymbol{\theta}}|\boldsymbol{x})=\big(\overline{F}_{\boldsymbol{\theta}}(\omega)\big)^{n-k}
\prod_{j=1}^kf_{\boldsymbol{\theta}}(x_j).
\]</span></p>
<p>Indeed, an observation of <span class="math inline">\(\omega\)</span> actually means that the amount
of the loss was at least <span class="math inline">\(\omega\)</span>, but the insurer paid
only a compensation of <span class="math inline">\(\omega\)</span>.</p>
</div>
</div>
<div id="properties-5" class="section level4 hasAnchor" number="10.4.3.3">
<h4><span class="header-section-number">10.4.3.3</span> Properties<a href="chap9.html#properties-5" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Maximum likelihood estimators have excellent theoretical properties. Under fairly general assumptions, they are asymptotically unbiased and efficient (i.e., they are the most precise), and always make the best use of the information contained in the sample.</p>
<p>Let’s now examine the behavior of maximum likelihood estimators in large samples. Let <span class="math inline">\(\widehat{\boldsymbol{\theta}}_n\)</span> be the maximum likelihood estimator of
<span class="math inline">\({\boldsymbol{\theta}}\)</span> obtained from a sample of size <span class="math inline">\(n\)</span>. When <span class="math inline">\(\Theta\)</span> is an open subset of
<span class="math inline">\({\mathbb{R}}^p\)</span>, and as long as the model is identifiable, let’s show that when the number <span class="math inline">\(n\)</span> of observations is
sufficiently large, <span class="math inline">\(\widehat{\boldsymbol{\theta}}_n-\boldsymbol{\theta}\)</span> is approximately multivariate normally distributed with mean <span class="math inline">\(\boldsymbol{0}\)</span> and covariance matrix the inverse of the Fisher information matrix <span class="math inline">\(\mathcal{I}\)</span>, i.e.</p>
<p><span class="math display">\[\begin{equation}
\label{ApproxNorMLE}
\widehat{\boldsymbol{\theta}}_n-\boldsymbol{\theta}\approx_{\text{loi}}\mathcal{N}or
(\boldsymbol{0},\mathcal{I}^{-1}).
\end{equation}\]</span></p>
<p>To justify this statement,
suppose that there exists a unique maximum of the log-likelihood and that <span class="math inline">\(\widehat{\boldsymbol{\theta}}_n\)</span>
is close to the true value <span class="math inline">\(\boldsymbol{\theta}\)</span> of the parameter. Let <span class="math inline">\(\boldsymbol{U}\)</span> be the gradient vector
of the log-likelihood, and <span class="math inline">\(\boldsymbol{H}\)</span> the corresponding Hessian matrix.
A Taylor expansion to the first order then gives</p>
<p><span class="math display">\[
{\boldsymbol{U}}(\boldsymbol{\theta})\approx
\underbrace{{\boldsymbol{U}}(\widehat{\boldsymbol{\theta}}_n)}_{=0\text{ by definition of }
\widehat{\boldsymbol{\theta}}_n}+{\boldsymbol{H}}(\widehat{\boldsymbol{\theta}}_n)
\Big(\boldsymbol{\theta}-\widehat{\boldsymbol{\theta}}_n\Big).
\]</span></p>
<p>Asymptotically, <span class="math inline">\({\boldsymbol{H}}\)</span> is equal to its mean value <span class="math inline">\(-\mathcal{I}\)</span>, so</p>
<p><span class="math display">\[
{\boldsymbol{U}}(\boldsymbol{\theta})\approx
-\mathcal{I}
\Big(\boldsymbol{\theta}-\widehat{\boldsymbol{\theta}}_n\Big)=\mathcal{I}
\Big(\widehat{\boldsymbol{\theta}}_n-{\boldsymbol{\theta}}\Big)
\]</span></p>
<p><span class="math display">\[
\Rightarrow \widehat{\boldsymbol{\theta}}_n-{\boldsymbol{\theta}}\approx \mathcal{I}^{-1}\boldsymbol{U}(\boldsymbol{\theta}).
\]</span></p>
<p>This last relation allows us to obtain the asymptotic covariance matrix
of the maximum likelihood estimator <span class="math inline">\(\widehat{\boldsymbol{\theta}}_n\)</span> of <span class="math inline">\(\boldsymbol{\theta}\)</span>, which
is given by</p>
<p><span class="math display">\[
\mathbb{E}\big[(\widehat{\boldsymbol{\theta}}_n-{\boldsymbol{\theta}})(\widehat{\boldsymbol{\theta}}_n-{\boldsymbol{\theta}})^\top\big]
\approx \mathcal{I}^{-1}\underbrace{\mathbb{E}[\boldsymbol{U}\boldsymbol{U}^\top]}_{=\mathcal{I}}\mathcal{I}=\mathcal{I}^{-1}.
\]</span></p>
<p>The central limit theorem ensures
that <span class="math inline">\(\boldsymbol{U}(\boldsymbol{\theta})\)</span> is approximately Gaussian (as the sum of <span class="math inline">\(n\)</span> independent random variables), so, in a large sample, we indeed have <span class="math inline">\(\eqref{ApproxNorMLE}\)</span>.</p>
</div>
</div>
<div id="likelihood-ratio-test" class="section level3 hasAnchor" number="10.4.4">
<h3><span class="header-section-number">10.4.4</span> Likelihood Ratio Test<a href="chap9.html#likelihood-ratio-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This test is often very useful for answering certain questions regarding the parameters. So, if <span class="math inline">\({\boldsymbol{\theta}}\)</span> is <span class="math inline">\(p\)</span>-dimensional, and if the question posed (<span class="math inline">\(H_0\)</span>) is that there are <span class="math inline">\(j\)</span> restrictions on the parameter domain of the form <span class="math inline">\(R_i({\boldsymbol{\theta}})=0\)</span>, <span class="math inline">\(i=1,2,\ldots,j\)</span>, where each of the functions <span class="math inline">\(R_i\)</span>
has continuous first partial derivatives with respect to the components of <span class="math inline">\({\boldsymbol{\theta}}\)</span>, and if the alternative (<span class="math inline">\(H_1\)</span>) is to say that there are no such restrictions, then the maximum likelihood estimator under the constraint (under <span class="math inline">\(H_0\)</span>), denoted as <span class="math inline">\(\widetilde{\boldsymbol{\theta}}\)</span>, and the unconstrained maximum likelihood estimator, denoted as <span class="math inline">\(\widehat{\boldsymbol{\theta}}\)</span>, are calculated. The test statistic
<span class="math display">\[
\mathcal{RV}_n=2\Big\{L(\widehat{\boldsymbol{\theta}}|\boldsymbol{X})-
L(\widetilde{\boldsymbol{\theta}}|\boldsymbol{X})\Big\}
\]</span>
is approximately chi-squared distributed with <span class="math inline">\(j\)</span> degrees of freedom (for sufficiently large <span class="math inline">\(n\)</span>).
We reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(T\)</span> is “too large,” i.e., if <span class="math inline">\(\mathcal{RV}_n&gt;\chi_{j;1-\alpha}^2\)</span>.</p>
</div>
<div id="other-estimation-methods" class="section level3 hasAnchor" number="10.4.5">
<h3><span class="header-section-number">10.4.5</span> Other Estimation Methods<a href="chap9.html#other-estimation-methods" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As we will see later, likelihood equations often do not have explicit solutions. Therefore, numerical methods are used, which proceed through iteration. Thus, an initial value as precise as possible for the parameters is required, which can be obtained using “ad hoc” methods.</p>
<p>“Ad hoc” methods represent a set of widely used practical methods, often without a real theoretical basis, which involve equating a number of sample values (i.e., calculated based on <span class="math inline">\({\hat F}_n\)</span>) to their population counterparts (i.e., calculated based on <span class="math inline">\(F_{\boldsymbol{\theta}}\)</span>). The choice of these quantities is guided by practical considerations: they are the ones that the actuary wishes to emphasize because their importance is crucial to the problem being addressed. Among this set of methods, we distinguish the method of moments and the method of quantiles.</p>
<div id="method-of-moments" class="section level4 hasAnchor" number="10.4.5.1">
<h4><span class="header-section-number">10.4.5.1</span> Method of Moments<a href="chap9.html#method-of-moments" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Suppose that <span class="math inline">\({\boldsymbol{\theta}}\)</span> is <span class="math inline">\(p\)</span>-dimensional. The method of moments consists of equating the first <span class="math inline">\(p\)</span> observed moments to their theoretical counterparts, i.e., <span class="math inline">\(\widehat{\boldsymbol{\theta}}\)</span> is the solution
of the system
<span class="math display">\[\begin{equation}
\label{MomMeth}
\int_{x\in\mathbb{R}}x^jd{\hat F}_n(x)=
\int_{x\in\mathbb{R}}x^jdF_{\boldsymbol{\theta}}(x),\hspace{2mm}
j=1,2,\ldots,p.
\end{equation}\]</span>
Note that there is no guarantee that the solution of the system will
be in <span class="math inline">\(\Theta\)</span>.</p>
</div>
<div id="method-of-quantiles" class="section level4 hasAnchor" number="10.4.5.2">
<h4><span class="header-section-number">10.4.5.2</span> Method of Quantiles<a href="chap9.html#method-of-quantiles" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The method of quantiles, on the other hand, involves selecting a number of observed quantiles, <span class="math inline">\({\hat q}_{\pi_1}\)</span>, <span class="math inline">\({\hat q}_{\pi_2}\)</span>, <span class="math inline">\(\ldots\)</span>,
<span class="math inline">\({\hat q}_{\pi_p}\)</span>, say, obtained by
<span class="math inline">\({\hat F}_n^{-1}(\pi_i)={\hat q}_{\pi_i}\)</span>, <span class="math inline">\(i=1,2,\ldots,p\)</span>,
and then taking <span class="math inline">\(\widehat{\boldsymbol{\theta}}\)</span> as the solution
of the system
<span class="math display">\[\begin{equation}
\label{QuantMeth}
F_{\boldsymbol{\theta}}({\hat q}_{\pi_i})=\pi_i,\hspace{2mm}
i=1,2,\ldots,p.
\end{equation}\]</span></p>
<p>Of course, the method of moments and the method of quantiles can be combined
by requiring that equations <span class="math inline">\(\eqref{MomMeth}\)</span> be satisfied for
<span class="math inline">\(j=1,2,\ldots,\ell\)</span> and those of <span class="math inline">\(\eqref{QuantMeth}\)</span> for <span class="math inline">\(\ell+1,\ldots,p\)</span>.</p>
<p>Finally, it should be noted that “ad hoc” methods should be used with caution. Their undeniable advantage is to provide initial values for iterative algorithms that obtain
maximum likelihood estimates.</p>
</div>
<div id="minimum-distance-type-method" class="section level4 hasAnchor" number="10.4.5.3">
<h4><span class="header-section-number">10.4.5.3</span> Minimum Distance Type Method<a href="chap9.html#minimum-distance-type-method" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>This is another type of approach, based on probabilistic distances: this class of methods consists of choosing <span class="math inline">\({\boldsymbol{\theta}}\)</span> to
minimize a “distance” between <span class="math inline">\(F_{\boldsymbol{\theta}}\)</span> and <span class="math inline">\({\hat F}_n\)</span>. Here are
some interesting special cases in the grouped data context:</p>
<ol style="list-style-type: decimal">
<li>Cramér-Von Mises type method:
<span class="math display">\[
{\hat\theta}=\arg\min_{{\boldsymbol{\theta}}\in\Theta}
\sum_{j=1}^rw_j\left(F_{\boldsymbol{\theta}}(c_j)-{\hat F}_n(c_j)
\right)^2,
\]</span>
where the weights <span class="math inline">\(w_1,w_2,\ldots,w_r\)</span> are selected by the actuary
to emphasize certain regions where the quality
of the fit is crucial (this is equivalent to fitting the nonlinear function <span class="math inline">\(F_{\boldsymbol{\theta}}\)</span> to the data points <span class="math inline">\((c_j,{\hat F}_n(c_j))\)</span> by weighted least squares);</li>
<li><span class="math inline">\(\chi^2\)</span> type method:
<span class="math display">\[
\widehat{\boldsymbol{\theta}}=\arg\min_{{\boldsymbol{\theta}}\in\Theta}
\sum_{j=1}^r\frac{\left(F_{\boldsymbol{\theta}}(c_j)-F_{\boldsymbol{\theta}}(c_{j-1})
-{\hat F}_n(c_j)+{\hat F}_n(c_{j-1})\right)^2}
{F_{\boldsymbol{\theta}}(c_j)-F_{\boldsymbol{\theta}}(c_{j-1})}.
\]</span></li>
</ol>
<p>Most of the time, <span class="math inline">\(\widehat{\boldsymbol{\theta}}\)</span> can only be obtained through
an iterative algorithm (such as the simplex method). It is
strongly recommended to verify the solution proposed by the
numerical method by evaluating the objective function at a few
points in the vicinity of the solution. Furthermore, the actuary
should keep in mind that the proposed solution may correspond only to
a local minimum.</p>
</div>
</div>
</div>
<div id="SecPCA" class="section level2 hasAnchor" number="10.5">
<h2><span class="header-section-number">10.5</span> Data Analysis<a href="chap9.html#SecPCA" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="principle-3" class="section level3 hasAnchor" number="10.5.1">
<h3><span class="header-section-number">10.5.1</span> Principle<a href="chap9.html#principle-3" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Actuaries often have vast amounts of data to analyze. Before opting for a parametric model <span class="math inline">\(\mathcal{F}\)</span>, it is often useful
to analyze the data without making any assumptions about them.
There are several types of methods for data analysis (in multivariate statistics): factorial methods, which involve projecting the point cloud onto a
subspace while retaining as much information as possible, and classification methods, which attempt to group the points.</p>
<p>Among factorial methods, three groups of techniques are generally distinguished: Principal Component Analysis
(PCA, based on multiple quantitative variables, ideally continuous), Correspondence Analysis for binary data (CA, for two qualitative variables represented by a contingency table), and Multiple Correspondence Analysis (MCA, for more than two qualitative variables and no quantitative variables).
This section briefly recalls the principles of data analysis.
For more details, we refer the reader to <span class="citation">(<a href="#ref-lebart1995statistique" role="doc-biblioref">Lebart, Morineau, and Piron 1995</a>)</span>.</p>
</div>
<div id="principal-component-analysis-pca" class="section level3 hasAnchor" number="10.5.2">
<h3><span class="header-section-number">10.5.2</span> Principal Component Analysis (PCA)<a href="chap9.html#principal-component-analysis-pca" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="variables-and-individuals" class="section level4 hasAnchor" number="10.5.2.1">
<h4><span class="header-section-number">10.5.2.1</span> Variables and Individuals<a href="chap9.html#variables-and-individuals" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>PCA provides representations and reductions of the information contained in
large numerical data tables <span class="math inline">\(\boldsymbol{X}\)</span>.
The element <span class="math inline">\(x_{ij}\)</span> of the <span class="math inline">\(n \times p\)</span> matrix <span class="math inline">\({\boldsymbol{X}}\)</span> represents the numerical value of the
<span class="math inline">\(j\)</span>th variable for the <span class="math inline">\(i\)</span>th individual (<span class="math inline">\(j=1,2,\ldots, p\)</span>, and <span class="math inline">\(i=1,2,\ldots, n\)</span>).
The data matrix <span class="math inline">\({\boldsymbol{X}}\)</span> thus has <span class="math inline">\(np\)</span> elements, which is generally very large.</p>
<p>The natural space for statisticians to represent data is the Euclidean space <span class="math inline">\({\mathbb{R}}^p\)</span>, in which
the sample takes the form of a cloud of <span class="math inline">\(n\)</span> points (each point corresponding to an individual).
We call this the {}, and point <span class="math inline">\(i\)</span> is the <span class="math inline">\(p\)</span>-dimensional vector
<span class="math inline">\(\boldsymbol{x}_i^{\text{v}}\)</span> defined as the <span class="math inline">\(i\)</span>th row of <span class="math inline">\({\boldsymbol{X}}\)</span>, i.e.
<span class="math display">\[\begin{equation}
\boldsymbol{x}_i^{\text{v}}=(x_{i1}, \ldots, x_{ip})^\top, \quad i=1,\ldots, n. \label{ACP1.5}
\end{equation}\]</span>
This provides a first cloud of points in <span class="math inline">\({\mathbb{R}}^p\)</span>, called the cloud of individuals or cloud of row points.
Each of the <span class="math inline">\(n\)</span> points in this cloud corresponds to a row of <span class="math inline">\(\boldsymbol{X}\)</span> and thus summarizes the measurements of the <span class="math inline">\(p\)</span> variables
for one of the <span class="math inline">\(n\)</span> individuals.</p>
<p>One of the peculiarities of data analysis is to consider an additional space
called the {}, in which there is a cloud of <span class="math inline">\(p\)</span> points representing the column vectors
of <span class="math inline">\({\boldsymbol{X}}\)</span>, i.e., the
<span class="math display">\[\begin{equation}
\boldsymbol{x}_j^{\text{o}}=(x_{1j},\ldots, x_{nj})^\top, \quad j=1,\ldots, p. \label{ACP1.6}
\end{equation}\]</span>
This provides a second cloud of points, in <span class="math inline">\({\mathbb{R}}^n\)</span> this time, called the cloud of variables or cloud of column points.
Each of the <span class="math inline">\(p\)</span> points in this cloud corresponds to a column of <span class="math inline">\(\boldsymbol{X}\)</span>, and thus, it summarizes the measurements of the same variable
taken for all <span class="math inline">\(n\)</span> individuals.</p>
<p>The interpretations of each of these spaces are simple: in the space of variables <span class="math inline">\({\mathbb{R}}^p\)</span>, <span class="math inline">\(\boldsymbol{x}_i^{\text{v}}\)</span> represents the <span class="math inline">\(p\)</span> characteristics or variables measured on the <span class="math inline">\(i\)</span>th individual, and in the space of observations <span class="math inline">\({\mathbb{R}}^n\)</span>, <span class="math inline">\(\boldsymbol{x}_j^{\text{o}}\)</span> represents the values taken by the <span class="math inline">\(j\)</span>th variable over all <span class="math inline">\(n\)</span> individuals.
The points in the space of variables <span class="math inline">\({\mathbb{R}}^p\)</span> thus represent individuals, and those in the space of observations <span class="math inline">\({\mathbb{R}}^n\)</span> represent variables.</p>
<p>The geometric proximities between row points and between column points actually reflect statistical associations, either between individuals or between variables. Thus,
proximity between two individual points in <span class="math inline">\({\mathbb{R}}^p\)</span> means that these two individuals have a similar behavior with respect to
all <span class="math inline">\(p\)</span> variables; proximity between two variable points in <span class="math inline">\({\mathbb{R}}^n\)</span> means that the <span class="math inline">\(n\)</span> individuals have a similar behavior with respect to these two variables.</p>
</div>
<div id="adjusting-the-cloud-of-individuals-in-the-space-of-variables" class="section level4 hasAnchor" number="10.5.2.2">
<h4><span class="header-section-number">10.5.2.2</span> Adjusting the Cloud of Individuals in the Space of Variables<a href="chap9.html#adjusting-the-cloud-of-individuals-in-the-space-of-variables" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>A simple way to visually understand the shape of a point cloud is to project it onto lines, or even better, onto planes, while minimizing the distortions that the projection implies.</p>
<p>Given the cloud of <span class="math inline">\(n\)</span> points in <span class="math inline">\({\mathbb{R}}^p\)</span>, we seek the line passing through the origin and determined by the unit direction vector
<span class="math inline">\(\boldsymbol{u}\)</span>, which provides the best fit in the least squares sense, i.e., minimizing
<span class="math inline">\(\sum^n_{i=1} d^2_i(\boldsymbol{u})\)</span>, where <span class="math inline">\(d_i(\boldsymbol{u})\)</span> represents the distance from point <span class="math inline">\(i\)</span> to the line determined by the vector <span class="math inline">\(\boldsymbol{u}\)</span>.
If <span class="math inline">\(p_i(\boldsymbol{u})\)</span> denotes the projection of the vector <span class="math inline">\(\boldsymbol{x}_i^{\text{v}}\)</span> onto the line determined by the vector <span class="math inline">\(\boldsymbol{u}\)</span>,
from the Pythagorean theorem, it follows that it is equivalent to maximizing
<span class="math display">\[\begin{equation}
\max_{\boldsymbol{u}} \sum^n_{i=1} p^2_i (\boldsymbol{u}) \label{ACP1.7}
\end{equation}\]</span>
over all normalized vectors.
By expressing <span class="math inline">\(p_i(\boldsymbol{u})\)</span> using the dot product, we have
<span class="math inline">\(p_i(\boldsymbol{u})=\boldsymbol{u}^\top\boldsymbol{x}_i^{\text{v}}\)</span>. Using the Euclidean norm notation <span class="math inline">\(||\cdot||\)</span>, <span class="math inline">\(\eqref{ACP1.7}\)</span> is equivalent to
<span class="math display">\[\begin{eqnarray*}
\max_{\boldsymbol{u}} \sum^n_{i=1} ||\boldsymbol{u}^\top \boldsymbol{x}_i^{\text{v}}||^2&amp;\Leftrightarrow&amp;\max_{\boldsymbol{u}} \sum^n_{i=1} \boldsymbol{u}^\top \boldsymbol{x}_i^{\text{v}} \boldsymbol{x}_i^{\text{v}t} \boldsymbol{u}\\
&amp;\Leftrightarrow&amp;\max_{\boldsymbol{u}} \boldsymbol{u}^\top \left(\sum^n_{i=1} \boldsymbol{x}_i^{\text{v}} \boldsymbol{x}_i^{\text{v}t}\right) \boldsymbol{u}\\
&amp;\Leftrightarrow&amp;\max_{\boldsymbol{u}} \boldsymbol{u}^\top {\boldsymbol{X}}^\top{\boldsymbol{X}} \; \boldsymbol{u}
\end{eqnarray*}\]</span>
subject to the constraint <span class="math inline">\(\boldsymbol{u}^\top\boldsymbol{u}=1\)</span>.
The problem to solve is thus stated as follows:
<span class="math display">\[\begin{equation}
\label{MaxP}
\max_{\boldsymbol{u}}  \boldsymbol{u}^\top{\boldsymbol{X}}^\top{\boldsymbol{X}} \; \boldsymbol{u}\text{ subject to the constraint }
\boldsymbol{u}^\top\boldsymbol{u}=1.
\end{equation}\]</span></p>
<p>This is a classic problem in differential calculus that is solved by introducing the Lagrangian function
<span class="math display">\[
\Psi (\boldsymbol{u}; \lambda)=\boldsymbol{u}^\top \; {\boldsymbol{X}}^\top{\boldsymbol{X}} \; \boldsymbol{u}- \lambda (\boldsymbol{u}^\top\boldsymbol{u}-1)
\]</span>
which is a function of <span class="math inline">\((p+1)\)</span> variables, the <span class="math inline">\(p\)</span> coordinates of <span class="math inline">\(\boldsymbol{u}\)</span> and <span class="math inline">\(\lambda\)</span>.
By setting the partial derivative of <span class="math inline">\(\Psi\)</span> with respect to <span class="math inline">\(\lambda\)</span> to zero, we obviously obtain the constraint <span class="math inline">\(\boldsymbol{u}^\top\boldsymbol{u}=1\)</span>;
setting the partial derivatives of <span class="math inline">\(\Psi\)</span> with respect to the coordinates of <span class="math inline">\(\boldsymbol{u}\)</span> to zero leads, after some manipulation of matrix calculations, to the system
<span class="math display">\[\begin{equation}
{\boldsymbol{X}}^\top{\boldsymbol{X}} \; \boldsymbol{u}- \lambda \boldsymbol{u}=0 \Leftrightarrow {\boldsymbol{X}}^\top{\boldsymbol{X}}\boldsymbol{u}=\lambda \boldsymbol{u}\label{ACP1.14}
\end{equation}\]</span>
where it can be seen that solving (<span class="math inline">\(\ref{MaxP}\)</span>) is identical to finding the eigenvalues and eigenvectors of the symmetric matrix <span class="math inline">\({\boldsymbol{X}}^\top{\boldsymbol{X}}\)</span> of dimension <span class="math inline">\(p\times p\)</span>. To determine which eigenpair is appropriate, it is sufficient to pre-multiply both sides of equation (<span class="math inline">\(\ref{ACP1.14}\)</span>) by <span class="math inline">\(\boldsymbol{u}^\top\)</span>,
<span class="math display">\[
\boldsymbol{u}^\top\; {\boldsymbol{X}}^\top{\boldsymbol{X}} \; \boldsymbol{u}= \lambda \boldsymbol{u}^\top\boldsymbol{u}=\lambda.
\]</span>
Returning to (<span class="math inline">\(\ref{MaxP}\)</span>), we can conclude that the appropriate eigenpair corresponds to the largest eigenvalue of <span class="math inline">\({\boldsymbol{X}}^\top{\boldsymbol{X}}\)</span>.</p>
<p>Linear algebra teaches us that all eigenvalues of <span class="math inline">\({\boldsymbol{X}}^\top{\boldsymbol{X}}\)</span> are non-negative, and the number of strictly positive eigenvalues is given by the rank <span class="math inline">\(r\)</span> of <span class="math inline">\({\boldsymbol{X}}\)</span> (with <span class="math inline">\(r \leq \min \{n,p\}\)</span>).
Furthermore, if <span class="math inline">\(\lambda\)</span> has multiplicity order <span class="math inline">\(k\)</span>, there exist <span class="math inline">\(k\)</span> orthogonal eigenvectors associated with that eigenvalue;
finally, the set of <span class="math inline">\(p\)</span> eigenvectors of <span class="math inline">\({\boldsymbol{X}}^\top{\boldsymbol{X}}\)</span> is orthogonal.</p>
<p>Let <span class="math inline">\(\lambda_1, \ldots, \lambda_r\)</span> be the nonzero eigenvalues of <span class="math inline">\({\boldsymbol{X}}^\top{\boldsymbol{X}}\)</span> arranged in descending order; in most applications, they are distinct, i.e., <span class="math inline">\(\lambda_1 &gt; \lambda_2 &gt; \ldots &gt; \lambda_r\)</span>.
Let <span class="math inline">\(\boldsymbol{u}_1, \boldsymbol{u}_2, \ldots, \boldsymbol{u}_r\)</span> be the normalized eigenvectors corresponding to these <span class="math inline">\(r\)</span> eigenvalues.
Thus, <span class="math inline">\(\boldsymbol{u}_1\)</span> determines the sought-after line, the solution to problem <span class="math inline">\(\eqref{MaxP}\)</span>, called the first factorial axis and denoted by <span class="math inline">\(F_1\)</span>.</p>
<p>If one wants to fit the cloud of <span class="math inline">\(n\)</span> points in <span class="math inline">\({\mathbb{R}}^p\)</span> with a hyperplane, optimized in the least squares sense, the same
formalism as that used for the line shows that the hyperplane sought is the one spanned by the eigenvectors
<span class="math inline">\(\boldsymbol{u}_1\)</span> and <span class="math inline">\(\boldsymbol{u}_2\)</span> of <span class="math inline">\({\boldsymbol{X}}^\top{\boldsymbol{X}}\)</span> corresponding to the eigenvalues <span class="math inline">\(\lambda_1\)</span> and <span class="math inline">\(\lambda_2\)</span>; it is
thus spanned by the lines determined by <span class="math inline">\(\boldsymbol{u}_1\)</span> and <span class="math inline">\(\boldsymbol{u}_2\)</span>
(i.e., the first two factorial axes <span class="math inline">\(F_1\)</span> and <span class="math inline">\(F_2\)</span>).</p>
<div class="example">
<p><span id="exm:unlabeled-div-6" class="example"><strong>Example 10.4  </strong></span>Figure <span class="math inline">\(\ref{ACM-exp}\)</span> shows a projection, emphasizing the large dispersion of the cloud, and a one for which the projection does not provide much information.</p>
</div>
</div>
<div id="adjustment-of-the-cloud-of-variables-in-the-space-of-observations" class="section level4 hasAnchor" number="10.5.2.3">
<h4><span class="header-section-number">10.5.2.3</span> Adjustment of the Cloud of Variables in the Space of Observations<a href="chap9.html#adjustment-of-the-cloud-of-variables-in-the-space-of-observations" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Now, let’s consider the space of observations <span class="math inline">\({\mathbb{R}}^n\)</span>, where the table <span class="math inline">\(\boldsymbol{X}\)</span> is represented by a cloud of points-variables,
with the <span class="math inline">\(n\)</span> coordinates representing the columns of <span class="math inline">\(\boldsymbol{X}\)</span>.
By analogy, the best fit by the least squares method of the cloud of <span class="math inline">\(p\)</span> points by a line determined by the normalized vector <span class="math inline">\(\boldsymbol{v}\)</span>
leads to the problem:
<span class="math display">\[\begin{eqnarray*}
\max_{\boldsymbol{v}} \sum^p_{j=1} ||\boldsymbol{v}^\top \boldsymbol{x}_j^{\text{o}}||^2 &amp;\Leftrightarrow&amp;\max_{\boldsymbol{v}}\boldsymbol{v}^\top
\left(\sum^p_{j=1} \boldsymbol{x}_j^{\text{o}} \boldsymbol{x}_j^{\text{o}t} \right)\boldsymbol{v}\\
&amp;\Leftrightarrow&amp;\max_{\boldsymbol{v}}\boldsymbol{v}^\top {\boldsymbol{X}}{\boldsymbol{X}}^\top \boldsymbol{v}
\end{eqnarray*}\]</span>
subject to the constraint <span class="math inline">\(\boldsymbol{v}^\top\boldsymbol{v}=1\)</span>,
where <span class="math inline">\({\boldsymbol{X}}{\boldsymbol{X}}^\top\)</span> is, this time, a symmetric matrix of dimension <span class="math inline">\(n\times n\)</span>.</p>
<p>By analogy with the problem <span class="math inline">\(\eqref{MaxP}\)</span>, it can be found that <span class="math inline">\(\boldsymbol{v}\)</span> is a solution of
<span class="math display">\[\begin{equation}
{\boldsymbol{X}}{\boldsymbol{X}}^\top\boldsymbol{v}=\mu \boldsymbol{v}\label{ACP1.20}
\end{equation}\]</span>
In other words, <span class="math inline">\(\boldsymbol{v}\)</span> is a normalized eigenvector of <span class="math inline">\({\boldsymbol{X}}{\boldsymbol{X}}^\top\)</span> corresponding to the largest eigenvalue <span class="math inline">\(\mu_1\)</span> of <span class="math inline">\({\boldsymbol{X}}{\boldsymbol{X}}^\top\)</span>.</p>
<div class="remark">
<p><span id="unlabeled-div-7" class="remark"><em>Remark</em>. </span>It is easy to see that any eigenvalue <span class="math inline">\(\lambda\)</span> of <span class="math inline">\({\boldsymbol{X}}^\top{\boldsymbol{X}}\)</span> is also an eigenvalue of <span class="math inline">\({\boldsymbol{X}}{\boldsymbol{X}}^\top\)</span>, and vice versa.
For example, suppose <span class="math inline">\({\boldsymbol{X}}^\top{\boldsymbol{X}} \boldsymbol{u}=\lambda \boldsymbol{u}\)</span>.
By pre-multiplying by <span class="math inline">\({\boldsymbol{X}}\)</span>, we have <span class="math inline">\(({\boldsymbol{X}}{\boldsymbol{X}}^\top){\boldsymbol{X}}\boldsymbol{u}= \lambda {\boldsymbol{X}} \boldsymbol{u}\)</span>,
which proves that <span class="math inline">\({\boldsymbol{X}}\boldsymbol{u}\)</span> is an eigenvector of <span class="math inline">\({\boldsymbol{X}}{\boldsymbol{X}}^\top\)</span> corresponding to the eigenvalue <span class="math inline">\(\lambda\)</span>.
Conversely, if <span class="math inline">\({\boldsymbol{X}}{\boldsymbol{X}}^\top\boldsymbol{v}=\mu \boldsymbol{v}\)</span>, by pre-multiplying by <span class="math inline">\({\boldsymbol{X}}^\top\)</span>, we get
<span class="math inline">\(({\boldsymbol{X}}^\top{\boldsymbol{X}}){\boldsymbol{X}}^\top\boldsymbol{v}= \mu {\boldsymbol{X}}^\top \boldsymbol{v}\)</span>, from which we see that <span class="math inline">\({\boldsymbol{X}}^\top \boldsymbol{v}\)</span> is an eigenvalue of <span class="math inline">\({\boldsymbol{X}}^\top{\boldsymbol{X}}\)</span>
corresponding to the eigenvalue <span class="math inline">\(\mu\)</span>. This implies that the <span class="math inline">\(n\)</span> eigenvalues (in descending order) of <span class="math inline">\({\boldsymbol{X}}{\boldsymbol{X}}^\top\)</span> are
<span class="math inline">\(\lambda_1, \ldots, \lambda_r\)</span>, with the remaining <span class="math inline">\(n-r\)</span> being zero.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-8" class="remark"><em>Remark</em> (Additional Elements). </span>The variables and individuals used to construct the optimal subspaces for representing proximities are called active elements.
However, there’s nothing preventing the actuary from placing additional elements (row or column points) in these subspaces
that did not participate in the analysis. These are called supplementary or illustrative elements.</p>
<p>Supplementary elements come into play after the analysis to enrich the interpretation of the factors.
They do not participate in the adjustment calculations and do not contribute to the formation of the factor axes.
They are positioned in either the cloud of individuals or variables by calculating their coordinates on the factor axes post-analysis.
You may also want to represent an additional nominal variable. To do this, you create as many groups of individuals
as there are levels in this variable and calculate their centroids. These centroid points are then positioned among the individual
points as supplementary elements.</p>
</div>
</div>
<div id="application-to-index-construction" class="section level4 hasAnchor" number="10.5.2.4">
<h4><span class="header-section-number">10.5.2.4</span> Application to Index Construction<a href="chap9.html#application-to-index-construction" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Significant amounts of information are available, especially from national statistical institutes
(INSEE in France, INS in Belgium), law enforcement agencies, national banks, as well as private polling or marketing organizations.
Incorporating such data into a pricing model can sometimes lead to significant improvements in the accuracy of pure premium calculations.</p>
<p>The following examples illustrate how the incorporation of such statistics into the pricing scheme can lead to a better risk assessment:</p>
<p>1.In home theft insurance, information about the neighborhood of the residence can be very useful. The type of housing (isolated villas, single-family houses, apartment buildings, etc.), the socio-economic profile of residents, and more can influence the covered risk.
2. In car theft insurance, actuaries can obtain statistics from police services to determine the most stolen car models and the most vulnerable areas.
3. In automobile insurance, it is useful to leverage the technical characteristics of the vehicle (available from manufacturers), especially for assessing their sportiness (and, therefore, less maneuverable). You can refer to <span class="citation">(<a href="#ref-ingenbleek1988sports" role="doc-biblioref">Ingenbleek and Lemaire 1988</a>)</span> for an application of PCA to determine an index reflecting the mechanical performance of automobiles.</p>
<p>However, it is not sufficient to incorporate such information directly into the pricing models that we will develop later in this chapter. The characteristics of vehicles or statistical sectors are contained in several hundred, or even thousands, of variables, whereas the characteristics of the insured and the covered risk are often summarized in a few dozen variables. Before incorporating them into a pricing model, the information must be summarized into relevant indices obtained using techniques like PCA.</p>
</div>
</div>
<div id="multiple-correspondence-analysis-mca" class="section level3 hasAnchor" number="10.5.3">
<h3><span class="header-section-number">10.5.3</span> Multiple Correspondence Analysis (MCA)<a href="chap9.html#multiple-correspondence-analysis-mca" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="descriptive-analysis-of-large-qualitative-data-sets" class="section level4 hasAnchor" number="10.5.3.1">
<h4><span class="header-section-number">10.5.3.1</span> Descriptive Analysis of Large Qualitative Data Sets<a href="chap9.html#descriptive-analysis-of-large-qualitative-data-sets" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Multiple Correspondence Analysis (MCA) is a powerful technique for the description of large qualitative data sets.
MCA can be seen as the analogue of PCA for qualitative variables.
Like PCA, the results are presented graphically (represented in factorial planes).</p>
<p>Here, we consider <span class="math inline">\(N\)</span> individuals described by <span class="math inline">\(Q\)</span> qualitative variables with <span class="math inline">\(J_1\)</span>, <span class="math inline">\(J_2\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(J_Q\)</span> categories. We denote <span class="math inline">\(J=\sum_{q=1}^Q J_q\)</span> as the total number of categories across all variables.</p>
</div>
<div id="complete-disjunctive-table" class="section level4 hasAnchor" number="10.5.3.2">
<h4><span class="header-section-number">10.5.3.2</span> Complete Disjunctive Table<a href="chap9.html#complete-disjunctive-table" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The starting table is a cross-tabulation of qualitative variables and individuals. Each individual is described by the categories of the <span class="math inline">\(Q\)</span> variables to which they belong. These raw data are therefore presented in the form of a table with <span class="math inline">\(N\)</span> rows and <span class="math inline">\(Q\)</span> columns.</p>
<div class="remark">
<p><span id="unlabeled-div-9" class="remark"><em>Remark</em>. </span>In practice, both categorical and quantitative variables are often available simultaneously. From now on, we assume that the quantitative variables have been made categorical. To do this, we distinguish between:</p>
<ol style="list-style-type: decimal">
<li>Variables that only take a few integer values (e.g., the number of dependents), which are made categorical by grouping values into meaningful classes (e.g., 0 dependents, 1-2 dependents, and 3 or more dependents).</li>
<li>Continuous variables (e.g., vehicle horsepower), which are made categorical by choosing quantiles as class boundaries (a partition into 4 or 5 classes is usually sufficient in practice).</li>
</ol>
</div>
<p>Each variable <span class="math inline">\(q\)</span> corresponds to a table <span class="math inline">\(\boldsymbol{Z}_q\)</span> with <span class="math inline">\(N\)</span> rows and <span class="math inline">\(J_q\)</span> columns. This table is such that its <span class="math inline">\(i\)</span>th row contains <span class="math inline">\(J_q-1\)</span> zeros and one 1 (in the column corresponding to the category of variable <span class="math inline">\(q\)</span> for individual <span class="math inline">\(i\)</span>).</p>
<p>The table <span class="math inline">\(\boldsymbol{Z}\)</span> with <span class="math inline">\(N\)</span> rows and <span class="math inline">\(J\)</span> columns describing the <span class="math inline">\(Q\)</span> characteristics of the <span class="math inline">\(N\)</span> individuals using binary coding is obtained by juxtaposing <span class="math inline">\(\boldsymbol{Z}_1,\boldsymbol{Z}_2,\ldots,\boldsymbol{Z}_Q\)</span>. <span class="math inline">\(\boldsymbol{Z}\)</span> is called the complete disjunctive table.</p>
</div>
<div id="burts-table" class="section level4 hasAnchor" number="10.5.3.3">
<h4><span class="header-section-number">10.5.3.3</span> Burt’s Table<a href="chap9.html#burts-table" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The complete disjunctive table <span class="math inline">\(\boldsymbol{Z}\)</span> is then transformed into a multiple contingency table <span class="math inline">\(\boldsymbol{B}\)</span>
(also known as Burt’s table) to make it usable for MCA. This table is obtained from <span class="math inline">\(\boldsymbol{Z}\)</span> through
<span class="math inline">\(\boldsymbol{B}=\boldsymbol{Z}^\top\boldsymbol{Z}\)</span>; <span class="math inline">\(\boldsymbol{B}\)</span> appears as a juxtaposition of contingency tables.
More precisely, this table is formed by the juxtaposition of <span class="math inline">\(Q^2\)</span> blocks where we distinguish between:</p>
<ol style="list-style-type: decimal">
<li>The block <span class="math inline">\(\boldsymbol{Z}_q^\top\boldsymbol{Z}_{q&#39;}\)</span> indexed by <span class="math inline">\((q,q&#39;)\)</span> of size <span class="math inline">\(J_q\times J_{q&#39;}\)</span>, which is nothing but the contingency table crossing the modalities of variables <span class="math inline">\(q\)</span> and <span class="math inline">\(q&#39;\)</span>.</li>
<li>The <span class="math inline">\(q\)</span>-th square block <span class="math inline">\(\boldsymbol{Z}_q^\top\boldsymbol{Z}_q\)</span>, which appears as a diagonal matrix of size <span class="math inline">\(J_q\times J_q\)</span>
(since two modalities of the same variable cannot be chosen simultaneously). The diagonal terms are the frequencies of the <span class="math inline">\(J_q\)</span> modalities of variable <span class="math inline">\(q\)</span>.</li>
</ol>
<p>MCA is the analysis of correspondences in a complete disjunctive table or, equivalently, in the corresponding Burt’s table.</p>
</div>
<div id="binary-correspondence-analysis" class="section level4 hasAnchor" number="10.5.3.4">
<h4><span class="header-section-number">10.5.3.4</span> Binary Correspondence Analysis<a href="chap9.html#binary-correspondence-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Since the Burt’s table can be seen as a juxtaposition of contingency tables crossing variables pairwise, the goal is to be able to analyze a contingency table. This is the subject of Binary Correspondence Analysis, or BCA.</p>
<p>Let’s assume that observations have been made on a set of <span class="math inline">\(N\)</span> individuals for two characters denoted <span class="math inline">\(I\)</span> and <span class="math inline">\(J\)</span>, taking <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span> possible values, respectively. These characters can be either quantitative (in which case grouping has been done into <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span> groups) or qualitative.</p>
<p>The result of observing the <span class="math inline">\(N\)</span> individuals can be represented as a table with <span class="math inline">\(n\)</span> rows and <span class="math inline">\(p\)</span> columns crossing the two characters in such a way that the intersection of the <span class="math inline">\(i\)</span>th row and <span class="math inline">\(j\)</span>th column contains the total number of times <span class="math inline">\(N_{ij}\)</span> that both the value <span class="math inline">\(i\)</span> of <span class="math inline">\(I\)</span> and the value <span class="math inline">\(j\)</span> of <span class="math inline">\(J\)</span> were observed. This table is called a “contingency table.”</p>
<p>By dividing all <span class="math inline">\(N_{ij}\)</span> by <span class="math inline">\(N\)</span>, we obtain the relative frequencies <span class="math inline">\(f_{ij}\)</span> defined as:</p>
<p><span class="math display">\[\begin{equation}
f_{ij}=\frac{N_{ij}}{N}; \label{1.69}
\end{equation}\]</span></p>
<p>The table of <span class="math inline">\(f_{ij}\)</span> is called the “frequency table.” It is supplemented by an additional row and column providing the marginal frequencies associated with the <span class="math inline">\(n\)</span> levels of <span class="math inline">\(I\)</span> and the <span class="math inline">\(p\)</span> levels of <span class="math inline">\(J\)</span>:</p>
<p><span class="math display">\[\begin{equation}
f_{i \bullet}=\sum_j f_{ij},\quad i=1,\ldots,n, \label{1.70}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
f_{\bullet j}=\sum_i f_{ij}, \quad j=1,\ldots, p. \label{1.71}
\end{equation}\]</span></p>
<p>BCA essentially involves performing PCA on the matrix <span class="math inline">\(\boldsymbol{X}\)</span>, where the element <span class="math inline">\(x_{ij}\)</span> is defined as:
<span class="math display">\[
x_{ij}=\frac{f_{ij}}{f_{i\bullet}\sqrt{f_{\bullet j}}}.
\]</span></p>
</div>
</div>
</div>
<div id="Sec923MC" class="section level2 hasAnchor" number="10.6">
<h2><span class="header-section-number">10.6</span> Scoring Methods<a href="chap9.html#Sec923MC" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="classification-methods" class="section level3 hasAnchor" number="10.6.1">
<h3><span class="header-section-number">10.6.1</span> Classification Methods<a href="chap9.html#classification-methods" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Linear regression, which will be discussed in Section <span class="math inline">\(\ref{Sec93MLMC}\)</span>, is used to predict a continuous variable based on explanatory variables, involving a linear combination of these explanatory variables. However, this method cannot model a dichotomous variable of the type - client (an indicator of whether or not a claim occurred during the year, for example).</p>
<p>In a simplified view, we aim to model such a dichotomous variable (denoted as <span class="math inline">\(Y\)</span>) using two quantitative variables, <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>. In a first approach, we can treat the qualitative variable as a quantitative variable (taking two values, <span class="math inline">\(0\)</span> if the person had no claims and <span class="math inline">\(1\)</span> if they had at least one) and use linear regression with the other two variables. In this case, we use the model:</p>
<p><span class="math display">\[
Y=\beta_0+\beta_1X_1+\beta_2X_2+\varepsilon,
\]</span></p>
<p>where <span class="math inline">\(\varepsilon\)</span> represents the error term.</p>
<p>This technique allows us to separate the space <span class="math inline">\((X_1,X_2)\)</span> into two parts using a hyperplane (in this case, a line). More formally, we aim to construct a function <span class="math inline">\(y=f(x_1,x_2)\)</span> such that the set of points <span class="math inline">\(\{(x_1,x_2)\in\mathbb{R}^2|f(x_1,x_2)&lt;1/2\}\)</span> corresponds to non-claimants (those for whom <span class="math inline">\(Y=0\)</span>), and <span class="math inline">\(\{(x_1,x_2)\in\mathbb{R}^2|f(x_1,x_2)&gt;1/2\}\)</span> corresponds to claimants (those for whom <span class="math inline">\(Y=1\)</span>). The boundary is defined as <span class="math inline">\(\{(x_1,x_2)\in\mathbb{R}^2|f(x_1,x_2)=1/2\}\)</span>. Note that the choice of the factor <span class="math inline">\(1/2\)</span> is arbitrary and corresponds to the natural boundary point separating <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>. Choosing a value closer to <span class="math inline">\(1\)</span> allows for better isolation of individuals with <span class="math inline">\(Y=1\)</span>, but many claimants may then be outside the correct region of the space. Linear regression corresponds to the case where <span class="math inline">\(f\)</span> is an affine function, and we separate the space using the line:</p>
<p><span class="math display">\[
f(x_1,x_2) = \widehat{\beta}_0+\widehat{\beta}_1 x_1+\widehat{\beta}_2 x_2 = 1/2,
\]</span></p>
<p>where <span class="math inline">\(\widehat{\beta}_i\)</span> are the estimators obtained by least squares. More generally, we can consider a curvilinear regression by regressing <span class="math inline">\(Y\)</span> not only on <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> but also on <span class="math inline">\(X_1\cdot X_2\)</span>, <span class="math inline">\(X_1^2\)</span>, and <span class="math inline">\(X_2^2\)</span>. In this case, the separation is given by:</p>
<p><span class="math display">\[
f(x_1,x_2) = \widehat{\beta}_0+\widehat{\beta}_1 x_1+\widehat{\beta}_2 x_2 +\widehat{\beta}_{1,1} x_1^2+\widehat{\beta}_{1,2} x_1\cdot x_2+\widehat{\beta}_{2,2} x_2^2= 1/2,
\]</span></p>
<p>Figure <span class="math inline">\(\ref{classif-1}\)</span> illustrates these two methods, where points with <span class="math inline">\(Y=0\)</span> are represented in white, and points with <span class="math inline">\(Y=1\)</span> are in black.</p>
<p>Another method that can be considered is the “nearest neighbors” method. For a given point <span class="math inline">\((X_1,X_2)\)</span>, we calculate the average value of <span class="math inline">\(Y\)</span> within a neighborhood and then round it to <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span>. The result is shown in Figure <span class="math inline">\(\ref{classif-2}\)</span>.</p>
<p>More formally, we choose a distance metric in the space of explanatory variables (<span class="math inline">\(X_1,X_2\)</span>). The Euclidean distance is a common choice, where the distance between two individuals, say <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>, is given by:</p>
<p><span class="math display">\[
d(X^i,X^j)=\sqrt{(X_1^i-X_1^j)^2+(X_2^i-X_2^j)^2}.
\]</span></p>
<p>For a point <span class="math inline">\((x_1,x_2)\)</span> in the space, the classification function <span class="math inline">\(f(x_1,x_2)\)</span> is the average of <span class="math inline">\(Y\)</span> obtained from the <span class="math inline">\(k\)</span> nearest neighbors of point <span class="math inline">\((x_1,x_2)\)</span> within the sample.</p>
<p>Therefore, if two customer classes exist within the population (the and the ), based on information about the individual (corresponding to the values of the rating variables <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>), the individual will be classified in the region of clients (lower left part in the plots, dominated by mostly white points) or in the region of clients (upper right part).</p>
</div>
<div id="definition-of-a-score" class="section level3 hasAnchor" number="10.6.2">
<h3><span class="header-section-number">10.6.2</span> Definition of a Score<a href="chap9.html#definition-of-a-score" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A score corresponds to a ranking of individuals based on their characteristics. These techniques are used by banks for credit approval (the score reflecting the probability of not being able to repay a loan). Insurers can also use these techniques by modeling the probability of having an accident: the lower this probability, the better the client.</p>
<p>As explained in Volume I, the production cycle is reversed in insurance. Indeed, insurers promise benefits in case of a claim, while premiums are set . Insurers must, therefore, carefully consider their acceptance policy. Once the insurance proposal is completed, the company would like to predict whether the insured will report claims or not, given their observable characteristics. Several methodologies are possible: file inspection, interviews, additional information searches, etc. However, due to costs, the number of cases to be studied, and the level of subjectivity introduced in the case-by-case examination, statistical classification methods are increasingly used for mass risks, as they are less costly, faster, and more systematic.</p>
<div class="remark">
<p><span id="unlabeled-div-10" class="remark"><em>Remark</em>. </span>Although the rest of this section will be devoted to the acceptance problem, the methods described here can be applied to solve many other problems that insurance companies face, including the detection of policyholders who are at risk of canceling their policies at the next renewal, for example. This can be done by analyzing the file containing the portfolio policies with the dependent variable being the indicator of the event “policy <span class="math inline">\(i\)</span> leaves the portfolio at the end of the period.” Make sure to include crucial information in this file, such as the number of years the policy has been in the portfolio, claims history, the number of modifications made to the contract, etc.</p>
</div>
<p>Let’s define the indicator variable <span class="math inline">\(Y\)</span>, taking values <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span> depending on whether the risk is good or not (typically, a good risk is an insured who reports no claims over the period). As with classification methods, the decision rule is based on a set of explanatory variables <span class="math inline">\(\mathbf{X}\)</span>: we consider the insured as a risk if <span class="math inline">\(\mathbf{X} \in \mathcal{A}\)</span>, and a risk otherwise, where <span class="math inline">\(\mathcal{A}\)</span> represents an acceptability domain. Formally, ranking prospective policyholders into acceptance and rejection amounts to choosing a partition of <span class="math inline">\({\mathbb{R}}^p\)</span> into an acceptance zone <span class="math inline">\(\mathcal{A}\)</span> and a rejection zone <span class="math inline">\(\overline{\mathcal{A}}={\mathbb{R}}^p\setminus \mathcal{A}\)</span>. Thus, for an insured whose characteristics are summarized in the vector <span class="math inline">\(\mathbf{x}\)</span>:</p>
<p><span class="math display">\[\begin{eqnarray*}
\mathbf{x} \in \mathcal{A} &amp;\Rightarrow&amp; \text{acceptance} \\
\mathbf{x} \in \overline{\mathcal{A}} &amp;\Rightarrow&amp; \text{rejection}.
\end{eqnarray*}\]</span></p>
<p>The simplest partition is the one generated by a hyperplane, but much more complex sets can be imagined. So, there are as many classifiers as partitions of <span class="math inline">\({\mathbb{R}}^p\)</span> into <span class="math inline">\(\mathcal{A}\)</span> and <span class="math inline">\(\overline{\mathcal{A}}\)</span>.</p>
<p>Since qualitative variables can always be replaced by numerical codes as described above, <span class="math inline">\(\mathbf{X}\)</span> takes values in <span class="math inline">\({\mathbb{R}}^p\)</span>. We will start by assuming that <span class="math inline">\(\mathbf{X}\)</span> is continuous, and later we will see how to handle cases where some components of <span class="math inline">\(\mathbf{X}\)</span> are discrete or categorical.</p>
</div>
<div id="principle-of-scoring" class="section level3 hasAnchor" number="10.6.3">
<h3><span class="header-section-number">10.6.3</span> Principle of Scoring<a href="chap9.html#principle-of-scoring" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The canonical score is the score that classifies individuals based on the probability of being a good client or not. It is the function that associates the tariff variables <span class="math inline">\(\mathbf{X}\)</span> with the score <span class="math inline">\(S^*\)</span> defined as:
<span class="math display">\[
S^*(\mathbf{x}) = \Pr[Y=1|\mathbf{X}=\mathbf{x}].
\]</span></p>
<p>We can then define two classes: the and the risks. The good risks correspond to individuals who have obtained a score lower than a threshold <span class="math inline">\(s\)</span> - set . It should be noted that with <span class="math inline">\(s\)</span> fixed, <span class="math inline">\(\Pr[S\leq s]\)</span> is the proportion of individuals retained, and <span class="math inline">\(\Pr[Y=0|S\leq s]\)</span> represents the proportion of good individuals among those retained.</p>
<p>In general, let’s denote $p_{1}=$ and <span class="math inline">\(p_{0}=1-p_{1}\)</span> as the unknown marginal probabilities (but which can be estimated from historical data). Assuming that the covariates are continuous and follow a density <span class="math inline">\(f(\mathbf{x})\)</span>, we can also denote <span class="math inline">\(f_{0}\)</span> and <span class="math inline">\(f_{1}\)</span> as the conditional densities:
<span class="math display">\[
f_{j}(\mathbf{x}) = f(\mathbf{x}|Y=j), \text{ where } j=0,1.
\]</span>
The discriminatory (or segmentation) power will be stronger the more different these densities are. Finally, we denote <span class="math inline">\(p_{j}(\mathbf{x})\)</span> as the probabilities of <span class="math inline">\(Y=j\)</span> given <span class="math inline">\(\mathbf{X}=\mathbf{x}\)</span>:
<span class="math display">\[
p_{j}(\mathbf{x}) = \Pr \left[ Y=j|\mathbf{X}=\mathbf{x}\right], \text{ where } j=0,1.
\]</span>
Using Bayes’ theorem (as recalled in Section 2.2.8), we can relate these quantities as follows:
<span class="math display">\[
p_{j}(\mathbf{x}) = \frac{p_{j}f_{j}(\mathbf{x})}{f(\mathbf{x})}, \text{ with } f(\mathbf{x}) = p_{0}f_{0}(\mathbf{x}) + p_{1}f_{1}(\mathbf{x}).
\]</span></p>
</div>
<div id="optimal-classification-and-threshold-selection" class="section level3 hasAnchor" number="10.6.4">
<h3><span class="header-section-number">10.6.4</span> Optimal Classification and Threshold Selection<a href="chap9.html#optimal-classification-and-threshold-selection" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Several methods can be used to determine the threshold <span class="math inline">\(s\)</span>. For this, let’s assume that the insurer can refuse to insure bad clients. Let <span class="math inline">\(g\)</span> be the insurer’s gain in case of a correct decision: accepting to insure a good client (which happens with probability <span class="math inline">\(\Pr [\mathbf{X}\in \mathcal{A},Y=0]\)</span>). Let <span class="math inline">\(c_{0}\)</span> be the cost associated with insuring a bad risk (with probability <span class="math inline">\(\Pr [\mathbf{X}\in \mathcal{A},Y=1]\)</span>), and let <span class="math inline">\(c_{1}\)</span> be the loss in revenue from refusing to insure a good client (with probability <span class="math inline">\(\Pr [\mathbf{X}\notin \mathcal{A},Y=0]\)</span>).</p>
<p>It is essential to consider the very different consequences of misclassifications for the company. Indeed, placing a bad policyholder among the good ones means accepting to cover a policyholder who will cause a claim (or multiple claims), exposing the company to heavy losses. Conversely, refusing a good risk costs little to the company (but could have disastrous commercial consequences if it happened too often). It is essential to introduce this distinction into the model.</p>
<p>By defining the profitability of the insurer as:
<span class="math display">\[\begin{eqnarray*}
R(\mathcal{A}) &amp;=&amp; g\Pr [\mathbf{X}\in \mathcal{A},Y=0] - c_{0}\Pr [\mathbf{X}\in \mathcal{A},Y=1] \\
&amp;&amp; - c_{1}\Pr [\mathbf{X}\notin \mathcal{A},Y=0],
\end{eqnarray*}\]</span>
the optimal acceptance region is given by:
<span class="math display">\[
\mathcal{A}^{*}=\underset{\mathcal{A}}{\text{argmax}}\{ R(\mathcal{A}) \}.
\]</span>
It can be noted that the insurer’s profitability can be rewritten as:
<span class="math display">\[
R(\mathcal{A}) = \int_{\mathbf{x}\in \mathcal{A}}\Big( \left( g+c_{1}\right) p_{0}f_{0}(\mathbf{x}) - c_{0}p_{1}f_{1}(\mathbf{x}) \Big)d\mathbf{x} - c_{1}p_{0}.
\]</span>
The optimal acceptance region <span class="math inline">\(\mathcal{A}^*\)</span> is the one for which the integrand is always positive, i.e.:
<span class="math display">\[
\mathcal{A}^{*} = \{ \mathbf{x}|\frac{f_{1}(\mathbf{x})}{f_{0}(\mathbf{x})} \leq \frac{(g+c_{1})p_0}{c_0 p_1} \},
\]</span>
or using Bayes’ formula with <span class="math inline">\(p_{j}(\mathbf{x}) = p_{j}f_{j}(\mathbf{x})/f(\mathbf{x})\)</span>:
<span class="math display">\[
\mathcal{A}^{*} = \{ \mathbf{x}|p_{1}(\mathbf{x}) \leq \frac{g+c_{1}}{g+c_{0}+c_{1}} \}.
\]</span>
One can then select policyholders using either the conditional laws of <span class="math inline">\(\mathbf{X}\)</span> given <span class="math inline">\(Y\)</span> (i.e., <span class="math inline">\(f_0\)</span> and <span class="math inline">\(f_1\)</span>), or the conditional laws of <span class="math inline">\(Y\)</span> given <span class="math inline">\(\mathbf{X}\)</span> (i.e., <span class="math inline">\(p_0(\mathbf{X})\)</span> and <span class="math inline">\(p_1(\mathbf{X})\)</span>).
However, it should be noted that while these two approaches are equivalent from a mathematical point of view, they reflect two substantially different perspectives:</p>
<ul>
<li>The law of <span class="math inline">\(\mathbf{X}|Y\)</span> is used in discriminant analysis (the question is whether an individual, whose status as a good or bad risk is known, will be classified among the or clients).</li>
<li>The law of <span class="math inline">\(Y|\mathbf{X}\)</span> is used in a predictive context (to which population - or clients - does an individual, whose characteristics <span class="math inline">\(\mathbf{X}\)</span> are known, have the highest chance of belonging?).</li>
</ul>
<div class="remark">
<p><span id="unlabeled-div-11" class="remark"><em>Remark</em>. </span>It is also possible to imagine that the costs of misclassification <span class="math inline">\(c_0\)</span> and <span class="math inline">\(c_1\)</span>, as well as the gain <span class="math inline">\(g\)</span>, depend on the characteristics of the insured individual, i.e. <span class="math inline">\(c_0=c_0(\mathbf{x})\)</span>, <span class="math inline">\(c_1=c_1(\mathbf{x})\)</span>, and <span class="math inline">\(g=g(\mathbf{x})\)</span>. The reasoning is very similar to the one followed above.</p>
</div>
</div>
<div id="practical-construction-of-a-score" class="section level3 hasAnchor" number="10.6.5">
<h3><span class="header-section-number">10.6.5</span> Practical Construction of a Score<a href="chap9.html#practical-construction-of-a-score" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Five main steps are fundamental when constructing a score:</p>
<ol style="list-style-type: decimal">
<li>The choice of the dichotomous criterion to model: not having a claim in the year, not having a severe claim in the year, not having a responsible claim in the year, …</li>
<li>The choice of the population: the difficulty arises from the fact that the population of insureds in the portfolio can be substantially different from the population applying for insurance (due to selection using a score, precisely). A portion of the insured population is used to calculate the score, while the rest is used to test its performance.</li>
<li>The choice of covariates <span class="math inline">\(\mathbf{X}\)</span>.</li>
<li>Model estimation: schematically, it is possible to use logistic or probit models and estimate the parameters using maximum likelihood.</li>
<li>Performance analysis: a number of tests and criteria can be used to assess the quality of the score’s discrimination (performance, selection, discrimination curves, for example).</li>
</ol>
<p>Let <span class="math inline">\(\mathcal{H}\)</span> designate the historical data used to construct the classifier (i.e., the observations <span class="math inline">\((\mathbf{x},Y)\)</span> recorded by the company in the past for a large number of individuals). This set will be partitioned into two subsets (most often randomly) that will be used to respectively estimate the parameters and evaluate the classifier, namely:</p>
<p><strong>The “training set”</strong></p>
<p>This is the subset of <span class="math inline">\(\mathcal{H}\)</span> used to determine the parameters. Most often, the training set has the form:
<span class="math display">\[\begin{equation}
\left\{ (\mathbf{x}_{0;k},0)\text{ }k=1,...,n_0,(\mathbf{x}_{1;k},1)\text{ }k=1,...,n_1\right\}\label{2.12}
\end{equation}\]</span>
where the <span class="math inline">\(\mathbf{x}_{0,j}\)</span>, numbering <span class="math inline">\(n_0\)</span>, correspond to policyholders classified as good risks, and the <span class="math inline">\(\mathbf{x}_{1,j}\)</span>, numbering <span class="math inline">\(n_1\)</span>, correspond to policyholders classified as bad risks.</p>
<p><strong>The “test set”</strong></p>
<p>This is the subset of <span class="math inline">\(\mathcal{H}\)</span> used to assess the classifier’s performance.</p>
<p>If a parametric model is selected, we estimate the densities of observable characteristics for good and bad policyholders by:
<span class="math display">\[
\widehat{f_1(\mathbf{x})} = f_1(\mathbf{x};\widehat{\boldsymbol{\theta}}) \text{ and }
\widehat{f_0(\mathbf{x})} = f_0(\mathbf{x};\widehat{\boldsymbol{\theta}}),
\]</span>
where <span class="math inline">\(\widehat{\boldsymbol{\theta}}\)</span> is the maximum likelihood estimator of <span class="math inline">\(\boldsymbol{\theta}.\)</span> The likelihood is given by:
<span class="math display">\[\begin{equation}
\mathcal{L}(\boldsymbol{\theta})=\prod_{j=1}^{n_0}\left(f_0(\mathbf{x}_{0,j},\boldsymbol{\theta}) p_0\right)\prod_{j=1}^{n_1}\left(f_1(\mathbf{x}_{1,j},\boldsymbol{\theta})p_1\right).   
\label{2.13}
\end{equation}\]</span>
To estimate the vector of parameters <span class="math inline">\((\boldsymbol{\theta},p_0,p_1)\)</span>, we write the log-likelihood:
<span class="math display">\[\begin{equation}
L(\boldsymbol{\theta},\boldsymbol{p})=\sum_{j=1}^{n_0 }\ln f_0(\mathbf{x}_{0,j};\boldsymbol{\theta})
+\sum_{j=1}^{n_1 }\ln f_1(\mathbf{x}_{1,j};\boldsymbol{\theta})+n_0\ln p_0+n_1\ln p_1.
\label{2.14}
\end{equation}\]</span>
We first maximize <span class="math inline">\(n_0\ln p_0+n_1\ln p_1\)</span> to obtain:
<span class="math display">\[
\hat{p}_0=\frac{n_0}{n_0+n_1} \text{ and } \hat{p}_1=1-\hat{p}_0=\frac{n_1}{n_0+n_1}.
\]</span>
This is the natural estimator of <span class="math inline">\(p_0\)</span>, i.e., the proportion of good clients in the training set. It is important to realize that we assume that the target population for new business is similar to the individuals in <span class="math inline">\(\mathcal{H}\)</span>.</p>
<p>We then maximize:
<span class="math display">\[\begin{equation}
\sum_{j=1}^{n_0 }\ln f_0(\mathbf{x}_{0,j};\boldsymbol{\theta})
+\sum_{j=1}^{n_1 }\ln f_1(\mathbf{x}_{1,j};\boldsymbol{\theta})\label{2.14 bis}
\end{equation}\]</span>
over <span class="math inline">\(\boldsymbol{\theta}\)</span> to obtain the maximum likelihood estimator of <span class="math inline">\(\boldsymbol{\theta}\)</span>.</p>
</div>
<div id="linear-discriminant-analysis" class="section level3 hasAnchor" number="10.6.6">
<h3><span class="header-section-number">10.6.6</span> (Linear) Discriminant Analysis<a href="chap9.html#linear-discriminant-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The objective here is to describe an individual’s membership in a predefined class based on their characteristics. We are interested in the law of <span class="math inline">\(\mathbf{X}|Y\)</span> described by the probability density functions <span class="math inline">\(f_0\)</span> and <span class="math inline">\(f_1\)</span>. Recall that the canonical score function <span class="math inline">\(S^*\)</span> leads to a classification rule based on the ratio <span class="math inline">\(f_1/f_0\)</span>.</p>
<p>::: {.example}
Suppose that, conditional on <span class="math inline">\(Y=j\)</span>, <span class="math inline">\(\mathbf{X}\sim\mathcal{N}or_p(\boldsymbol{\mu}_j,\boldsymbol{\Sigma})\)</span>. Note that the variance-covariance matrix does not depend on the value of <span class="math inline">\(j\)</span> for <span class="math inline">\(Y\)</span>. The ratio of densities is then an increasing function of:
<span class="math display">\[
\left( \mathbf{X}-\boldsymbol{\mu}_0\right)^\top \boldsymbol{\Sigma}^{-1}\left( \mathbf{X}-\boldsymbol{\mu}_0\right)-\left( \mathbf{X}-\boldsymbol{\mu}_1\right)^\top \boldsymbol{\Sigma}^{-1}\left( \mathbf{X}-\boldsymbol{\mu}_1\right).
\]</span>%
We define the score as:
<span class="math display">\[
S(\mathbf{X}) = \mathbf{X}^\top \boldsymbol{\Sigma}^{-1}\left( \boldsymbol{\mu}_1-\boldsymbol{\mu}_0\right).
\]</span>%</p>
<p>If the variance-covariance matrices are not equal, the score becomes:
<span class="math display">\[
S(\mathbf{X}) = \left( \mathbf{X}-\boldsymbol{\mu}_0\right)^\top \boldsymbol{\Sigma}_0^{-1}\left( \mathbf{X}-\boldsymbol{\mu}_0\right)-\left( \mathbf{X}-\boldsymbol{\mu}_1\right)^\top \boldsymbol{\Sigma}_1^{-1}\left( \mathbf{X}-\boldsymbol{\mu}_1\right),
\]</span>%
where <span class="math inline">\(\boldsymbol{\Sigma}_0\)</span> is the variance-covariance matrix of <span class="math inline">\(\mathbf{X}\)</span> if <span class="math inline">\(Y=0\)</span>, and <span class="math inline">\(\boldsymbol{\Sigma}_1\)</span> is that of <span class="math inline">\(\mathbf{X}\)</span> if <span class="math inline">\(Y=1\)</span>. In this case, we no longer have a linear criterion (as in the previous case), but a quadratic one.
:::</p>
</div>
<div id="discriminant-analysis" class="section level3 hasAnchor" number="10.6.7">
<h3><span class="header-section-number">10.6.7</span> Discriminant Analysis<a href="chap9.html#discriminant-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The objective here is to describe an individual’s membership in a predefined class based on their characteristics. We are interested in the law of <span class="math inline">\(\mathbf{X}|Y\)</span> described by the probability density functions <span class="math inline">\(f_0\)</span> and <span class="math inline">\(f_1\)</span>. Recall that the canonical score function <span class="math inline">\(S^*\)</span> leads to a classification rule based on the ratio <span class="math inline">\(f_1/f_0\)</span>.</p>
<p>::: {.example}
Suppose that, conditional on <span class="math inline">\(Y=j\)</span>, <span class="math inline">\(\mathbf{X}\sim\mathcal{N}or_p(\boldsymbol{\mu}_j,\boldsymbol{\Sigma})\)</span>. Note that the variance-covariance matrix does not depend on the value of <span class="math inline">\(j\)</span> for <span class="math inline">\(Y\)</span>. The ratio of densities is then an increasing function of:
<span class="math display">\[
\left( \mathbf{X}-\boldsymbol{\mu}_0\right)^\top \boldsymbol{\Sigma}^{-1}\left( \mathbf{X}-\boldsymbol{\mu}_0\right)-\left( \mathbf{X}-\boldsymbol{\mu}_1\right)^\top \boldsymbol{\Sigma}^{-1}\left( \mathbf{X}-\boldsymbol{\mu}_1\right).
\]</span>%
We define the score as:
<span class="math display">\[
S(\mathbf{X}) = \mathbf{X}^\top \boldsymbol{\Sigma}^{-1}\left( \boldsymbol{\mu}_1-\boldsymbol{\mu}_0\right).
\]</span>%</p>
<p>If the variance-covariance matrices are not equal, the score becomes:
<span class="math display">\[
S(\mathbf{X}) = \left( \mathbf{X}-\boldsymbol{\mu}_0\right)^\top \boldsymbol{\Sigma}_0^{-1}\left( \mathbf{X}-\boldsymbol{\mu}_0\right)-\left( \mathbf{X}-\boldsymbol{\mu}_1\right)^\top \boldsymbol{\Sigma}_1^{-1}\left( \mathbf{X}-\boldsymbol{\mu}_1\right),
\]</span>%
where <span class="math inline">\(\boldsymbol{\Sigma}_0\)</span> is the variance-covariance matrix of <span class="math inline">\(\mathbf{X}\)</span> if <span class="math inline">\(Y=0\)</span>, and <span class="math inline">\(\boldsymbol{\Sigma}_1\)</span> is that of <span class="math inline">\(\mathbf{X}\)</span> if <span class="math inline">\(Y=1\)</span>. In this case, we no longer have a linear criterion (as in the previous case), but a quadratic one.
:::</p>
</div>
<div id="the-disqual-method" class="section level3 hasAnchor" number="10.6.8">
<h3><span class="header-section-number">10.6.8</span> The DISQUAL Method<a href="chap9.html#the-disqual-method" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The discriminant analysis methods we have presented seem to be particularly well-suited for solving the acceptance problem. However, there is a significant challenge that needs to be overcome. In practice, the information available to the actuary about future policyholders is mostly composed of qualitative variables (binary, such as gender, or with multiple categories, such as socio-professional category), and integer variables that only take a few distinct values (such as the number of dependent children or the number of household vehicles, for example).</p>
<p>The joint density of these variables is far from resembling that of a multivariate normal distribution. To get closer to the assumptions of validity of discriminant analysis, we can first transform the qualitative variables into continuous and uncorrelated variables by performing a Multiple Correspondence Analysis (MCA) on all qualitative variables. We will then work with the coordinates of individuals on the factorial axes.</p>
<p>MCA allows us to replace the original qualitative characteristics, which may not always be suitable for scoring, with continuous variables. Then, a discriminant analysis is performed, and it is straightforward to return to the original variables to derive a score.</p>
<p>This strategy can be represented schematically as follows:</p>
</div>
<div id="the-probit-model" class="section level3 hasAnchor" number="10.6.9">
<h3><span class="header-section-number">10.6.9</span> The Probit Model<a href="chap9.html#the-probit-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The fact that <span class="math inline">\(Y\)</span> takes its values in <span class="math inline">\(\{0,1\}\)</span> makes any linear modeling approach outlined in Section <span class="math inline">\(\ref{Sec923MC}\)</span> inappropriate. The use of a latent variable, as we will see, is much more appropriate.</p>
<p>Here, we assume that there exists a latent variable <span class="math inline">\(Y^*\)</span> such that:
<span class="math display">\[
Y^* = \mathbf{X}^\top \boldsymbol{\beta} + \varepsilon, \text{ and }
Y = \mathbb{I}[Y^* \geq 0],
\]</span>
where <span class="math inline">\(\mathbb{I}[A]\)</span> is the indicator of the event <span class="math inline">\(A\)</span>, equal to 1 if the event occurred and 0 otherwise, and where <span class="math inline">\(\varepsilon \sim \mathcal{N}or(0,1)\)</span>. Therefore, the score associated with this model is given by:
<span class="math display">\[
S(\mathbf{x}) = \Pr[Y=1|\mathbf{X}=\mathbf{x}] = \Pr[Y^* \geq 0|\mathbf{X}=\mathbf{x}] = \Pr[\mathbf{x}^\top \boldsymbol{\beta} + \varepsilon \geq 0] = 1 - \Phi(-\mathbf{x}^\top \boldsymbol{\beta}) = \Phi(\mathbf{x}^\top \boldsymbol{\beta}),
\]</span>
where <span class="math inline">\(\Phi\)</span> denotes the cumulative distribution function of the <span class="math inline">\(\mathcal{N}or(0,1)\)</span> distribution.</p>
<p>For this model, the estimation of the parameters <span class="math inline">\(\boldsymbol{\beta}\)</span> is done by maximum likelihood. Given observations <span class="math inline">\((y_i, \mathbf{x}_i)\)</span>, <span class="math inline">\(i=1,2,\ldots,n\)</span>, this amounts to maximizing:
<span class="math display">\[
\mathcal{L} = \prod_{i=1}^n \left(\Phi(\mathbf{x}_i^\top\boldsymbol{\beta})\right)^{1-y_i} \left(1-\Phi(\mathbf{x}_i^\top\boldsymbol{\beta})\right)^{y_i}.
\]</span>
In fact, it is sufficient to solve the system of likelihood equations obtained by setting the gradient of the log-likelihood to zero:
<span class="math display">\[
\frac{\partial}{\partial \boldsymbol{\beta}} \ln \mathcal{L} = \sum_{i=1}^n \left((1-y_i) \frac{\mathbf{x}_i \phi(\mathbf{x}_i^\top\boldsymbol{\beta})}{\Phi(\mathbf{x}_i^\top\boldsymbol{\beta})} - y_i \frac{\mathbf{x}_i \phi(\mathbf{x}_i^\top\boldsymbol{\beta})}{1-\Phi(\mathbf{x}_i^\top\boldsymbol{\beta})}\right) = \boldsymbol{0},
\]</span>
where <span class="math inline">\(\phi\)</span> is the probability density function associated with the <span class="math inline">\(\mathcal{N}or(0,1)\)</span> distribution.</p>
<p>This system does not have an explicit solution. We will return more extensively to this nonlinear regression model in Section <span class="math inline">\(\ref{Sec96GLM}\)</span>.</p>
</div>
<div id="the-logit-model" class="section level3 hasAnchor" number="10.6.10">
<h3><span class="header-section-number">10.6.10</span> The Logit Model<a href="chap9.html#the-logit-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The idea here is also to use a latent variable <span class="math inline">\(Y^*\)</span>, but we assume that <span class="math inline">\(\varepsilon\)</span> follows a logistic distribution, with cumulative distribution function:
<span class="math display">\[
F(x) = \frac{1}{1+\exp(-x)}, \quad x \in \mathbb{R}.
\]</span></p>
<p>In this case, the score is given by:
<span class="math display">\[
S^*(\mathbf{x}) = \Pr[Y=1|\mathbf{X}=\mathbf{x}] = F(\mathbf{x}^\top\boldsymbol{\beta}) = \frac{1}{1+\exp(-\mathbf{x}^\top\boldsymbol{\beta})}.
\]</span></p>
<p>Additionally:
<span class="math display">\[
\Pr[Y=0|\mathbf{X}=\mathbf{x}] = F(-\mathbf{x}^\top\boldsymbol{\beta}) = \frac{1}{1+\exp(\mathbf{x}^\top\boldsymbol{\beta})}.
\]</span></p>
<p>Note that the ratio <span class="math inline">\(\Pr[Y=1|\boldsymbol{X}=\boldsymbol{x}] / \Pr[Y=0|\boldsymbol{X}=\boldsymbol{x}]\)</span> is called the odds ratio. For horse racing enthusiasts, odds of 5 to 1 mean that the probability of losing is 5 times greater than the probability of winning. So, when a horse is rated at 5 to 1, it means that 5 bettors have placed losing bets against 1 winning bet. In other words, there are 5 bettors out of 6 who have placed losing bets, resulting in odds of <span class="math inline">\(\frac{5/6}{1/6}=5\)</span>.</p>
<p>We will delve into the details of this nonlinear regression model in Section <span class="math inline">\(\ref{Sec96GLM}\)</span>.</p>
</div>
<div id="duality-of-approaches" class="section level3 hasAnchor" number="10.6.11">
<h3><span class="header-section-number">10.6.11</span> Duality of Approaches<a href="chap9.html#duality-of-approaches" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The approaches of discriminant analysis and logistic regression are, in fact, dual approaches; see <span class="citation">(<a href="#ref-celeux1994analyse" role="doc-biblioref">Celeux and Nakache 1994</a>)</span>, <span class="citation">(<a href="#ref-gourieroux1992courbes" role="doc-biblioref">Gourieroux 1992</a>)</span> and <span class="citation">(<a href="#ref-gourieroux1999statistique" role="doc-biblioref">Gouriéroux 1999</a>)</span>. Discriminant analysis is based on the specification of the conditional distributions of <span class="math inline">\({\boldsymbol{X}}|Y\)</span>, i.e., <span class="math inline">\(f_{0}\)</span> and <span class="math inline">\(f_{1}\)</span>. Thus, the canonical score can be expressed as
<span class="math display">\[
S^{\ast }\left( {\boldsymbol{x}}\right) =\Pr \left[ Y=1|{\boldsymbol{X}}={\boldsymbol{x}}%
\right] =\frac{p_{1}f_{1}\left(
{\boldsymbol{x}}\right) }{p_{0}f_{0}\left( {\boldsymbol{x}}\right)
+p_{1}f_{1}\left( {\boldsymbol{x}}\right) }.
\]</span>%
Under the assumptions of Example <span class="math inline">\(\ref{Example-analyse-disc}\)</span>
(Gaussian conditional distributions with the same covariance matrix), this canonical score can be written after simplifications as%
<span class="math display">\[
S^{\ast }\left( {\boldsymbol{x}}\right) =\frac{1}{1+\exp\big(-\Delta(\boldsymbol{x})\big)}
\]</span>
where
<span class="math display">\[\begin{eqnarray*}
\Delta(\boldsymbol{x})&amp;=&amp; \boldsymbol{X}^\top\boldsymbol{\Sigma}^{-1}\left( \boldsymbol{\mu }_{1}-\boldsymbol{\mu
}_{0}\right)-\ln \left( \frac{1-p_{1}}{p_{1}}\right)\\
&amp;&amp;-\frac{1}{2}\Big( \boldsymbol{\mu}_{0}^\top\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu }_{0}-\boldsymbol{\mu }_{1}^\top
\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu }_{1}\Big).
\end{eqnarray*}\]</span>
This corresponds to a logistic model with a constant term, i.e., $%
_{0}+{}^$ where <span class="math inline">\(\boldsymbol{\beta}=\boldsymbol{\Sigma}^{-1}\left( \boldsymbol{\mu }_{1}-\boldsymbol{\mu }_{0}\right)\)</span> and%
<span class="math display">\[
\beta _{0}=-\ln\left( \frac{1-p_{1}}{p_{1}}\right) -\frac{1}{2}%
\Big( \boldsymbol{\mu }_{0}^\top\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu }_{0}-%
\boldsymbol{\mu }_{1}^\top\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu }_{1}\Big) .
\]</span>%
In other words, the linear discriminant analysis described
in Example <span class="math inline">\(\ref{Example-analyse-disc}\)</span> is a special case
of the LOGIT model. Equivalently, the quadratic
discriminant analysis
obtained with different covariance matrices (second part of Example <span class="math inline">\(\ref{Example-analyse-disc}\)</span>) appears as a specific case of the LOGIT model when the explanatory variables include quadratic transformations of the components of <span class="math inline">\(\boldsymbol{X}\)</span>.</p>
</div>
<div id="performance-and-selection-curves" class="section level3 hasAnchor" number="10.6.12">
<h3><span class="header-section-number">10.6.12</span> Performance and Selection Curves<a href="chap9.html#performance-and-selection-curves" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Suppose the score <span class="math inline">\(S\)</span> is used to discriminate between two
subpopulations, good and bad clients, using a threshold <span class="math inline">\(s\)</span>. The idea of <span class="citation">(<a href="#ref-gourieroux1992courbes" role="doc-biblioref">Gourieroux 1992</a>)</span> is then to represent the performance of the
score by the performance curve
<span class="math display">\[
\mathcal{P}=\big\{(x(s),y(s))\big| s\in[0,1]\big\},
\]</span>
where
<span class="math display">\[
x(s)=\Pr[S\leq s]\text{ and }y(s)=\frac{\Pr[Y=0|S\leq
s]}{\Pr[Y=0]},
\]</span> whose explicit equation is <span class="math inline">\(y=\mathcal{P}(x)\)</span>. We define
the selection curve as
<span class="math display">\[
\mathcal{S}=\big\{(x(s),y(s))\big| s\in[0,1]\big\},
\]</span>
where
<span class="math display">\[
x(s)=\Pr[S\leq s]\text{ and }y(s)=\Pr[S\leq s|Y=0],
\]</span>
whose explicit equation is <span class="math inline">\(y=\mathcal{S}(x)\)</span>. The performance curve is necessarily increasing, while the
selection curve of a canonical score is always increasing and convex. Note also that the two curves are related by
the equation <span class="math inline">\(\mathcal{S}(x)=x\mathcal{P}(x)\)</span>. Both of these curves are depicted in Figure <span class="math inline">\(\ref{Fig-score}\)</span>.</p>
<p>Performance curves are invariant under strictly increasing transformations of the score: let <span class="math inline">\(h\)</span> be a strictly increasing transformation,
<span class="math display">\[
x_h(s)=\Pr[h(S)\leq s]=\Pr[S\leq h^{-1}(s)]=x(h^{-1}(s)),
\]</span>
and
<span class="math display">\[
y_h(s)=\frac{\Pr[Y=0|h(S)\leq s]}{\Pr[Y=0]}=y(h^{-1}(s)).
\]</span>
In other words, these curves do not take into account the
value of the score, but only the order it establishes.</p>
</div>
<div id="desirable-properties-of-a-score" class="section level3 hasAnchor" number="10.6.13">
<h3><span class="header-section-number">10.6.13</span> Desirable Properties of a Score<a href="chap9.html#desirable-properties-of-a-score" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For any score <span class="math inline">\(S\)</span> (not necessarily canonical),
it is desirable that it strongly depends on <span class="math inline">\(Y\)</span>. In
particular, it is possible to show that the random variables <span class="math inline">\(Y\)</span> and <span class="math inline">\(S\)</span> are
associated (this notion was presented in Section
8.5.3 of Volume 1) if, and only if, the
performance curve <span class="math inline">\(\mathcal{P}\)</span> is below the line
<span class="math inline">\(y=1\)</span>, or equivalently, if, and only if, the selection curve <span class="math inline">\(\mathcal{S}\)</span> is below the first bisector.</p>
<p>Furthermore, if the selection curve is increasing and convex,
then the selection curve can be viewed as the Lorenz curve associated with <span class="math inline">\(\Pr[Y=0|S]\)</span>.</p>
</div>
<div id="score-comparison" class="section level3 hasAnchor" number="10.6.14">
<h3><span class="header-section-number">10.6.14</span> Score Comparison<a href="chap9.html#score-comparison" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Performance curves can be used to compare scores with each other. In particular, a score <span class="math inline">\(S_1\)</span> is considered more effective than a score <span class="math inline">\(S_2\)</span> if, and only if, its performance curve is below that of <span class="math inline">\(S_2\)</span>.</p>
<p>We can also define the so-called score discrimination curve: if we denote <span class="math inline">\(G_0(s)=\Pr[S\leq s|Y=0]\)</span> and <span class="math inline">\(G_1(s)=\Pr[S\leq s|Y=1]\)</span>, then the score discrimination curve is the function <span class="math inline">\([0,1]\rightarrow[0,1]\)</span> defined as
<span class="math display">\[
\mathcal{D}(x)=G_1\circ G_0^{-1}(x), \hspace{2mm}x\in[0,1].
\]</span>
Note that this function is increasing and invariant under strictly increasing transformations of the score. Also, the concavity of this function is equivalent to having <span class="math inline">\(\Pr[Y=1|S=s]\)</span> increasing in <span class="math inline">\(s\)</span>.</p>
<p>The term “discrimination” in this curve comes from the following property: if the score is not discriminative, and thus <span class="math inline">\(G_0=G_1\)</span> (Y and S are independent), then the curve <span class="math inline">\(\mathcal{D}\)</span> coincides with the first bisector. On the other hand, for a population partitioned into two subsets, where <span class="math inline">\(G_0\)</span> and <span class="math inline">\(G_1\)</span> would then be concentrated at two points, the discrimination curve would be the curve <span class="math inline">\(y=0\)</span> on <span class="math inline">\([0,1[\)</span>. Between these two extreme cases, it is possible to consider the following preorder: score <span class="math inline">\(S_1\)</span> is more discriminative than score <span class="math inline">\(S_2\)</span> if, and only if, its discrimination curve is below that of <span class="math inline">\(S_2\)</span>.</p>
</div>
</div>
<div id="Sec93MLMC" class="section level2 hasAnchor" number="10.7">
<h2><span class="header-section-number">10.7</span> Linear Model<a href="chap9.html#Sec93MLMC" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="definition-36" class="section level3 hasAnchor" number="10.7.1">
<h3><span class="header-section-number">10.7.1</span> Definition<a href="chap9.html#definition-36" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Since generalized linear models extend the Gaussian linear model, it seems natural to begin by briefly recalling the main results related to the classic linear regression approach. We strongly recommend the interested reader to delve deeper into this subject by consulting the excellent work of <span class="citation">(<a href="#ref-cornillon2007regression" role="doc-biblioref">Cornillon and Matzner-Løber 2007</a>)</span>.</p>
<p>For a long time, models used to explain the variations of continuous variables <span class="math inline">\(Y_1, Y_2, \ldots, Y_n\)</span> in the presence of explanatory variables summarized in vectors <span class="math inline">\(\boldsymbol{x}_1, \boldsymbol{x}_2, \ldots, \boldsymbol{x}_n\)</span> took the form
<span class="math display">\[
Y_i=\beta_0+\sum_{j=1}^p\beta_jx_{ij}+\epsilon_i\text{ with }\epsilon_i\sim\mathcal{N}or(0,\sigma^2).
\]</span>
Equivalently, we have
<span class="math display">\[
Y_i\sim\mathcal{N}or\left(\beta_0+\sum_{j=1}^p\beta_jx_{ij},\sigma^2\right),\hspace{2mm}i=1,2,\ldots,n.
\]</span>
The observations <span class="math inline">\(Y_i\)</span> are assumed to be normally distributed with a mean of <span class="math inline">\(\beta_0+\sum_{j=1}^p\beta_jx_{ij}\)</span>, an affine function of the explanatory variables, and constant variance <span class="math inline">\(\sigma^2\)</span>. The linear combination of the explanatory variables <span class="math inline">\(\beta_0+\sum_{j=1}^p\beta_jx_{ij}\)</span> that yields <span class="math inline">\(\mathbb{E}[Y_i]\)</span> is called the score (or linear predictor) and will be denoted as <span class="math inline">\(\eta_i\)</span> subsequently.</p>
<p>Even though the linear model imposes serious limitations and its realism is questionable in many problems faced by actuaries, it remains highly significant because most more realistic models (including generalized linear models to be discussed in the next section) borrow many techniques from it.</p>
</div>
<div id="matrix-formalism" class="section level3 hasAnchor" number="10.7.2">
<h3><span class="header-section-number">10.7.2</span> Matrix Formalism<a href="chap9.html#matrix-formalism" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Matrix formalism is quite useful for analyzing the generalized linear model. We can easily see that the linear regression model can be vectorized as follows:
<span class="math display">\[\begin{equation}
\label{Rel0}
{\boldsymbol{Y}}={\boldsymbol{X}}\boldsymbol{\beta}+\boldsymbol{\epsilon}
\end{equation}\]</span>
where <span class="math inline">\({\boldsymbol{Y}}=\left(Y_1,Y_2,\ldots, Y_n\right)^\top\)</span> is an <span class="math inline">\(n\times 1\)</span> vector containing the variables to be explained, <span class="math inline">\(\boldsymbol{\beta}=\left(\beta_0,\beta_1, \ldots,\beta_p\right)^\top\)</span> is a <span class="math inline">\((p+1)\times 1\)</span> vector of parameters,
<span class="math display">\[
{\boldsymbol{X}}=\left(
\begin{array}{cc}
1&amp;\boldsymbol{x}_1^\top \\
1&amp;\boldsymbol{x}_2^\top \\
\vdots&amp;\vdots \\
1&amp;\boldsymbol{x}_n^\top
\end{array}
\right)=\left(
\begin{array}{ccccc}
1&amp;x_{11} &amp; x_{12} &amp; \ldots &amp; x_{1p} \\
1&amp;x_{21} &amp; x_{22} &amp; \ldots &amp; x_{2p} \\
\vdots&amp;\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1&amp;x_{n1} &amp; x_{n2} &amp; \ldots &amp; x_{np}
\end{array}
\right)
\]</span>
is an <span class="math inline">\(n\times (p+1)\)</span> matrix containing the explanatory variables, and <span class="math inline">\(\boldsymbol{\epsilon}=\left(\epsilon_1,\epsilon_2,\ldots,\epsilon_n\right)^\top\sim\mathcal{N}or_n(\boldsymbol{0},\sigma^2\boldsymbol{I})\)</span> is an <span class="math inline">\(n\times 1\)</span> vector representing the errors. The matrix <span class="math inline">\({\boldsymbol{X}}\)</span> is assumed to have rank <span class="math inline">\(p+1\)</span>, i.e., the square matrix <span class="math inline">\({\boldsymbol{X}}^\top{\boldsymbol{X}}\)</span> of dimension <span class="math inline">\((p+1)\times (p+1)\)</span> is assumed to be invertible.</p>
<p>With these notations, the observed value of the random vector <span class="math inline">\({\boldsymbol{Y}}\)</span> is the sum of a deterministic component <span class="math inline">\({\boldsymbol{X}}\boldsymbol{\beta}\)</span> and a random component <span class="math inline">\(\boldsymbol{\epsilon}\)</span> that models the noise. The deterministic component of the vector <span class="math inline">\({\boldsymbol{Y}}\)</span> represents the observations that would have been made in the absence of noise. The assumption <span class="math inline">\(\mathbb{E}[\boldsymbol{\epsilon}]=\boldsymbol{0}\)</span> means that the deterministic component of the vector <span class="math inline">\({\boldsymbol{Y}}\)</span> is its mean.</p>
</div>
<div id="parameter-estimation" class="section level3 hasAnchor" number="10.7.3">
<h3><span class="header-section-number">10.7.3</span> Parameter Estimation<a href="chap9.html#parameter-estimation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Suppose we have realizations <span class="math inline">\(y_1,y_2,\ldots,y_n\)</span> of the variables <span class="math inline">\(Y_1,Y_2,\ldots,Y_n\)</span>. The likelihood function associated with the observations <span class="math inline">\(y_1,y_2,\ldots,y_n\)</span> is
<span class="math display">\[\begin{eqnarray*}
\mathcal{L}(\boldsymbol{\beta},\sigma|{\boldsymbol{y}})&amp;=&amp;\left(\frac{1}{\sigma\sqrt{2\pi}}\right)^n
\prod_{i=1}^n\exp\left(-\frac{1}{2\sigma^2}(y_i-{\boldsymbol{x}}_i^\top\boldsymbol{\beta})^2\right)\\
&amp;=&amp;\left(\frac{1}{\sigma\sqrt{2\pi}}\right)^n
\exp\left(-\frac{1}{2\sigma^2}({\boldsymbol{y}}-{\boldsymbol{X}}\boldsymbol{\beta})^\top
({\boldsymbol{y}}-{\boldsymbol{X}}\boldsymbol{\beta})\right).
\end{eqnarray*}\]</span>
As explained above, the maximum likelihood estimator of <span class="math inline">\(\boldsymbol{\beta}\)</span> is the value of <span class="math inline">\(\boldsymbol{\beta}\)</span> that maximizes <span class="math inline">\(\mathcal{L}(\boldsymbol{\beta},\sigma|{\boldsymbol{y}})\)</span>. Intuitively, this is the value of <span class="math inline">\(\boldsymbol{\beta}\)</span> that makes the observations <span class="math inline">\(y_1,y_2,\ldots,y_n\)</span> most likely in the model <span class="math inline">\(\eqref{Rel0}\)</span>. This estimator is obtained as follows.</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-12" class="proposition"><strong>Proposition 10.1  </strong></span>The maximum likelihood estimator <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> of <span class="math inline">\(\boldsymbol{\beta}\)</span> is a solution of the normal equations
<span class="math display">\[
{\boldsymbol{X}}^\top{\boldsymbol{X}}\widehat{\boldsymbol{\beta}}-{\boldsymbol{X}}^\top{\boldsymbol{Y}}=\boldsymbol{0}
\Leftrightarrow {\boldsymbol{X}}^\top\Big({\boldsymbol{X}}\widehat{\boldsymbol{\beta}}-{\boldsymbol{Y}}\Big)=\boldsymbol{0}.
\]</span>
Since the matrix <span class="math inline">\({\boldsymbol{X}}^\top{\boldsymbol{X}}\)</span> has been assumed to be invertible, the system of normal equations has a unique solution given by
<span class="math display">\[\begin{equation}
\label{eqI5}
\widehat{\boldsymbol{\beta}}=\big({\boldsymbol{X}}^\top{\boldsymbol{X}}\big)^{-1}{\boldsymbol{X}}^\top{\boldsymbol{Y}}=\left(\sum_{i=1}^n\boldsymbol{x}_i\boldsymbol{x}_i^\top\right)^{-1}
\sum_{i=1}^n\boldsymbol{x}_iy_i,
\end{equation}\]</span>
which defines the maximum likelihood estimator of <span class="math inline">\(\boldsymbol{\beta}\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-13" class="proof"><em>Proof</em>. </span>The log-likelihood is given by
<span class="math display">\[
L(\boldsymbol{\beta},\sigma|{\boldsymbol{y}})=
-\frac{n}{2}\ln\sigma^2-\frac{1}{2\sigma^2}({\boldsymbol{y}}-{\boldsymbol{X}}\boldsymbol{\beta})^\top
({\boldsymbol{y}}-{\boldsymbol{X}}\boldsymbol{\beta})-\frac{n}{2}\ln(2\pi).
\]</span>
Now,
<span class="math display">\[
\sup_{(\boldsymbol{\beta},\sigma^2)}L(\boldsymbol{\beta},\sigma|{\boldsymbol{y}})
=\sup_{\sigma^2}\sup_{\boldsymbol{\beta}}L(\boldsymbol{\beta},\sigma|{\boldsymbol{y}}).
\]</span>
So, regardless of the value of <span class="math inline">\(\sigma^2\)</span>, maximizing <span class="math inline">\(L(\boldsymbol{\beta},\sigma|{\boldsymbol{y}})\)</span>
with respect to <span class="math inline">\(\boldsymbol{\beta}\)</span> is equivalent to minimizing
<span class="math display">\[
S_2(\boldsymbol{\beta})=
({\boldsymbol{y}}-{\boldsymbol{X}}\boldsymbol{\beta})^\top
({\boldsymbol{y}}-{\boldsymbol{X}}\boldsymbol{\beta}).
\]</span>
To minimize <span class="math inline">\(S_2\)</span> with respect to <span class="math inline">\(\boldsymbol{\beta}\)</span>, it is necessary for it to be a stationary point of this expression. Therefore, it is obtained by taking the derivative of <span class="math inline">\(S_2\)</span> with respect to
<span class="math inline">\(\boldsymbol{\beta}\)</span> and setting the gradient to <span class="math inline">\(\boldsymbol{0}\)</span>.
As
<span class="math display">\[
S_2(\boldsymbol{\beta})={\boldsymbol{y}}^\top{\boldsymbol{y}}-2{\boldsymbol{y}}^\top{\boldsymbol{X}}\boldsymbol{\beta}
+\boldsymbol{\beta}^\top{\boldsymbol{X}}^\top{\boldsymbol{X}}\boldsymbol{\beta}.
\]</span>
It follows that
<span class="math display">\[
\frac{\partial S_2(\boldsymbol{\beta})}{\partial\boldsymbol{\beta}}
=-2{\boldsymbol{X}}^\top{\boldsymbol{Y}}+2{\boldsymbol{X}}^\top{\boldsymbol{X}}\boldsymbol{\beta},
\]</span>
yielding the system of normal equations. Also, note that any solution to the normal equations corresponds to a minimum of the function <span class="math inline">\(S_2\)</span> because the Hessian matrix
is the positive definite matrix <span class="math inline">\(2{\boldsymbol{X}}^\top{\boldsymbol{X}}\)</span>.</p>
</div>
<p>If the matrix <span class="math inline">\({\boldsymbol{X}}^\top{\boldsymbol{X}}\)</span> is not invertible, the system of normal equations may have more than one solution, and the concept of the generalized inverse matrix is used (note, however, that the estimator of the mean <span class="math inline">\({\boldsymbol{X}}\widehat{\boldsymbol{\beta}}\)</span> is unique). This occurs if the columns of <span class="math inline">\(\boldsymbol{X}\)</span> are linearly dependent. This can happen, for example, if for each individual, the number of pre-university study years, the number of higher education study years, and the total number of study years are measured. In practice, it is sufficient to review the variables and eliminate any redundancy of information. Often, problems are caused by the determinant of <span class="math inline">\(\boldsymbol{X}^\top\boldsymbol{X}\)</span> being very close to zero, causing numerical instability when estimating <span class="math inline">\(\boldsymbol{\beta}\)</span> and <span class="math inline">\(\sigma^2\)</span>.</p>
<p>The normal equations can also be written as
<span class="math display">\[
\sum_{i=1}^n\boldsymbol{x}_i^\top(y_i-\boldsymbol{\beta}^\top\boldsymbol{x}_i)=\boldsymbol{0}\text{ for }i=1,2,\ldots,n.
\]</span>
Written in this way, they have a very simple and intuitive interpretation: <span class="math inline">\(y_i-\boldsymbol{\beta}^\top\boldsymbol{x}_i\)</span> is the residual associated with observation <span class="math inline">\(i\)</span>, and the normal equations thus impose orthogonality between the residual vector and the vector of explanatory variables. This orthogonality intuitively means that there is nothing left in the explanatory variables that can provide information about the residuals. We will see in the next section that this interpretation is preserved in generalized linear models.</p>
<p>By calculating the partial derivative of the log-likelihood <span class="math inline">\(L(\boldsymbol{\beta},\sigma|{\boldsymbol{y}})\)</span> with respect to <span class="math inline">\(\sigma^2\)</span>, we verify that the maximum likelihood estimator of <span class="math inline">\(\sigma^2\)</span> is
<span class="math display">\[
\widetilde{\sigma}^2=\sigma^2(\widehat{\boldsymbol{\beta}})=
\frac{S_2(\widehat{\boldsymbol{\beta}})}{n}=\frac{R_0^2}{n}.
\]</span>
We will see later that we often prefer an unbiased estimator of <span class="math inline">\(\sigma^2\)</span> over <span class="math inline">\(\widetilde{\sigma}^2\)</span>, which will be defined in <span class="math inline">\(\eqref{EstimSigma}\)</span>.</p>
</div>
<div id="prediction-matrix" class="section level3 hasAnchor" number="10.7.4">
<h3><span class="header-section-number">10.7.4</span> Prediction Matrix<a href="chap9.html#prediction-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Having obtained the estimate of the vector <span class="math inline">\(\boldsymbol{\beta}\)</span>, we can define an estimator
<span class="math inline">\(\widehat{{\boldsymbol{Y}}}={\boldsymbol{X}}\widehat{\boldsymbol{\beta}}\)</span> for the mean of the vector <span class="math inline">\({\boldsymbol{Y}}\)</span> and an estimator <span class="math inline">\(\widehat{\boldsymbol{\epsilon}}={\boldsymbol{Y}}-\widehat{{\boldsymbol{Y}}}\)</span>
for the unobservable vector <span class="math inline">\(\boldsymbol{\epsilon}\)</span>, which is called the residual vector. Note that we can still write
<span class="math display">\[
\widehat{{\boldsymbol{Y}}}={\boldsymbol{X}}\big({\boldsymbol{X}}^\top{\boldsymbol{X}}\big)^{-1}{\boldsymbol{X}}^\top{\boldsymbol{Y}},
\]</span>
which leads to the following definition.</p>
<div class="definition">
<p><span id="def:unlabeled-div-14" class="definition"><strong>Definition 10.1  </strong></span>The projection matrix (hat matrix) associated with a data matrix <span class="math inline">\({\boldsymbol{X}}\)</span> is the square <span class="math inline">\(n\times n\)</span> matrix <span class="math inline">\({\boldsymbol{H}}\)</span> defined as
<span class="math display">\[
{\boldsymbol{H}}={\boldsymbol{X}}\big({\boldsymbol{X}}^\top{\boldsymbol{X}}\big)^{-1}{\boldsymbol{X}}^\top.
\]</span></p>
</div>
<p>This matrix transforms <span class="math inline">\(\boldsymbol{Y}\)</span> into <span class="math inline">\(\widehat{{\boldsymbol{Y}}}\)</span> since <span class="math inline">\(\widehat{\boldsymbol{Y}}=\boldsymbol{X}\widehat{\boldsymbol{\beta}}={\boldsymbol{H}}\boldsymbol{Y}\)</span>. That’s why this matrix is also called the prediction matrix. Consequently,
<span class="math display">\[
\widehat{\boldsymbol{\epsilon}}={\boldsymbol{Y}}-{\boldsymbol{X}}\widehat{\boldsymbol{\beta}}
=({\boldsymbol{I}}-{\boldsymbol{H}}){\boldsymbol{Y}}.
\]</span></p>
<p>The projection matrix <span class="math inline">\({\boldsymbol{H}}\)</span> associated with <span class="math inline">\({\boldsymbol{X}}\)</span> has the following properties.</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-15" class="proposition"><strong>Proposition 10.2  </strong></span>Let <span class="math inline">\({\boldsymbol{X}}\)</span> be a real matrix of dimension <span class="math inline">\(n \times (p+1)\)</span>, with rank <span class="math inline">\(p+1\)</span>, and <span class="math inline">\({\boldsymbol{H}}\)</span> be the associated prediction matrix. Then:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\sum \limits ^n _ {i=1} h_{ii} = p+1,\)</span> so the trace of <span class="math inline">\(\boldsymbol{H}\)</span> is equal to the number of regression coefficients,</li>
<li><span class="math inline">\(\sum \limits ^n _{i=1} \sum \limits ^n _ {j=1} h^2 _{ij} = p+1,\)</span></li>
<li>$0 h_{ij} $ for all <span class="math inline">\(i\)</span>.</li>
<li>$ -1/2 h_{ij} /2 $ for any <span class="math inline">\(j \neq i\)</span>,</li>
<li>if $ h_{ii} = 1 $ or <span class="math inline">\(h_{ii} = 0,\)</span> then $ h_{ij} = 0 $ for $ j i $,</li>
<li>$(1-h_{ii})(1-h_{jj}) - h^2 _{ij} $</li>
<li>$ h_{ii} h_{jj} - h^2 _ {ij} .$</li>
</ol>
</div>
</div>
<div id="estimation-of-means-and-variance" class="section level3 hasAnchor" number="10.7.5">
<h3><span class="header-section-number">10.7.5</span> Estimation of Means and Variance<a href="chap9.html#estimation-of-means-and-variance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The following result gives the properties of the estimators <span class="math inline">\(\widehat{\boldsymbol{Y}}\)</span> for the mean of <span class="math inline">\(\boldsymbol{Y}\)</span> and <span class="math inline">\(\widehat{\boldsymbol{\varepsilon}}\)</span> for the vector of residuals.</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-16" class="proposition"><strong>Proposition 10.3  </strong></span>The random vector <span class="math inline">\(\widehat{{\boldsymbol{Y}}}\)</span> is an unbiased estimator of the mean of <span class="math inline">\({\boldsymbol{Y}}\)</span> with a variance-covariance matrix of <span class="math inline">\(\sigma^2{\boldsymbol{H}}\)</span>. The vector <span class="math inline">\(\widehat{\boldsymbol{\epsilon}}\)</span> of estimated residuals is centered, and its variance-covariance matrix is <span class="math inline">\(\sigma^2({\boldsymbol{I}}-{\boldsymbol{H}})\)</span>. Furthermore, these two vectors are uncorrelated.</p>
</div>
<p>The components of the vector <span class="math inline">\(\widehat{\boldsymbol{\epsilon}}\)</span> are generally correlated; their correlation depends on the design matrix <span class="math inline">\({\boldsymbol{X}}\)</span>.</p>
<p>Since
<span class="math display">\[\begin{eqnarray*}
\mathbb{E}\left[\sum_{i=1}^n\widehat{\epsilon}_i^2\right]&amp;=&amp;\mathbb{E}\left[{\boldsymbol{Y}}^\top{\boldsymbol{Y}}-
\widehat{\boldsymbol{\beta}}^\top{\boldsymbol{X}}^\top{\boldsymbol{X}}\widehat{\boldsymbol{\beta}}\right]\\
&amp;=&amp;\mathbb{E}\left[
{\boldsymbol{Y}}^\top({\boldsymbol{I}}-{\boldsymbol{H}}){\boldsymbol{Y}}\right]\\
&amp;=&amp;\text{Trace}\big(\sigma^2({\boldsymbol{I}}-{\boldsymbol{H}})\big)=
\sigma^2(n-p-1),
\end{eqnarray*}\]</span>
we only need to consider
<span class="math display">\[\begin{equation}
\label{EstimSigma}
\widehat{\sigma}^2=\frac{1}{n-p-1}\sum_{i=1}^n\widehat{\epsilon}_i^2.
\end{equation}\]</span></p>
</div>
<div id="measurement-of-fit-quality-the-coefficient-of-determination" class="section level3 hasAnchor" number="10.7.6">
<h3><span class="header-section-number">10.7.6</span> Measurement of Fit Quality: The Coefficient of Determination<a href="chap9.html#measurement-of-fit-quality-the-coefficient-of-determination" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To assess the quality of the fit provided by the model, the coefficient of determination is generally used, or the percentage of variance explained by the model, defined as
<span class="math display">\[
R^2=1-\frac{\sum_{i=1}^n\big(\widehat{y}_i-y_i\big)^2}
{\sum_{i=1}^n\big(y_i-\overline{y}\big)^2}=
\frac{\sum_{i=1}^n\big(\widehat{y}_i-\overline{y}\big)^2}
{\sum_{i=1}^n\big(y_i-\overline{y}\big)^2}.
\]</span>
The value of <span class="math inline">\(R^2\)</span> is between 0 and 1, with the model being better as <span class="math inline">\(R^2\)</span> is closer to 1.
The coefficient of determination measures “the proportion of variability in <span class="math inline">\(Y\)</span> due to its linear regression by least squares on the explanatory variables <span class="math inline">\(X_i\)</span>.”
In the case where there is only one explanatory variable (i.e., <span class="math inline">\(p=2\)</span>), <span class="math inline">\(R^2\)</span> is the square of the linear correlation coefficient between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X_1\)</span>.
It should be noted that <span class="math inline">\(R^2\)</span> is only useful if the model includes an intercept term <span class="math inline">\(\beta_0\)</span>.</p>
</div>
<div id="standardized-residuals" class="section level3 hasAnchor" number="10.7.7">
<h3><span class="header-section-number">10.7.7</span> Standardized Residuals<a href="chap9.html#standardized-residuals" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Unlike the theoretical residuals <span class="math inline">\(\epsilon_i\)</span> (unobservable), the estimated residuals <span class="math inline">\(\widehat{\epsilon}_i\)</span> do not have a constant variance and are generally correlated. Therefore, standardized residuals are preferred, given by
<span class="math display">\[
T_i=\frac{\widehat{\epsilon}_i}{\widehat{\sigma}\sqrt{1-h_{ii}}}
\]</span>
which remedies these inconveniences.</p>
</div>
<div id="inferential-results-for-parameters" class="section level3 hasAnchor" number="10.7.8">
<h3><span class="header-section-number">10.7.8</span> Inferential Results for Parameters<a href="chap9.html#inferential-results-for-parameters" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s continue the statistical analysis from before by completing the properties of the estimators <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> and <span class="math inline">\(\widehat{\sigma}^2\)</span>. The following results are fundamental.</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-17" class="proposition"><strong>Proposition 10.4  </strong></span></p>
<ol style="list-style-type: decimal">
<li>The estimator <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> given in <span class="math inline">\(\eqref{eqI5}\)</span> has a distribution of <span class="math inline">\(\mathcal{N}or_{p+1}\left(\boldsymbol{\beta},\sigma^2\big({\boldsymbol{X}}^\top{\boldsymbol{X}}\big)^{-1}\right)\)</span>.</li>
<li>The estimator <span class="math inline">\(\widehat{\sigma}^2\)</span> given in <span class="math inline">\(\eqref{EstimSigma}\)</span> is such that
<span class="math display">\[
\frac{(n-p-1)\widehat{\sigma}^2}{\sigma^2}
\]</span>
follows a chi-squared distribution with <span class="math inline">\(n-p-1\)</span> degrees of freedom.</li>
<li>The estimators <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> and <span class="math inline">\(\widehat{\sigma}^2\)</span> are independent.</li>
</ol>
</div>
</div>
<div id="testing-a-simple-hypothesis" class="section level3 hasAnchor" number="10.7.9">
<h3><span class="header-section-number">10.7.9</span> Testing a Simple Hypothesis<a href="chap9.html#testing-a-simple-hypothesis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let <span class="math inline">\(\boldsymbol{\beta}_0\)</span> be a fixed value of the parameter vector. To test the null hypothesis <span class="math inline">\(H_0:\boldsymbol{\beta}=\boldsymbol{\beta}_0\)</span>, against its negation, we will apply the likelihood ratio test.</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-18" class="proposition"><strong>Proposition 10.5  </strong></span>Let <span class="math inline">\(\boldsymbol{\beta}_0\)</span> be a fixed value of the parameter. We reject <span class="math inline">\(H_0\)</span> at the confidence level <span class="math inline">\(\alpha\)</span> when
<span class="math display">\[
\sum_{i=1}^n(y_i-{\boldsymbol{x}}_i^\top\boldsymbol{\beta}_0)^2-
\sum_{i=1}^n(y_i-{\boldsymbol{x}}_i^\top\widehat{\boldsymbol{\beta}})^2\geq\widehat{\sigma}^2
F_{p+1,n-p-1;1-\alpha},
\]</span>
where <span class="math inline">\(F_{p+1,n-p-1;1-\alpha}\)</span> is the <span class="math inline">\((1-\alpha)\)</span> quantile of the Fisher-Snedecor distribution with <span class="math inline">\(p+1\)</span> and <span class="math inline">\(n-p-1\)</span> degrees of freedom.</p>
</div>
</div>
<div id="comparison-of-nested-models" class="section level3 hasAnchor" number="10.7.10">
<h3><span class="header-section-number">10.7.10</span> Comparison of Nested Models<a href="chap9.html#comparison-of-nested-models" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Suppose we have to choose between a model <span class="math inline">\(M_0\)</span> and another model <span class="math inline">\(M_1\)</span>, such that <span class="math inline">\(M_0\)</span> involves only a subset of the explanatory variables included in <span class="math inline">\(M_1\)</span>. Formally, <span class="math inline">\(M_0\)</span> is obtained by setting <span class="math inline">\(\beta_j=0\)</span> for a set of indices <span class="math inline">\(j\in \mathcal{E}_0\)</span>. In this case, we have nested models.</p>
<p>The choice between <span class="math inline">\(M_0\)</span> and <span class="math inline">\(M_1\)</span> is therefore equivalent to testing the nullity of the <span class="math inline">\(\beta_j\)</span> for <span class="math inline">\(j\in \mathcal{E}_0\)</span>. We will base our decision on the likelihood ratio statistic
<span class="math display">\[
\frac{\max_{(\boldsymbol{\beta},\sigma^2)\in M_1}\mathcal{L}(\boldsymbol{\beta},\sigma^2|\boldsymbol{y})}
{\max_{(\boldsymbol{\beta},\sigma^2)\in M_0}\mathcal{L}(\boldsymbol{\beta},\sigma^2|\boldsymbol{y})}
\]</span>
and reject <span class="math inline">\(M_0\)</span> in favor of <span class="math inline">\(M_1\)</span> if this ratio is sufficiently large.</p>
<p>By denoting <span class="math inline">\(\widehat{\boldsymbol{\beta}}_1\)</span> and <span class="math inline">\(\widehat{\sigma}_1^2\)</span> as the maximum likelihood estimators in model <span class="math inline">\(M_1\)</span>, and <span class="math inline">\(\widehat{\boldsymbol{\beta}}_0\)</span> and <span class="math inline">\(\widehat{\sigma}_0^2\)</span> as the maximum likelihood estimators in model <span class="math inline">\(M_0\)</span>, we can still use the test statistic
<span class="math display">\[
\frac{\sum_{i=1}^n\big(y_i-\widehat{\boldsymbol{\beta}}_0^\top\boldsymbol{x}_i\big)^2
-\sum_{i=1}^n\big(y_i-\widehat{\boldsymbol{\beta}}_1^\top\boldsymbol{x}_i\big)^2}
{\sum_{i=1}^n\big(y_i-\widehat{\boldsymbol{\beta}}_1^\top\boldsymbol{x}_i\big)^2}.
\]</span>
If model <span class="math inline">\(M_0\)</span> has <span class="math inline">\(p_0+1\)</span> parameters and model <span class="math inline">\(M_1\)</span> has <span class="math inline">\(p_1+1\)</span> parameters, this statistic, multiplied by <span class="math inline">\((n-p_1-1)/(p_1-p_0)\)</span>, follows the Fisher distribution with parameters <span class="math inline">\(p_1-p_0\)</span> and <span class="math inline">\(n-p_1-1\)</span>.</p>
<p>::: {.example}[Test of Linear Constraints on Parameters]</p>
<p>Suppose that, in the context of credit risk analysis, we have data on a married couple’s individual income levels, one for the husband and one for the wife. We may wonder if it is relevant to include both of these variables as explanatory factors in the model, i.e., to use a model of the form:
<span class="math display">\[
\ldots+\beta_j\times\text{ husband&#39;s income }+\beta_{j+1}\times\text{ wife&#39;s income }+\ldots
\]</span>
or, on the contrary, if only the total household income matters. In this case, we can test the hypothesis <span class="math inline">\(H_0:\beta_j=\beta_{j+1}\)</span>. We will proceed as above with model <span class="math inline">\(M_0\)</span> defined by the constraint <span class="math inline">\(\beta_j=\beta_{j+1}\)</span> while model <span class="math inline">\(M_1\)</span> allows these two parameters to differ.
:::</p>
</div>
<div id="confidence-regions" class="section level3 hasAnchor" number="10.7.11">
<h3><span class="header-section-number">10.7.11</span> Confidence Regions<a href="chap9.html#confidence-regions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Confidence regions are defined by the set of parameters <span class="math inline">\(\boldsymbol{\beta}\)</span> that are not rejected, at the given level, as possible values of the parameter based on the maximum likelihood ratio test. These are regions in the parameter space that appear reasonable given the collected observations.</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-19" class="proposition"><strong>Proposition 10.6  </strong></span>The confidence region for <span class="math inline">\(\boldsymbol{\beta}\)</span> at the <span class="math inline">\(1-\alpha\)</span> level is defined by the set of values of <span class="math inline">\(\boldsymbol{\beta}\)</span> such that
<span class="math display">\[
(\boldsymbol{\beta}-\widehat{\boldsymbol{\beta}})^\top{\boldsymbol{X}}^\top{\boldsymbol{X}}
(\boldsymbol{\beta}-\widehat{\boldsymbol{\beta}})\leq \widehat{\sigma}^2
F_{p+1,n-p-1;1-\alpha}.
\]</span></p>
</div>
</div>
<div id="confidence-intervals" class="section level3 hasAnchor" number="10.7.12">
<h3><span class="header-section-number">10.7.12</span> Confidence Intervals<a href="chap9.html#confidence-intervals" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Rather than confidence regions, most commercial software often presents confidence intervals for individual parameters. These intervals are typically obtained by noting that the <span class="math inline">\(j\)</span>-th component of <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span>, <span class="math inline">\(\widehat{\beta}_j\)</span>, follows a normal distribution with mean <span class="math inline">\(\beta_j\)</span> and variance <span class="math inline">\(\sigma^2\big({\boldsymbol{X}}^\top{\boldsymbol{X}}\big)_{jj}^{-1}\)</span>, where <span class="math inline">\(\big({\boldsymbol{X}}^\top{\boldsymbol{X}}\big)_{jj}^{-1}\)</span> is the <span class="math inline">\(j\)</span>-th diagonal element of the inverse of the matrix <span class="math inline">\({\boldsymbol{X}}^\top{\boldsymbol{X}}\)</span>.</p>
<p>When <span class="math inline">\(\sigma^2\)</span> is known, the confidence interval for <span class="math inline">\(\beta_j\)</span> is formed by the values <span class="math inline">\(\xi\in\mathbb{R}\)</span> that satisfy
<span class="math display">\[
|\widehat{\beta}_j-\xi|\leq\sigma\sqrt{\big({\boldsymbol{X}}^\top{\boldsymbol{X}}\big)_{jj}^{-1}}z_{\alpha/2},
\]</span>
where <span class="math inline">\(z_{\alpha/2}\)</span> is the <span class="math inline">\((1-\alpha/2)\)</span> quantile associated with the <span class="math inline">\(\mathcal{N}or(0,1)\)</span> distribution. When <span class="math inline">\(\sigma^2\)</span> is unknown, a <span class="math inline">\(1-\alpha\)</span> confidence interval for <span class="math inline">\(\beta_j\)</span> is given by the set of values <span class="math inline">\(\xi\in\mathbb{R}\)</span> that satisfy
<span class="math display">\[
|\widehat{\beta}_j-\xi|\leq\widehat{\sigma}
\sqrt{\big({\boldsymbol{X}}^\top{\boldsymbol{X}}\big)_{jj}^{-1}}t_{n-p-1;1-\alpha/2},
\]</span>
where <span class="math inline">\(t_{n-p-1;1-\alpha/2}\)</span> is the <span class="math inline">\((1-\alpha/2)\)</span> quantile associated with the Student’s t-distribution with <span class="math inline">\(n-p-1\)</span> degrees of freedom.</p>
<div class="remark">
<p><span id="unlabeled-div-20" class="remark"><em>Remark</em>. </span>The confidence intervals defined above are not adequate if you want to consider multiple parameters simultaneously because they do not account for the parameter dependencies. When the parameter <span class="math inline">\(\boldsymbol{\beta}\)</span> has two components <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span>, the confidence region, which in this case is an ellipse, consists of pairs <span class="math inline">\((\beta_1, \beta_2)\)</span> that reasonably explain the observations, taking into account the correlation between <span class="math inline">\(\hat\beta_1\)</span> and <span class="math inline">\(\hat\beta_2\)</span>. In contrast, the separate confidence intervals for parameters <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> locate the value of one component without considering the value taken by the other.</p>
</div>
</div>
<div id="measures-of-influence" class="section level3 hasAnchor" number="10.7.13">
<h3><span class="header-section-number">10.7.13</span> Measures of Influence<a href="chap9.html#measures-of-influence" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="principle-4" class="section level4 hasAnchor" number="10.7.13.1">
<h4><span class="header-section-number">10.7.13.1</span> Principle<a href="chap9.html#principle-4" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The results of the least squares fitting of a linear model to a set of observations can be significantly altered by the removal or perturbation of certain data points. Various statistics have been defined to quantify the influence of each observation on the model’s fit. These statistics are primarily based on the residuals <span class="math inline">\(\hat{\epsilon}_i\)</span> and the projection matrix <span class="math inline">\(\boldsymbol{H}\)</span>.</p>
<p>Here, we present the approach based on omission, primarily relying on the comparison between the results obtained when fitting the model to the entire dataset and those obtained after fitting the model with one or more observations omitted.</p>
</div>
<div id="effect-of-omitting-an-observation" class="section level4 hasAnchor" number="10.7.13.2">
<h4><span class="header-section-number">10.7.13.2</span> Effect of Omitting an Observation<a href="chap9.html#effect-of-omitting-an-observation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The characteristics obtained after omitting observation <span class="math inline">\(i\)</span> will be denoted with the subscript <span class="math inline">\((i)\)</span>. Thus, <span class="math inline">\(\widehat{\boldsymbol{\beta}}_{(i)}\)</span> is the estimated regression vector based on the remaining <span class="math inline">\(n-1\)</span> observations, namely <span class="math inline">\(y_1, \ldots, y_{i-1}, y_{i+1}, \ldots, y_n\)</span>. We can study the effect of each observation on the estimations in a regression model.</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-21" class="proposition"><strong>Proposition 10.7  </strong></span>After omitting the <span class="math inline">\(i\)</span>th observation, the least squares estimators <span class="math inline">\(\widehat{\boldsymbol{\beta}}_{(i)}\)</span> and <span class="math inline">\(\hat{\sigma}^2_{(i)}\)</span> for the parameters <span class="math inline">\(\boldsymbol{\beta}\)</span> and <span class="math inline">\(\sigma^2\)</span> of the linear model satisfy the following equations:</p>
<p><span class="math display">\[
\widehat{\boldsymbol{\beta}}_{(i)} = \widehat{\boldsymbol{\beta}} - (\boldsymbol{X}^\top\boldsymbol{X})^{-1}\boldsymbol{x}_i \cdot \frac{\epsilon_i}{1-h_{ii}}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
(n-p-2) \hat{\sigma}^2_{(i)} = (n-p-1) \hat{\sigma}^2 - \frac{\epsilon_i^2}{1-h_{ii}}
\]</span></p>
</div>
<p>The two preceding formulas demonstrate that the estimators <span class="math inline">\(\widehat{\boldsymbol{\beta}}_{(i)}\)</span> and <span class="math inline">\(\hat{\sigma}^2_{(i)}\)</span> depend solely on <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span>, <span class="math inline">\(\hat{\sigma}^2\)</span>, and the <span class="math inline">\(i\)</span>th diagonal element of <span class="math inline">\(\boldsymbol{H}\)</span>. Therefore, when omitting an observation, it is not necessary to perform model fitting again to compute these values.</p>
</div>
<div id="diagnostic-measures-based-on-residuals" class="section level4 hasAnchor" number="10.7.13.3">
<h4><span class="header-section-number">10.7.13.3</span> Diagnostic Measures Based on Residuals<a href="chap9.html#diagnostic-measures-based-on-residuals" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>To assess the suitability of an observation for the model, we can examine whether its omission has an impact on its prediction. Specifically, we want to determine if observation <span class="math inline">\(y_i\)</span> is close enough to its predicted value <span class="math inline">\(\hat{y}_{(i)}\)</span> obtained by excluding the <span class="math inline">\(i\)</span>th observation from the calculation. Since <span class="math inline">\(y_i\)</span> is not used in the calculation of <span class="math inline">\(\hat{y}_{(i)}\)</span>, the random variables <span class="math inline">\(y_i\)</span> and <span class="math inline">\(\hat{y}_{(i)}\)</span> are uncorrelated. Their difference <span class="math inline">\(y_i - \hat{y}_{(i)}\)</span> has a variance given by:</p>
<p><span class="math display">\[
\sigma^2\left(1+\boldsymbol{x}_i^\top (\boldsymbol{X}_{(i)}^\top \boldsymbol{X}_{(i)})^{-1}\boldsymbol{x}_i\right)
\]</span></p>
<p>When the parameter <span class="math inline">\(\sigma^2\)</span> is unknown, it is estimated by the residual variance <span class="math inline">\(\hat{\sigma}^2_{(i)}\)</span> obtained from the regression equation after removing the <span class="math inline">\(i\)</span>th observation. This estimator is independent of <span class="math inline">\(Y_i\)</span>. This leads to the definition of the following statistics:</p>
<p><span class="math display">\[\begin{eqnarray*}
T^\ast_i &amp;=&amp; \frac{Y_i - \hat{Y}_{(i)}}{\hat{\sigma}_{(i)} \sqrt{1+\boldsymbol{x}_i^\top (\boldsymbol{X}_{(i)}^\top \boldsymbol{X}_{(i)})^{-1}\boldsymbol{x}_i}} \\
&amp;=&amp; \frac{\hat{\epsilon}_i}{\hat{\sigma}_{(i)} \sqrt{1-h_{ii}}}, \quad i=1,2,\ldots,n,
\end{eqnarray*}\]</span></p>
<p>called cross-validation residuals. The probability distribution of <span class="math inline">\(T^\ast_i\)</span> is given by the following theorem.</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-22" class="proposition"><strong>Proposition 10.8  </strong></span>Assuming that the matrix <span class="math inline">\(\boldsymbol{X}\)</span> has rank <span class="math inline">\(p+1\)</span>, if the removal of the <span class="math inline">\(i\)</span>th row of <span class="math inline">\(\boldsymbol{X}\)</span> does not change its rank, then the cross-validation residuals <span class="math inline">\(T^\ast_i\)</span>, <span class="math inline">\(i=1, \ldots, n\)</span>, follow the Student’s t-distribution with <span class="math inline">\(n-p-2\)</span> degrees of freedom.</p>
</div>
<p>Empirical evidence shows that to detect “outlier” observations, standardized residuals <span class="math inline">\(T_i\)</span> and cross-validation residuals <span class="math inline">\(T^\ast_i\)</span> are equivalent. Nevertheless, several authors prefer <span class="math inline">\(T^\ast_i\)</span> over $T_i” for the following reasons:</p>
<ol style="list-style-type: decimal">
<li>The <span class="math inline">\(T^\ast_i\)</span>, <span class="math inline">\(i=1,2, \ldots, n\)</span>, are identically distributed and follow the Student’s t-distribution with <span class="math inline">\((n-p-2)\)</span> degrees of freedom.</li>
<li>A simple calculation shows that <span class="math inline">\(T^\ast_i = T_i \sqrt{\frac{n-p-1}{n-p-T_i^2}}\)</span>. This relationship demonstrates that <span class="math inline">\(T^\ast_i\)</span> is a monotonic function of <span class="math inline">\(T_i\)</span> and is more sensitive to observations with large residuals.</li>
<li>Since <span class="math inline">\(\hat{\sigma}_{(i)}\)</span> is independent of <span class="math inline">\(y_i\)</span>, this estimator is robust to gross errors in the <span class="math inline">\(i\)</span>th observation, which can occur during data acquisition.</li>
</ol>
</div>
<div id="outlier-observations" class="section level4 hasAnchor" number="10.7.13.4">
<h4><span class="header-section-number">10.7.13.4</span> Outlier Observations<a href="chap9.html#outlier-observations" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We now aim to clarify what is meant by an outlier observation in a linear model.</p>
<div class="definition">
<p><span id="def:unlabeled-div-23" class="definition"><strong>Definition 10.2  </strong></span>An outlier data point is a point <span class="math inline">\((\boldsymbol{x}_i^\top, y_i)\)</span> for which the associated value <span class="math inline">\(t^\ast_i\)</span> of <span class="math inline">\(T^\ast_i\)</span> is high (compared to the threshold given by the Student’s t-distribution).</p>
</div>
<p>Outlier data points are typically detected by plotting the <span class="math inline">\(t^\ast_i\)</span> (or the <span class="math inline">\(t_i\)</span> or the partial residuals) sequentially or as a function of other variables such as <span class="math inline">\(y_i\)</span> or <span class="math inline">\(\boldsymbol{x}_i\)</span>. Detecting outlier data depends solely on the magnitude of the residuals.</p>
<p>Graphical representations of residuals often allow not only the detection of outlier data but also the verification of the model’s validity. We will recall some of the most common representations.</p>
<p><strong>Empirical Distribution Representation</strong></p>
<p>One way to represent residuals is by plotting the histogram, smoothed density, etc., of the empirical distribution of residuals. The use of such representations to check the assumption of normality of data makes sense only for large samples.</p>
<p><strong>Representations as a Function of Fitted Values</strong></p>
<p>Residuals can also be plotted as a function of fitted values. We have seen that when the assumptions associated with the model are correct, residuals and predicted values are uncorrelated. Therefore, the plot of points should not exhibit any particular structure. This type of plot provides insights into the validity of linearity assumptions and the homogeneity of error variance. For example, curvature in the shape of residuals suggests that the linearity assumption may not be appropriate, while a monotonic behavior of residual variability with <span class="math inline">\(\hat{y}_i\)</span> indicates non-constant error variance.</p>
</div>
<div id="diagnostic-measures-based-on-the-projection-matrix" class="section level4 hasAnchor" number="10.7.13.5">
<h4><span class="header-section-number">10.7.13.5</span> Diagnostic Measures Based on the Projection Matrix<a href="chap9.html#diagnostic-measures-based-on-the-projection-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The arrangement of points in the space of regressors plays an important role. The projection matrix <span class="math inline">\(\boldsymbol{H}\)</span> partially evaluates this influence. Recall that the components of <span class="math inline">\(\widehat{\boldsymbol{\epsilon}}\)</span> are centered variables with variance equal to <span class="math inline">\(\sigma^2(1 - h_{ii})\)</span>, where</p>
<p><span class="math display">\[
h_{ii} = \boldsymbol{x}_i^\top (\boldsymbol{X}^\top\boldsymbol{X})^{-1}\boldsymbol{x}_i, \quad i=1,2, \ldots, n.
\]</span></p>
<p>The estimated residual variance is lower when <span class="math inline">\(h_{ii}\)</span> is greater, and the value of <span class="math inline">\(h_{ii}\)</span> measures the influence of observation <span class="math inline">\(y_i\)</span> on the fitted value <span class="math inline">\(\hat{y}_i\)</span>. The matrix <span class="math inline">\(\boldsymbol{H}\)</span> is symmetric and idempotent as it projects <span class="math inline">\(\boldsymbol{Y}\)</span> onto the subspace spanned by the column vectors of matrix <span class="math inline">\(\boldsymbol{X}\)</span>.</p>
<p>The <span class="math inline">\(i\)</span>th component of <span class="math inline">\(\widehat{\boldsymbol{Y}}\)</span> is:</p>
<p><span class="math display">\[
\hat{Y}_i = \sum_{j=1}^n h_{ij} Y_j = h_{ii} Y_i + \sum_{j \neq i} h_{ij} Y_j, \quad i=1,2, \ldots, n,
\]</span></p>
<p>and thus, <span class="math inline">\(h_{ii}\)</span> represents the “weight” of observation <span class="math inline">\(y_i\)</span> in determining the prediction <span class="math inline">\(\hat{y}_i\)</span>. In particular, if <span class="math inline">\(h_{ii} = 1\)</span>, <span class="math inline">\(\hat{y}_i\)</span> is determined solely by observation <span class="math inline">\(y_i\)</span>. Furthermore, if <span class="math inline">\(h_{ii} = 0\)</span>, observation <span class="math inline">\(y_i\)</span> has no influence on <span class="math inline">\(\hat{y}_i\)</span>.</p>
<p>Since the trace of <span class="math inline">\(\boldsymbol{H}\)</span> is equal to <span class="math inline">\(p+1\)</span>, the average of <span class="math inline">\(h_{ii}\)</span> is <span class="math inline">\((p+1)/n\)</span>. When <span class="math inline">\(h_{ii}\)</span> is “large,” as we have seen, the corresponding observation is influential. This has led to labeling points for which <span class="math inline">\(h_{ii} &gt; 2(p+1)/n\)</span> as influential. Other authors prefer to set the bounds at 0.2 and 0.5: values of <span class="math inline">\(h_{ii} \leq 0.2\)</span> are normal, and values between 0.2 and 0.5 are considered influential.</p>
</div>
<div id="cooks-distance" class="section level4 hasAnchor" number="10.7.13.6">
<h4><span class="header-section-number">10.7.13.6</span> Cook’s Distance<a href="chap9.html#cooks-distance" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Under the assumption of normality of observations and when <span class="math inline">\(\sigma\)</span> is unknown, the confidence region for the vector <span class="math inline">\(\boldsymbol{\beta}\)</span> of the linear model coefficients is given by:
<span class="math display">\[
C_{\alpha} = \left\{\boldsymbol{\beta}\in\mathbb{R}^{p+1} \, \middle| \, (\boldsymbol{\beta} - \widehat{\boldsymbol{\beta}})^\top\boldsymbol{X}^\top\boldsymbol{X}
(\boldsymbol{\beta} - \widehat{\boldsymbol{\beta}}) \leq \hat \sigma^2  p  F_{p+1, n-p-1; 1-\alpha}\right\}.
\]</span>
This inequality defines an ellipsoid centered at the point <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span>. The influence of the <span class="math inline">\(i\)</span>th observation can be measured by the displacement of this ellipsoid when the <span class="math inline">\(i\)</span>th observation is omitted.</p>
<p>Cook introduced the statistic:
<span class="math display">\[
C_i = \frac{(\widehat{\boldsymbol{\beta}}_{(i)} - \widehat{\boldsymbol{\beta}})^\top\boldsymbol{X}^\top\boldsymbol{X}
(\widehat{\boldsymbol{\beta}}_{(i)} - \widehat{\boldsymbol{\beta}})}{\hat \sigma^2 (p+1)},
\]</span>
to detect the influence of the <span class="math inline">\(i\)</span>th observation on the regression coefficients. This statistic is called Cook’s distance. One can consider it as a weighted distance between <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> and <span class="math inline">\(\widehat{\boldsymbol{\beta}}_{(i)}\)</span>. At first glance, it seems that to calculate Cook’s distance for all observations, one would need to perform <span class="math inline">\(n+1\)</span> regressions (one using all the data and <span class="math inline">\(n\)</span> using reduced data). However, it can be shown that
<span class="math display">\[
C_i = \frac{h_{ii}}{(p+1)(1-h_{ii})} T^2_i.
\]</span>
Thus, <span class="math inline">\(C_i\)</span> can be computed using quantities already calculated during the fitting of the full model. Furthermore, this relationship shows that Cook’s distance <span class="math inline">\(C_i\)</span> is an increasing function of the square of the standardized residual and <span class="math inline">\(h_{ii}\)</span>. When <span class="math inline">\(C_i\)</span> is large, the corresponding observation has a simultaneous influence on all model parameters. Cook suggests comparing each <span class="math inline">\(C_i\)</span> to the quantiles of the Fisher-Snedecor distribution with <span class="math inline">\(p+1\)</span> and <span class="math inline">\(n-p-1\)</span> degrees of freedom, even though the <span class="math inline">\(C_i\)</span> do not exactly follow such a distribution. This is not a rigorous test.</p>
</div>
<div id="likelihood-distance" class="section level4 hasAnchor" number="10.7.13.7">
<h4><span class="header-section-number">10.7.13.7</span> Likelihood Distance<a href="chap9.html#likelihood-distance" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>If <span class="math inline">\(L (\boldsymbol{\beta}, \sigma^2)\)</span> denotes the log-likelihood for <span class="math inline">\(\boldsymbol{\beta}\)</span> and <span class="math inline">\(\sigma^2\)</span>, then the statistic:
<span class="math display">\[
LD_i = 2 \cdot \left\{ L(\widehat{\boldsymbol{\beta}}, \widehat{\sigma}^2) - L (\widehat{\boldsymbol{\beta}}_{(i)}, \widehat \sigma^2_{(i)})\right\},
\]</span>
is called the likelihood distance. It can also be shown that
<span class="math display">\[
LD_i = n \ln \left(\frac{n(n-p-1-T^2_i)}{(n-1)(n-p-1)}\right)+ \frac{(n-1) T^2_i}{(1-h_{ii})(n-p-1-T^2_i)}-1.
\]</span>
This measure is useful when considering the joint influence of observation <span class="math inline">\(i\)</span> on the estimators of <span class="math inline">\(\boldsymbol{\beta}\)</span> and <span class="math inline">\(\sigma^2\)</span>. The likelihood distance is compared to the <span class="math inline">\(1-\alpha\)</span> percentile of the chi-squared distribution with <span class="math inline">\(p+1\)</span> degrees of freedom.</p>
</div>
</div>
<div id="weighted-least-squares" class="section level3 hasAnchor" number="10.7.14">
<h3><span class="header-section-number">10.7.14</span> Weighted Least Squares<a href="chap9.html#weighted-least-squares" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="definition-37" class="section level4 hasAnchor" number="10.7.14.1">
<h4><span class="header-section-number">10.7.14.1</span> Definition<a href="chap9.html#definition-37" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Let’s now consider the model where the observations <span class="math inline">\(Y_1, Y_2, \ldots, Y_n\)</span> have the representation:
<span class="math display">\[
Y_i = \beta_0 + \sum_{j=1}^p \beta_j x_{ij} + \epsilon_i \text{ with } \epsilon_i \sim \mathcal{N}or(0, \sigma^2/w_i),
\]</span>
where <span class="math inline">\(w_i\)</span> is a weight associated with observation <span class="math inline">\(i\)</span>. Typically, this weight arises when <span class="math inline">\(Y_i\)</span> is the average of <span class="math inline">\(w_i\)</span> observations. Note that a high weight is associated with low variance. In other words, deviations from the mean <span class="math inline">\(\beta_0 + \sum_{j=1}^p \beta_j x_{ij}\)</span> will be less tolerated as the weights increase. Equivalently, we have:
<span class="math display">\[
Y_i \sim \mathcal{N}or\left(\beta_0 + \sum_{j=1}^p \beta_j x_{ij}, \frac{\sigma^2}{w_i}\right), \hspace{2mm} i=1,2,\ldots,n.
\]</span></p>
<p>The matrix formalism is also useful for analyzing this model. It is easy to see that the linear regression model can be rewritten vectorially as in <span class="math inline">\(\eqref{Rel0}\)</span>, where <span class="math inline">\({\boldsymbol{Y}}\)</span>, <span class="math inline">\(\boldsymbol{\beta}\)</span>, <span class="math inline">\({\boldsymbol{X}}\)</span> are as defined previously, and <span class="math inline">\(\boldsymbol{\epsilon} \sim \mathcal{N}or_n(\boldsymbol{0}, \sigma^2\boldsymbol{W})\)</span>, with:
<span class="math display">\[
\boldsymbol{W}= \begin{pmatrix}
1/w_1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; 1/w_2 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; 1/w_n
\end{pmatrix}.
\]</span></p>
</div>
<div id="parameter-estimation-1" class="section level4 hasAnchor" number="10.7.14.2">
<h4><span class="header-section-number">10.7.14.2</span> Parameter Estimation<a href="chap9.html#parameter-estimation-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The likelihood function for the observations <span class="math inline">\(y_1, y_2, \ldots, y_n\)</span> is:
<span class="math display">\[\begin{eqnarray*}
\mathcal{L}(\boldsymbol{\beta},\sigma|\boldsymbol{y}) &amp;=&amp;
\frac{1}{(2\pi)^{n/2}\sigma|\boldsymbol{W}|^{1/2}}
\exp\left(-\frac{1}{2\sigma^2}(\boldsymbol{y}-{\boldsymbol{X}}\boldsymbol{\beta})^\top\boldsymbol{W}^{-1}
(\boldsymbol{y}-{\boldsymbol{X}}\boldsymbol{\beta})\right).
\end{eqnarray*}\]</span>
Regardless of the value of <span class="math inline">\(\sigma^2\)</span>, maximizing <span class="math inline">\(L(\boldsymbol{\beta},\sigma|\boldsymbol{y}) = \ln \mathcal{L}(\boldsymbol{\beta},\sigma|\boldsymbol{y})\)</span> with respect to <span class="math inline">\(\boldsymbol{\beta}\)</span> is equivalent to minimizing:
<span class="math display">\[\begin{eqnarray*}
S_2(\boldsymbol{\beta}) &amp;=&amp;
({\boldsymbol{y}}-{\boldsymbol{X}}\boldsymbol{\beta})^\top\boldsymbol{W}^{-1}
({\boldsymbol{y}}-{\boldsymbol{X}}\boldsymbol{\beta})\\
&amp;=&amp; \sum_{i=1}^n w_i(y_i-\boldsymbol{x}_i^\top\boldsymbol{\beta})^2,
\end{eqnarray*}\]</span>
where the column vector <span class="math inline">\({\boldsymbol{x}}_i\)</span> contains the elements of the <span class="math inline">\(i\)</span>th row of matrix <span class="math inline">\({\boldsymbol{X}}\)</span>. This means minimizing a weighted sum of squares of the differences between the response <span class="math inline">\(y_i\)</span> and the linear predictor <span class="math inline">\(\boldsymbol{x}_i^\top\boldsymbol{\beta}\)</span>. Observations for which the weight <span class="math inline">\(w_i\)</span> is high will be penalized during the minimization of <span class="math inline">\(S_2(\boldsymbol{\beta})\)</span>: less deviation between <span class="math inline">\(y_i\)</span> and <span class="math inline">\(\boldsymbol{x}_i^\top\boldsymbol{\beta}\)</span> will be tolerated for these indices.</p>
<p>To find <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> that minimizes <span class="math inline">\(S_2\)</span>, we need to find the stationary point of this expression. It is obtained by taking the derivative of <span class="math inline">\(S_2\)</span> with respect to <span class="math inline">\(\boldsymbol{\beta}\)</span> and setting the gradient equal to <span class="math inline">\(\boldsymbol{0}\)</span>. The maximum likelihood estimator <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> of <span class="math inline">\(\boldsymbol{\beta}\)</span> satisfies the normal equations:
<span class="math display">\[
{\boldsymbol{X}}^\top\boldsymbol{W}{\boldsymbol{X}}\widehat{\boldsymbol{\beta}}-{\boldsymbol{X}}^\top\boldsymbol{W}{\boldsymbol{Y}}=0,
\]</span>
which will give:
<span class="math display">\[\begin{equation}
\label{eqI5}
\widehat{\boldsymbol{\beta}} = \big({\boldsymbol{X}}^\top\boldsymbol{W}{\boldsymbol{X}}\big)^{-1}{\boldsymbol{X}}^\top\boldsymbol{W}{\boldsymbol{Y}},
\end{equation}\]</span>
defining the maximum likelihood estimator of <span class="math inline">\(\boldsymbol{\beta}\)</span>.</p>
</div>
<div id="projection-matrix" class="section level4 hasAnchor" number="10.7.14.3">
<h4><span class="header-section-number">10.7.14.3</span> Projection Matrix<a href="chap9.html#projection-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Having obtained the estimate of the vector <span class="math inline">\(\boldsymbol{\beta}\)</span>, we can define an estimator <span class="math inline">\(\widehat{{\boldsymbol{Y}}}={\boldsymbol{X}}\widehat{\boldsymbol{\beta}}\)</span> for the mean of the vector <span class="math inline">\({\boldsymbol{Y}}\)</span> and the vector of residuals <span class="math inline">\(\widehat{\boldsymbol{\epsilon}}={\boldsymbol{Y}}-\widehat{{\boldsymbol{Y}}}\)</span>. Note that we can still write:
<span class="math display">\[
\widehat{{\boldsymbol{Y}}}={\boldsymbol{X}}\big({\boldsymbol{X}}^\top\boldsymbol{W}{\boldsymbol{X}}\big)^{-1}{\boldsymbol{X}}^\top\boldsymbol{W}{\boldsymbol{Y}}.
\]</span>
By defining the projection matrix (hat matrix) as the square <span class="math inline">\(n\times n\)</span> matrix <span class="math inline">\(\boldsymbol{H}\)</span>:
<span class="math display">\[
{\boldsymbol{H}}={\boldsymbol{X}}\big({\boldsymbol{X}}^\top\boldsymbol{W}{\boldsymbol{X}}\big)^{-1}{\boldsymbol{X}}^\top\boldsymbol{W},
\]</span>
we can see that this matrix maps <span class="math inline">\({\boldsymbol{Y}}\)</span> to <span class="math inline">\(\widehat{{\boldsymbol{Y}}}\)</span> since <span class="math inline">\(\widehat{\boldsymbol{Y}}=\boldsymbol{X}\widehat{\boldsymbol{\beta}}={\boldsymbol{H}}\boldsymbol{Y}\)</span>. Note that <span class="math inline">\(\boldsymbol{H}\)</span> does not depend on the <span class="math inline">\(\boldsymbol{Y}\)</span> but only on the weights <span class="math inline">\(\boldsymbol{W}\)</span> and the regressors <span class="math inline">\(\boldsymbol{X}\)</span>.</p>
</div>
</div>
</div>
<div id="additive-models" class="section level2 hasAnchor" number="10.8">
<h2><span class="header-section-number">10.8</span> Additive Models<a href="chap9.html#additive-models" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="principle-5" class="section level3 hasAnchor" number="10.8.1">
<h3><span class="header-section-number">10.8.1</span> Principle<a href="chap9.html#principle-5" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the score <span class="math inline">\(\eta_i\)</span>, we cannot be sure that continuous variables will intervene linearly. Thus, if the first variable is continuous (think, for example, of the age of the insured person), it might be advantageous to consider a model of the form:
<span class="math display">\[
Y_i \sim \mathcal{N}or(\eta_i, \sigma^2) \text{ where } \eta_i = \beta_0 + f(x_{i1}) + \sum_{j=2}^p \beta_j x_{ij}.
\]</span>
The function <span class="math inline">\(f\)</span>, to be estimated based on the data, will express the relationship between the score <span class="math inline">\(\eta_i\)</span> and <span class="math inline">\(x_{i1}\)</span>, taking into account the other variables <span class="math inline">\(x_{i2},\ldots,x_{ip}\)</span>. This is precisely the advantage of this approach compared to the prior determination of classes for a quantitative variable: when this approach is part of a regression model, it corrects for the other explanatory variables.</p>
<div class="remark">
<p><span id="unlabeled-div-24" class="remark"><em>Remark</em>. </span>Another, more traditional approach is to substitute various transformations of the variable (e.g., using polynomials, sinusoids, etc.) to capture non-linear influence. However, this approach is much less convincing than estimating <span class="math inline">\(f\)</span> directly. It leads to an increase in the number of parameters to estimate and imposes an arbitrary choice of transformations for the explanatory variables (which can be erroneous).</p>
</div>
<p>The innovation of additive models is to allow the score to be a non-linear additive function of the covariates. Specifically, we will consider the form:
<span class="math display">\[
\eta = c + \sum_{j=1}^p f_j(x_j)
\]</span>
for the score, where the functions <span class="math inline">\(f_j(\cdot)\)</span>, <span class="math inline">\(j=1,\ldots,p\)</span>, assumed to be smooth, reflect the influence of the explanatory variables <span class="math inline">\(x_1,\ldots,x_p\)</span> on the response <span class="math inline">\(y\)</span>. If all functions <span class="math inline">\(f_j\)</span> are linear, we are reduced to the linear model.</p>
<p>To explain how an additive model is fitted, we will proceed in two successive steps. We will start by addressing the simple model <span class="math inline">\(y=f(x)+\epsilon\)</span> where the error <span class="math inline">\(\epsilon\)</span> is normally distributed. We will see how it is possible to estimate the unknown function <span class="math inline">\(f(\cdot)\)</span>, which reflects the influence of <span class="math inline">\(x\)</span> on <span class="math inline">\(y\)</span>, using smoothing or local linear fitting techniques. Then, we will move on to the case of multiple explanatory variables <span class="math inline">\(y=\sum_{j=1}^p f_j(x_j)+\epsilon\)</span>.</p>
</div>
<div id="single-regressor-case" class="section level3 hasAnchor" number="10.8.2">
<h3><span class="header-section-number">10.8.2</span> Single Regressor Case<a href="chap9.html#single-regressor-case" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s begin by considering the following elementary model. We have <span class="math inline">\(n\)</span> observations <span class="math inline">\((x_i,y_i)\)</span>, <span class="math inline">\(i=1,\ldots,n\)</span>, where the response variable <span class="math inline">\(y_i\)</span> and the regressor <span class="math inline">\(x_i\)</span> are continuous. The influence of <span class="math inline">\(x_i\)</span> on <span class="math inline">\(y_i\)</span> is modeled using a function <span class="math inline">\(f(\cdot)\)</span>, assumed to be smooth, plus additive noise, i.e.:
<span class="math display">\[\begin{equation}
\label{AM1}
y_i = f(x_i) + \epsilon_i
\end{equation}\]</span>
where the errors <span class="math inline">\(\epsilon_i\)</span> are assumed to be independent with a <span class="math inline">\(\mathcal{N}or(0,\sigma^2)\)</span> distribution. The specification <span class="math inline">\(\eqref{AM1}\)</span> allows us to free ourselves from the linearity constraint imposed in classical regression (where <span class="math inline">\(f(x_i)=\beta_0+\beta_1x_i\)</span>). The estimation of <span class="math inline">\(f(\cdot)\)</span> will be done using smoothing techniques, and we will focus specifically on linear smoothers, in the sense that <span class="math inline">\(\widehat{f}(x_i)\)</span> can be expressed as a linear combination of the values <span class="math inline">\(y_1,\ldots,y_n\)</span>, i.e.:
<span class="math display">\[\begin{equation}
\label{FormLin}
\widehat{f}(x_i) = \sum_{j=1}^n h_{ij} y_j
\end{equation}\]</span>
where the weights <span class="math inline">\(h_{ij}=h(x_i,x_j)\)</span> depend on the location <span class="math inline">\(x_i\)</span> where the response <span class="math inline">\(f(x_i)\)</span> needs to be estimated. If we define the vector <span class="math inline">\(\boldsymbol{f}=(f(x_1),\ldots,f(x_n))^\top\)</span>, <span class="math inline">\(\eqref{FormLin}\)</span> can be rewritten as <span class="math inline">\(\widehat{\boldsymbol{f}}=\boldsymbol{H}\boldsymbol{y}\)</span>.</p>
<div id="loess-method" class="section level4 hasAnchor" number="10.8.2.1">
<h4><span class="header-section-number">10.8.2.1</span> Loess Method<a href="chap9.html#loess-method" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>Principle: Weighted Least Squares</strong></p>
<p>This method proposed by <span class="citation">(<a href="#ref-cleveland1979robust" role="doc-biblioref">Cleveland 1979</a>)</span> is part of the family of local polynomial regressions. It consists of locally approximating <span class="math inline">\(f(\cdot)\)</span> with a line (note that it is also possible to locally approximate <span class="math inline">\(f\)</span> with a constant, in which case, we obtain weighted moving averages, or with a polynomial of degree 2. Here, we consider only local linear fitting). This approach was developed by <span class="citation">(<a href="#ref-cleveland1988locally" role="doc-biblioref">Cleveland and Devlin 1988</a>)</span> and <span class="citation">(<a href="#ref-cleveland1991computational" role="doc-biblioref">Cleveland and Grosse 1991</a>)</span>. The idea is to use the <span class="math inline">\(\lambda\)</span> nearest neighbors of <span class="math inline">\(x\)</span> to estimate <span class="math inline">\(f(x)\)</span>. The neighborhood is defined based on the explanatory variables: we use the <span class="math inline">\(\lambda\)</span> observations with explanatory variables closest to <span class="math inline">\(x\)</span> to estimate the response <span class="math inline">\(f(x)\)</span>.</p>
<p>The Loess method can be broken down as follows: given the <span class="math inline">\(n\)</span> observations <span class="math inline">\((x_i,y_i)\)</span>, <span class="math inline">\(i=1,2,\ldots,n\)</span>,</p>
<ol style="list-style-type: decimal">
<li>Identify the <span class="math inline">\(\lambda\)</span> nearest neighbors of <span class="math inline">\(x\)</span> (let <span class="math inline">\(\mathcal{V}(x)\)</span> be the set of these neighbors).</li>
<li>Calculate the distance <span class="math inline">\(\Delta(x)\)</span> between <span class="math inline">\(x\)</span> and the furthest of its <span class="math inline">\(\lambda\)</span> nearest neighbors as
<span class="math display">\[
\Delta(x) = \max_{i\in\mathcal{V}(x)} |x-x_i|;
\]</span></li>
<li>Assign weights
<span class="math display">\[
w_i(x) = K\left(\frac{|x-x_i|}{\Delta(x)}\right)
\]</span>
to each element in <span class="math inline">\(\mathcal{V}(x)\)</span>. The function <span class="math inline">\(K(\cdot)\)</span> assigning weights <span class="math inline">\(w_i(x)\)</span> to the observations in <span class="math inline">\(\mathcal{V}(x)\)</span> should have the following properties:</li>
</ol>
<ul>
<li><span class="math inline">\(K(u) \geq 0\)</span> for all <span class="math inline">\(u\)</span></li>
<li><span class="math inline">\(K(u) = 0\)</span> for <span class="math inline">\(u &gt; 1\)</span></li>
<li><span class="math inline">\(K\)</span> is non-decreasing on (0,1).
As suggested by <span class="citation">(<a href="#ref-cleveland1979robust" role="doc-biblioref">Cleveland 1979</a>)</span>, we will use the function <span class="math inline">\(K(\cdot)\)</span> given by
<span class="math display">\[
K(u) = \left\{
\begin{array}{l}
(1-u^3)^3\text{ for }0\leq u &lt; 1\\
0\text{ otherwise}.
\end{array}
\right.
\]</span>
This way, more weight is given to points in the nearest neighborhood of <span class="math inline">\(x\)</span>.</li>
</ul>
<ol start="4" style="list-style-type: decimal">
<li><span class="math inline">\(\widehat{f}(x)\)</span> is obtained by regressing the <span class="math inline">\(y_i\)</span>, <span class="math inline">\(i\in\mathcal{V}(x)\)</span>, on the corresponding <span class="math inline">\(x_i\)</span> using weighted least squares, and then using the regression line to predict the response corresponding to <span class="math inline">\(x\)</span>.</li>
</ol>
<p>This approach provides a response in the form of <span class="math inline">\(\eqref{FormLin}\)</span>.</p>
<p>In the case we are dealing with here, that is, with a single regressor, the fitted value <span class="math inline">\(\widehat{y}_i=\widehat{\beta}_0(x_i) +\widehat{\beta}_1(x_i)x_i\)</span> is obtained by determining <span class="math inline">\(\widehat{\beta}_0(x_i)\)</span>
and <span class="math inline">\(\widehat{\beta}_1(x_i)\)</span> in a way that minimizes
<span class="math display">\[
\sum_{k\in\mathcal{V}(x_i)}w_k(x_i)\big(y_k-{\beta}_0(x_i)-{\beta}_1(x_i)x_k\big)^2
\]</span>
which yields
<span class="math display">\[
\widehat{y}_i=\sum_{k=1}^nh_k(x_i)y_k,
\]</span>
where <span class="math inline">\(h_k(x_i)\)</span> does not depend on the <span class="math inline">\(y_j\)</span>, <span class="math inline">\(j=1,\ldots,n\)</span> (<span class="math inline">\(h_k(x_i)\)</span> depends only
on the regressors).</p>
<p>If we are interested in the response for an unobserved value <span class="math inline">\(x\)</span>, the model used to estimate <span class="math inline">\(f(x)\)</span> is therefore
<span class="math display">\[
y_i=\beta_0(x)+\beta_1(x)x_i+\epsilon_i\text{ for }i\in\mathcal{V}(x)
\]</span>
where the estimates <span class="math inline">\(\widehat{\beta}_0(x)\)</span> and <span class="math inline">\(\widehat{\beta}_1(x)\)</span> of the parameters <span class="math inline">\(\beta_0(x)\)</span> and <span class="math inline">\(\beta_1(x)\)</span> are obtained by minimizing
<span class="math display">\[
\sum_{k\in\mathcal{V}(x)}w_k(x)\big(y_k-{\beta}_0(x)-{\beta}_1(x)x_k\big)^2.
\]</span>
This ultimately gives <span class="math inline">\(\widehat{f}(x)=\widehat{\beta}_0(x) +\widehat{\beta}_1(x)x\)</span>.</p>
<p><strong>Confidence Intervals</strong></p>
<p>If we denote <span class="math inline">\(\widehat{\boldsymbol{y}}\)</span> as the vector of fitted values <span class="math inline">\((\widehat{f}(x_1),\ldots,\widehat{f}(x_n))^\top\)</span> and <span class="math inline">\(\widehat{\boldsymbol{\varepsilon}}\)</span> as the vector of residuals, we have
<span class="math display">\[
\widehat{\boldsymbol{y}} = \boldsymbol{H}\boldsymbol{y}\text{ and }\widehat{\boldsymbol{\varepsilon}} = (\boldsymbol{I}-\boldsymbol{H})\boldsymbol{y}.
\]</span>
Therefore, both <span class="math inline">\(\widehat{\boldsymbol{y}}\)</span> and <span class="math inline">\(\widehat{\boldsymbol{\varepsilon}}\)</span> follow multivariate normal distributions with variance-covariance matrices <span class="math inline">\(\sigma^2\boldsymbol{H}\boldsymbol{H}^\top\)</span> and <span class="math inline">\(\sigma^2(\boldsymbol{I}-\boldsymbol{H})(\boldsymbol{I}-\boldsymbol{H})^\top\)</span>, respectively.
This allows us to obtain confidence intervals for <span class="math inline">\(f(x)\)</span>. If we define
<span class="math display">\[
\delta_k = \text{Trace}\Big((\boldsymbol{I}-\boldsymbol{H})(\boldsymbol{I}-\boldsymbol{H})^\top\Big)^k\text{ for }k=1,2,
\]</span>
we can easily see that
<span class="math display">\[
\mathbb{E}\left[\sum_{i=1}^n\widehat{\epsilon}_i^2\right] = \sigma^2\delta_1
\]</span>
so that
<span class="math display">\[
\widehat{\sigma^2} = \frac{1}{\delta_1}\sum_{i=1}^n\widehat{\epsilon}_i^2
\]</span>
is an unbiased estimator of <span class="math inline">\(\sigma^2\)</span>. Furthermore,
<span class="math display">\[
\frac{\delta_1^2}{\delta_2}\times\frac{\widehat{\sigma}^2}{\sigma^2}
\]</span>
approximately follows the chi-squared distribution with <span class="math inline">\(\delta_1^2/\delta_2\)</span> degrees
of freedom (where <span class="math inline">\(\delta_1^2/\delta_2\)</span> is rounded to the nearest integer).
Therefore,
<span class="math display">\[
\frac{\widehat{f}(x)-f(x)}{\widehat{\sigma}\sqrt{\sum_{i=1}^nh_i^2(x)}}
\]</span>
approximately follows the Student’s t-distribution with <span class="math inline">\(\delta_1^2/\delta_2\)</span> degrees of freedom. This allows
for obtaining confidence intervals for the response <span class="math inline">\(f(\cdot)\)</span> at different points <span class="math inline">\(x\)</span>.</p>
<p><strong>Smoothing Parameter</strong></p>
<p>As the reader may have noticed, the Loess approach depends on the number <span class="math inline">\(\lambda\)</span> of points contained in the neighborhood <span class="math inline">\(\mathcal{V}(x_0)\)</span> of the considered point <span class="math inline">\(x_0\)</span>. The number of nearest neighbors, most often expressed as a percentage of the dataset size, acts as the smoothing parameter. The selection of an optimal value for <span class="math inline">\(\lambda\)</span> is discussed below.</p>
<p>When comparing models, it is helpful to have a measure of their complexity. This can be done using a degree of freedom associated with the smoothers satisfying <span class="math inline">\(\eqref{FormLin}\)</span>. The number of degrees of freedom <span class="math inline">\(DF_\lambda\)</span> is provided by the trace of the matrix <span class="math inline">\(\boldsymbol{H}_\lambda\)</span>, whose elements are the <span class="math inline">\(h_{ij}\)</span> involved in <span class="math inline">\(\eqref{FormLin}\)</span>. This choice stems from the fact that in the classical linear regression model, as seen earlier, the trace of the matrix that maps observations <span class="math inline">\(y_i\)</span> to fitted values <span class="math inline">\(\widehat{y}_i=\widehat{\boldsymbol{\beta}}^\top\boldsymbol{x}_i\)</span> equals the number of parameters (i.e., the dimension of <span class="math inline">\(\boldsymbol{\beta}\)</span>).</p>
<p><strong>Fit Quality Measure</strong></p>
<p>Measuring the quality of fit is more complex for models that aim to estimate a function than for classical parametric models. Most criteria involve both a measure of fit quality and a measure of model complexity. Minimizing such criteria also helps in selecting the smoothing parameter.</p>
<p>Let <span class="math inline">\(\widehat{\sigma}^2\)</span> be the sum of squared residuals. <span class="citation">(<a href="#ref-hurvich1998smoothing" role="doc-biblioref">Hurvich, Simonoff, and Tsai 1998</a>)</span> proposed the following criterion:
<span class="math display">\[
AICC1=n\ln\widehat{\sigma}^2+n\frac{\delta_1/\delta_2(n+DF_\lambda)}{\delta_1^2/\delta_2-2}
\]</span>
called AIC corrected. The first term in AICC1 measures the quality of the fit, while the second term assesses the complexity of the model.
This criterion helps select the optimal value of <span class="math inline">\(\lambda\)</span> (i.e., the one minimizing AICC1).</p>
<p><strong>Extension to Multiple Regressors</strong></p>
<p>Extending the Loess method to more than one regressor is straightforward.
In this case, we consider a model of the form
<span class="math display">\[
y_i=f(x_{i1},x_{i2},\ldots,x_{ip})+\epsilon_i.
\]</span>
Note that this model is not additive.
The approach involves locally approximating the function <span class="math inline">\(f\)</span> of the explanatory variables by a hyperplane and then proceeding as explained above for the single regressor case. It is important in this case that the different regressors have comparable values (so that multidimensional neighborhoods are not determined solely based on the variable with the largest values). To achieve this, the explanatory variables are often normalized (e.g., by dividing them by the interquartile range). The distance used is typically the Euclidean distance in <span class="math inline">\({\mathbb{R}}^p\)</span>.</p>
<div class="remark">
<p><span id="unlabeled-div-25" class="remark"><em>Remark</em>. </span>Depending on the size of the dataset, one can consider using “k-d trees” to define multidimensional neighborhoods.</p>
</div>
</div>
<div id="SubSubSplines" class="section level4 hasAnchor" number="10.8.2.2">
<h4><span class="header-section-number">10.8.2.2</span> Penalized Maximum Likelihood and Cubic Splines<a href="chap9.html#SubSubSplines" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>Principle: Penalized Least Squares</strong></p>
<p>An ingenious way to estimate the function <span class="math inline">\(f(\cdot)\)</span> in <span class="math inline">\(\eqref{AM1}\)</span> is to minimize the objective function
<span class="math display">\[\begin{equation}
\label{Obj}
\mathcal{O}(f)=\sum_{i=1}^n\Big(y_i-f(x_i)\Big)^2+\lambda\int_{u\in{\mathbb{R}}}\big(f&#39;&#39;(u)\big)^2du.
\end{equation}\]</span>
The first term in <span class="math inline">\(\mathcal{O}(f)\)</span> ensures that <span class="math inline">\(f(\cdot)\)</span> fits the data as closely as possible, while the second term penalizes excessive irregularity in the fit. This technique assumes that <span class="math inline">\(f(\cdot)\)</span> is twice continuously differentiable and that <span class="math inline">\(f&#39;&#39;(\cdot)\)</span> is square-integrable. The integral in <span class="math inline">\(\eqref{Obj}\)</span> measures the irregularity of the function <span class="math inline">\(f(\cdot)\)</span>. Note that two functions differing only by a linear term will have the same second derivatives and, consequently, the same level of irregularity as measured in <span class="math inline">\(\eqref{Obj}\)</span>. This property is particularly useful in a regression context.</p>
<p>The objective function <span class="math inline">\(\eqref{Obj}\)</span> can be seen as a penalized normal log-likelihood (PML) approach: a penalty term for the irregularity of the estimator is added to the log-likelihood before maximizing it. This approach dates back to the work of actuary E. Whittaker, who used a similar technique to smooth mortality tables as early as 1923. For more details, see, for example, <span class="citation">(<a href="#ref-hastie1990generalized" role="doc-biblioref">Hastie and Tibshirani 1990</a>)</span>.</p>
<p>When <span class="math inline">\(\lambda\to +\infty\)</span>, the term penalizing the irregularity of <span class="math inline">\(f(\cdot)\)</span> dominates, forcing the second derivative of <span class="math inline">\(f(\cdot)\)</span> to vanish everywhere, resulting in a straight line as the solution. For large values of <span class="math inline">\(\lambda\)</span>, the integral dominates in <span class="math inline">\(\eqref{Obj}\)</span>, and the resulting estimator will have very low curvature. Conversely, when <span class="math inline">\(\lambda\to 0\)</span>, the penalty disappears, and a perfect interpolation is obtained (when the <span class="math inline">\(x_i\)</span> are distinct).</p>
<p>Suppose first that <span class="math inline">\(x_1&lt;x_2&lt;\ldots &lt;x_n\)</span>.
The solution <span class="math inline">\(\widehat{f}_\lambda\)</span> of the minimization of <span class="math inline">\(\eqref{Obj}\)</span> is a cubic spline
with knots at <span class="math inline">\(x_1,x_2,\ldots,x_n\)</span> (meaning that <span class="math inline">\(\widehat{f}_\lambda\)</span> coincides
with a cubic polynomial on each interval <span class="math inline">\((x_i,x_{i+1})\)</span> and has continuous first and
second derivatives at each <span class="math inline">\(x_i\)</span>). This allows us to reduce the minimization of <span class="math inline">\(\eqref{Obj}\)</span> to that of
<span class="math display">\[\begin{equation}
\label{ObjNew}
(\boldsymbol{y}-\boldsymbol{f})^\top(\boldsymbol{y}-\boldsymbol{f})+\lambda\boldsymbol{f}^\top\boldsymbol{K}\boldsymbol{f}
\end{equation}\]</span>
where <span class="math inline">\(\boldsymbol{y}^\top=(y_1,\cdots,y_n)\)</span>, <span class="math inline">\(\boldsymbol{f}^\top=(f(x_1),\cdots,f(x_n))\)</span>, and
<span class="math inline">\(\boldsymbol{K}=\boldsymbol{D}^\top\boldsymbol{C}^{-1}\boldsymbol{D}\)</span>
with <span class="math inline">\(\boldsymbol{D}\)</span> being a tri-diagonal matrix of size <span class="math inline">\((n-2)\times n\)</span> given by
{
<span class="math display">\[
\boldsymbol{D}=\left(
\begin{array}{cccccccc}
\frac{1}{\Delta_1}&amp;-\left(\frac{1}{\Delta_1}+\frac{1}{\Delta_2}\right) &amp; \frac{1}{\Delta_2} &amp; 0 &amp; 0&amp;\cdots &amp; 0 \\
0 &amp; \frac{1}{\Delta_2} &amp; -\left(\frac{1}{\Delta_2}+\frac{1}{\Delta_3}\right) &amp; \frac{1}{\Delta_3} &amp; 0&amp; \cdots &amp; 0 \\
0 &amp; 0&amp;\frac{1}{\Delta_3} &amp; -\left(\frac{1}{\Delta_3}+\frac{1}{\Delta_4}\right) &amp; \frac{1}{\Delta_4} &amp;  \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots&amp;\vdots&amp;\ddots &amp; \vdots \\
0&amp;0&amp;0&amp;0&amp;0&amp;\cdots &amp; \frac{1}{\Delta_{n-1}}\\
0&amp;0&amp;0&amp;0&amp;0&amp;\cdots &amp; -\left(\frac{1}{\Delta_{n-2}}+\frac{1}{\Delta_{n-1}}\right)\\
0&amp;0&amp;0&amp;0&amp;0&amp;\cdots&amp; \frac{1}{\Delta_{n-2}}
\end{array}
\right),
\]</span>
}
where <span class="math inline">\(\Delta_i=x_{i+1}-x_i\)</span>, and <span class="math inline">\(\boldsymbol{C}\)</span> is a symmetric tri-diagonal matrix of size
<span class="math inline">\((n-2)\times (n-2)\)</span> given by
{
<span class="math display">\[
\boldsymbol{C}=\frac{1}{6}\left(
\begin{array}{cccccc}
2\left(\Delta_1+\Delta_2\right) &amp; \Delta_2 &amp; 0 &amp; \cdots &amp; 0&amp;0 \\
\Delta_2 &amp; 2(\Delta_2+\Delta_3) &amp; \Delta_3 &amp;  \cdots &amp; 0&amp;0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots   \\
0&amp;0&amp;0&amp;\cdots &amp; 2(\Delta_{n-3}+\Delta_{n-2})&amp;\Delta_{n-2}\\
0&amp;0&amp;0&amp;\cdots &amp; \Delta_{n-2} &amp; 2(\Delta_{n-2}+\Delta_{n-1})
\end{array}
\right).
\]</span>
}
The solution <span class="math inline">\(\widehat{f}_\lambda\)</span> can then be obtained by setting the gradient <span class="math inline">\(-2(\boldsymbol{y}-\boldsymbol{f})+2\lambda\boldsymbol{K}\boldsymbol{f}\)</span> of the objective function <span class="math inline">\(\eqref{ObjNew}\)</span> to zero, which gives
<span class="math display">\[
\widehat{\boldsymbol{f}}_\lambda=(\boldsymbol{I}+\lambda\boldsymbol{K})^{-1}\boldsymbol{y}
\]</span>
which has the form <span class="math inline">\(\eqref{FormLin}\)</span> with <span class="math inline">\(\boldsymbol{H}=(\boldsymbol{I}+\lambda\boldsymbol{K})^{-1}\)</span>. Note that it is not necessary to explicitly invert <span class="math inline">\(\boldsymbol{I}+\lambda\boldsymbol{K}\)</span>
to obtain <span class="math inline">\(\widehat{f}_\lambda\)</span>, and it is more efficient to use numerical techniques such as the Reinsch algorithm.</p>
<div class="remark">
<p><span id="unlabeled-div-26" class="remark"><em>Remark</em>. </span>In the case where each observation <span class="math inline">\((x_i,y_i)\)</span> is weighted
with <span class="math inline">\(w_i\)</span>, the objective function becomes
<span class="math display">\[
\mathcal{O}_w(f)=\sum_{i=1}^nw_i\Big(y_i-f(x_i)\Big)^2+\lambda\int_{u\in{\mathbb{R}}}\big(f&#39;&#39;(u)\big)^2du.
\]</span>
Again, the minimum of <span class="math inline">\(\mathcal{O}_w(f)\)</span> is obtained by using a cubic spline for <span class="math inline">\(f\)</span>, which allows us to write
<span class="math display">\[
\mathcal{O}_w(f)=(\boldsymbol{y}-\boldsymbol{f})^\top\boldsymbol{W}(\boldsymbol{y}-\boldsymbol{f})+\lambda\boldsymbol{f}^\top\boldsymbol{K}\boldsymbol{f}
\]</span>
where the diagonal matrix <span class="math inline">\(\boldsymbol{W}\)</span> contains the weights. The solution is then given by
<span class="math display">\[
\widehat{\boldsymbol{f}}_\lambda=(\boldsymbol{W}+\lambda\boldsymbol{K})^{-1}\boldsymbol{W}\boldsymbol{y}.
\]</span>
The weight <span class="math inline">\(w_i\)</span> can, for example, represent the number of observations <span class="math inline">\(y_i\)</span> in the sample corresponding to the same value <span class="math inline">\(x_i\)</span>.</p>
</div>
<p><strong>Smoothing Parameter</strong></p>
<p>Measuring the complexity of the model is done exactly as in Loess. The choice of <span class="math inline">\(\lambda\)</span> is often made using the cross-validation criterion:
<span class="math display">\[
CV(\lambda)=\frac{1}{n}\sum_{i=1}^n\left(y_i-\widehat{f}_\lambda^{-i}(x_i)\right)^2
\]</span>
where <span class="math inline">\(\widehat{f}_\lambda^{-i}(x_i)\)</span> is the estimate of <span class="math inline">\(f(x_i)\)</span> obtained using the sample <span class="math inline">\(\{(y_j,x_j),\hspace{2mm} j\neq i\}\)</span>, of size <span class="math inline">\(n-1\)</span>.
The cross-validation criterion favors the predictive power of the selected model (while the sum of squares of residuals favors the quality of the fit to a given set of observations). In fact, <span class="math inline">\(\widehat{f}_\lambda^{-i}(x_i)\)</span> is the prediction of <span class="math inline">\(y_i\)</span> provided by the data excluding the <span class="math inline">\(i\)</span>-th observation, so the difference <span class="math inline">\(y_i-\widehat{f}_\lambda^{-i}(x_i)\)</span> measures the quality of the prediction provided by the model.</p>
<p>The value of <span class="math inline">\(\widehat{f}_\lambda^{-i}(x_i)\)</span> for cubic splines is given by:
<span class="math display">\[
\widehat{f}_\lambda^{-i}(x_i)=\sum_{j\neq i}\frac{h_{ij}}{1-h_{ii}}y_j
\]</span>
where the weights <span class="math inline">\(\frac{h_{ij}}{1-h_{ii}}\)</span> sum to 1.
The idea is straightforward: zero weight is assigned to observation <span class="math inline">\(i\)</span>, and the weights are normalized (to have a sum equal to 1). Therefore:
<span class="math display">\[
\widehat{f}_\lambda^{-i}(x_i)=\frac{1}{1-h_{ii}}\widehat{f}_\lambda(x_i)-\frac{h_{ii}}{1-h_{ii}}y_i
\]</span>
so that:
<span class="math display">\[
CV(\lambda)=\frac{1}{n}\sum_{i=1}^n\left(\frac{y_i-\widehat{f}_\lambda(x_i)}{1-h_{ii}}\right)^2
\]</span></p>
<div class="remark">
<p><span id="unlabeled-div-27" class="remark"><em>Remark</em>. </span>Sometimes, the generalized cross-validation criterion is also used, replacing <span class="math inline">\(h_{ii}\)</span> with the average <span class="math inline">\(\frac{1}{n}\sum_{i=1}^nh_{ii}\)</span>, which results in:
<span class="math display">\[\begin{eqnarray*}
GCV(\lambda)&amp;=&amp;\frac{1}{n\left(1-\frac{1}{n}\sum_{i=1}^nh_{ii}\right)^2}
\sum_{i=1}^n\left(y_i-\widehat{f}_\lambda(x_i)\right)^2\\
&amp;=&amp;\frac{1}{n\left(1-\frac{DF_\lambda}{n}\right)^2}
\sum_{i=1}^n\left(y_i-\widehat{f}_\lambda(x_i)\right)^2.
\end{eqnarray*}\]</span></p>
</div>
</div>
</div>
<div id="estimation-with-multiple-regressors-backfitting" class="section level3 hasAnchor" number="10.8.3">
<h3><span class="header-section-number">10.8.3</span> Estimation with Multiple Regressors: Backfitting<a href="chap9.html#estimation-with-multiple-regressors-backfitting" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now, suppose that we have observations <span class="math inline">\((y_i,\boldsymbol{x}_i)\)</span>, <span class="math inline">\(i=1,2,\ldots,n\)</span>, where
<span class="math inline">\(\boldsymbol{x}_i^\top=(x_{i1},\ldots,x_{ip})\)</span> are <span class="math inline">\(p\)</span> continuous explanatory variables.
We consider the model:
<span class="math display">\[\begin{equation}
\label{Star}
y_i=c+\sum_{j=1}^pf_j(x_{ij})+\epsilon_i
\end{equation}\]</span>
where the errors <span class="math inline">\(\epsilon_i\)</span> are assumed to be independent with a <span class="math inline">\(\mathcal{N}ormal(0,\sigma^2)\)</span> distribution.
Our goal is to estimate the functions <span class="math inline">\(f_1(\cdot),\ldots,f_p(\cdot)\)</span> representing the effect of the explanatory variables on the response. To achieve this, we will use the backfitting algorithm, which will successively re-adjust the partial residuals for each of the explanatory variables.</p>
<p>Let’s show how model <span class="math inline">\(\eqref{Star}\)</span> can be handled based on the simple case <span class="math inline">\(\eqref{AM1}\)</span>.
The principle consists of, given a first estimation <span class="math inline">\(\widehat{f_k}(\cdot)\)</span> of <span class="math inline">\(f_k(\cdot)\)</span>, <span class="math inline">\(k=1,2,\ldots,p\)</span>, to re-estimate <span class="math inline">\(f_j(\cdot)\)</span> by adjusting the residuals obtained from the <span class="math inline">\(f_k(\cdot)\)</span>, <span class="math inline">\(k\neq j\)</span>. These residuals, denoted as <span class="math inline">\(r_i^{(j)}\)</span>, are given by:
<span class="math display">\[
r_i^{(j)}=y_i-\widehat{c}-\sum_{k\neq j}\widehat{f_k}(x_{ik}),\hspace{2mm}i=1,2,\ldots,n,
\]</span>
which are then fitted to the values of the <span class="math inline">\(j\)</span>-th regressor <span class="math inline">\(x_{ij}\)</span>. This process continues until the results stabilize. The key idea behind the backfitting algorithm is that, for any <span class="math inline">\(j\)</span>:
<span class="math display">\[
\mathbb{E}\left[Y-c-\sum_{k\neq j}f_k(X_k)\Big|X_j\right]=f_j(X_j),
\]</span>
so the residuals <span class="math inline">\(r_i^{(j)}\)</span>, <span class="math inline">\(i=1,\ldots,n\)</span>, reflect the part of the behavior of the dependent variable attributable to the <span class="math inline">\(j\)</span>-th regressor.</p>
<p>From now on, let’s denote by <span class="math inline">\(\boldsymbol{f}_j^\top=(f_j(x_{j1}),\cdots,f_j(x_{jn}))\)</span> the vector of evaluations of <span class="math inline">\(f_j(\cdot)\)</span>
at the observed values of the <span class="math inline">\(j\)</span>-th regressor. As it is obviously possible to absorb the constant <span class="math inline">\(c\)</span> into any of the
functions <span class="math inline">\(f_1,\ldots,f_p\)</span>, from now on, we will take <span class="math inline">\(\widehat{c}=\overline{y}\)</span> for identifiability purposes.
The main idea of the method is to define residuals <span class="math inline">\(r_1^{(j)},\cdots,r_n^{(j)}\)</span> that will be explained using an additive model
with the <span class="math inline">\(j\)</span>-th regressor. More precisely, the algorithm proceeds as follows:</p>
<ul>
<li><strong>Initialization</strong>:
<span class="math inline">\(\widehat{c}\gets\overline{y}\)</span>,
<span class="math inline">\(\widehat{\boldsymbol{f}_j}^{(0)}\gets \boldsymbol{0}\)</span>, <span class="math inline">\(j=1,2,\ldots,p\)</span>.</li>
<li><strong>Cycle</strong>:
For <span class="math inline">\(r=1,2,\ldots,\)</span> and <span class="math inline">\(j=1,2,\ldots,p\)</span>, update <span class="math inline">\(\widehat{\boldsymbol{f}}_j^{(r)}\)</span> as follows:
<span class="math display">\[
\widehat{\boldsymbol{f}}_j^{(r+1)}\gets \boldsymbol{H}_{\lambda_j}\left(\boldsymbol{y}-(\overline{y},\ldots,\overline{y})^\top-\sum_{k&lt;j}\widehat{\boldsymbol{f}}_k^{(r+1)}-
\sum_{k&gt;j}\widehat{\boldsymbol{f}}_k^{(r)}\right)
\]</span>
where <span class="math inline">\(\boldsymbol{H}_{\lambda_j}\)</span> is the smoothing matrix applied to the partial residual obtained
by subtracting the prediction from the observation <span class="math inline">\(\boldsymbol{y}\)</span> using all regressors except the <span class="math inline">\(j\)</span>-th one.</li>
<li><strong>Stopping criterion</strong>:
Iterate the above step and stop when the sum of the squares of residuals
<span class="math display">\[
\left(\boldsymbol{y}-(\overline{y},\ldots,\overline{y})^\top-\sum_{j=1}^p\widehat{\boldsymbol{f}}_k^{(r+1)}\right)^\top
\left(\boldsymbol{y}-(\overline{y},\ldots,\overline{y})^\top-\sum_{j=1}^p\widehat{\boldsymbol{f}}_k^{(r+1)}\right)
\]</span>
ceases to decrease.</li>
</ul>
<p>Note that the influence of each of the regressors can be estimated using a different smoothing parameter.</p>
</div>
<div id="comparison-of-different-approaches" class="section level3 hasAnchor" number="10.8.4">
<h3><span class="header-section-number">10.8.4</span> Comparison of Different Approaches<a href="chap9.html#comparison-of-different-approaches" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s consider simulations with 50 observations following a
model <span class="math inline">\(Y_i=(X_i+1)^2-1+\epsilon_i\)</span>, where the errors
<span class="math inline">\(\epsilon\sim\mathcal{N}ormal(0,1/2)\)</span> are independent, and where
<span class="math inline">\(X_i\sim\mathcal{N}ormal(0,1)\)</span>.</p>
<p>Figure <span class="math inline">\(\ref{figure:regression-lineaire}\)</span> shows the least squares estimation of <span class="math inline">\(Y\)</span> on <span class="math inline">\(X\)</span> on the left, and <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span> on the right. As
shown on the left-hand side, the least squares principle consists of minimizing the sum of squares of distances between
<span class="math inline">\(Y_i\)</span> and <span class="math inline">\(\widehat{Y}_i\)</span> (represented vertically).</p>
<p>Figure <span class="math inline">\(\ref{figure:regression-ACP}\)</span> shows the result of
PCA of the cloud <span class="math inline">\((X_1,Y_1),...,(X_n,Y_n)\)</span>, where the first axis is obtained by
minimizing the sum of squares of distances from <span class="math inline">\((X_i,Y_i)\)</span> to the
line (corresponding to the orthogonal projection of <span class="math inline">\((X_i,Y_i)\)</span>
onto the line). Note that the obtained line is relatively
close to that obtained by linear regression.</p>
</div>
</div>
<div id="Sec96GLM" class="section level2 hasAnchor" number="10.9">
<h2><span class="header-section-number">10.9</span> Generalized Linear Models (GLM)<a href="chap9.html#Sec96GLM" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="a-brief-history-of-actuarial-applications-of-regression-models" class="section level3 hasAnchor" number="10.9.1">
<h3><span class="header-section-number">10.9.1</span> A Brief History of Actuarial Applications of Regression Models<a href="chap9.html#a-brief-history-of-actuarial-applications-of-regression-models" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For a long time, actuaries limited themselves to using the Gaussian linear model when quantifying the impact of explanatory variables on an interest phenomenon (frequency or cost of claims, probability of insured events, etc.). Now that the complexity of statistical problems faced by actuaries has greatly increased, it is crucial to turn to models that better account for the reality of insurance than the linear model does. The linear model imposes a series of constraints that are not very compatible with the reality of numbers or claim costs: approximate Gaussian probability density, linearity of the score, and homoscedasticity. Even though it is possible to overcome some of these constraints by transforming the response variable using well-chosen functions, the linear approach comes with many disadvantages (working on an artificial scale, difficulties in returning to the original quantities, etc.).</p>
<p>A first step in using models more appropriate to the reality of insurance was taken in actuarial science at the end of the 20th century by actuaries in London’s City University, who applied Generalized Linear Models (GLMs). These models, introduced in statistics by <span class="citation">(<a href="#ref-nelder1972generalized" role="doc-biblioref">Nelder and Wedderburn 1972</a>)</span>, allow for the abandonment of the normality assumption by treating responses whose distribution is part of the exponential family in a unified way. This family includes, in addition to the normal distribution, the Poisson, binomial, Gamma, and Inverse Gaussian distributions. Notable works in this area include <span class="citation">(<a href="#ref-gourieroux1984pseudo" role="doc-biblioref">Gourieroux, Monfort, and Trognon 1984</a>)</span>.</p>
<p>Poisson regression (and related models such as negative binomial regression) is now a tool of choice for developing automobile insurance pricing, largely replacing the general linear model and logistic regression for analyzing claim counts. The breakthrough of this method within insurance companies dates back to the inclusion of procedures in widely-used statistical software (with SAS leading the way) that allow for the application of this technique (specifically, the GENMOD procedure). In addition to the maximum likelihood approach, GLM techniques allow the analysis of a wide range of phenomena from a quasi-likelihood perspective by specifying only the mean-variance structure. French econometricians have proven fundamental results regarding the convergence of estimators obtained in this way. See, for example, <span class="citation">(<a href="#ref-gourieroux1984pseudo" role="doc-biblioref">Gourieroux, Monfort, and Trognon 1984</a>)</span>.</p>
<p>Recently, GLM techniques have been successfully applied to life insurance problems (establishing mortality tables, estimating demographic indicators, mortality projection, etc.). See <span class="citation">(<a href="#ref-delwarde2005construction" role="doc-biblioref">Delwarde and Denuit 2005</a>)</span> for numerous examples.</p>
<p>This section is based on <span class="citation">(<a href="#ref-mccullagh1989generalized" role="doc-biblioref">McCullagh and Nelder 1989</a>)</span>, <span class="citation">(<a href="#ref-dobson2001introduction" role="doc-biblioref">Dobson 2001</a>)</span>, and <span class="citation">(<a href="#ref-fahrmeir1994multivariate" role="doc-biblioref">Fahrmeir et al. 1994</a>)</span>.
We emphasize that maximum likelihood estimators can be obtained using repeated weighted least squares adjustments by defining appropriate pseudo-responses. This allows us to relax the assumption of linearity of the score by transitioning to generalized additive models.</p>
<p>To better understand the generalizations of the Gaussian linear model discussed in this section, let us recall that within the framework of this model, we seek to model a variable <span class="math inline">\(Y\)</span> using a set of explanatory variables <span class="math inline">\(\mathbf{X}=(X_1,\ldots,X_p)^\top\)</span>. Naturally, linear regression assumes that
<span class="math display">\[
Y \sim \mathcal{N}(\mu, \sigma^2)\text{ where }\mu = \mathbf{X}^\top\boldsymbol{\beta}.
\]</span>
This model proposed by Legendre and Gauss in the early 19th century, and extensively studied by Fisher in the 1920s, has become established in econometrics but is challenging to use in insurance.</p>
<p>The variables we seek to model in insurance are costs (taking values in <span class="math inline">\(\mathbb{R}^+\)</span>), numbers of claims (taking values in <span class="math inline">\(\mathbb{N}\)</span>), or indicators of being a claimant in a given year (taking values in <span class="math inline">\(\{0,1\}\)</span>). In the latter case, we saw that latent variables could be an interesting solution. Specifically, we considered models of the form
<span class="math display">\[
Y \sim \text{Binomial}(1, \mu)\text{ where }\mu = \mathbb{E}[Y] = F(\mathbf{X}^\top\boldsymbol{\beta}),
\]</span>
where <span class="math inline">\(F\)</span> denotes the cumulative distribution function associated with the logistic distribution (for LOGIT models) or the centered and scaled Gaussian distribution (for PROBIT models).</p>
<p>In general, we want to retain the linear structure of the score with respect to <span class="math inline">\(\boldsymbol{\beta}\)</span> and consider that the expectation of <span class="math inline">\(Y\)</span> is a transformation of this linear combination. More precisely, we now wish to move to regression models of the form
<span class="math display">\[
Y \sim \text{Distribution}(\mu)\text{ where }\mu = \mathbb{E}[Y] = g^{-1}(\mathbf{X}^\top\boldsymbol{\beta}),
\]</span>
where <span class="math inline">\(g^{-1}\)</span> is a “well-chosen” function, and <span class="math inline">\(\text{Distribution}\)</span> denotes a parametric distribution that properly models our variable of interest.</p>
<p>This type of approach is the basis of so-called “generalized linear models,” which extend the Gaussian model to a specific family of distributions known as the exponential (natural) family.</p>
</div>
<div id="definition-38" class="section level3 hasAnchor" number="10.9.2">
<h3><span class="header-section-number">10.9.2</span> Definition<a href="chap9.html#definition-38" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this section, we will focus on the family of generalized linear models. This class includes, in addition to the normal distribution, probability distributions with two parameters <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\phi\)</span>, whose density (discrete or continuous) can be expressed in the form
<span class="math display">\[\begin{equation}
\label{Rel2}
f(y|\theta,\phi) = \exp\left(\frac{y\theta - b(\theta)}{\phi} + c(y,\phi)\right),\quad y \in \mathcal{S},
\end{equation}\]</span>
where the support <span class="math inline">\(\mathcal{S}\)</span> is a subset of <span class="math inline">\(\mathbb{N}\)</span> or <span class="math inline">\(\mathbb{R}\)</span>. The parameter <span class="math inline">\(\theta\)</span> is called the natural parameter, and <span class="math inline">\(\phi\)</span> is the dispersion parameter. Often, weighting is required, and <span class="math inline">\(\phi\)</span> is replaced by <span class="math inline">\(\phi/\omega\)</span>, where <span class="math inline">\(\omega\)</span> is a known a priori weight.</p>
<p>Let’s examine some examples of common distributions whose density can be expressed in the form <span class="math inline">\(\eqref{Rel2}\)</span>.</p>
<p>::: {.example}[Normal Distribution]</p>
<p>The normal distribution <span class="math inline">\(\mathcal{N}(\mu, \sigma^2)\)</span> has a density that can be expressed in the form <span class="math inline">\(\eqref{Rel2}\)</span>, with <span class="math inline">\(\mathcal{S} = \mathbb{R}\)</span>, <span class="math inline">\(\theta = \mu\)</span>, <span class="math inline">\(b(\theta) = \theta^2/2\)</span>, <span class="math inline">\(\phi = \sigma^2\)</span>, and
<span class="math display">\[
c(y,\phi) = -\frac{1}{2}\left(\frac{y^2}{\sigma^2} + \ln(2\pi\sigma^2)\right).
\]</span>
:::</p>
<p>::: {.example}<a href="chap15.html#poisson-distribution-1">Poisson Distribution</a></p>
<p>For the Poisson distribution <span class="math inline">\(\mathcal{P}oi(\lambda)\)</span>, we have
<span class="math display">\[
f(y|\lambda) = \exp(-\lambda)\frac{\lambda^y}{y!} = \exp\left(y\ln\lambda - \lambda - \ln y!\right), \quad y \in \mathbb{N},
\]</span>
where <span class="math inline">\(\mathcal{S} = \mathbb{N}\)</span>, <span class="math inline">\(\theta = \ln\lambda\)</span>, <span class="math inline">\(\phi = 1\)</span>, <span class="math inline">\(b(\theta) = \exp(\theta) = \lambda\)</span>, and <span class="math inline">\(c(y,\phi) = -\ln y!\)</span>.
:::</p>
<p>::: {.example}<a href="chap15.html#binomial-distribution">Binomial Distribution</a></p>
<p>The <span class="math inline">\(\mathcal{B}in(n,p)\)</span> distribution has a density that can be expressed in the form <span class="math inline">\(\eqref{Rel2}\)</span> with <span class="math inline">\(\mathcal{S} = \mathbb{N}\)</span>, <span class="math inline">\(\theta = \ln\{p/(1-p)\}\)</span>, <span class="math inline">\(b(\theta) = n\ln(1+\exp(\theta))\)</span>, <span class="math inline">\(\phi = 1\)</span>, and <span class="math inline">\(c(y,\phi) = \ln\left(\frac{n!}{y!(n-y)!}\right)\)</span>.
:::</p>
<p>::: {.example}<a href="chap15.html#gamma-distribution-1">Gamma Distribution</a></p>
<p>The density associated with the Gamma distribution can be rewritten as
<span class="math display">\[
\frac{1}{\Gamma(\nu)}\left(\frac{\nu}{\mu}\right)^\nu y^{\nu-1}\exp\left(-\frac{\nu}{\mu}y\right),
\]</span>
which can be expressed in the form <span class="math inline">\(\eqref{Rel2}\)</span> with <span class="math inline">\(\mathcal{S} = \mathbb{R}^+\)</span>, <span class="math inline">\(\theta = -\frac{1}{\mu}\)</span>, <span class="math inline">\(b(\theta) = -\ln(-\theta)\)</span>, and <span class="math inline">\(\phi = \nu^{-1}\)</span>.
:::</p>
<p>Not all probability distributions whose density can be expressed in the form <span class="math inline">\(\eqref{Rel2}\)</span> have dispersion parameters <span class="math inline">\(\phi\)</span>. Thus, the examples above teach us, for instance, that for the Poisson distribution, <span class="math inline">\(\phi = 1\)</span>. For distributions with a dispersion parameter <span class="math inline">\(\phi\)</span>, it controls the variance, as we will see later. The pure premium depends only on the natural parameter <span class="math inline">\(\theta\)</span>. Therefore, when actuaries are interested only in the pure premium, the parameter <span class="math inline">\(\theta\)</span> is the parameter of interest, while <span class="math inline">\(\phi\)</span> is considered a nuisance parameter. However, the parameter <span class="math inline">\(\phi\)</span> is also crucial as it controls the dispersion (and hence the risk).</p>
</div>
<div id="mean-and-variance" class="section level3 hasAnchor" number="10.9.3">
<h3><span class="header-section-number">10.9.3</span> Mean and Variance<a href="chap9.html#mean-and-variance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For a random variable <span class="math inline">\(Y\)</span> whose density can be expressed in the form (<span class="math inline">\(\ref{Rel2}\)</span>), we can express the first two moments of <span class="math inline">\(Y\)</span> using the functions <span class="math inline">\(b\)</span> and <span class="math inline">\(c\)</span>. To do this, let
<span class="math display">\[
U = \frac{\partial}{\partial\theta}\ln f(Y|\theta,\phi).
\]</span>
and
<span class="math display">\[
U&#39; = \frac{\partial^2}{\partial\theta^2} \ln f(Y|\theta,\phi),
\]</span>
so that the Fisher information is <span class="math inline">\(\text{Var}[U] = -\mathbb{E}[U&#39;]\)</span> by <span class="math inline">\(\eqref{ExpressFisher}\)</span>.</p>
<div class="proposition">
<p><span id="prp:Prop963MoyVar" class="proposition"><strong>Proposition 10.9  </strong></span>For a random variable <span class="math inline">\(Y\)</span> whose density is of the form (<span class="math inline">\(\ref{Rel2}\)</span>), we have
<span class="math display">\[
\mathbb{E}[Y] = b&#39;(\theta)\text{ and } \mathbb{V}[Y] = \frac{b&#39;&#39;(\theta)\phi}{\omega},
\]</span>
where <span class="math inline">\(&#39;\)</span> and <span class="math inline">\(&#39;&#39;\)</span> denote the first and second derivatives with respect to <span class="math inline">\(\theta\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-28" class="proof"><em>Proof</em>. </span>We know from Proposition <span class="math inline">\(\ref{PropInfoFisher}\)</span> that <span class="math inline">\(\mathbb{E}[U] = 0\)</span>. It suffices to observe that
<span class="math display">\[
\frac{d}{d\theta}\ln f(y|\theta,\phi) = \frac{\partial}{\partial\theta}\left(\frac{y\theta - b(\theta)}{\phi/\omega} + c(y,\phi)\right) = \frac{y - b&#39;(\theta)}{\phi/\omega},
\]</span>
which gives
<span class="math display">\[
\mathbb{E}[U] = \frac{\mathbb{E}[Y] - b&#39;(\theta)}{\phi/\omega} = 0,
\]</span>
yielding the announced expression for the mean of <span class="math inline">\(Y\)</span>.
Furthermore, since <span class="math inline">\(\mathbb{E}[U] = 0\)</span>,
<span class="math display">\[
\mathbb{V}[U] = \mathbb{E}[U^2] = \mathbb{E}\left[\left(\frac{Y - b&#39;(\theta)}{\phi/\omega}\right)^2\right] = \frac{\mathbb{V}[Y]}{(\phi/\omega)^2}
\]</span>
and
<span class="math display">\[\begin{eqnarray*}
\mathbb{E}[U^2] &amp;=&amp;
\int_{y\in\mathcal{S}}\left(\frac{\partial}{\partial\theta}\ln f(y|\theta,\phi)\right)^2f(y|\theta,\phi)dy\\
&amp;=&amp;
\int_{y\in\mathcal{S}}\frac{\partial}{\partial\theta}\ln f(y|\theta,\phi)\frac{\partial}{\partial\theta}f(y|\theta,\phi)dy\\
&amp;=&amp;\mathbb{E}\left[- \frac{\partial^2}{\partial\theta^2}\ln f(Y|\theta,\phi)\right]=\frac{b&#39;&#39;(\theta)}{\phi/\omega}.
\end{eqnarray*}\]</span>
Thus,
<span class="math display">\[
\mathbb{V}[U] = \mathbb{E}[-U&#39;] = \frac{b&#39;&#39;(\theta)}{\phi/\omega}.
\]</span>
Combining the last two equalities gives the desired result.</p>
</div>
<p>Therefore, the variance of <span class="math inline">\(Y\)</span> appears as the product of two functions:</p>
<ol style="list-style-type: decimal">
<li>the first one, <span class="math inline">\(b&#39;&#39;(\theta)\)</span>, which depends solely on the parameter <span class="math inline">\(\theta\)</span>, is called the variance function;</li>
<li>the second one is independent of <span class="math inline">\(\theta\)</span> and depends solely on <span class="math inline">\(\phi\)</span>.</li>
</ol>
<p>Denoting <span class="math inline">\(\mu = \mathbb{E}[Y]\)</span>, we see that the parameter <span class="math inline">\(\theta\)</span> is related to the mean <span class="math inline">\(\mu\)</span> as indicated in Proposition <span class="math inline">\(\ref{Prop963MoyVar}\)</span>. The variance function can thus be defined in terms of <span class="math inline">\(\mu\)</span>; we will henceforth denote it as <span class="math inline">\(V(\mu)\)</span>.</p>
<p>The variance function is very important in various models, as can be seen in Table <span class="math inline">\(\ref{FoncVar}\)</span>.
It is important to note that, except for the normal distribution case, the variance of <span class="math inline">\(Y\)</span> is always a function of the mean and increases with the mean for Poisson, Gamma, and Inverse Gaussian distributions (with a fixed <span class="math inline">\(\phi\)</span> parameter).</p>
<table>
<caption><span id="tab:FoncVar">Table 10.1: </span>Variance functions associated with common probability distributions</caption>
<thead>
<tr class="header">
<th align="center">Probability Distribution</th>
<th align="center"><span class="math inline">\(V(\mu)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Normal</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">Poisson</td>
<td align="center"><span class="math inline">\(\mu\)</span></td>
</tr>
<tr class="odd">
<td align="center">Gamma</td>
<td align="center"><span class="math inline">\(\mu^2\)</span></td>
</tr>
<tr class="even">
<td align="center">Binomial</td>
<td align="center"><span class="math inline">\(\mu(1-\mu)\)</span></td>
</tr>
</tbody>
</table>
</div>
<div id="regression-model" class="section level3 hasAnchor" number="10.9.4">
<h3><span class="header-section-number">10.9.4</span> Regression Model<a href="chap9.html#regression-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider independent but not identically distributed random variables <span class="math inline">\(Y_1,Y_2,\ldots,Y_n\)</span> whose densities are of the form (<span class="math inline">\(\ref{Rel2}\)</span>). More precisely, suppose that the probability density of <span class="math inline">\(Y_i\)</span> is
<span class="math display">\[\begin{equation}
\label{Rel3}
f(y_i|\theta_i,\phi)=\exp\left(\frac{y_i\theta_i-b(\theta_i)}{\phi/\omega_i}+c(y_i,\phi)\right),
\hspace{2mm}y_i\in\mathcal{S}.
\end{equation}\]</span>
Then, the joint density of <span class="math inline">\(Y_1,Y_2,\ldots, Y_n\)</span> is
<span class="math display">\[\begin{eqnarray*}
f(\boldsymbol{y}|\boldsymbol{\theta},\phi)&amp;=&amp;\prod_{i=1}^nf(y_i|\theta_i,\phi)\\
&amp;=&amp;\exp\left(\frac{\sum_{i=1}^ny_i\theta_i-\sum_{i=1}^nb(\theta_i)}{\phi/\omega_i}+
\sum_{i=1}^nc(y_i,\phi)\right).
\end{eqnarray*}\]</span>
Of course, the likelihood is <span class="math inline">\(\mathcal{L}(\boldsymbol{\theta},\phi|\boldsymbol{y})=f(\boldsymbol{y}|\boldsymbol{\theta},\phi)\)</span>.
We assume that the <span class="math inline">\(\theta_i\)</span> are functions of a set of <span class="math inline">\(p+1\)</span> parameters <span class="math inline">\(\beta_0,\beta_1,\ldots,\beta_p\)</span>, for example. More precisely, denoting <span class="math inline">\(\mu_i\)</span> as the mean of <span class="math inline">\(Y_i\)</span>, we assume that
<span class="math display">\[
g(\mu_i)=\beta_0+\sum_{j=1}^p\beta_jx_{ij}=\boldsymbol{x}_i^\top\boldsymbol{\beta}=\eta_i
\]</span>
where
the monotone and differentiable function <span class="math inline">\(g\)</span> is called the link function, the vector <span class="math inline">\(\boldsymbol{x}_i\)</span> contains explanatory variables related to individual <span class="math inline">\(i\)</span>, and the vector <span class="math inline">\(\boldsymbol{\beta}\)</span> contains the <span class="math inline">\(p+1\)</span> parameters.</p>
<p>Thus, a generalized linear model consists of three elements:</p>
<ol style="list-style-type: decimal">
<li>the random variables to be explained <span class="math inline">\(Y_1,Y_2,\ldots,Y_n\)</span> whose densities are of the form (<span class="math inline">\(\ref{Rel3}\)</span>);</li>
<li>a set of parameters
<span class="math inline">\(\boldsymbol{\beta}=(\beta_0,\beta_1,\ldots,\beta_p)^\top\)</span>
belonging to a non-empty open set in <span class="math inline">\({\mathbb{R}}^{p+1}\)</span> and explanatory variables
<span class="math inline">\({\boldsymbol{X}}=(\boldsymbol{x}_1,\boldsymbol{x}_2,\ldots, \boldsymbol{x}_n)^\top\)</span>:
the <span class="math inline">\(n\times (p+1)\)</span> matrix <span class="math inline">\({\boldsymbol{X}}\)</span> is assumed to have rank <span class="math inline">\(p+1\)</span>,
i.e., the square matrix <span class="math inline">\({\boldsymbol{X}}^\top{\boldsymbol{X}}\)</span> of dimension <span class="math inline">\((p+1)\times (p+1)\)</span> is
invertible;</li>
<li>a link function <span class="math inline">\(g\)</span> such that
<span class="math display">\[
g(\mu_i)=\boldsymbol{x}_i^\top\boldsymbol{\beta}\text{ where }
\mu_i=\mathbb{E}[Y_i]
\]</span>
which relates the linear predictor
<span class="math inline">\(\eta_i=\boldsymbol{x}_i^\top\boldsymbol{\beta}\)</span> to the mean <span class="math inline">\(\mu_i\)</span> of
<span class="math inline">\(Y_i\)</span>.</li>
</ol>
<p>Most of the time, explanatory variables are all categorical in an insurance rate. Consider the example given in Section <span class="math inline">\(\ref{Sec965New}\)</span> of an insurance company segmenting based on gender, the sportiness of the vehicle, and the age of the insured (3 age classes, namely less than 30 years old, 30-65 years old, and over 65 years old). An insured individual will be represented by a binary vector that provides the values of the variables used to encode the individual’s characteristics.</p>
<p>We choose as the reference level (i.e., the one for which all <span class="math inline">\(X_i\)</span> values are 0) the modalities most represented in the portfolio. The results will then be interpreted as over- or under-risk compared to this reference class. Thus, the vector (0,1,1,0) represents a male insured under 30 years old driving a sporty vehicle. The linear predictor (or score) will be of the form <span class="math inline">\(\beta_0+\sum_{j=1}^4\beta_jX_j\)</span>, and the number or average cost of claims is generally a non-decreasing function of the score. The intercept, <span class="math inline">\(\beta_0\)</span>, represents the score associated with the reference class (i.e., men between 30 and 65 years old with non-sporty vehicles); if <span class="math inline">\(\beta_j &gt; 0\)</span>, it indicates that presenting the modality translated by <span class="math inline">\(X_j\)</span> is a factor aggravating the risk compared to that of the reference individual, while <span class="math inline">\(\beta_j &lt; 0\)</span> will indicate classes of insured individuals less risky than the reference individuals.</p>
<p>Let’s now examine some examples.</p>
<p>::: {.example}[Gaussian Regression]</p>
<p>The classical linear model, where <span class="math inline">\(Y_i\sim\mathcal{N}or(\mu_i,\sigma^2)\)</span> with <span class="math inline">\(\mu_i=\boldsymbol{x}_i^\top\boldsymbol{\beta}\)</span>. The identity function serves as the link function here. This model has been studied in detail in Section <span class="math inline">\(\ref{Sec93MLMC}\)</span>.
:::</p>
<div class="example">
<p><span id="exm:unlabeled-div-29" class="example"><strong>Example 10.5  (Binomial Regression) </strong></span>Binomial regression is obtained by considering <span class="math inline">\(Y_i\sim\mathcal{B}in(1,q_i)\)</span>. The quantity to be explained, <span class="math inline">\(q_i\)</span>, can be, for example, the probability that policy <span class="math inline">\(i\)</span> produces at least one claim. Since <span class="math inline">\(q_i\in[0,1]\)</span>, modeling is done as <span class="math inline">\(q_i=F(\boldsymbol{x}_i^\top\boldsymbol{\beta})\)</span>, where <span class="math inline">\(F\)</span> is a distribution function, or equivalently, <span class="math inline">\(\boldsymbol{x}_i^\top\boldsymbol{\beta}=F^{-1}(q_i)\)</span>. Although theoretically any distribution function <span class="math inline">\(F\)</span> could be used as a link function, one of the following three functions is usually used:</p>
<ol style="list-style-type: decimal">
<li>The logit model in which
<span class="math display">\[\begin{eqnarray*}
&amp;&amp;\text{logit}(q_i)=\ln\frac{q_i}{1-q_i}=\boldsymbol{x}_i^\top\boldsymbol{\beta}\\
&amp; \Leftrightarrow &amp;
q_i=\frac{\exp(\boldsymbol{x}_i^\top\boldsymbol{\beta})}{1+\exp(\boldsymbol{x}_i^\top\boldsymbol{\beta})}
=\frac{\exp\eta_i}{1+\exp\eta_i}.
\end{eqnarray*}\]</span></li>
<li>The probit model in which
<span class="math display">\[
\text{probit}(q_i)=\Phi^{-1}(q_i)=\boldsymbol{x}_i^\top\boldsymbol{\beta}
\]</span>
<span class="math display">\[
\Leftrightarrow
q_i=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\boldsymbol{x}_i^\top\boldsymbol{\beta}}\exp\left(-\frac{z^2}{2}\right)dz.
\]</span></li>
<li>The complementary log-log model in which
<span class="math display">\[
\text{cloglog}(q_i)=\ln\Big(-\ln(1-q_i)\Big)=\boldsymbol{x}_i^\top\boldsymbol{\beta}
\]</span>
<span class="math display">\[
\Leftrightarrow
q_i=1-\exp\big(-\exp(\boldsymbol{x}_i^\top\boldsymbol{\beta})\big).
\]</span></li>
</ol>
<p>Unlike the logit and probit functions, the complementary log-log function is not symmetric around 0.5. For small values of <span class="math inline">\(q\)</span> (as is often the case in actuarial science), there is practically no difference between the logistic and complementary log-log transformations. The use of the complementary log-log transformation assumes that the probabilities of success and failure should be treated differently.</p>
<p>You can see the shapes of the three functions described above in Figure <span class="math inline">\(\ref{PLC}\)</span>. However, this graph does not provide a perfect indication of the potential differences between the three regression models. It should be noted that the score is linear in <span class="math inline">\(\boldsymbol{\beta}\)</span>. Therefore, if we replace the link function <span class="math inline">\(F\)</span> with its standardized version
<span class="math display">\[
F_{\text{stand}}(z)=F\left(\frac{z-\mu}{\sigma}\right)
\]</span>
where <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span> are the mean and variance associated with <span class="math inline">\(F\)</span>, respectively,
then
<span class="math display">\[
q_i=F(\boldsymbol{x}_i^\top\boldsymbol{\beta})=F_{\text{stand}}(\boldsymbol{x}_i^\top\widetilde{\boldsymbol{\beta}})
\]</span>
where <span class="math inline">\(\widetilde{\beta}_0=\mu+\sigma\beta_0\)</span> and <span class="math inline">\(\widetilde{\beta}_j=\sigma\beta_j\)</span>,
<span class="math inline">\(j=1,2\ldots,p\)</span>. Consequently, the three models presented above can only be compared on appropriate scales for the score, i.e., after centering and scaling.
The expectations associated with the normal, logistic, and Gumbel distributions involved in the probit, logit, and cloglog links are 0, 0, and <span class="math inline">\(\Gamma &#39;(1)=-0.5772\)</span>, respectively, where <span class="math inline">\(\Gamma &#39;\)</span>
denotes the first derivative of the Gamma function. The associated variances are 1, <span class="math inline">\(\frac{\pi^2}{3}\)</span>, and <span class="math inline">\(\frac{\pi^2}{6}\)</span>. After standardization, the functions associated with the logit and probit models are almost identical. However, in the cloglog model, the probability tends to zero and one more quickly as the score diverges to <span class="math inline">\(-\infty\)</span> or <span class="math inline">\(+\infty\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-30" class="example"><strong>Example 10.6  (Poisson Regression) </strong></span>The log-linear Poisson regression is obtained by considering <span class="math inline">\(Y_i\sim\mathcal{P}oi(\lambda_i)\)</span>, with the link function induced by the natural parameter, i.e.,
<span class="math display">\[
\ln\lambda_i=\boldsymbol{x}_i^\top\boldsymbol{\beta}\Leftrightarrow
\lambda_i=\exp\big(\boldsymbol{x}_i^\top\boldsymbol{\beta}\big).
\]</span>
Most of the time, we have a measure of exposure to risk, and we consider
<span class="math inline">\(Y_i\sim\mathcal{P}oi(d_i\lambda_i)\)</span>, where <span class="math inline">\(d_i\)</span> is the duration of coverage granted to insured individual <span class="math inline">\(i\)</span> (this duration multiplies the annual frequency <span class="math inline">\(\lambda_i\)</span> under the assumption of a Poisson process governing the occurrence of claims).</p>
</div>
</div>
<div id="canonical-link-function" class="section level3 hasAnchor" number="10.9.5">
<h3><span class="header-section-number">10.9.5</span> Canonical Link Function<a href="chap9.html#canonical-link-function" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Each of the probability laws in the exponential family has a specific link function called the canonical link function, defined by <span class="math inline">\(\theta = \eta\)</span>, where <span class="math inline">\(\theta\)</span> is the natural parameter. The canonical link is such that <span class="math inline">\(g(\mu_i) = \theta_i\)</span>. Now, <span class="math inline">\(\mu_i = b&#39;(\theta_i)\)</span>, so <span class="math inline">\(g^{-1} = b&#39;\)</span>. The canonical link functions for various probability laws are listed in Table <span class="math inline">\(\ref{FoncCan}\)</span>.</p>
<table>
<caption><span id="tab:FoncCan">Table 10.2: </span>Canonical links associated with common probability laws</caption>
<thead>
<tr class="header">
<th align="center">Probability Law</th>
<th align="center">Canonical Link Function</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Normal</td>
<td align="center"><span class="math inline">\(\eta = \mu\)</span></td>
</tr>
<tr class="even">
<td align="center">Poisson</td>
<td align="center"><span class="math inline">\(\eta = \ln \mu\)</span></td>
</tr>
<tr class="odd">
<td align="center">Gamma</td>
<td align="center"><span class="math inline">\(\eta = 1/\mu\)</span></td>
</tr>
<tr class="even">
<td align="center">Binomial</td>
<td align="center"><span class="math inline">\(\eta = \ln \mu - \ln(1-\mu)\)</span></td>
</tr>
</tbody>
</table>
</div>
<div id="likelihood-equations" class="section level3 hasAnchor" number="10.9.6">
<h3><span class="header-section-number">10.9.6</span> Likelihood Equations<a href="chap9.html#likelihood-equations" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In practice, the regression coefficients <span class="math inline">\(\beta_0, \beta_1, \ldots, \beta_p\)</span>, and the dispersion parameter <span class="math inline">\(\phi\)</span> are unknown and must be estimated from the data. In this section, we focus on estimating the regression coefficients <span class="math inline">\(\boldsymbol{\beta}\)</span> using the maximum likelihood method. This involves maximizing the log-likelihood:</p>
<p><span class="math display">\[\begin{eqnarray*}
L\big(\boldsymbol{\theta}(\boldsymbol{\beta})|\boldsymbol{y},\phi\big) &amp;=&amp; \sum_{i=1}^n \ln f(y_i|\theta_i,\phi) \\
&amp;=&amp; \sum_{i=1}^n \frac{y_i\theta_i - b(\theta_i)}{\phi/\omega_i} + \sum_{i=1}^n c(y_i,\phi)
\end{eqnarray*}\]</span></p>
<p>where <span class="math inline">\(\mathbb{E}[Y_i] = b&#39;(\theta_i) = \mu_i\)</span> and <span class="math inline">\(g(\mu_i) = \boldsymbol{x}_i^\top\boldsymbol{\beta} = \eta_i\)</span>, with <span class="math inline">\(g\)</span> being a monotone and differentiable function. Finding the maximum likelihood estimators of <span class="math inline">\(\beta_0, \beta_1, \ldots, \beta_p\)</span> involves solving the equations:</p>
<p><span class="math display">\[\begin{equation}
\label{SystMaxLik} U_j = 0 \text{ for } j = 0,1,\ldots,p,
\end{equation}\]</span></p>
<p>where</p>
<p><span class="math display">\[\begin{eqnarray*}
U_j &amp;=&amp; \frac{\partial L\big(\boldsymbol{\theta}(\boldsymbol{\beta})|\boldsymbol{y},\phi\big)}{\partial\beta_j} \\
&amp;=&amp; \sum_{i=1}^n \frac{(y_i-\mu_i)x_{ij}}{{\mathbb{V}}[Y_i]g&#39;(\mu_i)}.
\end{eqnarray*}\]</span></p>
<p>If we choose the canonical link function, the likelihood equations become:</p>
<p><span class="math display">\[\begin{equation}
\sum_{i=1}^n \omega_i(y_i-\mu_i)x_{ij} = 0 \text{ for } j = 0,1,\ldots,p,
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\omega_i\)</span> is the weight associated with each observation.</p>
<p>These equations can be interpreted as orthogonality relationships between the explanatory variables and the estimation residuals.</p>
</div>
<div id="solving-likelihood-equations" class="section level3 hasAnchor" number="10.9.7">
<h3><span class="header-section-number">10.9.7</span> Solving Likelihood Equations<a href="chap9.html#solving-likelihood-equations" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The maximum likelihood estimators <span class="math inline">\(\hat{\beta}_j\)</span> of the parameters <span class="math inline">\(\beta_j\)</span> are solutions to the system <span class="math inline">\(\eqref{SystMaxLik}\)</span>. In general, the equations that make up this system do not have explicit solutions and must be solved numerically. One common numerical method for this is the Newton-Raphson method, which we briefly explain below.</p>
<p>Let <span class="math inline">\({\boldsymbol{U}}(\boldsymbol{\beta})\)</span> be the gradient vector of the log-likelihood, with its <span class="math inline">\(j\)</span>-th component defined as
<span class="math display">\[
U_j(\boldsymbol{\beta}) = \frac{\partial}{\partial\beta_j}L(\boldsymbol{\beta}|\boldsymbol{y}),
\]</span>
and let <span class="math inline">\({\boldsymbol{H}}(\boldsymbol{\beta})\)</span> be the Hessian matrix of <span class="math inline">\(L(\boldsymbol{\beta}|\boldsymbol{y})\)</span>, with its <span class="math inline">\((j,k)\)</span> element defined as
<span class="math display">\[
\frac{\partial^2}{\partial\beta_j\partial\beta_k}L(\boldsymbol{\beta}|\boldsymbol{y}).
\]</span>
For <span class="math inline">\(\boldsymbol{\beta}^*\)</span> close to <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span>, a limited Taylor expansion gives
<span class="math display">\[
0 = {\boldsymbol{U}}(\widehat{\boldsymbol{\beta}}) \approx {\boldsymbol{U}}(\boldsymbol{\beta}^*) + {\boldsymbol{H}}(\boldsymbol{\beta}^*) \Big(\widehat{\boldsymbol{\beta}} - \boldsymbol{\beta}^*\Big)
\]</span>
which can be rewritten as
<span class="math display">\[
{\boldsymbol{U}}(\boldsymbol{\beta}^*) + {\boldsymbol{H}}(\boldsymbol{\beta}^*) \Big(\widehat{\boldsymbol{\beta}} - \boldsymbol{\beta}^*\Big) \approx 0
\]</span>
or
<span class="math display">\[\begin{equation}
\label{NewtRaph}
\widehat{\boldsymbol{\beta}} \approx \boldsymbol{\beta}^* - {\boldsymbol{H}}^{-1}(\boldsymbol{\beta}^*) {\boldsymbol{U}}(\boldsymbol{\beta}^*).
\end{equation}\]</span>
This suggests an iterative procedure to obtain the maximum likelihood estimator <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> of <span class="math inline">\(\boldsymbol{\beta\)</span>: starting from an initial value <span class="math inline">\(\widehat{\boldsymbol{\beta}}^{(0)}\)</span> that is expected to be close to <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span>, we define the <span class="math inline">\((r+1)\)</span>-th approximate value <span class="math inline">\(\widehat{\boldsymbol{\beta}}^{(r+1)}\)</span> of <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> from the <span class="math inline">\(r\)</span>-th <span class="math inline">\(\widehat{\boldsymbol{\beta}}^{(r)}\)</span> by
<span class="math display">\[\begin{equation}
\label{AlgoNR}
\widehat{\boldsymbol{\beta}}^{(r+1)} \approx \widehat{\boldsymbol{\beta}}^{(r)} - {\boldsymbol{H}}^{-1}(\widehat{\boldsymbol{\beta}}^{(r)}) {\boldsymbol{U}}(\widehat{\boldsymbol{\beta}}^{(r)}).
\end{equation}\]</span>
This iterative procedure to obtain the maximum likelihood estimator corresponds to the Newton-Raphson method.</p>
<div class="remark">
<p><span id="unlabeled-div-31" class="remark"><em>Remark</em>. </span>There is a clever iterative method for solving the likelihood equations. At each step <span class="math inline">\(r\)</span>, you only need to minimize a weighted least squares criterion of the form
<span class="math display">\[
\sum_{k=1}^n w_k(z_k-\boldsymbol{x}_k^\top\boldsymbol{\beta})^2
\]</span>
where the pseudo-responses <span class="math inline">\(z_k\)</span> are given by
<span class="math display">\[
z_k = \boldsymbol{x}_k^\top\boldsymbol{\beta}^{(r)} + (y_k-\mu_k)\frac{\partial\eta_k}{\partial\mu_k}
\]</span>
and the weights are
<span class="math display">\[
w_k^{-1} = \left(\frac{\partial\eta_k}{\partial\mu_k}\right)^2 V(\mu_k).
\]</span>
In these formulas, <span class="math inline">\(\mu_k\)</span> and <span class="math inline">\(\eta_k\)</span> are calculated for the current values <span class="math inline">\(\boldsymbol{\beta}^{(r)}\)</span> of the parameter <span class="math inline">\(\boldsymbol{\beta}\)</span>. The procedure is stopped when the difference between <span class="math inline">\(\boldsymbol{\beta}^{(r)}\)</span> and <span class="math inline">\(\boldsymbol{\beta}^{(r-1)}\)</span> is sufficiently small.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-32" class="example"><strong>Example 10.7  (Binomial Regression) </strong></span>If the observations <span class="math inline">\(y_i\)</span> follow a <span class="math inline">\(\mathcal{B}in(n_i,q_i)\)</span> distribution, <span class="math inline">\(i=1,\ldots,n\)</span>, we have
<span class="math display">\[
U_j(\boldsymbol{\beta}) = \sum_{i=1}^n \frac{y_i-n_iq_i}{q_i(1-q_i)} \frac{x_{ij}}{g&#39;(q_i)}.
\]</span>
%where
%<span class="math display">\[
%g&#39;(q_i) = \frac{\partial \eta_i}{\partial q_i} = \left(\frac{\partial q_i}{\partial \eta_i}\right)^{-1}.
%\]</span></p>
<p>In this case, <span class="math inline">\(\widehat{\boldsymbol{\beta}}^{(r+1)}\)</span> is obtained by an ordinary linear regression of the dependent random variable <span class="math inline">\({\boldsymbol{z}}_r\)</span>, where the <span class="math inline">\(i\)</span>-th element is
<span class="math display">\[
\widehat{\eta}_{ir} + \frac{(y_i-n_i\widehat{q}_{ir})g&#39;(\widehat{q}_{ir})}{n_i}
\]</span>
as a function of the <span class="math inline">\(p\)</span> explanatory variables, using weights <span class="math inline">\(v_{ir}\)</span> where
<span class="math display">\[
v_{ir} = \frac{n_i}{\widehat{q}_{ir}(1-\widehat{q}_{ir})\big(g&#39;(\widehat{q}_{ir})\big)^2}
\]</span>
and <span class="math inline">\(\widehat{q}_{ir}\)</span> and <span class="math inline">\(\widehat{\eta}_{ir}\)</span> are the success probabilities and linear predictors for observation <span class="math inline">\(i\)</span>, evaluated using the <span class="math inline">\(r\)</span>-th iteration <span class="math inline">\(\widehat{\boldsymbol{\beta}}^{(r)}\)</span>.</p>
<p>To initiate the iterative process, you can use the initial estimates of <span class="math inline">\(q_i\)</span> given by <span class="math inline">\(\widehat{q}_{i0} = \frac{y_i+0.5}{n_i+1}\)</span>, the initial weights
<span class="math inline">\(v_{ir} = \frac{n_i}{\widehat{q}_{i0}(1-\widehat{q}_{i0})\big(g&#39;(\widehat{q}_{i0})\big)^2}\)</span>, and the initial values of pseudo-variables <span class="math inline">\({\boldsymbol{z}}\)</span> where <span class="math inline">\(z_{i0} = \widehat{\eta}_{i0} = g(\widehat{q}_{i0})\)</span>. The values of <span class="math inline">\(z_{i0}\)</span> are then regressed against the <span class="math inline">\(p\)</span> explanatory variables using a weighted least squares method (with weights being the <span class="math inline">\(v_{i0}\)</span>). The coefficients of the explanatory variables are the components of <span class="math inline">\(\widehat{\boldsymbol{\beta}}^{(1)}\)</span>. You can then obtain
<span class="math display">\[\begin{eqnarray*}
\widehat{\eta}_{i1} &amp; = &amp; \widehat{\boldsymbol{\beta}}^{(1)}\boldsymbol{x}_i\\
\widehat{q}_{i1} &amp; = &amp; g^{-1}(\widehat{\eta}_{i1})\\
v_{i1} &amp; = &amp; \frac{n_i}{\widehat{q}_{i1}(1-\widehat{q}_{i1})\big(g&#39;(\widehat{q}_{i1})\big)^2}\\
z_{i1} &amp; = &amp; \widehat{\eta}_{i1} + \frac{(y_i-n_i\widehat{q}_{i1})g&#39;(\widehat{q}_{i1})}{n_i}.
\end{eqnarray*}\]</span>
Then, <span class="math inline">\(\widehat{\boldsymbol{\beta}}^{(2)}\)</span> results from a regression of <span class="math inline">\(z_{i1}\)</span> against the explanatory variables, taking into account the weights <span class="math inline">\(v_{i1}\)</span>, and so on. You stop the process when the difference between <span class="math inline">\(\widehat{\boldsymbol{\beta}}^{(r)}\)</span> and <span class="math inline">\(\widehat{\boldsymbol{\beta}}^{(r+1)}\)</span> is sufficiently small.</p>
<p>In the special case of logistic regression, the pseudo-observations are
<span class="math display">\[
z_i = \widehat{\eta}_i + \frac{y_i-n_i\widehat{q}_i}{n_i\widehat{q}_i(1-\widehat{q}_i)}
\]</span>
and the weights are given by
<span class="math display">\[
v_i = n_iq_i(1-q_i).
\]</span>
Note that the weights are simply the variances of <span class="math inline">\(Y_i\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-34" class="example"><strong>Example 10.8  (Poisson Regression) </strong></span>If the observations <span class="math inline">\(n_i\)</span> follow a <span class="math inline">\(\mathcal{P}oi(\lambda_i)\)</span> distribution, the gradient vector of <span class="math inline">\(L(\boldsymbol{\beta}|\boldsymbol{n})\)</span>, of dimension <span class="math inline">\(p+1\)</span>, is given by
<span class="math display">\[
\boldsymbol{U}(\boldsymbol{\beta}) = \sum_{i=1}^n \boldsymbol{x}_i(n_i-{\lambda}_i),
\]</span>
where we’ve added an extra component <span class="math inline">\(x_{i0}=1\)</span> to the vector <span class="math inline">\(\boldsymbol{x}_i\)</span>. The Hessian matrix, of dimension <span class="math inline">\((p+1)\times(p+1)\)</span>, is given by
<span class="math display">\[
\boldsymbol{H}(\boldsymbol{\beta}) = -\sum_{i=1}^n \boldsymbol{x}_i\boldsymbol{x}_i^\top{\lambda}_i = -\boldsymbol{X}^\top\text{diag}(\boldsymbol{\lambda})\boldsymbol{X},
\]</span>
where diag<span class="math inline">\((\boldsymbol{\lambda})\)</span> denotes the diagonal matrix of dimension <span class="math inline">\(n\times n\)</span> with main elements <span class="math inline">\(\lambda_1,\ldots,\lambda_n\)</span>.</p>
<p>The iterative procedure to obtain the maximum likelihood estimator <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> of <span class="math inline">\(\boldsymbol{\beta}\)</span> is as follows: starting from an initial value <span class="math inline">\(\widehat{\boldsymbol{\beta}}^{(0)}\)</span> that is expected to be close to <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span>, you define the <span class="math inline">\((r+1)\)</span>-th approximate value <span class="math inline">\(\widehat{\boldsymbol{\beta}}^{(r+1)}\)</span> of <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> from the <span class="math inline">\(r\)</span>-th <span class="math inline">\(\widehat{\boldsymbol{\beta}}^{(r)}\)</span> by
<span class="math display">\[\begin{eqnarray}
\label{NewtRaph}
\widehat{\boldsymbol{\beta}}^{(r+1)}
%&amp;=&amp;\widehat{\boldsymbol{\beta}}^{(r)}
%-{\Hvec}^{-1}(\widehat{\boldsymbol{\beta}}^{(r)})
%{\Uvec}(\widehat{\boldsymbol{\beta}}^{(r)})\nonumber\\
&amp;=&amp;\widehat{\boldsymbol{\beta}}^{(r)}+\big(\boldsymbol{X}^\top\text{diag}(\widehat{\boldsymbol{\lambda}}^{(r)})\boldsymbol{X}\big)^{-1}
\boldsymbol{X}^\top(\boldsymbol{n}-\widehat{\boldsymbol{\lambda}}^{(r)}).
\end{eqnarray}\]</span>
A good initial value <span class="math inline">\(\widehat{\boldsymbol{\beta}}^{(0)}\)</span> can be obtained by taking <span class="math inline">\(\widehat{\beta_0}^{(0)}=\ln\overline{n}\)</span>, where <span class="math inline">\(\overline{n}\)</span> is the average number of claims per policy, and <span class="math inline">\(\widehat{\beta_j}^{(0)}=0\)</span> for <span class="math inline">\(j=1,\ldots,p\)</span>. Note that <span class="math inline">\(\widehat{\boldsymbol{\beta}}^{(0)}\)</span> corresponds to the homogeneous Poisson model.</p>
<div class="example">
<p><span id="exm:unlabeled-div-33" class="example"><strong>Example 10.9  (Iterative Algorithm for Poisson Model) </strong></span>The iterative algorithm providing the maximum likelihood estimator of <span class="math inline">\(\boldsymbol{\beta}\)</span> in the Poisson model can also be written as follows:
<span class="math display">\[\begin{eqnarray*}
\widehat{\boldsymbol{\beta}}^{(r+1)} &amp; = &amp; \widehat{\boldsymbol{\beta}}^{(r)} + \left(\sum_{i=1}^n\widehat{\lambda}_i^{(r)}\boldsymbol{x}_i\boldsymbol{x}_i^\top\right)^{-1}
\sum_{i=1}^n\boldsymbol{x}_i(n_i-\widehat{\lambda}_i^{(r)})\\
&amp; = &amp; \left(\sum_{i=1}^n\left(\sqrt{\widehat{\lambda}_i^{(r)}}\boldsymbol{x}_i\right)\left(\sqrt{\widehat{\lambda}_i^{(r)}}\boldsymbol{x}_i\right)^\top\right)^{-1}\\
&amp; &amp; \sum_{i=1}^n\left(\sqrt{\widehat{\lambda}_i^{(r)}}\boldsymbol{x}_i\right)\left(\sqrt{\widehat{\lambda}_i^{(r)}}
\frac{n_i-\widehat{\lambda}_i^{(r)}}
{\widehat{\lambda}_i^{(r)}}+\sqrt{\widehat{\lambda}_i^{(r)}}\boldsymbol{x}_i^\top\widehat{\boldsymbol{\beta}}^{(r)}\right).
\end{eqnarray*}\]</span>
Comparing the recurrence relation above to <span class="math inline">\(\eqref{eqI5}\)</span>, we see that <span class="math inline">\(\widehat{\boldsymbol{\beta}}^{(r+1)}\)</span> is nothing but the least squares estimator associated with the linear regression model
<span class="math display">\[
\sqrt{\widehat{\lambda}_i^{(r)}}\left(\frac{n_i-\widehat{\lambda}_i^{(r)}}
{\widehat{\lambda}_i^{(r)}}+\boldsymbol{x}_i^\top\widehat{\boldsymbol{\beta}}^{(r)}\right)
=\left(\sqrt{\widehat{\lambda}_i^{(r)}}\boldsymbol{x}_i\right)^\top\widehat{\boldsymbol{\beta}}^{(r+1)}
+\epsilon_i
\]</span>
where <span class="math inline">\(\epsilon_i\)</span> is a centered Gaussian error term. The maximum likelihood estimator of the parameter <span class="math inline">\(\boldsymbol{\beta}\)</span> can thus be obtained using an iterative least squares method.</p>
<p>Equivalently, <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> can be obtained
through a weighted least squares fit of pseudo-variables
<span class="math display">\[
z_i^{(r)} = \frac{n_i-\widehat{\lambda}_i^{(r)}}
{\widehat{\lambda}_i^{(r)}}+\boldsymbol{x}_i^\top\widehat{\boldsymbol{\beta}}^{(r)}
\]</span>
on <span class="math inline">\(\boldsymbol{x}_i\)</span>, where the weights <span class="math inline">\(\sqrt{\widehat{\lambda}_i^{(r)}}\)</span> change at each iteration.</p>
</div>
<div id="fisher-information" class="section level3 hasAnchor" number="10.9.8">
<h3><span class="header-section-number">10.9.8</span> Fisher Information<a href="chap9.html#fisher-information" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <span class="math inline">\((jk)\)</span> element of the Fisher information matrix <span class="math inline">\(\mathcal{I}\)</span> is given by <span class="math inline">\((\mathcal{I})_{jk} = \mathbb{E}[U_jU_k]\)</span>. The contribution of each observation <span class="math inline">\(Y_i\)</span> to <span class="math inline">\((\mathcal{I})_{jk}\)</span> is
<span class="math display">\[\begin{eqnarray*}
\mathbb{E}\left[\frac{\partial\ln
f_{\theta_i}(Y_i)}{\partial\beta_j} \frac{\partial\ln
f_{\theta_i}(Y_i)}{\partial\beta_k}\right]
&amp;=&amp;\mathbb{E}\left[\frac{(Y_i-\mu_i)^2x_{ij}x_{ik}}{\left({\mathbb{V}}[Y_i]\right)^2}
\left(\frac{\partial\mu_i}{\partial\eta_i}\right)^2\right]\\
&amp;=&amp;\frac{x_{ij}x_{ik}}{{\mathbb{V}}[Y_i]}
\left(\frac{\partial\mu_i}{\partial\eta_i}\right)^2.
\end{eqnarray*}\]</span>
Therefore,
<span class="math display">\[\begin{eqnarray}
\label{RelFish}
(\mathcal{I})_{jk}
&amp;=&amp;\sum_{i=1}^n\frac{x_{ij}x_{ik}}{{\mathbb{V}}[Y_i]}
\left(\frac{\partial\mu_i}{\partial\eta_i}\right)^2.
\end{eqnarray}\]</span></p>
<p>::: {.example}[Binomial Regression]</p>
<p>If the observations <span class="math inline">\(y_i\)</span> follow a <span class="math inline">\(\mathcal{B}in(n_i,q_i)\)</span> distribution, <span class="math inline">\(i=1,\ldots,n\)</span>,
the <span class="math inline">\((j,k)\)</span> element of the Fisher information matrix is given by
<span class="math display">\[\begin{eqnarray*}
\mathcal{I}_{jk}(\boldsymbol{\beta})&amp;=&amp;
\mathbb{E}\left[\sum_{i=1}^n\frac{Y_i-n_iq_i}{q_i(1-q_i)}\frac{x_{ij}}{g&#39;(q_i)}
\sum_{\ell=1}^n\frac{Y_\ell-n_\ell q_\ell}{q_\ell(1-q_\ell)}\frac{x_{k\ell}}{g&#39;(q_\ell)}\right].
\end{eqnarray*}\]</span>
Now, note that
<span class="math display">\[
\mathbb{E}\Big[(Y_i-n_iq_i)(Y_\ell-n_\ell q_\ell)\Big]=\mathbb{C}[Y_i,Y_\ell]=0
\mbox{ for }i\neq \ell
\]</span>
since the observations are assumed to be independent, while when <span class="math inline">\(i=\ell\)</span> we obtain
<span class="math display">\[
\mathbb{E}\Big[(Y_i-n_iq_i)^2\Big]=\mathbb{V}[Y_i]=n_iq_i(1-q_i)
\]</span>
from which we derive
<span class="math display">\[
\mathcal{I}_{jk}(\boldsymbol{\beta})=\sum_{i=1}^n\frac{n_i}{q_i(1-q_i)\big(g&#39;(q_i)\big)^2}
x_{ij}x_{ik}.
\]</span></p>
</div>
</div>
<p>::: {.example}[Poisson Regression]</p>
<p>If the observations <span class="math inline">\(n_i\)</span> follow a <span class="math inline">\(\mathcal{P}oi(\lambda_i)\)</span> distribution,
<span class="math display">\[\begin{eqnarray*}
\mathbb{C}[U_j,U_k]&amp;=&amp;\mathbb{E}[U_jU_k]\\
&amp;=&amp;\mathbb{E}\left[\sum_{i_1=1}^nx_{i_1j}(N_{i_1}-\lambda_{i_1}),\sum_{i_2=1}^nx_{i_2k}(N_{i_2}-\lambda_{i_2})\right]\\
&amp;=&amp;\sum_{i_1=1}^n\sum_{i_2=1}^nx_{i_1j}x_{i_2k}\underbrace{\mathbb{C}[N_{i_1},N_{i_2}]}_{=0\text{ if }i_1\neq i_2}\\
&amp;=&amp;\sum_{i=1}^nx_{ij}x_{ik}\mathbb{V}[N_i]=\sum_{i=1}^nx_{ij}x_{ik}\lambda_i.
\end{eqnarray*}\]</span>
The Fisher information matrix <span class="math inline">\(\mathcal{I}\)</span> is given by
<span class="math display">\[
\mathcal{I}=\sum_{i=1}^n\boldsymbol{x}_i\boldsymbol{x}_i^\top\lambda_i.
\]</span></p>
<p>If the matrix <span class="math inline">\(\boldsymbol{X}\)</span> is of rank <span class="math inline">\(p+1\)</span>, then <span class="math inline">\(\boldsymbol{H}\)</span> is non-singular and, moreover, negative definite. This ensures that the solution of the likelihood equations corresponds to a maximum of <span class="math inline">\(L(\boldsymbol{\beta})\)</span>.</p>
<p>:::</p>
<div class="remark">
<p><span id="unlabeled-div-35" class="remark"><em>Remark</em>. </span>When you have a large number of observations,
<span class="math display">\[
{\boldsymbol{H}}(\boldsymbol{\beta})\approx
\mathbb{E}[{\boldsymbol{H}}(\boldsymbol{\beta})]=
-\mathcal{I}(\boldsymbol{\beta}),
\]</span>
so an alternative to <span class="math inline">\(\eqref{AlgoNR}\)</span> is given by
<span class="math display">\[\begin{equation}
\label{FisherScoring}
\widehat{\boldsymbol{\beta}}_{r+1}\approx\widehat{\boldsymbol{\beta}}_r
+\mathcal{I}^{-1}(\widehat{\boldsymbol{\beta}}_r)
{\boldsymbol{U}}(\widehat{\boldsymbol{\beta}}_r).
\end{equation}\]</span>
This second iterative scheme is called Fisher’s method of scoring.</p>
<p>Note that <span class="math inline">\(\mathbb{E}[\boldsymbol{H}]=-\mathbb{E}[\boldsymbol{U}\boldsymbol{U}^\top]=-\mathcal{I}\)</span>.
This allows us to interpret <span class="math inline">\(\mathcal{I}\)</span> in terms of information quantity.
Indeed, if <span class="math inline">\(\boldsymbol{H}\)</span>, and thus <span class="math inline">\(\mathcal{I}\)</span>, is small, the likelihood will have a weak curvature,
and determining the maximum likelihood estimator will be less straightforward.</p>
</div>
</div>
<div id="confidence-interval-for-parameters" class="section level3 hasAnchor" number="10.9.9">
<h3><span class="header-section-number">10.9.9</span> Confidence Interval for Parameters<a href="chap9.html#confidence-interval-for-parameters" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="likelihood-ratio-method" class="section level4 hasAnchor" number="10.9.9.1">
<h4><span class="header-section-number">10.9.9.1</span> Likelihood Ratio Method<a href="chap9.html#likelihood-ratio-method" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The likelihood ratio method is based on the likelihood profile,
defined for the parameter <span class="math inline">\(\beta_j\)</span> as the function
<span class="math display">\[
\mathcal{L}_j(\beta_j|\boldsymbol{y})=\max_{\beta_0,\ldots,\beta_{j-1},\beta_{j+1},\ldots,\beta_p}\mathcal{L}(\boldsymbol{\beta}|\boldsymbol{y}).
\]</span>
If <span class="math inline">\(\widehat{\boldsymbol{\beta}}_{\text{MV}}\)</span> is the maximum likelihood estimator of <span class="math inline">\(\boldsymbol{\beta}\)</span>,
then <span class="math inline">\(2\big\{L(\widehat{\boldsymbol{\beta}}_{\text{MV}}|\boldsymbol{y})-L_j(\beta_j|\boldsymbol{y})\big\}\)</span> is approximately chi-squared distributed with one degree of freedom,
provided that <span class="math inline">\(\beta_j\)</span> is the true parameter value, where <span class="math inline">\(L_j(\beta_j|\boldsymbol{y})=\ln\mathcal{L}_j(\beta_j|\boldsymbol{y})\)</span>.
Therefore, a <span class="math inline">\(1-\alpha\)</span> confidence interval for <span class="math inline">\(\beta_j\)</span> is given by the set of values <span class="math inline">\(\xi\)</span> such that the difference
<span class="math inline">\(L(\widehat{\boldsymbol{\beta}}_{\text{MV}}|\boldsymbol{y})-L_j(\xi|\boldsymbol{y})\)</span> is small enough, or equivalently, such that <span class="math inline">\(2\big\{L(\widehat{\boldsymbol{\beta}}_{\text{MV}}|\boldsymbol{y})-L_j(\xi|\boldsymbol{y})\big\}\leq\chi_{1-\alpha,1}^2\)</span>, i.e.
<span class="math display">\[
IC=\left\{\xi\in\mathbb{R} \big|L_j(\xi|\boldsymbol{y})\geq L(\widehat{\boldsymbol{\beta}}_{\text{MV}}|\boldsymbol{y})-\frac{1}{2}\chi_{1-\alpha,1}^2\right\}.
\]</span></p>
<p>The endpoints of this interval are obtained numerically by approximating the likelihood function with a second-degree surface.
Specifically, we use the approximation
<span class="math display">\[
L(\boldsymbol{\beta}|\boldsymbol{y})\approx L(\boldsymbol{\beta}_0|\boldsymbol{y})+(\boldsymbol{\beta}-\boldsymbol{\beta}_0)^\top\boldsymbol{U}(\boldsymbol{\beta})+\frac{1}{2}
(\boldsymbol{\beta}-\boldsymbol{\beta}_0)^\top\boldsymbol{H}(\boldsymbol{\beta})(\boldsymbol{\beta}-\boldsymbol{\beta}_0)
\]</span>
which should be of good quality for <span class="math inline">\(\boldsymbol{\beta}_0\)</span> sufficiently close to <span class="math inline">\(\boldsymbol{\beta}\)</span>.
By approximating <span class="math inline">\(\boldsymbol{H}(\boldsymbol{\beta})\)</span> by its mathematical expectation <span class="math inline">\(-\mathcal{I}\)</span>, we obtain
<span class="math display">\[
L(\boldsymbol{\beta}|\boldsymbol{y})\approx L(\boldsymbol{\beta}_0|\boldsymbol{y})+(\boldsymbol{\beta}-\boldsymbol{\beta}_0)^\top\boldsymbol{U}(\boldsymbol{\beta})-\frac{1}{2}
(\boldsymbol{\beta}-\boldsymbol{\beta}_0)^\top\mathcal{I}(\boldsymbol{\beta}-\boldsymbol{\beta}_0).
\]</span></p>
</div>
<div id="wald-method" class="section level4 hasAnchor" number="10.9.9.2">
<h4><span class="header-section-number">10.9.9.2</span> Wald Method<a href="chap9.html#wald-method" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Thanks to the normal approximation <span class="math inline">\(\eqref{ApproxNorMLE}\)</span> for <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span>, a confidence interval at the <span class="math inline">\(1-\alpha\)</span> confidence level for <span class="math inline">\(\beta_j\)</span> is given by
<span class="math display">\[
\left[\hat{{\beta}}_j\pm z_{\alpha/2}\sqrt{v_{jj}}\right]
\]</span>
where <span class="math inline">\(v_{jj}\)</span> is the diagonal element <span class="math inline">\((jj)\)</span> of <span class="math inline">\(\mathcal{I}^{-1}\)</span>.
This confidence interval is often called the Wald interval.
The diagonal elements of <span class="math inline">\(\mathcal{I}^{-1}\)</span> reflect the precision of the point estimates <span class="math inline">\(\hat{{\beta}}_j\)</span>, while the off-diagonal elements estimate the covariances between the estimators of the <span class="math inline">\(\beta_j\)</span>.</p>
</div>
</div>
<div id="model-comparison" class="section level3 hasAnchor" number="10.9.10">
<h3><span class="header-section-number">10.9.10</span> Model Comparison<a href="chap9.html#model-comparison" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The model’s fit to a dataset involves replacing
the means <span class="math inline">\(\mu_i\)</span> with the initial observations <span class="math inline">\(y_i\)</span>. It is
clear that in general, <span class="math inline">\(y_i\neq\mu_i\)</span>. We must therefore ask whether
the differences between <span class="math inline">\(y_i\)</span> and <span class="math inline">\(\mu_i\)</span> reflect a misspecification
of the model or can be attributed to chance. To do this, we can use various statistics.</p>
<div id="deviance" class="section level4 hasAnchor" number="10.9.10.1">
<h4><span class="header-section-number">10.9.10.1</span> Deviance<a href="chap9.html#deviance" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Here, we consider generalized linear models based on the same density (<span class="math inline">\(\ref{Rel3}\)</span>) and having the same link function but differing in the number of parameters they use.</p>
<p>We define the quality of a model by taking the saturated model, which has as many parameters as observations and thus provides a perfect description of the data, as a reference. The saturated model is characterized by <span class="math inline">\(\widehat{\mu}_i=y_i\)</span> for <span class="math inline">\(i=1,2,\ldots,n\)</span>; the likelihood associated with this model will now be denoted as <span class="math inline">\(\mathcal{L}(\boldsymbol{y}|\boldsymbol{y})\)</span>. In practice, this model is not interesting as it merely reproduces the observations without summarizing them. Let’s consider a model for which the parameter <span class="math inline">\(\boldsymbol{\beta}\)</span> has dimension <span class="math inline">\(p+1&lt;n\)</span>. The model will fit the data well when <span class="math inline">\(\mathcal{L}(\widehat{\boldsymbol{\mu}}|\boldsymbol{y})\approx \mathcal{L}(\boldsymbol{y}|\boldsymbol{y})\)</span> and poorly when <span class="math inline">\(\mathcal{L}(\widehat{\boldsymbol{\mu}}|\boldsymbol{y})&lt;&lt;&lt; \mathcal{L}(\boldsymbol{y}|\boldsymbol{y})\)</span>. This suggests the likelihood ratio statistic
<span class="math display">\[
\Lambda=\frac{\mathcal{L}(\boldsymbol{y}|\boldsymbol{y})}
{\mathcal{L}(\widehat{\boldsymbol{\mu}}|\boldsymbol{y})}
\]</span>
as a measure of the quality of data fit by a model, or equivalently,
<span class="math display">\[
\ln\Lambda=\ln \mathcal{L}(\boldsymbol{y}|\boldsymbol{y})- \ln \mathcal{L}(\widehat{\boldsymbol{\mu}}|\boldsymbol{y}).
\]</span>
A large value of <span class="math inline">\(\ln\Lambda\)</span> suggests that the model is of poor quality. Let’s define the statistic <span class="math inline">\(D=2\ln\Lambda\)</span>; this is called the reduced deviance in the context of generalized linear models. The unreduced deviance is given by <span class="math inline">\(D^*=\phi D\)</span>.</p>
<p>The quality of a model’s fit to the data is often assessed by the deviance, mainly due to its close relationship with the likelihood ratio test. A small deviance value indicates a good fit since the likelihood of the model is close to that of the saturated model. Conversely, a large deviance value indicates a poor fit.</p>
<p>If the model describes the observed data well, <span class="math inline">\(D\)</span> is approximately chi-squared distributed with <span class="math inline">\(n-p-1\)</span> degrees of freedom. An observed value <span class="math inline">\(D_{\text{obs}}\)</span> that is “too large” suggests a poor-quality model. In practice, we consider the model to be of poor quality if
<span class="math display">\[
D_{\text{obs}}&gt;\chi_{n-p-1;1-\alpha}^2,
\]</span>
where <span class="math inline">\(\chi_{n-p-1;1-\alpha}^2\)</span> is the <span class="math inline">\(1-\alpha\)</span> quantile of the chi-squared distribution with <span class="math inline">\(n-p-1\)</span> degrees of freedom.</p>
<p>::: {.example}[Binomial Regression]</p>
<p>If the observations <span class="math inline">\(y_i\)</span> follow a <span class="math inline">\(\mathcal{B}in(n_i,q_i)\)</span> distribution, the log-likelihood is given by
<span class="math display">\[
\sum_{i=1}^n\left(\ln\left(\begin{array}{c}n_i \\y_i\end{array}\right)+y_i\ln \widehat{q}_i+(n_i-y_i)\ln(1-\widehat{q}_i)
\right).
\]</span>
In the saturated model, <span class="math inline">\(q_i\)</span> is estimated as <span class="math inline">\(y_i/n_i\)</span>, so the log-likelihood is
<span class="math display">\[
\sum_{i=1}^n\left(\ln\left(\begin{array}{c}n_i \\y_i\end{array}\right)+y_i\ln \frac{y_i}{n_i}+(n_i-y_i)\ln\frac{n_i-y_i}{n_i}
\right).
\]</span>
Thus, the deviance is
<span class="math display">\[\begin{eqnarray*}
D&amp;=&amp;2\sum_{i=1}^n\left(y_i\ln \frac{y_i/n_i}{\widehat{q}_i}+(n_i-y_i)\ln\frac{1-y_i/n_i}{1-\widehat{q}_i}
\right)\\
&amp;=&amp;2\sum_{i=1}^n\left(y_i\ln \frac{y_i}{\widehat{y}_i}+(n_i-y_i)
\ln\frac{n_i-y_i}{n_i-\widehat{y}_i}\right)
\end{eqnarray*}\]</span>
where <span class="math inline">\(\widehat{y}_i=n_i\widehat{q}_i\)</span> is the value predicted by the current model for observation <span class="math inline">\(i\)</span>.</p>
<p>It is important to note
that the chi-squared approximation for <span class="math inline">\(D\)</span>
can be quite poor when some <span class="math inline">\(n_i\)</span> are small and the
fitted probabilities <span class="math inline">\(\widehat{q}_i\)</span> are close to 0 or 1.
:::</p>
<div class="remark">
<p><span id="unlabeled-div-36" class="remark"><em>Remark</em>. </span>In the particular case where the observations <span class="math inline">\(y_i\)</span> follow a <span class="math inline">\(\mathcal{B}er(q_i)\)</span> distribution,
the deviance does not provide a measure of model quality.
In this case, the model’s log-likelihood is
<span class="math display">\[
\sum_{i=1}^n\Big(y_i\ln \widehat{q}_i+(1-y_i)\ln(1-\widehat{q}_i)
\Big).
\]</span>
The saturated model is characterized by <span class="math inline">\(\widehat{q}_i=y_i\)</span>, which makes the associated log-likelihood zero because <span class="math inline">\(y_i=0\)</span> or <span class="math inline">\(1\)</span>. The deviance is then given by
<span class="math display">\[\begin{eqnarray}
\label{eq3.7}
D&amp;=&amp;-2\sum_{i=1}^n\Big(y_i\ln \widehat{q}_i+(1-y_i)\ln(1-\widehat{q}_i)\Big)\nonumber\\
&amp;=&amp;-2\sum_{i=1}^n\Big(y_i\ln \frac{\widehat{q}_i}{1-\widehat{q}_i}
+\ln(1-\widehat{q}_i)\Big).
\end{eqnarray}\]</span>
By differentiating
<span class="math display">\[
L(\boldsymbol{\beta}|\boldsymbol{y})=\sum_{i=1}^n\Big(y_i\ln q_i+(1-y_i)\ln(1-q_i)
\Big)
\]</span>
with respect to <span class="math inline">\(\beta_j\)</span>, we obtain
<span class="math display">\[
\frac{\partial}{\partial\beta_j}L(\boldsymbol{\beta}|\boldsymbol{y})=
\sum_{i=1}^n\left(\frac{y_i}{q_i}-\frac{1-y_i}{1-\widehat{q}_i}\right)q_i(1-q_i)x_{ij}
\]</span>
from which we derive
<span class="math display">\[\begin{eqnarray*}
\sum_{j=1}^p\beta_j\frac{\partial}{\partial\beta_j}L(\boldsymbol{\beta}|\boldsymbol{y})&amp;=&amp;
\sum_{i=1}^n(y_i-q_i)\sum_{j=1}^p\beta_jx_{ij}\\
&amp;=&amp;\sum_{i=1}^n(y_i-q_i)\ln\frac{q_i}{1-q_i}.
\end{eqnarray*}\]</span>
The left-hand side, when evaluated at the maximum likelihood estimator <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> of <span class="math inline">\(\boldsymbol{\beta}\)</span>, satisfies
<span class="math display">\[
\sum_{i=1}^n(y_i-\widehat{q}_i)\mbox{logit}(\widehat{q}_i)=0
\]</span>
<span class="math display">\[
\Leftrightarrow \sum_{i=1}^ny_i\mbox{logit}(\widehat{q}_i)
= \sum_{i=1}^n\widehat{q}_i\mbox{logit}(\widehat{q}_i).
\]</span>
Taking this into account in <span class="math inline">\(\eqref{eq3.7}\)</span>, we get
<span class="math display">\[
D=-2\sum_{i=1}^n\Big\{\widehat{q}_i\mbox{logit}(\widehat{q}_i)+\ln(1-\widehat{q}_i)\Big\}.
\]</span>
It can be seen that the deviance depends only on the fitted values <span class="math inline">\(\widehat{q}_i\)</span> of <span class="math inline">\(q_i\)</span> and not on the observations <span class="math inline">\(y_i\)</span>; therefore, <span class="math inline">\(D\)</span> does not provide any information about the quality of the fit to the observations and cannot be used to measure model adequacy.</p>
</div>
<p>::: {.example}[Poisson Regression]</p>
<p>Let <span class="math inline">\(L(\widehat{\boldsymbol{\lambda}}|\boldsymbol{n})\)</span> denote the log-likelihood of the fitted model, where
<span class="math inline">\(\widehat{\boldsymbol{\lambda}}=(\widehat{\lambda}_1,\widehat{\lambda}_2,\ldots,\widehat{\lambda}_n)\)</span>
with <span class="math inline">\(\widehat{\lambda}_i=\exp(\widehat{\boldsymbol{\beta}}^\top\boldsymbol{x}_i)\)</span>.
Since
<span class="math display">\[
\exp(-\lambda)\frac{\lambda^k}{k!}\leq \exp(-k)\frac{k^k}{k!}
\]</span>
for any <span class="math inline">\(k\)</span> and <span class="math inline">\(\lambda\)</span>, the
maximum log-likelihood that can be obtained in the model specifying that <span class="math inline">\(N_i\)</span> are
independent Poisson variables is obtained for <span class="math inline">\(N_i\sim\mathcal{P}oi(n_i)\)</span>.
In this case, there are as many parameters as observations,
which is <span class="math inline">\(n\)</span> parameters. Let <span class="math inline">\(L(\boldsymbol{n}|\boldsymbol{n})\)</span> denote the log-likelihood of this model (which predicts <span class="math inline">\(n_i\)</span> for the
<span class="math inline">\(i\)</span>th observation). The deviance is then given by
<span class="math display">\[\begin{eqnarray*}
D(\boldsymbol{n},\widehat{\boldsymbol{\lambda}})&amp;=&amp;2\Big\{L(\boldsymbol{n}|\boldsymbol{n})-L(\widehat{\boldsymbol{\lambda}}|\boldsymbol{n})\Big\}\\
&amp;=&amp;2\ln\left\{\prod_{i=1}^n\exp(-n_i)\frac{n_i^{n_i}}{n_i!}\right\}
-2\ln\left\{\prod_{i=1}^n\exp(-\widehat{\lambda}_i)\frac{\widehat{\lambda}_i^{n_i}}{n_i!}\right\}\nonumber\\
&amp; = &amp; \sum_{i=1}^n\left\{n_i\ln\frac{n_i}{\widehat{\lambda}_i}-(n_i-\widehat{\lambda}_i)\right\}\label{DefDev}
\end{eqnarray*}\]</span>
where we set <span class="math inline">\(y\ln y=0\)</span> when <span class="math inline">\(y=0\)</span>. Since the inclusion of an intercept <span class="math inline">\(\beta_0\)</span> ensures that
<span class="math inline">\(\eqref{EqVrais983}\)</span> holds, in this case, the deviance is written as
<span class="math display">\[\begin{equation}
\label{DefDevInt}
D(\boldsymbol{n},\widehat{\boldsymbol{\lambda}})=\sum_{i=1}^nn_i\ln\frac{n_i}{\widehat{\lambda}_i}.
\end{equation}\]</span></p>
<p>:::</p>
<div class="remark">
<p><span id="unlabeled-div-37" class="remark"><em>Remark</em> (Pseudo-$R^2$ for Count Data). </span>Once the fitted model (i.e., relevant explanatory variables selected and maximum likelihood estimator <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> of <span class="math inline">\(\boldsymbol{\beta}\)</span> obtained), it is crucial to evaluate its quality, i.e., its ability to describe the number of claims affecting the various policyholders in the portfolio. This assessment can be done using the deviance as described above.</p>
<p>Suppose the observations <span class="math inline">\(N_i\)</span> follow a <span class="math inline">\(\mathcal{P}oi(\lambda_i)\)</span> distribution.
The generalization of the usual <span class="math inline">\(R^2\)</span> statistic from linear regression is based on the decomposition of deviance into
<span class="math display">\[
D(\boldsymbol{n},\overline{\boldsymbol{n}})=D(\boldsymbol{n},\widehat{\boldsymbol{\lambda}})+D(\widehat{\boldsymbol{\lambda}},\overline{\boldsymbol{n}})
\]</span>
where</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(D(\boldsymbol{n},\overline{\boldsymbol{n}})\)</span> is the deviance of the model containing only the intercept <span class="math inline">\(\beta_0\)</span>
(i.e., the one not using explanatory variables for which <span class="math inline">\(\widehat{\lambda}_i=\overline{n} =\frac{1}{n}\sum_{j=1}^nn_j\)</span>, <span class="math inline">\(i=1,2,\ldots,n\)</span>) given by
<span class="math display">\[
D(\boldsymbol{n},\overline{\boldsymbol{n}})=\sum_{i=1}^nn_i\ln\frac{n_i}{\overline{n}};
\]</span></li>
<li><span class="math inline">\(D(\boldsymbol{n},\widehat{\boldsymbol{\lambda}})\)</span> is the deviance associated with the considered model;</li>
<li><span class="math inline">\(D(\widehat{\boldsymbol{\lambda}},\overline{\boldsymbol{n}})\)</span> is the deviance unexplained by the model, given by
<span class="math display">\[
D(\widehat{\boldsymbol{\lambda}},\overline{\boldsymbol{n}})=\sum_{i=1}^n\widehat{\lambda}_i\ln\frac{\widehat{\lambda}_i}{\overline{n}}
-(\widehat{\lambda}_i-\overline{n}).
\]</span></li>
</ol>
<p>This decomposition leads to the definition
<span class="math display">\[\begin{eqnarray*}
R_D^2&amp;=&amp;1-\frac{D(\widehat{\boldsymbol{\lambda}},\overline{\boldsymbol{n}})}{D(\boldsymbol{n},\overline{\boldsymbol{n}})}\\
&amp;=&amp;\frac{\sum_{i=1}^nn_i\ln\frac{\widehat{\lambda}_i}{\overline{n}}-(n_i-\widehat{\lambda}_i)}
{\sum_{i=1}^nn_i\ln\frac{n_i}{\overline{n}}}
\end{eqnarray*}\]</span>
with the convention <span class="math inline">\(y\ln y=0\)</span> when <span class="math inline">\(y=0\)</span>.
The Pseudo-<span class="math inline">\(R^2\)</span>
measures the reduction in deviance resulting from the inclusion of explanatory variables in the model.
The statistic <span class="math inline">\(R_D^2\)</span> has the following desirable properties: it is always between 0 and 1,
increases when new variables are added to the model, and has an interpretation in terms of information as the proportional reduction in Kullback-Liebler contrast resulting from the inclusion of new variables. Only the last property requires the correct specification of the distribution of <span class="math inline">\(N_i\)</span>.</p>
</div>
</div>
<div id="pearsons-goodness-of-fit-statistic" class="section level4 hasAnchor" number="10.9.10.2">
<h4><span class="header-section-number">10.9.10.2</span> Pearson’s Goodness-of-Fit Statistic<a href="chap9.html#pearsons-goodness-of-fit-statistic" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Pearson’s chi-squared statistic
<span class="math display">\[
X^2=\sum_{i=1}^n\omega_i\frac{(y_i-\mu_i)^2}{\mathbb{V}[Y_i]}
\]</span>
is used to measure the quality
of a fit.
The statistic <span class="math inline">\(X^2\)</span>, like deviance <span class="math inline">\(D\)</span>, follows
an exact chi-squared distribution in the particular case of the
Gaussian linear model. This result is approximately true
in other cases.</p>
</div>
</div>
<div id="hypothesis-tests-on-parameters" class="section level3 hasAnchor" number="10.9.11">
<h3><span class="header-section-number">10.9.11</span> Hypothesis Tests on Parameters<a href="chap9.html#hypothesis-tests-on-parameters" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We want to test the hypothesis <span class="math inline">\(H_0:\boldsymbol{\beta}=\boldsymbol{\beta}_0=(\beta_0,\beta_1,\ldots,\beta_q)^\top\)</span>
against <span class="math inline">\(H_1:\boldsymbol{\beta}=\boldsymbol{\beta}_1=(\beta_0,\beta_1,\ldots,\beta_p)^\top\)</span>
where <span class="math inline">\(q&lt;p&lt;n\)</span>. This amounts to testing the joint nullity of <span class="math inline">\(\beta_{q+1},\ldots,\beta_p\)</span>. We then use the statistic <span class="math inline">\(\Delta\)</span>, which is the difference between the deviances of the two models, namely
<span class="math display">\[
\Delta=D_0-D_1 = 2\left(\ln
L_{\hat{\boldsymbol{\beta}}_1}(\boldsymbol{y})- \ln
L_{\hat{\boldsymbol{\beta}}_0}(\boldsymbol{y})\right)\geq 0.
\]</span>
It can be shown that <span class="math inline">\(\Delta\)</span> is approximately chi-squared distributed with <span class="math inline">\(p-q\)</span> degrees of freedom. We reject <span class="math inline">\(H_0\)</span> in favor of <span class="math inline">\(H_1\)</span> when
<span class="math display">\[
\Delta_{obs}&gt;\chi_{p-q;1-\alpha}^2,
\]</span>
where <span class="math inline">\(\chi_{p-q;1-\alpha}^2\)</span> is the <span class="math inline">\(1-\alpha\)</span> quantile of the chi-squared distribution with <span class="math inline">\(p-q\)</span> degrees of freedom.</p>
<p>The interest of this type of test arises when the actuary wonders whether certain levels of categorical variables should be grouped. Indeed, the null hypothesis test of regression coefficients only indicates if the level in question should be merged with the reference level. However, it could be the case that two levels of a categorical variable are statistically equivalent but differ from the reference level. In such cases, you would want to perform a test like <span class="math inline">\(H_0:\beta_1=\beta_2\)</span>. Specifically, in the example used in Section <span class="math inline">\(\ref{Sec965New}\)</span> to illustrate the coding of categorical variables using binary coding, you could test <span class="math inline">\(H_0:\beta_3=0\)</span> and <span class="math inline">\(H_0:\beta_4=0\)</span>, which would tell you if people under 30 or over 60 differ from those aged 30-65, but also <span class="math inline">\(H_0:\beta_3=\beta_4\)</span> to see if it makes sense to group those under 30 with those over 65.</p>
</div>
<div id="estimation-of-the-dispersion-parameter" class="section level3 hasAnchor" number="10.9.12">
<h3><span class="header-section-number">10.9.12</span> Estimation of the Dispersion Parameter<a href="chap9.html#estimation-of-the-dispersion-parameter" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The estimation of the dispersion parameter
<span class="math inline">\(\phi\)</span> is based on the deviance <span class="math inline">\(D\)</span>.
As <span class="math inline">\(\mathbb{E}[D]\approx n-p-1\)</span>, we could estimate <span class="math inline">\(\phi\)</span>
by
<span class="math display">\[
\widetilde{\phi}=\frac{1}{n-p-1}D.
\]</span>
However, this estimator is rarely used in practice because it is
very unstable. To avoid these issues, a Taylor expansion of
the second order of
the log-likelihood provides us with
<span class="math display">\[
\widehat{\phi}=\frac{1}{n-p-1}(\boldsymbol{y}-\widehat{\boldsymbol{\mu}})^\top
\mathcal{I}_n(\widehat{\boldsymbol{\mu}})(\boldsymbol{y}-\widehat{\boldsymbol{\mu}});
\]</span>
this estimator is often called the Pearson’s <span class="math inline">\(X^2\)</span> estimation.</p>
</div>
<div id="residual-analysis" class="section level3 hasAnchor" number="10.9.13">
<h3><span class="header-section-number">10.9.13</span> Residual Analysis<a href="chap9.html#residual-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The goodness-of-fit measures discussed above (deviance and
Pearson’s statistic) provide global indications of model quality.
However, a careful analysis of residuals allows us to discover where any discrepancy between the model and the observations may come from, and thus improve the initial model, if necessary.</p>
<p>Residuals are based on the difference between the observation <span class="math inline">\(y_i\)</span>
and the predicted value <span class="math inline">\(\mu_i\)</span>. They help check
the adequacy of the model and also detect particular observations (often called “outliers”). These outliers
can have a significant influence on parameter estimates, and it is important to re-estimate the model after removing these observations to ensure the stability of the results obtained.</p>
<p>There are two situations where a model is considered inadequate based on global measures such as deviance or Pearson’s statistic: either a small number of observations are poorly described by the model, or
all observations show a systematic deviation from the model.</p>
<p>A graphical representation of residuals can detect deviations from the model for most types of dependent variables. However, when the dependent variable can only take a few values (as in logistic regression, for example), residual analysis may have limited utility.</p>
<p>Two types of residuals are commonly used in the context of generalized linear models: Pearson residuals and deviance residuals.</p>
<div id="pearson-residuals" class="section level4 hasAnchor" number="10.9.13.1">
<h4><span class="header-section-number">10.9.13.1</span> Pearson Residuals<a href="chap9.html#pearson-residuals" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>They are defined as
<span class="math display">\[
r_i^P=\frac{\sqrt{w_i}(y_i-\mu_i)}{\sqrt{V(\mu_i)}}.
\]</span>
The name of this first type of residuals comes from the fact that <span class="math inline">\(r_i^P\)</span>
can be seen as the square root of the contribution of the <span class="math inline">\(i\)</span>-th observation to the Pearson statistic, i.e.
<span class="math display">\[
\sum_{i=1}^n\{r_i^P\}^2=X^2.
\]</span></p>
<p>::: {.example}[Binomial Regression]</p>
<p>If the observations <span class="math inline">\(y_i\)</span> follow a <span class="math inline">\(\mathcal{B}in(n_i,q_i)\)</span> distribution, the Pearson residual is
<span class="math display">\[
r_i^P=\frac{y_i-\widehat{y}_i}{\sqrt{n_i\widehat{q}_i(1-\widehat{q}_i)}}.
\]</span>
:::</p>
<p>::: {.example}[Poisson Regression]</p>
<p>If the observations <span class="math inline">\(n_i\)</span> follow a <span class="math inline">\(\mathcal{P}oi(\lambda)\)</span> distribution, the Pearson residual is
<span class="math display">\[
r_i^P=\frac{n_i-\widehat{\lambda}_i}{\sqrt{\widehat{\lambda}_i}}.
\]</span>
:::</p>
</div>
<div id="deviance-residuals" class="section level4 hasAnchor" number="10.9.13.2">
<h4><span class="header-section-number">10.9.13.2</span> Deviance Residuals<a href="chap9.html#deviance-residuals" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We have seen that deviance is a measure of the goodness of fit provided by a model. We can consider that each observation <span class="math inline">\(y_i\)</span> contributes an amount <span class="math inline">\(d_i\)</span> to the deviance <span class="math inline">\(D\)</span>, i.e.
<span class="math display">\[
D=\sum_{i=1}^nd_i.
\]</span>
Deviance residuals are then defined as the square root of the contribution <span class="math inline">\(d_i\)</span> of the <span class="math inline">\(i\)</span>-th observation to deviance <span class="math inline">\(D\)</span>, multiplied by the sign of the raw residual <span class="math inline">\(y_i-\mu_i\)</span>, i.e.
<span class="math display">\[
r_i^D=\mbox{sign}(y_i-\mu_i)\sqrt{d_i}.
\]</span>
Thus,
<span class="math display">\[
\sum_{i=1}^n\{r_i^D\}^2=D.
\]</span>
% Deviance residuals can also be standardized to give them approximately unit variance by dividing them by <span class="math inline">\(\sqrt{1-h_{ii}}\)</span>, i.e.
% <span class="math display">\[
% r_i^{DS}=\frac{r_i^D}{\sqrt{1-h_{ii}}}.
% \]</span></p>
<p>::: {.example}[Logistic Regression]</p>
<p>If the observations <span class="math inline">\(y_i\)</span> follow a <span class="math inline">\(\mathcal{B}in(n_i,q_i)\)</span> distribution, the deviance residual is
<span class="math display">\[\begin{eqnarray*}
r_i^D %&amp; = &amp; \mbox{sign}(y_i-\widehat{y}_i)\sqrt{d_i}\\
&amp; = &amp; \mbox{sign}(y_i-\widehat{y}_i)\sqrt{2y_i\ln\left(\frac{y_i}{\widehat{y}_i}\right)+
2(n_i-y_i)\ln\left(\frac{n_i-y_i}{n_i-\widehat{y}_i}\right)}.
\end{eqnarray*}\]</span></p>
<p>:::</p>
<p>::: {.example}[Poisson Regression]</p>
<p>If the observations <span class="math inline">\(n_i\)</span> follow a <span class="math inline">\(\mathcal{P}oi(\lambda_i)\)</span> distribution, the deviance residual is
<span class="math display">\[
r_i^D=\mbox{sign}(n_i-\widehat{\lambda}_i)\sqrt{2\left\{n_i\ln\frac{n_i}{\widehat{\lambda}_i}-(n_i-\widehat{\lambda}_i)\right\}}
\]</span>
where <span class="math inline">\(y\ln y=0\)</span> when <span class="math inline">\(y=0\)</span>. % Thus, <span class="math inline">\(\{r_i^D\}^2\)</span> is the contribution of the observation to the deviance statistic <span class="math inline">\(D\)</span>.</p>
<p>:::</p>
<div class="remark">
<p><span id="unlabeled-div-38" class="remark"><em>Remark</em>. </span>Another interesting quantity is obtained by taking the difference between the deviance obtained using the <span class="math inline">\(n\)</span> observations and that obtained by removing the <span class="math inline">\(i\)</span>-th observation from the data set (i.e., based on a data set with <span class="math inline">\(n-1\)</span> observations). This allows measuring the overall influence of <span class="math inline">\(y_i\)</span> on the model.</p>
<p>Note that the exact computation of these quantities takes time (since the model must be fitted <span class="math inline">\(n\)</span> times using a data set of size <span class="math inline">\(n-1\)</span>). To avoid excessive computation time, various approximations are used in practice for the difference in deviances obtained by omitting the observation <span class="math inline">\(y_i\)</span>.</p>
</div>
</div>
<div id="representation-of-residuals" class="section level4 hasAnchor" number="10.9.13.3">
<h4><span class="header-section-number">10.9.13.3</span> Representation of Residuals<a href="chap9.html#representation-of-residuals" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The residuals introduced above can be plotted against various statistics, each providing different information about deviations from the model. Residuals can be represented against the observation number (index plot), which helps identify observations that contribute to large residuals (and, consequently, to model inadequacy).</p>
<p>Residuals can also be represented against the predicted values <span class="math inline">\(\widehat{\mu}_i\)</span> or the linear predictors <span class="math inline">\(\widehat{\eta}_i\)</span>. They can also be plotted against each of the explanatory variables.</p>
<p>%Additionally, a normal QQ-plot of standardized residuals can be used to assess model adequacy. A good model should have a QQ-plot close to the first bisector. If the residuals are too skewed, the curve will not pass through the origin, and if they have a thick distribution tail, the curve will be concave. However, it’s important to keep in mind that residuals from non-normal models are generally skewed.</p>
<p>%::: {.remark}
%
%It is advisable not to plot residuals against observed values in the case of Poisson regression because
%<span class="math display">\[
%\Cov\big[N_i-\mathbb{E}[N_i],N_i\big]=\Var[N_i]=\mathbb{E}[N_i]
%\]</span>
%if <span class="math inline">\(N_i\sim\mathcal{P}oi(\lambda_i)\)</span>, which suggests a trend in the residual plot (large values of <span class="math inline">\(r_i\)</span> accompanying large values of <span class="math inline">\(n_i\)</span>), resulting in higher residuals for higher <span class="math inline">\(n_i\)</span>. For this reason, it is preferable to plot residuals against predicted values since in this case
%<span class="math display">\[
%\Cov\big[N_i-\mathbb{E}[N_i],\mathbb{E}[N_i]\big]=0.
%\]</span>
%:::</p>
</div>
<div id="influential-observations" class="section level4 hasAnchor" number="10.9.13.4">
<h4><span class="header-section-number">10.9.13.4</span> Influential Observations<a href="chap9.html#influential-observations" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>An observation <span class="math inline">\(y_i\)</span> is said to have influence on a considered model when a small change in <span class="math inline">\(y_i\)</span> or its omission leads to significantly different parameter estimates for the model. Such an observation has a substantial impact on the study’s conclusions. However, an observation with influence is not necessarily an outlier; it can be close to other observations and have a small residual. The leverage effect of an observation <span class="math inline">\(y_i\)</span> on the predicted value <span class="math inline">\(\widehat{y}_j\)</span> is the derivative of <span class="math inline">\(\widehat{y}_j\)</span> with respect to <span class="math inline">\(y_i\)</span>, indicating how the predicted values of other observations vary with changes in <span class="math inline">\(y_i\)</span>. To measure this effect, the projection matrix <span class="math inline">\({\boldsymbol{H}}\)</span> is used, which maps observations <span class="math inline">\(y_i\)</span> to their predictions <span class="math inline">\(\widehat{y}_i\)</span>, i.e.,
<span class="math display">\[
\widehat{\boldsymbol{y}}={\boldsymbol{H}}\boldsymbol{y}.
\]</span>
In the case of generalized linear models, we have
<span class="math display">\[
{\boldsymbol{H}}={\boldsymbol{V}}^{1/2}{\boldsymbol{X}}\Big({\boldsymbol{X}}^\top{\boldsymbol{V}}{\boldsymbol{X}}\Big)^{-1}
{\boldsymbol{X}}^\top{\boldsymbol{V}}^{1/2}
\]</span>
where <span class="math inline">\({\boldsymbol{V}}\)</span> is a diagonal matrix defined as
<span class="math display">\[
{\boldsymbol{V}}=\mbox{diag}\left(\{V(\mu_i)\}^2\left(\frac{d\eta_i}{d\mu_i}\right)^2
\frac{\phi}{\omega_i} \right).
\]</span></p>
<p>The diagonal elements of <span class="math inline">\({\boldsymbol{H}}\)</span> provide information about the influence of each observation. It is worth noting that this measure depends on both the explanatory variables and the estimates of the parameters <span class="math inline">\(\beta_0,\beta_1,\ldots,\beta_p\)</span>. Since the trace of <span class="math inline">\({\boldsymbol{H}}\)</span> equals <span class="math inline">\(p+1\)</span>, the average value of the diagonal terms is <span class="math inline">\((p+1)/n\)</span>. Values corresponding to diagonal terms that exceed, say, twice the average value of <span class="math inline">\((p+1)/n\)</span> should be carefully examined.</p>
</div>
<div id="cooks-distance-1" class="section level4 hasAnchor" number="10.9.13.5">
<h4><span class="header-section-number">10.9.13.5</span> Cook’s Distance<a href="chap9.html#cooks-distance-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Cook’s distance is used to identify observations that influence all parameters <span class="math inline">\(\beta_0,\beta_1,\ldots,\beta_p\)</span>. Similar to linear models, the idea is to determine the maximum likelihood estimator <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> of the parameter <span class="math inline">\(\boldsymbol{\beta}\)</span> and then calculate the same estimator <span class="math inline">\(\widehat{\boldsymbol{\beta}}_{(i)}\)</span> obtained by removing the <span class="math inline">\(i\)</span>-th observation from the dataset. Cook’s distance is then given by
<span class="math display">\[
C_i=\frac{1}{p+1}(\widehat{\boldsymbol{\beta}}-\widehat{\boldsymbol{\beta}}_{(i)})^\top
{\boldsymbol{X}}^\top{\boldsymbol{V}}{\boldsymbol{X}}(\widehat{\boldsymbol{\beta}}-\widehat{\boldsymbol{\beta}}_{(i)})
\]</span>
which can be interpreted as a distance between <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> and <span class="math inline">\(\widehat{\boldsymbol{\beta}}_{(i)}\)</span>. If <span class="math inline">\(C_i\)</span> is large, it indicates that observation <span class="math inline">\(y_i\)</span> has a significant influence on parameter estimation. However, the calculation of <span class="math inline">\(C_i\)</span> is computationally expensive because all parameters must be re-estimated once <span class="math inline">\(y_i\)</span> is removed. In actuarial science, this calculation is often infeasible due to a large number of observations. To avoid re-fitting the model <span class="math inline">\(n\)</span> times, various approximations are often used in practice.</p>
</div>
</div>
<div id="the-practice-of-generalized-linear-models" class="section level3 hasAnchor" number="10.9.14">
<h3><span class="header-section-number">10.9.14</span> The Practice of Generalized Linear Models<a href="chap9.html#the-practice-of-generalized-linear-models" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="choice-of-distribution" class="section level4 hasAnchor" number="10.9.14.1">
<h4><span class="header-section-number">10.9.14.1</span> The Importance of Choosing the Exponential Subfamily<a href="chap9.html#choice-of-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The few examples mentioned in the introduction are often sufficient in practice: modeling claim costs with a Gamma regression model and modeling counts with a Poisson regression model. However, the choice of the subfamily is not neutral when it comes to pricing.</p>
<p>Consider the following (simplified) example based on three observations:
<span class="math display">\[
\begin{tabular}{|l|lll|}
\hline Observation $i$ &amp; 1 &amp; 2 &amp; 3 \\ \hline
Response variable $Y_{i}$ (claim costs) &amp; 1 &amp; 2 &amp; 8 \\
Explanatory variable $X_{i}$ (vehicle power) &amp; 1 &amp; 2 &amp; 3 \\
\hline
\end{tabular}%
\]</span>
We aim to fit a generalized linear model, i.e., <span class="math inline">\(g\left(\mathbb{E}[Y]\right) = \alpha + \beta X\)</span> where <span class="math inline">\(g\)</span> is a link function. Figure <span class="math inline">\(\ref{Fig-GLM-trois-lois-1}\)</span> illustrates the influence of the choice of the probability distribution (for the canonical link function), considering <span class="math inline">\(Y_i\)</span> to follow a normal, Poisson, and Gamma distribution, respectively.</p>
<p>While the three distributions provide similar results at the edges (for values of <span class="math inline">\(X\)</span> close to <span class="math inline">\(1\)</span> or <span class="math inline">\(3\)</span>), they exhibit significantly different behavior elsewhere. In particular, compared to the other two distributions, the Gamma distribution implies higher claim costs (at equal power) for low and high power vehicles, while offering lower costs for vehicles with average power. As this cost reflects the premium, and if the true model is a Poisson distribution, the interpretation of this graph is as follows:
- With a normal model, vehicles with average power pay for the risk of others by having a higher premium than their actual risk.
- With a Gamma model, high or very low power vehicles pay for the risk of vehicles with average power, which are underpriced.</p>
<p>Therefore, even though this analysis should be nuanced by considering the impact of the link function, it’s essential to note that the choice of the distribution for the response variable has a substantial impact on the resulting pure premiums.</p>
</div>
<div id="Section-tweedie" class="section level4 hasAnchor" number="10.9.14.2">
<h4><span class="header-section-number">10.9.14.2</span> Total Cost Modeling: The Importance of Zero-Claim Contracts<a href="chap9.html#Section-tweedie" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Suppose the variable of interest <span class="math inline">\(Y\)</span> represents the (total) annual premiums for all policies in the portfolio. Since a large number of policies have no claims, the variable <span class="math inline">\(Y\)</span> will be zero for most observations. A Gamma distribution (for example) cannot model this type of behavior (see Chapter 6 of Volume 1 for the difference between collective and individual models).</p>
<p>The Tweedie distribution allows for modeling this behavior by adding a Dirac measure at <span class="math inline">\(0\)</span> to a probability distribution with support in <span class="math inline">\({\mathbb{R}}^+\)</span>. The distribution of <span class="math inline">\(Y\)</span> is then a compound Poisson distribution,
<span class="math display">\[
Y \sim \mathcal{CP}oi\left({\mu^{2-\gamma}}{\phi(2-\gamma)},
\mathcal{G}am\left(-\frac{2-\gamma}{\phi(1-\gamma)},\phi(2-\gamma)\mu^{\gamma-1}\right)\right),
\]</span>
where <span class="math inline">\(1&lt;\gamma&lt;2\)</span>. This results in a variance function of the form <span class="math inline">\(V(\mu)=\phi \mu^\gamma\)</span>.
The Poisson model is recovered when <span class="math inline">\(\gamma \rightarrow 1\)</span>, and a Gamma distribution is obtained when <span class="math inline">\(\gamma \rightarrow 2\)</span>. It is even possible to obtain a much broader class of distributions, including cases where <span class="math inline">\(\gamma&gt;2\)</span>, by considering stable distributions.</p>
<div class="remark">
<p><span id="unlabeled-div-39" class="remark"><em>Remark</em>. </span>These models were used in automobile insurance in <span class="citation">(<a href="#ref-jorgensen1994fitting" role="doc-biblioref">Jørgensen and Paes De Souza 1994</a>)</span>, where a value of <span class="math inline">\(\gamma\)</span> close to 1.5 was obtained.</p>
</div>
</div>
<div id="the-importance-of-the-link-function" class="section level4 hasAnchor" number="10.9.14.3">
<h4><span class="header-section-number">10.9.14.3</span> The Importance of the Link Function<a href="chap9.html#the-importance-of-the-link-function" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>As noted earlier, the choice of the exponential subfamily considered is not neutral in terms of pricing. The same holds true for the choice of the link function. Continuing with the same three observations, Figure <span class="math inline">\(\ref{Fig-GLM-trois-lois-2}\)</span> illustrates the influence of the link function (for a Poisson and a Gamma distribution for <span class="math inline">\(Y\)</span>). It’s important to note that even with the same family of distributions, the choice of the link function can have a significant impact.</p>
<p>However, it’s common practice to use the logarithmic link function since it has the advantage of providing a multiplicative model. In this case, the coefficients <span class="math inline">\(\beta_j\)</span> have a straightforward interpretation in terms of multipliers.</p>
<p>While the choice of the link function is significant for pricing, it’s also possible to treat this function as unknown and estimate it from the data. The Box-Cox transformation allows for a simple parametric form:</p>
<p><span class="math display">\[
g(x) = \begin{cases}
\frac{x^\lambda - 1}{\lambda}, &amp; \text{if } \lambda \neq 0, \\
\log(x), &amp; \text{if } \lambda = 0.
\end{cases}
\]</span></p>
<p>Note that <span class="math inline">\(\lambda = 1\)</span> corresponds to an identity link function (additive model), and <span class="math inline">\(\lambda \rightarrow 0\)</span> corresponds to a logarithmic link function (multiplicative model). If <span class="math inline">\(\lambda = -1\)</span>, it corresponds to an inverse link function. Many common link functions belong to this family. It’s possible to estimate <span class="math inline">\(\lambda\)</span> that maximizes the likelihood of the model.</p>
</div>
</div>
</div>
<div id="generalized-additive-models-gams" class="section level2 hasAnchor" number="10.10">
<h2><span class="header-section-number">10.10</span> Generalized Additive Models (GAMs)<a href="chap9.html#generalized-additive-models-gams" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="principle-6" class="section level3 hasAnchor" number="10.10.1">
<h3><span class="header-section-number">10.10.1</span> Principle<a href="chap9.html#principle-6" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Just as additive models allowed for the incorporation of nonlinear effects of explanatory variables without specifying their form in advance when the response variables were Gaussian, Generalized Additive Models (GAMs) offer the same extension for Poisson, Binomial, and Gamma regression models.</p>
<p>As discussed earlier, the maximum likelihood estimates of the parameters <span class="math inline">\(\beta_0, \beta_1, \ldots, \beta_p\)</span> involved in the linear score <span class="math inline">\(\eta\)</span> of a GLM can be obtained by fitting a linear model to pseudo-observations (using weighted least squares). This same approach is used in GAMs. Specifically, we define pseudo-observations as:</p>
<p><span class="math display">\[
z_i^{(k)} = \widehat{\eta}_i^{(k)} + \frac{y_i - \widehat{\mu}_i^{(k)}}{\widehat{D}_i^{(k)}}
\]</span></p>
<p>Where <span class="math inline">\(\widehat{\mu}_i^{(k)} = g^{-1}(\widehat{\eta}_i^{(k)})\)</span> and <span class="math inline">\(\widehat{\eta}_i^{(k)} = \widehat{c} + \sum_{j=1}^p \widehat{f}_j^{(k)}(x_{ij})\)</span> for <span class="math inline">\(i = 1, 2, \ldots, n\)</span>. Additionally:</p>
<p><span class="math display">\[
\widehat{D}_i^{(k)} = \frac{\partial}{\partial \eta} g^{-1}(\widehat{\eta}_i^{(k)})
\]</span></p>
<p>Each pseudo-observation <span class="math inline">\(z_i^{(k)}\)</span> is associated with weights:</p>
<p><span class="math display">\[
\pi_i^{(k)} = \left(\frac{\widehat{D}_i^{(k)}}{\widehat{\sigma}_i^{(k)}}\right)^2
\]</span></p>
<p>Where:</p>
<p><span class="math display">\[
\widehat{\sigma}_i^{(k)} = \sqrt{\frac{\phi V\big(g^{-1}(\widehat{\eta}_i^{(k)})\big)}{\omega_i}}
\]</span></p>
<p>The new estimation <span class="math inline">\(\widehat{f}_j^{(k+1)}\)</span> of <span class="math inline">\(f_j\)</span> is obtained by regressing <span class="math inline">\(z_i^{(k)}\)</span> on <span class="math inline">\(\boldsymbol{x}_i\)</span>, taking into account the weights <span class="math inline">\(\pi_i^{(k)}\)</span>. The same technique used for estimating functions <span class="math inline">\(f_1(\cdot), \ldots, f_p(\cdot)\)</span> in additive models is applied here.</p>
<p>Formalized as an algorithm, the estimation in GAMs is carried out as follows:</p>
<ul>
<li><strong>Initialization</strong>: <span class="math inline">\(\widehat{c} \gets g(\overline{y})\)</span> and <span class="math inline">\(\widehat{\boldsymbol{f}}_j^{(0)} \gets 0\)</span>, for <span class="math inline">\(j = 1, 2, \ldots, p\)</span>.</li>
<li><strong>Cycle</strong>: For <span class="math inline">\(k = 1, 2, \ldots\)</span>, we construct pseudo-observations <span class="math inline">\(z_i^{(k)}\)</span> and associate them with weights <span class="math inline">\(\pi_i^{(k)}\)</span>. Then, we fit <span class="math inline">\(z_i^{(k)}\)</span> to <span class="math inline">\(c + \sum_{j=1}^p f_j(x_{ij})\)</span> using an additive model, as described earlier:
<ul>
<li>Initialize <span class="math inline">\(\widehat{c} \gets \overline{z}^{(k)}\)</span> and <span class="math inline">\(\widehat{\boldsymbol{f}}_j^{(0)} \gets \widehat{\boldsymbol{f}}_j^{(k)}\)</span></li>
<li>Reevaluate <span class="math inline">\(\widehat{\boldsymbol{f}}_j^{(1)} \to \boldsymbol{H}_{\lambda_j}\left(\boldsymbol{z}^{(k)}-(\overline{z}^{(k)},\ldots,\overline{z}^{(k)})^\top- \sum_{s&lt;j}\widehat{\boldsymbol{f}}_s^{(1)}-\sum_{s&gt;j}\widehat{\boldsymbol{f}}_s^{(0)}\right)\)</span></li>
<li>Iterate (ii) and stop when the sum of squared residuals ceases to decrease.</li>
</ul></li>
<li><strong>Stopping Criterion</strong>: Stop when variations in <span class="math inline">\(f_j\)</span> become negligible.</li>
</ul>
<p>As we can see, each step of the iterative algorithm leading to the estimates of the functions <span class="math inline">\(f_j(\cdot)\)</span> requires a complete backfitting to review the estimation of these functions based on the pseudo-variables obtained at that stage.</p>
<div id="SecMVlocal" class="section level4 hasAnchor" number="10.10.1.1">
<h4><span class="header-section-number">10.10.1.1</span> Local Maximum Likelihood<a href="chap9.html#SecMVlocal" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Another approach is to directly extend the Loess method to Generalized Additive Models (GAMs). In this case, we will use local adjustments within GLMs based on maximum likelihood, similar to how the Loess method operated with Gaussian log-likelihood.</p>
<p>More precisely, given a point <span class="math inline">\(x\)</span> where we want to estimate the response, we determine a neighborhood <span class="math inline">\(\mathcal{V}(x)\)</span> and weights <span class="math inline">\(w_i(x)\)</span> just like in Loess. The goal is to solve the likelihood equations:</p>
<p><span class="math display">\[
\boldsymbol{X}^\top\boldsymbol{\Omega}(x)\big(\boldsymbol{y}-\boldsymbol{\mu}(\widehat{\beta}(x))\big)=\boldsymbol{0}
\]</span></p>
<p>Where the diagonal matrix <span class="math inline">\(\boldsymbol{\Omega}(x)\)</span> includes the weights <span class="math inline">\(w_1(x), \ldots, w_n(x)\)</span>.</p>
<p>::: {.example}[Logistic Regression]</p>
<p>Consider observations <span class="math inline">\((y_i, x_i)\)</span> where <span class="math inline">\(y_i\)</span> follows a <span class="math inline">\(\mathcal{B}in(n_i, q_i)\)</span> distribution, where <span class="math inline">\(q_i\)</span> is a function of <span class="math inline">\(x_i\)</span>. If we want to estimate the <span class="math inline">\(q_i = q(x_i)\)</span>, we will determine the neighborhood <span class="math inline">\(\mathcal{V}(x)\)</span> and estimate the function <span class="math inline">\(f\)</span> in:</p>
<p><span class="math display">\[
q(x) = \frac{\exp\big(f(x)\big)}{1+\exp\big(f(x)\big)}
\]</span></p>
<p>By maximizing:</p>
<p><span class="math display">\[
L\big(\beta_0(x), \beta_1(x)\big) = \sum_{\xi \in \mathcal{V}(x)} w_\xi(x)\ln\ell_\xi
\big(\beta_0(x), \beta_1(x)\big)
\]</span></p>
<p>Where:</p>
<p><span class="math display">\[
\ell_\xi\big(\beta_0(x), \beta_1(x)\big)
= \left(\begin{array}{c}n_\xi \\y_\xi\end{array}\right)\big(q_\xi(x)\big)^{y_\xi}\big(1-q_\xi(x)\big)^{n_\xi-y_\xi}
\]</span></p>
<p>And:</p>
<p><span class="math display">\[
q_\xi(x) = \frac{\exp\big(\beta_0(x)+\beta_1(x)\xi\big)}{1+\exp\big(\beta_0(x)+\beta_1(x)\xi\big)}.
\]</span></p>
<p>Finally, the fitted value of <span class="math inline">\(q(x)\)</span> is obtained as:</p>
<p><span class="math display">\[
\widehat{q(x)} = \frac{\exp\big(\widehat{f}(x)\big)}{1+\exp\big(\widehat{f}(x)\big)}
\]</span></p>
<p>Where:</p>
<p><span class="math display">\[
\widehat{f}(x) = \widehat{\beta}_0(x) + \widehat{\beta}_1(x)x.
\]</span></p>
<p>:::</p>
</div>
</div>
<div id="in-practice" class="section level3 hasAnchor" number="10.10.2">
<h3><span class="header-section-number">10.10.2</span> In Practice…<a href="chap9.html#in-practice" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Most often, actuaries have a large number of categorical variables and only a few continuous variables. Suppose the explanatory variables <span class="math inline">\(\boldsymbol{x}_i\)</span> related to the insured individual <span class="math inline">\(i\)</span> are reorganized as:</p>
<p><span class="math display">\[
\boldsymbol{x}_i=(1,x_{i1},\ldots,x_{if},x_{i,f+1},\ldots,x_{i,f+c})
\]</span></p>
<p>Where <span class="math inline">\(x_{i1}, \ldots, x_{if}\)</span> are the <span class="math inline">\(f\)</span> binary variables used to encode categorical variables describing the insured individual <span class="math inline">\(i\)</span>, and <span class="math inline">\(x_{i,f+1}, \ldots, x_{i,f+c}\)</span> are the <span class="math inline">\(c\)</span> continuous variables related to this individual. The linear predictor <span class="math inline">\(\eta_i\)</span> for this individual will take the form:</p>
<p><span class="math display">\[
\eta_i=c+\sum_{j=1}^f\beta_jx_{ij}+\sum_{j=1}^cf_j(x_{i,f+j}).
\]</span></p>
<p>One can go even further and introduce interaction effects between a categorical variable and a continuous variable (typically, between gender and age of the insured individual in the context of auto insurance).</p>
</div>
</div>
<div id="practical-case-of-auto-insurance-pricing" class="section level2 hasAnchor" number="10.11">
<h2><span class="header-section-number">10.11</span> Practical Case of Auto Insurance Pricing<a href="chap9.html#practical-case-of-auto-insurance-pricing" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="portfolio-description" class="section level3 hasAnchor" number="10.11.1">
<h3><span class="header-section-number">10.11.1</span> Portfolio Description<a href="chap9.html#portfolio-description" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this section, we consider an automobile insurance portfolio, which we analyze using the widely used SAS software within insurance companies. The methods proposed can, of course, be adapted to other types of coverage (damage insurance for vehicles, fire insurance for buildings, theft insurance, travel cancellation insurance, etc.), taking into account specific characteristics of these contracts. The data comes from the portfolio of a large insurance company operating in Belgium. We have observations related to 158,061 policies observed during the year 1997. The variables included in the dataset are listed in Table <span class="math inline">\(\ref{TabVar}\)</span>.</p>
<p>Right from the start, let’s clarify some important points. First of all, except for the variables , , and describing the claims, all others are known variables {} to the insurer (meaning the insurer can use these variables to personalize the premium amount charged to the policyholder for risk coverage). Clearly, unknown risk characteristics of the insured at the beginning of the period cannot be used in the {} pricing grid. If necessary, the insurer will use them for other purposes (as we will see later). Among the explanatory variables available as listed in Table <span class="math inline">\(\ref{TabVar}\)</span>, we distinguish different types:</p>
<ol style="list-style-type: decimal">
<li>Those related to the policyholder (, , and in our example).</li>
<li>Those related to the insured vehicle (, , , and in our example).</li>
<li>Those related to the coverage chosen by the policyholder ( and , in our example).</li>
</ol>
<p>These variables are described in detail below. Note that information related to the insureds’ past claims (such as the number and/or cost of claims or a summary provided by the position in a bonus-malus scale or a reduction-increase coefficient applied to the premium) should not normally be incorporated into the {} pricing scheme. Indeed, the insurer will adjust the premium amount based on the claims reported by the insured using credibility theory or bonus-malus systems (described in the following chapters). Therefore, including past claims in {} pricing would result in a double penalty for policyholders who reported claims and underpricing for those who did not report any claims to the company.</p>
<table>
<caption><span id="tab:TabVar">Table 10.3: </span>Variables included in the dataset</caption>
<thead>
<tr class="header">
<th align="center">Varible</th>
<th align="center">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">AGES</td>
<td align="center">Age of the policyholder</td>
</tr>
<tr class="even">
<td align="center">AGGLOM</td>
<td align="center">Type of residential area of the policyholder</td>
</tr>
<tr class="odd">
<td align="center">CARB</td>
<td align="center">Fuel type</td>
</tr>
<tr class="even">
<td align="center">CTOT</td>
<td align="center">Total cost of claims (in euros)</td>
</tr>
<tr class="odd">
<td align="center">DUR</td>
<td align="center">Coverage duration (in days)</td>
</tr>
<tr class="even">
<td align="center">FRAC</td>
<td align="center">Premium fraction</td>
</tr>
<tr class="odd">
<td align="center">GARACCESS</td>
<td align="center">Coverage extent</td>
</tr>
<tr class="even">
<td align="center">IND</td>
<td align="center">Claim code</td>
</tr>
<tr class="odd">
<td align="center">KW</td>
<td align="center">Vehicle power</td>
</tr>
<tr class="even">
<td align="center">NSIN</td>
<td align="center">Number of claims</td>
</tr>
<tr class="odd">
<td align="center">SEXE</td>
<td align="center">Policyholder’s gender</td>
</tr>
<tr class="even">
<td align="center">SPORT</td>
<td align="center">Sportiness of the vehicle</td>
</tr>
<tr class="odd">
<td align="center">USAGE</td>
<td align="center">Vehicle usage</td>
</tr>
</tbody>
</table>
<p>Before starting the modeling of the number and cost of claims, it is essential to have a good understanding of the portfolio you are working with. Therefore, take the time to describe in detail the different rating variables and examine the composition of the portfolio to be analyzed.</p>
<p>We are working here with a policy dataset. Such a dataset has as many rows as policies in the portfolio during the considered period. It summarizes the information available at the beginning of the period for each of the contracts and describes the claims related to them. In addition to this dataset, the insurer also has a claims dataset where all the characteristics of the claims made by the policies in the portfolio are recorded (circumstances in which these claims occurred, people involved, presence of bodily injuries, etc.). These two datasets are linked through the policy number.</p>
<div class="remark">
<p><span id="unlabeled-div-40" class="remark"><em>Remark</em>. </span>The phase of building the database is crucial for the premium that will result from the analysis: how can you arrive at a correct premium using erroneous, incomplete, or outdated data? Therefore, it is essential for the actuary to be involved in building the database and not delegate the task without control to the IT department or a young intern fresh out of school. The data extraction and cleaning phase represents a significant portion of the study time.</p>
<p>It is important:
1. That the data is homogeneous, meaning it concerns policies from the same portfolio with similar conditions (if not, explanatory variables differentiating the categories of insureds or identifying the company that issued the policies should be introduced).
2. When merging data from multiple sources, to systematically identify their source (for example, when merging databases related to policies sold by brokers or directly, you should add a variable indicating the distribution channel through which the policy was issued).
3. To carefully examine missing data and not ignore it under any circumstances. Often, omitted information reveals certain characteristics of the policy. Therefore, a level should be added to categorical variables indicating when information is missing. It is only when you have ensured that the omissions are random that you can neglect policies with missing information.</p>
<p>Most companies are now aware of the need to have as many data as possible and of good quality. Building and maintaining databases are among the most important concerns of large financial groups. In this regard, the widespread use of electronic transmission of information (the broker or agent enters data to receive an online price quote) helps avoid encoding errors or missing values (since the quote is provided only if all fields are duly completed).</p>
</div>
</div>
<div id="variables-describing-claims" class="section level3 hasAnchor" number="10.11.2">
<h3><span class="header-section-number">10.11.2</span> Variables Describing Claims<a href="chap9.html#variables-describing-claims" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="number-of-claims" class="section level4 hasAnchor" number="10.11.2.1">
<h4><span class="header-section-number">10.11.2.1</span> Number of Claims ()<a href="chap9.html#number-of-claims" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>This is the number of claims reported by the insured to the company, not the number of claims caused by the insured during the year. The insured may decide (rightly or wrongly) to compensate the injured third party in the case of a minor loss (such as a scratch on an old car door, for example, for which a young owner might prefer a 100 Euro note, signifying festivities and student celebrations, rather than repairing the vehicle). It is obvious that the number of claims compensated directly by the insured depends on the insurer’s {} pricing policy (understood as how claims reported and resulting in compensation by the company are penalized). Therefore, you should be particularly careful when considering changes to the {} mechanisms for personalizing premium amounts. We will revisit these issues in the following chapters.</p>
<p>In auto liability insurance, deserves special attention as claim costs often do not lend themselves to detailed segmentation. Additionally, will play a central role in the {} personalization of premium amounts (most commercial systems, such as bonus-malus mechanisms, only incorporate the number of claims into the formula for premium reevaluation during the contract).</p>
<p>The significant advantage of is that it is generally known with precision by the company. Except for claims occurring at the end of the period, which will probably only be reported to the insurer at the beginning of the next period, claims are usually reported promptly to the company. This can happen either due to a contractual clause requiring reporting within a certain timeframe under penalty of loss of coverage or because the insured wishes to be compensated as quickly as possible.</p>
<p>The average frequency of claims for the portfolio is 12.45% per year. Table <span class="math inline">\(\ref{AjusPoisson}\)</span> shows that the maximum number of claims reported by a policyholder is 5. More specifically, 140,276 (or 88.75%) policyholders reported no claims, 16,085 (or 10.18%) reported 1 claim, 1,522 (or 0.96%) reported 2 claims, 159 (or 0.10%) policyholders reported 3 claims, 17 policyholders (or 0.01%) reported 4 claims, and 2 policyholders (less than 0.01%) reported 5 claims during the year 1997.</p>
<p>Table <span class="math inline">\(\ref{AjusPoisson}\)</span> describes the fitting of the observed distribution of to a Poisson distribution with a parameter <span class="math inline">\(\lambda\)</span> identical for all policies. The maximum likelihood estimator of the parameter is <span class="math inline">\(\widehat{\lambda} = 0.1245\)</span>. It is clear that the fit is very poor and is rejected without hesitation by a chi-squared test (observed chi-squared statistic value is 783.75, with a p-value less than <span class="math inline">\(10^{-4}\)</span>). If you observe the sequence of signs:</p>
<p><span class="math display">\[
\widehat{\Pr[\texttt{Nsin}=k]} - \Pr[\mathcal{P}oi(\widehat{\lambda})=k],\hspace{2mm}k\in{\mathbb{N}},
\]</span></p>
<p>given in the last column of Table <span class="math inline">\(\ref{AjusPoisson}\)</span>, you can clearly see the sequence of signs +,-,+,+,… This sequence is not random. It is due to a very important property of Poisson mixtures known as the “Shaked’s Two Crossings Theorem” (see Property 3.7.6 of Volume I).</p>
<p>The sequence of signs thus supports the hypothesis of a mixed Poisson distribution for at the portfolio level. This indicates that the portfolio is heterogeneous and justifies the {} differentiation of policyholders.</p>
<table>
<caption><span id="tab:AjusPoisson">Table 10.4: </span>Observed claims in the portfolio and adjustment
by a Poisson distribution</caption>
<colgroup>
<col width="21%" />
<col width="34%" />
<col width="36%" />
<col width="7%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Number of Claims</th>
<th align="center">Number of Observed Policies</th>
<th align="center">Number of Predicted Policies</th>
<th align="center">Sign</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">0</td>
<td align="center">140,276</td>
<td align="center">139,553.33</td>
<td align="center">+</td>
</tr>
<tr class="even">
<td align="center">1</td>
<td align="center">16,085</td>
<td align="center">17,379.16</td>
<td align="center">-</td>
</tr>
<tr class="odd">
<td align="center">2</td>
<td align="center">1,522</td>
<td align="center">1,082.15</td>
<td align="center">+</td>
</tr>
<tr class="even">
<td align="center">3</td>
<td align="center">159</td>
<td align="center">44.92</td>
<td align="center">+</td>
</tr>
<tr class="odd">
<td align="center">4</td>
<td align="center">17</td>
<td align="center">1.40</td>
<td align="center">+</td>
</tr>
<tr class="even">
<td align="center">5</td>
<td align="center">2</td>
<td align="center">0.04</td>
<td align="center">+</td>
</tr>
<tr class="odd">
<td align="center">6+</td>
<td align="center">0</td>
<td align="center">0.00</td>
<td align="center"></td>
</tr>
</tbody>
</table>
<p>%Let us now examine the adjustment of by the negative binomial distribution (obtained by assuming <span class="math inline">\(\Theta\)</span>
%has a <span class="math inline">\(\mathcal{G}(a,a)\)</span> distribution).</p>
</div>
<div id="claims-occurrence" class="section level4 hasAnchor" number="10.11.2.2">
<h4><span class="header-section-number">10.11.2.2</span> Claims Occurrence ()<a href="chap9.html#claims-occurrence" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>This is a binary variable indicating whether the insured has reported
at least one claim during the year, i.e.,
<span class="math display">\[
\texttt{IND} = \mathbb{I}[\texttt{NSIN} \geq 1] = \left\{
\begin{array}{l}
1, \text{ if }\texttt{NSIN} \geq 1,\\
0, \text{ otherwise}.
\end{array}
\right.
\]</span></p>
<p>Out of the 158,061 insureds in the portfolio, 140,276 (or 88.75%) did not report any claims, and 17,785 (or 11.25%) triggered the insurer’s coverage during the year.</p>
</div>
<div id="total-claim-cost-ctot" class="section level4 hasAnchor" number="10.11.2.3">
<h4><span class="header-section-number">10.11.2.3</span> Total Claim Cost <code>CTOT</code><a href="chap9.html#total-claim-cost-ctot" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>This represents the claims expense for the year 1997, i.e., the total cost (in euros) borne by the insured and covered by the company. It is the sum of payments, reserves, and claims management expenses that occurred in 1997. The monetary risk, therefore, has two components: the occurrence component , which informs us whether the insured has triggered the insurer’s coverage at least once during the period under consideration, and the total cost of reported claims (zero if =0). Therefore, can be represented as
<span class="math display">\[
\texttt{CTOT} = \texttt{IND} \times \texttt{CTOT}_+
\]</span>
where <span class="math inline">\(\texttt{CTOT}_+\)</span> has the same distribution as <span class="math inline">\(\texttt{CTOT}\)</span> given <span class="math inline">\(\texttt{CTOT}&gt;0\)</span> or as <span class="math inline">\(\texttt{CTOT}\)</span> given <span class="math inline">\(\text{IND}=1\)</span>. The variable <span class="math inline">\(\texttt{CTOT}_+\)</span> is strictly positive, whereas is mostly zero (zero for 88.75% of the insureds in our example).</p>
<p>If we consider the cost of each claim individually, we have
<span class="math display">\[
\texttt{CTOT} = \sum_{k=1}^{\texttt{NSIN}} C_k
\]</span>
where <span class="math inline">\(C_k\)</span> is the cost (assumed strictly positive) of the <span class="math inline">\(k\)</span>th claim (with the convention <span class="math inline">\(\texttt{CTOT}=0\)</span> if <span class="math inline">\(\texttt{NSIN}=0\)</span>). The average cost <span class="math inline">\(\overline{C}\)</span> of a claim affecting a policy is then given by
<span class="math display">\[
\overline{C} = \frac{\texttt{CTOT}_+}{\texttt{NSIN}}, \text{ if }\texttt{NSIN}&gt;0,
\]</span>
and <span class="math inline">\(\overline{C}=0\)</span> if <span class="math inline">\(\texttt{NSIN}=0\)</span>. When studying claim costs, it is important to take into account the number of claims generated by the insured. Therefore, <span class="math inline">\(\overline{C}\)</span> is often analyzed by introducing a weight .</p>
<div class="remark">
<p><span id="unlabeled-div-41" class="remark"><em>Remark</em>. </span>Another variable often used is the S/P ratio by
categories of policies (i.e., the total claims cost caused by this category of insureds divided by the collected premium amount). This variable depends on the tariff in force when the observations were collected. It introduces the old tariff into the development of the new one, and this usage is not recommended.</p>
</div>
<p>We know that only 17,785 insureds triggered the insurer’s coverage during the year 1997. Let’s focus on the claims experience of these insureds (i.e., the variable <span class="math inline">\(\texttt{CTOT}_+\)</span>). The skewness coefficient is 84.31, indicating a significant left skewness. More precisely, 25% of the policies that led to indemnity payments by the company had claims with amounts less than 145.02 Euros, 50% had amounts less than 566.51 Euros, and 75% had amounts less than 1,450.67 Euros. The mean of <span class="math inline">\(\texttt{CTOT}_+\)</span> is 1,807.46 Euros. Therefore, the empirical pure premium is
<span class="math display">\[
\widehat{\mathbb{E}[\texttt{CTOT}_+]}\widehat{\mathbb{E}[\texttt{IND}]} = \widehat{\mathbb{E}[\texttt{CTOT}]} = 203.38 Euros.
\]</span>
The coefficient of variation of <span class="math inline">\(\texttt{CTOT}_+\)</span> is 982.61, with a standard deviation of 17,760.39.</p>
<p>The four policies that caused the highest claims amounts were 452,839 Euros, 499,601 Euros, 499,917 Euros, and 1,989,568 Euros. It is often necessary to cap claims to analyze their costs. A common approach is to cap them at a quantile, for example, <span class="math inline">\(q_{0.99}\)</span>=19,927.4 Euros.%, <span class="math inline">\(q_{0.95}\)</span>=4,275.32 Euros, or <span class="math inline">\(q_{0.90}\)</span>=3,021.87 Euros.</p>
</div>
</div>
<div id="measuring-exposure-to-risk-the-variable" class="section level3 hasAnchor" number="10.11.3">
<h3><span class="header-section-number">10.11.3</span> Measuring Exposure to Risk: the Variable<a href="chap9.html#measuring-exposure-to-risk-the-variable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This represents the number of days the policy was in force during the year 1997. It will be used to measure exposure to risk. It allows us to account for the fact that a claim reported by a policy in force for 1 month is a worse sign for the insurer than a claim related to a policy in force for the entire year.</p>
<div class="remark">
<p><span id="unlabeled-div-42" class="remark"><em>Remark</em>. </span>Note in passing that exposure to risk should ideally be measured by the number of kilometers driven rather than the number of days the policy was in force (the vehicle could well remain in the garage during certain periods, therefore not exposed to risk). However, measuring the annual mileage is extremely difficult, so most European insurers have abandoned the idea of introducing annual mileage into their pricing criteria and have preferred to use “proxies” such as vehicle usage (vehicles used for professional purposes likely cover more kilometers per year) or fuel type (the choice of diesel is often justified by covering longer distances).</p>
</div>
<p>In our portfolio, the average coverage duration is 323.93 days. Some policies were in force for only one day. Figure <span class="math inline">\(\ref{HistDuree}\)</span> gives an idea of the coverage periods for the different policies in the portfolio. We can see a majority of policies covered for the entire year and a more or less uniform distribution of coverage durations less than a year. Typically, policies with exposure to risk of less than a year are new policies and cancellations. The portfolio we are studying is relatively stable.</p>
</div>
<div id="characteristics-of-the-policyholder" class="section level3 hasAnchor" number="10.11.4">
<h3><span class="header-section-number">10.11.4</span> Characteristics of the Policyholder<a href="chap9.html#characteristics-of-the-policyholder" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="variable" class="section level4 hasAnchor" number="10.11.4.1">
<h4><span class="header-section-number">10.11.4.1</span> Variable<a href="chap9.html#variable" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>This is a quantitative variable with integer values representing the age of the policyholder (in completed years) as of January 1, 1997. More than age itself, it is driving experience that we hope to capture through this variable. An equivalent variable (with a high correlation between them) often more acceptable to clients is the length of time the driver’s license has been held.</p>
<p>Policies often include the concept of the habitual driver of the vehicle, and the personal characteristics determining the premium amount are those of the habitual driver mentioned in the specific conditions, not those of the policyholder. It is worth noting that personal characteristics (such as ) often refer to the policyholder, who is not necessarily the driver of the vehicle. Therefore, conclusions obtained based on this data should be considered with caution.</p>
<p>Now let’s examine the age structure of insureds in our portfolio. This is described in Figure <span class="math inline">\(\ref{HistAge1}\)</span>. Let’s calculate the observed claim frequency by age. This is a Poisson regression model with a single explanatory variable, namely, the age of the insured treated as a categorical variable. If <span class="math inline">\(\lambda_k\)</span> represents the annual claim frequency of insureds aged <span class="math inline">\(k\)</span> years, the likelihood associated with the data is
<span class="math display">\[
\mathcal{L} = \prod_{i=1}^n \exp(-d_i\lambda_{\texttt{AGES}_i}) \frac{(d_i\lambda_{\texttt{AGES}_i})^{n_i}}{n_i!}
\]</span>
where <span class="math inline">\(\texttt{AGES}_i\)</span> represents the age of insured <span class="math inline">\(i\)</span>, <span class="math inline">\(d_i\)</span> is the exposure duration for policy <span class="math inline">\(i\)</span>, and <span class="math inline">\(n_i\)</span> is the number of claims related to this policy. Maximizing the log-likelihood is equivalent to solving the system
<span class="math display">\[
\frac{\partial}{\partial\lambda_k}\left\{\sum_{i|\texttt{AGES}_i=k}d_i\lambda_k+\sum_{i|\texttt{AGES}_i=k}n_i\ln\lambda_k\right\}=0,
\]</span>
which provides the maximum likelihood estimators of claim frequencies at each age, given by
<span class="math display">\[
\widehat{\lambda}_k = \frac{\sum_{i|\texttt{AGES}_i=k}n_i}{\sum_{i|\texttt{AGES}_i=k}d_i}.
\]</span>
Therefore, it is necessary to divide the number of claims by the total exposure duration for each age, not by the number of policies. Figure <span class="math inline">\(\ref{HistAge2}\)</span> presents <span class="math inline">\(\widehat{\lambda}_k\)</span> as a function of age <span class="math inline">\(k\)</span>. It is clear that young drivers have a higher claim frequency, approaching 30%. With age, the annual claim frequency tends to decrease, slightly increasing among the older insureds.</p>
<p>Now, let’s see if can explain certain variations in claim costs. To do this, we restrict our study to insureds who reported at least one claim. The mean of the variable by age is described in Figure <span class="math inline">\(\ref{AgeCout}\)</span>. No particular structure is detected when examining the graph, indicating that the variable may influence claim frequency but not their cost.</p>
<p>It should be emphasized that the above analysis is purely marginal. The apparent influence of age on claim frequency could thus be caused by another variable strongly correlated with age (we refer the reader to the comments in Volume I).</p>
</div>
<div id="variable-1" class="section level4 hasAnchor" number="10.11.4.2">
<h4><span class="header-section-number">10.11.4.2</span> Variable <a href="chap9.html#variable-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>This binary qualitative variable indicates whether an antitheft device is installed in the vehicle:</p>
<p><span class="math display">\[
\texttt{VOL}= \left\{
\begin{array}{l}
0,\text{ if the vehicle has no antitheft device},\\
1,\text{ if the vehicle has an antitheft device}.
\end{array}
\right.
\]</span></p>
<p>In the portfolio, 122,869 policies (77.91%) do not have an antitheft device, while 34,192 policies (21.68%) have one. Figure <span class="math inline">\(\ref{Vol}\)</span> shows that policies with vehicles equipped with antitheft devices tend to have a lower claim frequency and lower average claim costs.</p>
</div>
<div id="variable-2" class="section level4 hasAnchor" number="10.11.4.3">
<h4><span class="header-section-number">10.11.4.3</span> Variable <a href="chap9.html#variable-2" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>This variable describes whether the policyholder has opted for personal insurance coverage, which provides additional coverage for personal injuries:</p>
<p><span class="math display">\[
\texttt{PERS}= \left\{
\begin{array}{l}
0,\text{ if personal insurance is not chosen},\\
1,\text{ if personal insurance is chosen}.
\end{array}
\right.
\]</span></p>
<p>In the portfolio, 107,389 policies (68.04%) do not include personal insurance, while 50,672 policies (31.96%) have it. Figure <span class="math inline">\(\ref{Pers}\)</span> illustrates the influence of personal insurance on claim frequency and average claim costs. Policies with personal insurance coverage tend to have a slightly higher claim frequency but lower average claim costs.</p>
</div>
<div id="variable-3" class="section level4 hasAnchor" number="10.11.4.4">
<h4><span class="header-section-number">10.11.4.4</span> Variable <a href="chap9.html#variable-3" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>This binary variable indicates whether the policyholder has had previous insurance coverage:</p>
<p><span class="math display">\[
\texttt{RES}= \left\{
\begin{array}{l}
0,\text{ if the policyholder has not had previous insurance},\\
1,\text{ if the policyholder has had previous insurance}.
\end{array}
\right.
\]</span></p>
<p>In the portfolio, 144,117 policies (91.27%) are for policyholders without previous insurance, while 13,944 policies (8.83%) are for policyholders with previous insurance coverage. Figure <span class="math inline">\(\ref{Res}\)</span> shows that policyholders with previous insurance tend to have a slightly lower claim frequency but slightly higher average claim costs.</p>
</div>
<div id="variable-4" class="section level4 hasAnchor" number="10.11.4.5">
<h4><span class="header-section-number">10.11.4.5</span> Variable <a href="chap9.html#variable-4" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>This variable describes the policyholder’s profession, which is categorized into different groups. Unfortunately, the specific categories are not provided in the text. However, it is mentioned that policyholders with certain professions have a higher claim frequency. Figure <span class="math inline">\(\ref{Prof}\)</span> illustrates the influence of the policyholder’s profession on claim frequency and average claim costs.</p>
</div>
<div id="variable-5" class="section level4 hasAnchor" number="10.11.4.6">
<h4><span class="header-section-number">10.11.4.6</span> Variable <a href="chap9.html#variable-5" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>This binary variable indicates whether the policyholder has opted for additional coverage for voluntary civil liability (VPP stands for “Volontaire de Protection Personnelle” in French):</p>
<p><span class="math display">\[
\texttt{VPP}= \left\{
\begin{array}{l}
0,\text{ if voluntary civil liability coverage is not chosen},\\
1,\text{ if voluntary civil liability coverage is chosen}.
\end{array}
\right.
\]</span></p>
<p>In the portfolio, 138,779 policies (87.91%) do not include voluntary civil liability coverage, while 19,282 policies (12.19%) have it. Figure <span class="math inline">\(\ref{Vpp}\)</span> shows that policies with voluntary civil liability coverage tend to have a slightly higher claim frequency but slightly lower average claim costs.</p>
</div>
</div>
<div id="vehicle-characteristics" class="section level3 hasAnchor" number="10.11.5">
<h3><span class="header-section-number">10.11.5</span> Vehicle Characteristics<a href="chap9.html#vehicle-characteristics" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="variable-6" class="section level4 hasAnchor" number="10.11.5.1">
<h4><span class="header-section-number">10.11.5.1</span> Variable <a href="chap9.html#variable-6" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>This variable represents the car brand. Unfortunately, specific brand names are not provided in the text. However, it is mentioned that different car brands have varying impacts on claim frequency and claim costs. The impact of the car brand on claim frequency and average claim costs is illustrated in Figure <span class="math inline">\(\ref{Marque}\)</span>.</p>
</div>
<div id="variable-7" class="section level4 hasAnchor" number="10.11.5.2">
<h4><span class="header-section-number">10.11.5.2</span> Variable <a href="chap9.html#variable-7" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>This variable represents the age of the insured vehicle in completed years. Policies with different vehicle ages have varying impacts on claim frequency and claim costs. The influence of vehicle age on claim frequency and average claim costs is illustrated in Figure <span class="math inline">\(\ref{Carage}\)</span>.</p>
</div>
<div id="variable-8" class="section level4 hasAnchor" number="10.11.5.3">
<h4><span class="header-section-number">10.11.5.3</span> Variable <a href="chap9.html#variable-8" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>This variable represents the driver’s bonus-malus class, which is a measure of the driver’s claim history and experience. Policies with different bonus-malus classes have varying impacts on claim frequency and claim costs. The influence of the bonus-malus class on claim frequency and average claim costs is illustrated in Figure <span class="math inline">\(\ref{Dami}\)</span>.</p>
</div>
<div id="variable-9" class="section level4 hasAnchor" number="10.11.5.4">
<h4><span class="header-section-number">10.11.5.4</span> Variable <a href="chap9.html#variable-9" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>This variable represents the power of the insured vehicle, measured in horsepower. Policies with vehicles of different power levels have varying impacts on claim frequency and claim costs. The influence of vehicle power on claim frequency and average claim costs is illustrated in Figure <span class="math inline">\(\ref{Puiss}\)</span>.</p>
</div>
<div id="variable-10" class="section level4 hasAnchor" number="10.11.5.5">
<h4><span class="header-section-number">10.11.5.5</span> Variable <a href="chap9.html#variable-10" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>This binary qualitative variable describes the extent of coverage chosen by the insured:</p>
<p><span class="math display">\[
\texttt{GARACCESS}= \left\{
\begin{array}{l}
0,\text{ if only liability insurance (RC) is subscribed by the insured},\\
1,\text{ if, in addition to liability insurance, the insured has subscribed}\\
\hspace{10mm}\text{for additional coverage, such as}\\
\hspace{10mm}\text{property damage or theft, for example}.
\end{array}
\right.
\]</span></p>
<p>Two types of behavior can be expected here:</p>
<ol style="list-style-type: decimal">
<li>The theory of moral hazard applies, and more claims are observed from individuals with more coverage. Similarly, according to the signaling theory, insured individuals who have only subscribed to liability insurance have done so knowingly, knowing that they are less risky. Therefore, the 0 category of should appear as a factor decreasing claims.</li>
<li>The subscription to more extensive coverage reflects a stronger aversion to risk on the part of the individuals involved. This would result in a particularly cautious driving behavior and, therefore, fewer claims. In this case, the 0 category of becomes an aggravating factor.</li>
</ol>
<p>Actuaries, being pragmatic by nature, do not try to determine which mechanism prevails and limit themselves to observing, based on data, the effect of different modalities of .</p>
<p>In our portfolio, 93,194 insured individuals have exclusively subscribed to liability insurance (representing 58.96% of the portfolio), while the other 64,867 (41.04% of the portfolio) have subscribed to additional coverage. In terms of the influence of the extent of coverage on claim frequency, Figure <span class="math inline">\(\ref{HistGar}\)</span> shows the observed claim frequency and the average claim cost per claim as a function of the extent of coverage. It can be seen that both claim frequency and average claim cost per claim are higher for policies that only include liability insurance. This technically justifies the reductions in liability insurance premiums granted to insured individuals who subscribe to other optional coverages.</p>
</div>
</div>
<div id="interaction-between-rating-variables-1" class="section level3 hasAnchor" number="10.11.6">
<h3><span class="header-section-number">10.11.6</span> Interaction Between Rating Variables<a href="chap9.html#interaction-between-rating-variables-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Often, an interaction between the age and gender of the insured is observed. By interaction, it is meant here that the influence of the insured’s age on claim frequency varies depending on whether one is male or female. In our portfolio, if we distinguish between males and females, we obtain the claim frequencies by age described in Figure <span class="math inline">\(\ref{FreqSinFemmes}\)</span>. It can be observed that young males have a slightly higher claim frequency compared to young females, and then the claim frequencies are quite similar. We do not continue the modeling of interactions here.</p>
</div>
<div id="initial-screening-of-rating-variables" class="section level3 hasAnchor" number="10.11.7">
<h3><span class="header-section-number">10.11.7</span> Initial Screening of Rating Variables<a href="chap9.html#initial-screening-of-rating-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="chi-squared-test-of-independence" class="section level4 hasAnchor" number="10.11.7.1">
<h4><span class="header-section-number">10.11.7.1</span> Chi-squared Test of Independence<a href="chap9.html#chi-squared-test-of-independence" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>An effective way to perform an initial screening among the variables at our disposal is to conduct chi-squared tests based on contingency tables. Due to sample sizes, it is better to work with than with . If we cross-tabulate with , many expected cell counts are less than 5, making it invalid to perform a chi-squared test based on such a contingency table. However, if we cross-tabulate and , we obtain the contingency table described in Table <span class="math inline">\(\ref{ContIndSex}\)</span> (the expected cell counts under the independence hypothesis are indicated in parentheses), on which we can base the chi-squared test. The observed value of the chi-squared statistic for independence is 20.33 (for 1 degree of freedom), which gives a <span class="math inline">\(p\)</span>-value less than <span class="math inline">\(10^{-4}\)</span>. Thus, there is a strong association between gender and whether or not an insured individual has had a claim.</p>
<table>
<caption><span id="tab:ContIndSex">Table 10.5: </span>Contingency table cross-tabulating IND and SEXE</caption>
<thead>
<tr class="header">
<th align="center">IND</th>
<th align="center">SEXE</th>
<th align="center">Female</th>
<th align="center">Male</th>
<th align="center">Total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">No claim</td>
<td align="center">Female</td>
<td align="center">36,722</td>
<td align="center">103,554</td>
<td align="center">140,276</td>
</tr>
<tr class="even">
<td align="center"></td>
<td align="center">Male</td>
<td align="center">(36,972)</td>
<td align="center">(103,304)</td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="center">One or more claims</td>
<td align="center">Female</td>
<td align="center">4,937</td>
<td align="center">12,848</td>
<td align="center">17,785</td>
</tr>
<tr class="even">
<td align="center"></td>
<td align="center">Male</td>
<td align="center">(4,687.5)</td>
<td align="center">(13,098)</td>
<td align="center"></td>
</tr>
</tbody>
</table>
<p>The rejection of the independence hypothesis indicates that the gender of the policyholder influences the variable and, therefore, also the variable . However, non-rejection does not allow us to conclude, partly for reasons related to the error of the second kind (which is not controlled), and partly because the lack of influence on does not imply a lack of influence on (since the explanatory variable can influence the distribution in the 1, 2, claims categories).</p>
<p>By successively cross-tabulating all rating variables with , we obtain the results shown in Table <span class="math inline">\(\ref{TabChi2}\)</span>. It can be seen that does not seem to influence . Therefore, we could exclude this variable from further analysis.</p>
<table>
<caption><span id="tab:TabChi2">Table 10.6: </span>Results of chi-squared tests on contingency tables cross-tabulating rating variables and IND</caption>
<colgroup>
<col width="15%" />
<col width="43%" />
<col width="27%" />
<col width="13%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Variable</th>
<th align="center">Observed Value of Q Statistic</th>
<th align="center">Degrees of Freedom</th>
<th align="center">p-value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">FRAC</td>
<td align="center">375.47</td>
<td align="center">3</td>
<td align="center"><span class="math inline">\(&lt;.0001\)</span></td>
</tr>
<tr class="even">
<td align="center">CARB</td>
<td align="center">139.22</td>
<td align="center">1</td>
<td align="center"><span class="math inline">\(&lt;.0001\)</span></td>
</tr>
<tr class="odd">
<td align="center">SPORT</td>
<td align="center">6.73</td>
<td align="center">1</td>
<td align="center">0.0094</td>
</tr>
<tr class="even">
<td align="center">GARACCESS</td>
<td align="center">22.60</td>
<td align="center">1</td>
<td align="center"><span class="math inline">\(&lt;.0001\)</span></td>
</tr>
<tr class="odd">
<td align="center">AGGLOM</td>
<td align="center">202.04</td>
<td align="center">2</td>
<td align="center"><span class="math inline">\(&lt;.0001\)</span></td>
</tr>
<tr class="even">
<td align="center">USAGE</td>
<td align="center">0.07</td>
<td align="center">1</td>
<td align="center">0.7956</td>
</tr>
<tr class="odd">
<td align="center">KW</td>
<td align="center">23.40</td>
<td align="center">2</td>
<td align="center"><span class="math inline">\(&lt;.0001\)</span></td>
</tr>
</tbody>
</table>
</div>
<div id="true-and-apparent-dependencies-1" class="section level4 hasAnchor" number="10.11.7.2">
<h4><span class="header-section-number">10.11.7.2</span> True and Apparent Dependencies<a href="chap9.html#true-and-apparent-dependencies-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Note that the rejection of the independence hypothesis between and indicates that the gender of the policyholder influences the probability of having at least one claim during the period. However, this influence can be either a true dependency, where the probability of occurrence genuinely depends on the gender of the policyholder, or an apparent dependency, where the probability of having at least one claim depends on a variable correlated with gender (whether hidden, like aggressiveness while driving, or observable, like the age of the policyholder, if age structures differ between males and females). In the latter case, the dependency between the gender of the policyholder and having at least one claim would disappear if the third variable were taken into account.</p>
</div>
</div>
<div id="analysis-of-claim-frequencies" class="section level3 hasAnchor" number="10.11.8">
<h3><span class="header-section-number">10.11.8</span> Analysis of Claim Frequencies<a href="chap9.html#analysis-of-claim-frequencies" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="poisson-regression-model-for-the-number-of-claims" class="section level4 hasAnchor" number="10.11.8.1">
<h4><span class="header-section-number">10.11.8.1</span> Poisson Regression Model for the Number of Claims<a href="chap9.html#poisson-regression-model-for-the-number-of-claims" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Let <span class="math inline">\(N_i\)</span> be the number of claims reported by insured individual <span class="math inline">\(i\)</span> during the year 1997, where <span class="math inline">\(i=1,2,\ldots,n\)</span>. We denote <span class="math inline">\(d_i\)</span> as the exposure to risk for individual <span class="math inline">\(i\)</span>. The Poisson model assumes that the conditional distribution of <span class="math inline">\(N_i\)</span> given <span class="math inline">\(\boldsymbol{x}_i\)</span> follows a Poisson distribution. Therefore, we need to specify its mean <span class="math inline">\(\mathbb{E}[N_i|\boldsymbol{x}_i]\)</span>. Since the mean is strictly positive, it is typically modeled as an exponential linear form:</p>
<p><span class="math display">\[\begin{equation}
\label{RegPoiss1}
\mathbb{E}[N_i|\boldsymbol{x}_i]=d_i\exp(\boldsymbol{\beta}^\top\boldsymbol{x}_i), \quad i=1,2,\ldots,n,
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{\beta}\)</span> is a vector of unknown regression coefficients. Thus, we work with the model:</p>
<p><span class="math display">\[\begin{equation}
\label{RegPoiss2}
N_i\sim\mathcal{P}oi\left(d_i\exp(\boldsymbol{\beta}^\top\boldsymbol{x}_i)\right), \quad i=1,2,\ldots,n.
\end{equation}\]</span></p>
<p>When all variables are categorical, each insured individual is represented by a vector <span class="math inline">\(\boldsymbol{x}_i\)</span> where the components are either 0 or 1. In this case, the annual frequency <span class="math inline">\(\lambda_i=\exp(\boldsymbol{\beta}^\top\boldsymbol{x}_i)\)</span> appears as a product of enhancement or reduction factors relative to the frequency of the reference individual in the portfolio. More precisely,</p>
<p><span class="math display">\[\begin{eqnarray*}
\lambda_i&amp;=&amp;\exp(\boldsymbol{\beta}^\top\boldsymbol{x}_i)\\
&amp;=&amp;\exp(\beta_0)\prod_{j=1}^p\exp(\beta_jx_{ij})\\
&amp;=&amp;\exp(\beta_0)\prod_{j|x_{ij}=1}\exp(\beta_j).
\end{eqnarray*}\]</span></p>
<p>Therefore, <span class="math inline">\(\exp(\beta_0)\)</span> is the annual frequency of the reference individual in the portfolio, while each of the factors <span class="math inline">\(\exp(\beta_j)\)</span> represents the influence of a segmentation criterion (if <span class="math inline">\(\beta_j&gt;0\)</span>, individuals with this characteristic will experience a premium increase compared to the reference premium <span class="math inline">\(\exp(\beta_0)\)</span>, whereas <span class="math inline">\(\beta_j&lt;0\)</span> indicates a premium reduction).</p>
<p>The parameters have the following interpretation: considering the characteristic coded by the <span class="math inline">\(j\)</span>-th binary variable,</p>
<p><span class="math display">\[
\frac{\mathbb{E}[N_i|\text{ characteristic present}]}{\mathbb{E}[N_i|\text{ characteristic absent}]}=\exp(\beta_j).
\]</span></p>
<p>Based on this equation, <span class="math inline">\(\exp(\beta_j)\)</span> is the factor by which we need to multiply the claim frequency of individuals not presenting the characteristic coded by the <span class="math inline">\(j\)</span>-th binary variable to obtain the claim frequency of individuals presenting this characteristic.</p>
</div>
<div id="treatment-of-age-with-a-generalized-additive-model" class="section level4 hasAnchor" number="10.11.8.2">
<h4><span class="header-section-number">10.11.8.2</span> Treatment of Age with a Generalized Additive Model<a href="chap9.html#treatment-of-age-with-a-generalized-additive-model" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We begin with a Poisson regression model of the GAM type to detect the influence of the variable . Specifically, the effect of the policyholder’s age on the annual claim frequency is represented by a function <span class="math inline">\(f_{\texttt{AGES}}\)</span> decomposed using cubic splines and then estimated based on available statistics. Table <span class="math inline">\(\ref{EstGam}\)</span> shows the fit of the linear part of the model. We learn that the sport code is not relevant (<span class="math inline">\(p\)</span>-value of 0.6981). Similarly (<span class="math inline">\(p\)</span>-value of 0.3612), the variable could be removed from the model. Table <span class="math inline">\(\ref{AnDev}\)</span> compares the models with and without the nonlinear part in . We can see that the influence of this continuous rating variable is clearly nonlinear (<span class="math inline">\(p\)</span>-value less than 0.0001).</p>
<table style="width:100%;">
<caption><span id="tab:EstGam">Table 10.7: </span>Parameter estimation for the linear part of the Poisson GAM model</caption>
<colgroup>
<col width="32%" />
<col width="12%" />
<col width="14%" />
<col width="13%" />
<col width="27%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Parameter</th>
<th align="center">Estimate</th>
<th align="center">Error</th>
<th align="center"><span class="math inline">\(t\)</span> value</th>
<th align="center">Pr <span class="math inline">\(&gt; &amp;#124;t&amp;#124;\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Intercept</td>
<td align="center">-1.42117</td>
<td align="center">0.08396</td>
<td align="center">-16.93</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="even">
<td align="center">CARB Diesel</td>
<td align="center">0.17744</td>
<td align="center">0.01582</td>
<td align="center">11.21</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="odd">
<td align="center">CARB Essence</td>
<td align="center">0</td>
<td align="center">.</td>
<td align="center">.</td>
<td align="center">.</td>
</tr>
<tr class="even">
<td align="center">FRAC Annuel</td>
<td align="center">-0.20345</td>
<td align="center">0.01475</td>
<td align="center">-13.79</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="odd">
<td align="center">FRAC Fractionné</td>
<td align="center">0</td>
<td align="center">.</td>
<td align="center">.</td>
<td align="center">.</td>
</tr>
<tr class="even">
<td align="center">SPORT Non sportif</td>
<td align="center">-0.02708</td>
<td align="center">0.06983</td>
<td align="center">-0.39</td>
<td align="center">0.6981</td>
</tr>
<tr class="odd">
<td align="center">SPORT Sportif</td>
<td align="center">0</td>
<td align="center">.</td>
<td align="center">.</td>
<td align="center">.</td>
</tr>
<tr class="even">
<td align="center">GARACCESS RC Seule</td>
<td align="center">0.09806</td>
<td align="center">0.01494</td>
<td align="center">6.56</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="odd">
<td align="center">GARACCESS RC+Accessoires</td>
<td align="center">0</td>
<td align="center">.</td>
<td align="center">.</td>
<td align="center">.</td>
</tr>
<tr class="even">
<td align="center">AGGLOM Rural</td>
<td align="center">-0.22783</td>
<td align="center">0.01519</td>
<td align="center">-15</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="odd">
<td align="center">AGGLOM Urbain</td>
<td align="center">0</td>
<td align="center">.</td>
<td align="center">.</td>
<td align="center">.</td>
</tr>
<tr class="even">
<td align="center">SEXE Femme</td>
<td align="center">0.05386</td>
<td align="center">0.01646</td>
<td align="center">3.27</td>
<td align="center">0.0011</td>
</tr>
<tr class="odd">
<td align="center">SEXE Homme</td>
<td align="center">0</td>
<td align="center">.</td>
<td align="center">.</td>
<td align="center">.</td>
</tr>
<tr class="even">
<td align="center">KW Cylindrée moyenne</td>
<td align="center">0.09516</td>
<td align="center">0.01852</td>
<td align="center">5.14</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="odd">
<td align="center">KW Grosse cylindrée</td>
<td align="center">0.20646</td>
<td align="center">0.02393</td>
<td align="center">8.63</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="even">
<td align="center">KW Petite cylindrée</td>
<td align="center">0</td>
<td align="center">.</td>
<td align="center">.</td>
<td align="center">.</td>
</tr>
<tr class="odd">
<td align="center">USAGE Privé</td>
<td align="center">-0.03167</td>
<td align="center">0.03468</td>
<td align="center">-0.91</td>
<td align="center">0.3612</td>
</tr>
<tr class="even">
<td align="center">USAGE Professionnel</td>
<td align="center">0</td>
<td align="center">.</td>
<td align="center">.</td>
<td align="center">.</td>
</tr>
<tr class="odd">
<td align="center">Linear(AGES)</td>
<td align="center">-0.01267</td>
<td align="center">0.00050444</td>
<td align="center">-25.11</td>
<td align="center">$&lt;.0001</td>
</tr>
</tbody>
</table>
<table>
<caption><span id="tab:AnDev">Table 10.8: </span>Fit of the nonlinear part of the Poisson GAM</caption>
<colgroup>
<col width="10%" />
<col width="15%" />
<col width="18%" />
<col width="26%" />
<col width="11%" />
<col width="8%" />
<col width="8%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">splines</th>
<th align="center">Smoothing.Parameter</th>
<th align="center">DF..Degrees.of.Freedom.</th>
<th align="center">GCV..Generalized.Cross.Validation.</th>
<th align="center">Num.Unique.Obs</th>
<th align="center">Chi.Square</th>
<th align="center">Pr…ChiSq</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Spline(AGES)</td>
<td align="center">0.999630</td>
<td align="center">13.629696</td>
<td align="center">0.6607040</td>
<td align="center">78</td>
<td align="center">511.6004</td>
<td align="center">$&lt;.0001</td>
</tr>
</tbody>
</table>
<p>Omitting the variables and does not significantly degrade the model (the deviance changes from 85,798.498 to 85,799.492). We can see the results of the fit in Table <span class="math inline">\(\ref{EstGam2}\)</span>. Table <span class="math inline">\(\ref{AnDev2}\)</span> informs us that the nonlinear effect of is still highly significant.</p>
<table style="width:100%;">
<caption><span id="tab:EstGam2">Table 10.9: </span>Parameter estimation for the linear part of the Poisson GAM model without the SPORT and USAGE variables</caption>
<colgroup>
<col width="32%" />
<col width="12%" />
<col width="14%" />
<col width="13%" />
<col width="27%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Parameter</th>
<th align="center">Estimate</th>
<th align="center">Error</th>
<th align="center"><span class="math inline">\(t\)</span> value</th>
<th align="center">Pr <span class="math inline">\(&gt; &amp;#124;t&amp;#124;\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Intercept</td>
<td align="center">-1.47918</td>
<td align="center">0.03291</td>
<td align="center">-44.95</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="even">
<td align="center">CARB Diesel</td>
<td align="center">0.17828</td>
<td align="center">0.01576</td>
<td align="center">11.31</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="odd">
<td align="center">CARB Essence</td>
<td align="center">0</td>
<td align="center">.</td>
<td align="center">.</td>
<td align="center">.</td>
</tr>
<tr class="even">
<td align="center">FRAC Annuel</td>
<td align="center">-0.20299</td>
<td align="center">0.01475</td>
<td align="center">-13.76</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="odd">
<td align="center">FRAC Fractionné</td>
<td align="center">0</td>
<td align="center">.</td>
<td align="center">.</td>
<td align="center">.</td>
</tr>
<tr class="even">
<td align="center">GARACCESS RC Seule</td>
<td align="center">0.09752</td>
<td align="center">0.01492</td>
<td align="center">6.54</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="odd">
<td align="center">GARACCESS RC+Accessoires</td>
<td align="center">0</td>
<td align="center">.</td>
<td align="center">.</td>
<td align="center">.</td>
</tr>
<tr class="even">
<td align="center">AGGLOM Rural</td>
<td align="center">-0.22785</td>
<td align="center">0.01519</td>
<td align="center">-15</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="odd">
<td align="center">AGGLOM Urbain</td>
<td align="center">0</td>
<td align="center">.</td>
<td align="center">.</td>
<td align="center">.</td>
</tr>
<tr class="even">
<td align="center">SEXE Femme</td>
<td align="center">0.05378</td>
<td align="center">0.01646</td>
<td align="center">3.27</td>
<td align="center">0.0011</td>
</tr>
<tr class="odd">
<td align="center">SEXE Homme</td>
<td align="center">0</td>
<td align="center">.</td>
<td align="center">.</td>
<td align="center">.</td>
</tr>
<tr class="even">
<td align="center">KW Cylindrée moyenne</td>
<td align="center">0.0955</td>
<td align="center">0.01851</td>
<td align="center">5.16</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="odd">
<td align="center">KW Grosse cylindrée</td>
<td align="center">0.21003</td>
<td align="center">0.02345</td>
<td align="center">8.96</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="even">
<td align="center">KW Petite cylindrée</td>
<td align="center">0</td>
<td align="center">.</td>
<td align="center">.</td>
<td align="center">.</td>
</tr>
<tr class="odd">
<td align="center">Linear(AGES)</td>
<td align="center">-0.01267</td>
<td align="center">0.00050334</td>
<td align="center">-25.17</td>
<td align="center">$&lt;.0001</td>
</tr>
</tbody>
</table>
<table>
<caption><span id="tab:AnDev2">Table 10.10: </span>Fit of the nonlinear part of the Poisson GAM without the SPORT and USAGE variables</caption>
<colgroup>
<col width="10%" />
<col width="15%" />
<col width="18%" />
<col width="26%" />
<col width="11%" />
<col width="8%" />
<col width="10%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">splines</th>
<th align="center">Smoothing.Parameter</th>
<th align="center">DF..Degrees.of.Freedom.</th>
<th align="center">GCV..Generalized.Cross.Validation.</th>
<th align="center">Num.Unique.Obs</th>
<th align="center">Chi.Square</th>
<th align="center">Pr…..ChiSq</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Spline(AGES)</td>
<td align="center">0.999646</td>
<td align="center">13.440987</td>
<td align="center">0.678901</td>
<td align="center">78</td>
<td align="center">486.5207</td>
<td align="center">$&lt;.0001</td>
</tr>
</tbody>
</table>
<p>Now let’s examine the influence of on the claim frequency. Figure <span class="math inline">\(\ref{AllfAGES}\)</span> provides information on this: it displays the coefficient by which the frequency of the reference class should be multiplied based on the age of the policyholder, i.e., <span class="math inline">\(\exp\left(\widehat{\beta}_{\texttt{AGES}}\texttt{AGES}+\widehat{f}_{\texttt{AGES}}(\texttt{AGES})\right)\)</span>.</p>
<p>Based on Figure <span class="math inline">\(\ref{AllfAGES}\)</span>, we decide to categorize the variable as follows:
<span class="math display">\[
\texttt{AGESGROUP}= \left\{
\begin{array}{l}
\text{Beginner, if }18-21\text{ years},\\
\text{Young, if }22-30\text{ years},\\
\text{Experienced, if }31-55\text{ years},\\
\text{Senior, if }56-75\text{ years},\\
\text{Elderly driver, if over }75\text{ years}.
\end{array}
\right.
\]</span>
Of course, this choice has an arbitrary component. Let’s now examine the variable . The portfolio contains 1,212 beginner drivers (0.77%), 22,685 young drivers (14.35%), 89,326 experienced drivers (56.51%), 40,199 seniors (25.43%), and 4,639 elderly drivers (2.93%). A chi-square independence test between and leads to its rejection (observed test statistic value of 860.15 for 4 degrees of freedom, yielding a <span class="math inline">\(p\)</span>-value less than <span class="math inline">\(0.0001\)</span>). Figure <span class="math inline">\(\ref{HistAgesGroup}\)</span> shows that the most represented class is that of experienced drivers. We also observe a “bowl-like” shape in the number and average claim amounts: high values for the young driver classes, a clear decrease over the years, and a slight increase for the elderly driver class.</p>
</div>
<div id="estimation-of-annual-claim-frequencies" class="section level4 hasAnchor" number="10.11.8.3">
<h4><span class="header-section-number">10.11.8.3</span> Estimation of Annual Claim Frequencies<a href="chap9.html#estimation-of-annual-claim-frequencies" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Now that the variable has been categorized, we revert to a Poisson regression model of the GLM type. The model fitting is given in Table <span class="math inline">\(\ref{AjusGENMOD}\)</span>, while Table <span class="math inline">\(\ref{Type3}\)</span> provides the <span class="math inline">\(p\)</span>-values of the tests used to exclude each variable from the model one by one (Type 3 analysis in SAS jargon).
The point estimates of <span class="math inline">\(\beta_j\)</span> are provided in the third column of Table <span class="math inline">\(\ref{AjusGENMOD}\)</span>, with the first two columns identifying the level to which the regression coefficient refers. Rows with 0 correspond to the reference levels of the different rating variables.
The “Wald <span class="math inline">\(95\%\)</span> Conf Limit” column contains the lower and upper bounds of the <span class="math inline">\(95\%\)</span> confidence intervals for the parameters, calculated using the formula
<span class="math display">\[
\text{Coeff}\, \beta_j \pm 1.96 \, \text{Std Error}\, \beta_j,
\]</span>
where 1.96 is the <span class="math inline">\(97.5\%\)</span> quantile of the standard normal distribution, and Std Error is the square root of the <span class="math inline">\(j\)</span>-th diagonal element of <span class="math inline">\(\widehat{\boldsymbol{\Sigma}}\)</span> given in the fourth column.</p>
<p>The “Chi-Sq” and “Pr<span class="math inline">\(&gt;\)</span>ChiSq” columns, which represent the associated <span class="math inline">\(p\)</span>-value, are used to test if the coefficient <span class="math inline">\(\beta_j\)</span> is significantly different from 0. This test is performed using the Wald statistic
<span class="math display">\[
\frac{(\text{Coeff}\, \beta_j)^2}{(\text{Std Error}\, \beta_j)^2},
\]</span>
which is approximately chi-squared with 1 degree of freedom. We reject the nullity of <span class="math inline">\(\beta_j\)</span> when the <span class="math inline">\(p\)</span>-value is less than <span class="math inline">\(5\%\)</span>.</p>
<p>The Type 3 analysis allows us to examine the contribution of each variable compared to a model without it. The “ChiSquare” column calculates, for each variable, twice the difference between the log-likelihood obtained for the model containing all variables and the log-likelihood of the model without the specific explanatory variable. This statistic is asymptotically distributed as a chi-squared with DF degrees of freedom, where DF is the number of parameters associated with the examined explanatory variable. The last column provides the <span class="math inline">\(p\)</span>-value associated with the likelihood ratio test, allowing us to assess the contribution of this explanatory variable to modeling the studied phenomenon.</p>
<table>
<caption><span id="tab:AjusGENMOD">Table 10.11: </span>Poisson regression model fitting</caption>
<colgroup>
<col width="16%" />
<col width="2%" />
<col width="6%" />
<col width="10%" />
<col width="23%" />
<col width="23%" />
<col width="7%" />
<col width="9%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Parameter</th>
<th align="center">DF</th>
<th align="center">Estimate</th>
<th align="center">Standard Error</th>
<th align="center">Wald 95% Confidence Limits (Lower)</th>
<th align="center">Wald 95% Confidence Limits (Upper)</th>
<th align="center">Chi-Square</th>
<th align="center">Pr <span class="math inline">\(&gt;\)</span> ChiSq</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Intercept</td>
<td align="center">1</td>
<td align="center">-1.9575</td>
<td align="center">0.0176</td>
<td align="center">-1.992</td>
<td align="center">-1.923</td>
<td align="center">12387.1</td>
<td align="center">&lt;.0001</td>
</tr>
<tr class="even">
<td align="center">CARB Diesel</td>
<td align="center">1</td>
<td align="center">0.1869</td>
<td align="center">0.0159</td>
<td align="center">0.1558</td>
<td align="center">0.2179</td>
<td align="center">138.68</td>
<td align="center">&lt;.0001</td>
</tr>
<tr class="odd">
<td align="center">CARB Essence</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">.</td>
<td align="center">.</td>
</tr>
<tr class="even">
<td align="center">FRAC Annuel</td>
<td align="center">1</td>
<td align="center">-0.277</td>
<td align="center">0.0147</td>
<td align="center">-0.3058</td>
<td align="center">-0.2481</td>
<td align="center">353.8</td>
<td align="center">&lt;.0001</td>
</tr>
<tr class="odd">
<td align="center">FRAC Fractionné</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">.</td>
<td align="center">.</td>
</tr>
<tr class="even">
<td align="center">SPORT Sportif</td>
<td align="center">1</td>
<td align="center">0.0391</td>
<td align="center">0.0698</td>
<td align="center">-0.0978</td>
<td align="center">0.176</td>
<td align="center">0.31</td>
<td align="center">0.5754</td>
</tr>
<tr class="odd">
<td align="center">SPORT Non sportif</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">.</td>
<td align="center">.</td>
</tr>
<tr class="even">
<td align="center">GARACCESS RC+Accessoires</td>
<td align="center">1</td>
<td align="center">-0.1342</td>
<td align="center">0.0149</td>
<td align="center">-0.1635</td>
<td align="center">-0.1049</td>
<td align="center">80.68</td>
<td align="center">&lt;.0001</td>
</tr>
<tr class="odd">
<td align="center">GARACCESS RC Seule</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">.</td>
<td align="center">.</td>
</tr>
<tr class="even">
<td align="center">AGGLOM Urbain</td>
<td align="center">1</td>
<td align="center">0.2376</td>
<td align="center">0.0152</td>
<td align="center">0.2078</td>
<td align="center">0.2674</td>
<td align="center">244.51</td>
<td align="center">&lt;.0001</td>
</tr>
<tr class="odd">
<td align="center">AGGLOM Rural</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">.</td>
<td align="center">.</td>
</tr>
<tr class="even">
<td align="center">SEXE Femme</td>
<td align="center">1</td>
<td align="center">0.0701</td>
<td align="center">0.0165</td>
<td align="center">0.0378</td>
<td align="center">0.1023</td>
<td align="center">18.13</td>
<td align="center">&lt;.0001</td>
</tr>
<tr class="odd">
<td align="center">SEXE Homme</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">.</td>
<td align="center">.</td>
</tr>
<tr class="even">
<td align="center">KW Grosse cylindrée</td>
<td align="center">1</td>
<td align="center">0.1274</td>
<td align="center">0.0203</td>
<td align="center">0.0876</td>
<td align="center">0.1672</td>
<td align="center">39.35</td>
<td align="center">&lt;.0001</td>
</tr>
<tr class="odd">
<td align="center">KW Petite cylindrée</td>
<td align="center">1</td>
<td align="center">-0.0867</td>
<td align="center">0.0185</td>
<td align="center">-0.1231</td>
<td align="center">-0.0504</td>
<td align="center">21.91</td>
<td align="center">&lt;.0001</td>
</tr>
<tr class="even">
<td align="center">KW Cylindrée moyenne</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">.</td>
<td align="center">.</td>
</tr>
<tr class="odd">
<td align="center">AGESGRP Débutant</td>
<td align="center">1</td>
<td align="center">0.7902</td>
<td align="center">0.0578</td>
<td align="center">0.6769</td>
<td align="center">0.9034</td>
<td align="center">186.92</td>
<td align="center">&lt;.0001</td>
</tr>
<tr class="even">
<td align="center">AGESGRP Conducteur âgé</td>
<td align="center">1</td>
<td align="center">-0.189</td>
<td align="center">0.0482</td>
<td align="center">-0.2135</td>
<td align="center">-0.0244</td>
<td align="center">6.09</td>
<td align="center">0.1360</td>
</tr>
<tr class="odd">
<td align="center">AGESGRP Jeune</td>
<td align="center">1</td>
<td align="center">0.3891</td>
<td align="center">0.0184</td>
<td align="center">0.353</td>
<td align="center">0.4252</td>
<td align="center">446.29</td>
<td align="center">&lt;.0001</td>
</tr>
<tr class="even">
<td align="center">AGESGRP Senior</td>
<td align="center">1</td>
<td align="center">-0.232</td>
<td align="center">0.0192</td>
<td align="center">-0.2696</td>
<td align="center">-0.1943</td>
<td align="center">145.77</td>
<td align="center">&lt;.0001</td>
</tr>
<tr class="odd">
<td align="center">AGESGRP Expérimenté</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">.</td>
<td align="center">.</td>
</tr>
<tr class="even">
<td align="center">USAGE Professionnel</td>
<td align="center">1</td>
<td align="center">0.0269</td>
<td align="center">0.0347</td>
<td align="center">-0.0411</td>
<td align="center">0.0949</td>
<td align="center">0.6</td>
<td align="center">0.4386</td>
</tr>
<tr class="odd">
<td align="center">USAGE Privé</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">.</td>
<td align="center">.</td>
</tr>
</tbody>
</table>
<table>
<caption><span id="tab:Type3">Table 10.12: </span>Likelihood ratio test statistics for Type 3 analysis</caption>
<thead>
<tr class="header">
<th align="center">Source</th>
<th align="center">DF</th>
<th align="center">Chi-Square</th>
<th align="center">Pr &gt; ChiSq</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">CARB</td>
<td align="center">1</td>
<td align="center">136.63</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="even">
<td align="center">FRAC</td>
<td align="center">1</td>
<td align="center">357.5</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="odd">
<td align="center">SPORT</td>
<td align="center">1</td>
<td align="center">0.31</td>
<td align="center">0.5775</td>
</tr>
<tr class="even">
<td align="center">GARACCESS</td>
<td align="center">1</td>
<td align="center">81.36</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="odd">
<td align="center">AGGLOM</td>
<td align="center">1</td>
<td align="center">237.72</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="even">
<td align="center">SEXE</td>
<td align="center">1</td>
<td align="center">17.96</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="odd">
<td align="center">KW</td>
<td align="center">2</td>
<td align="center">78.79</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="even">
<td align="center">AGESGRP</td>
<td align="center">4</td>
<td align="center">878.05</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="odd">
<td align="center">USAGE</td>
<td align="center">1</td>
<td align="center">0.6</td>
<td align="center">0.4404</td>
</tr>
</tbody>
</table>
<p>We start by excluding the variable , which is considered the least significant (<span class="math inline">\(p\)</span>-value of 57.75% in Table <span class="math inline">\(\ref{Type3}\)</span>). The resulting model (results not provided) still yields a high <span class="math inline">\(p\)</span>-value for the variable (44.28%). Therefore, we move directly to the next model by removing the variable. Next, to simplify the pricing model, we consider whether it would be possible to group certain levels of the variable. To do this, we test the hypothesis of equality of coefficients taken two by two, as shown in Table <span class="math inline">\(\ref{2a2}\)</span>. The obtained <span class="math inline">\(p\)</span>-value for the levels “Senior” and “Elderly driver” clearly indicates that we can group these levels (into one senior level) for the subsequent analysis. The final model, taking into account all these modifications, is presented in Tables <span class="math inline">\(\ref{AjusGENMODFin}\)</span> and <span class="math inline">\(\ref{Type3Fin}\)</span>.</p>
<table>
<caption><span id="tab:2a2">Table 10.13: </span>Tests of equality of coefficients taken two by two for the variable {AGESGRP}</caption>
<thead>
<tr class="header">
<th align="center">Contrast</th>
<th align="center">DF</th>
<th align="center">Chi-Square</th>
<th align="center">Pr &gt; ChiSq</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Beginner-Young</td>
<td align="center">1</td>
<td align="center">41.21</td>
<td align="center"><span class="math inline">\(&lt;\)</span>.0001</td>
</tr>
<tr class="even">
<td align="center">Elderly-Driver</td>
<td align="center">1</td>
<td align="center">5.06</td>
<td align="center">0.2450</td>
</tr>
<tr class="odd">
<td align="center">Young-Experienced</td>
<td align="center">1</td>
<td align="center">421.67</td>
<td align="center"><span class="math inline">\(&lt;\)</span>.0001</td>
</tr>
<tr class="even">
<td align="center">Experienced-Elderly</td>
<td align="center">1</td>
<td align="center">151.15</td>
<td align="center"><span class="math inline">\(&lt;\)</span>.0001</td>
</tr>
</tbody>
</table>
<table>
<caption><span id="tab:AjusGENMODFin">Table 10.14: </span>Poisson regression model fit, final model</caption>
<colgroup>
<col width="29%" />
<col width="3%" />
<col width="8%" />
<col width="13%" />
<col width="23%" />
<col width="10%" />
<col width="10%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Parameter</th>
<th align="center">DF</th>
<th align="center">Estimate</th>
<th align="center">Standard Error</th>
<th align="center">Wald 95% Confidence Limits</th>
<th align="center">Chi-Square</th>
<th align="center">Pr &gt; ChiSq</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Intercept</td>
<td align="center">1</td>
<td align="center">-1.9564</td>
<td align="center">0.0176</td>
<td align="center">(-1.9909, -1.922)</td>
<td align="center">12398.00</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="even">
<td align="center">CARB Diesel</td>
<td align="center">1</td>
<td align="center">0.1856</td>
<td align="center">0.0158</td>
<td align="center">(0.1547, 0.2165)</td>
<td align="center">138.28</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="odd">
<td align="center">CARB Gasoline</td>
<td align="center">0</td>
<td align="center">0.0000</td>
<td align="center">0.0000</td>
<td align="center">(0, 0)</td>
<td align="center">0.00</td>
<td align="center">.</td>
</tr>
<tr class="even">
<td align="center">ANNUAL Fraction</td>
<td align="center">1</td>
<td align="center">-0.2751</td>
<td align="center">0.0147</td>
<td align="center">(-0.3039, -0.2463)</td>
<td align="center">350.38</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="odd">
<td align="center">FRACTIONED Fraction</td>
<td align="center">0</td>
<td align="center">0.0000</td>
<td align="center">0.0000</td>
<td align="center">(0, 0)</td>
<td align="center">0.00</td>
<td align="center">.</td>
</tr>
<tr class="even">
<td align="center">GARAGE ACCESSORIES RC+Accessories</td>
<td align="center">1</td>
<td align="center">-0.1340</td>
<td align="center">0.0149</td>
<td align="center">(-0.1632, -0.1048)</td>
<td align="center">80.71</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="odd">
<td align="center">GARAGE ACCESSORIES RC Only</td>
<td align="center">0</td>
<td align="center">0.0000</td>
<td align="center">0.0000</td>
<td align="center">(0, 0)</td>
<td align="center">0.00</td>
<td align="center">.</td>
</tr>
<tr class="even">
<td align="center">URBAN AGGLOMERATION</td>
<td align="center">1</td>
<td align="center">0.2377</td>
<td align="center">0.0152</td>
<td align="center">(0.2079, 0.2674)</td>
<td align="center">244.69</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="odd">
<td align="center">RURAL AGGLOMERATION</td>
<td align="center">0</td>
<td align="center">0.0000</td>
<td align="center">0.0000</td>
<td align="center">(0, 0)</td>
<td align="center">0.00</td>
<td align="center">.</td>
</tr>
<tr class="even">
<td align="center">GENDER Female</td>
<td align="center">1</td>
<td align="center">0.0691</td>
<td align="center">0.0164</td>
<td align="center">(0.0369, 0.1014)</td>
<td align="center">17.67</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="odd">
<td align="center">GENDER Male</td>
<td align="center">0</td>
<td align="center">0.0000</td>
<td align="center">0.0000</td>
<td align="center">(0, 0)</td>
<td align="center">0.00</td>
<td align="center">.</td>
</tr>
<tr class="even">
<td align="center">LARGE DISPLACEMENT KW</td>
<td align="center">1</td>
<td align="center">0.1296</td>
<td align="center">0.0198</td>
<td align="center">(0.0907, 0.1684)</td>
<td align="center">42.81</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="odd">
<td align="center">SMALL DISPLACEMENT KW</td>
<td align="center">1</td>
<td align="center">-0.0860</td>
<td align="center">0.0185</td>
<td align="center">(-0.1223, -0.0497)</td>
<td align="center">21.57</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="even">
<td align="center">MEDIUM DISPLACEMENT KW</td>
<td align="center">0</td>
<td align="center">0.0000</td>
<td align="center">0.0000</td>
<td align="center">(0, 0)</td>
<td align="center">0.00</td>
<td align="center">.</td>
</tr>
<tr class="odd">
<td align="center">AGESGRP Beginner</td>
<td align="center">1</td>
<td align="center">0.7894</td>
<td align="center">0.0578</td>
<td align="center">(0.6761, 0.9026)</td>
<td align="center">186.65</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="even">
<td align="center">AGESGRP Young</td>
<td align="center">1</td>
<td align="center">0.3891</td>
<td align="center">0.0184</td>
<td align="center">(0.3531, 0.4252)</td>
<td align="center">448.41</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="odd">
<td align="center">AGESGRP Senior</td>
<td align="center">1</td>
<td align="center">-0.2213</td>
<td align="center">0.0185</td>
<td align="center">(-0.2576, -0.185)</td>
<td align="center">142.67</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="even">
<td align="center">AGESGRP Experienced</td>
<td align="center">0</td>
<td align="center">0.0000</td>
<td align="center">0.0000</td>
<td align="center">(0, 0)</td>
<td align="center">0.00</td>
<td align="center">.</td>
</tr>
</tbody>
</table>
<table>
<caption><span id="tab:Type3Fin">Table 10.15: </span>Likelihood ratio statistics for Type 3 analysis, final model</caption>
<thead>
<tr class="header">
<th align="center">Source</th>
<th align="center">DF</th>
<th align="center">Chi.Square</th>
<th align="center">Pr…ChiSq</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">CARB</td>
<td align="center">1</td>
<td align="center">136.18</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="even">
<td align="center">FRACTION</td>
<td align="center">1</td>
<td align="center">353.97</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="odd">
<td align="center">GARAGE ACCESSORIES</td>
<td align="center">1</td>
<td align="center">81.39</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="even">
<td align="center">AGGLOMERATION</td>
<td align="center">1</td>
<td align="center">237.89</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="odd">
<td align="center">GENDER</td>
<td align="center">1</td>
<td align="center">17.51</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="even">
<td align="center">KW</td>
<td align="center">2</td>
<td align="center">83.70</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="odd">
<td align="center">AGESGRP</td>
<td align="center">3</td>
<td align="center">877.65</td>
<td align="center">$&lt;.0001</td>
</tr>
</tbody>
</table>
<p>For the insured individual <span class="math inline">\(i\)</span> characterized by a vector of explanatory variables <span class="math inline">\(\boldsymbol{x}_i\)</span>, the predicted annual frequency is <span class="math inline">\(\exp(\boldsymbol{x}_i^\top\widehat{\boldsymbol{\beta}})\)</span>. This will also be the case for new insured individuals with the same characteristics as insured individual <span class="math inline">\(i\)</span> (the implicit assumption being that new policies are taken out by individuals who perfectly match the characteristics of the insured individuals that form the basis of the tariff construction; this assumes, among other things, that the company effectively manages adverse selection through a careful acceptance policy).</p>
</div>
<div id="overdispersion" class="section level4 hasAnchor" number="10.11.8.4">
<h4><span class="header-section-number">10.11.8.4</span> Overdispersion<a href="chap9.html#overdispersion" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The Poisson model imposes strong constraints on the
dependence between the count variable <span class="math inline">\(N_i\)</span> and the risk factors <span class="math inline">\(\boldsymbol{x}_i\)</span>, since
<span class="math display">\[\begin{equation}
\label{MoyVarEq995}
\mathbb{E}[N_i|\boldsymbol{x}_i]=\mathbb{V}[N_i|\boldsymbol{x}_i]=d_i\exp(\boldsymbol{\beta}^\top\boldsymbol{x}_i).
\end{equation}\]</span>
This implies an equality between the mean number of claims and the variability of this number within each risk class. However, it should be noted that the convergence of the maximum likelihood pseudo-estimators obtained in the Poisson model allows their use even if the Poisson distribution is not appropriate (provided that the conditional mean is correctly specified).</p>
<p>In practice, to check the validity of constraint <span class="math inline">\(\eqref{MoyVarEq995}\)</span>, we calculate the empirical mean and variance of claims for each risk class, denoted as <span class="math inline">\(\widehat{m}_k\)</span> and <span class="math inline">\(\widehat{\sigma}_k^2\)</span>, respectively. Then, we plot the data points <span class="math inline">\(\{(\widehat{m}_k, \widehat{\sigma}_k^2), k=1,2,\ldots\}\)</span> in a graph. This allows us to observe how the variance changes with the mean. When the points cluster around the first bisector, it can be considered that the first two conditional moments are equal, supporting the Poisson model. Conversely, when <span class="math inline">\(\widehat{\sigma}_k^2&gt;\widehat{m}_k\)</span>, we often observe overdispersion, meaning that some classes have <span class="math inline">\(\widehat{\sigma}_k^2&gt;\widehat{m}_k\)</span>. This phenomenon is often due to omitted variables.</p>
<p>We can understand this phenomenon by considering two risk classes, <span class="math inline">\(C_1\)</span> and <span class="math inline">\(C_2\)</span>, without overdispersion (<span class="math inline">\(\widehat{\sigma}_1^2=\widehat{m}_1\)</span> and <span class="math inline">\(\widehat{\sigma}_2^2=\widehat{m}_2\)</span>), but which have been mistakenly combined. In the class <span class="math inline">\(C_1\cup C_2\)</span>, the mean is given by
<span class="math display">\[
\widehat{m}=p_1\widehat{m}_1+p_2\widehat{m}_2,
\]</span>
where <span class="math inline">\(p_1\)</span> and <span class="math inline">\(p_2\)</span> represent the relative weights of <span class="math inline">\(C_1\)</span> and <span class="math inline">\(C_2\)</span>, respectively. The variance becomes
<span class="math display">\[
\widehat{\sigma}^2=p_1\widehat{\sigma}_1^2+p_2\widehat{\sigma}_2^2+p_1(\widehat{m}_1-\widehat{m})^2+p_2(\widehat{m}_2-\widehat{m})^2.
\]</span>
Therefore, in <span class="math inline">\(C_1\cup C_2\)</span>, there is overdispersion because <span class="math inline">\(\widehat{\sigma}^2&gt;\widehat{m}\)</span>, with equality only possible if <span class="math inline">\(\widehat{m}_1=\widehat{m}_2\)</span>. It is thus understandable that the omission of important explanatory variables can lead to overdispersion of observations within risk classes.</p>
<p>Based on our portfolio, the data points <span class="math inline">\(\{(\widehat{m}_k, \widehat{\sigma}_k^2), k=1,2,\ldots\}\)</span> are represented in the graph in Figure <span class="math inline">\(\ref{FigSurdisp}\)</span>.
The effect of overdispersion is clearly visible.</p>
</div>
<div id="consequences-of-specification-error-in-the-poisson-model" class="section level4 hasAnchor" number="10.11.8.5">
<h4><span class="header-section-number">10.11.8.5</span> Consequences of Specification Error in the Poisson Model<a href="chap9.html#consequences-of-specification-error-in-the-poisson-model" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>As mentioned earlier, the Poisson model is relatively restrictive as it assumes equidispersion of the data. Often, as in our dataset, this assumption is not met. It is interesting to examine what happens if the Poisson model is misspecified. Specifically, suppose that the true form of the conditional mean is now
<span class="math display">\[\begin{equation}
\label{SpecMoy}
\mathbb{E}[N_i|\boldsymbol{x}_i]=d_i\exp(\boldsymbol{\beta}_0^\top\boldsymbol{x}_i),
\end{equation}\]</span>
where <span class="math inline">\(\boldsymbol{\beta}_0\)</span> is the true parameter value, but the conditional distribution of <span class="math inline">\(N_i\)</span> given <span class="math inline">\(\boldsymbol{x}_i\)</span> is not Poisson. The estimator <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> obtained by solving the likelihood equations is no longer a maximum likelihood estimator, but rather an estimator calculated with specification error. This is referred to as the pseudo-maximum likelihood estimator. However, the pseudo-maximum likelihood estimator based on the Poisson model with mean <span class="math inline">\(\exp(\boldsymbol{\beta}^\top\boldsymbol{x}_i)\)</span> is consistent for the true value <span class="math inline">\(\boldsymbol{\beta}_0\)</span> if the mean is of the form <span class="math inline">\(\eqref{SpecMoy}\)</span>. This comes from the likelihood equations obtained in the Poisson regression model, which can still be written as
<span class="math display">\[
\mathbb{E}\Big[\big(N_i-\mathbb{E}[N_i|\boldsymbol{x}_i]\big)\boldsymbol{x}_i\Big]=\boldsymbol{0}
\]</span>
<span class="math display">\[
\Leftrightarrow\mathbb{E}\Big[d_i\exp(\boldsymbol{\beta}_0^\top\boldsymbol{x}_i)-d_i\exp(\widehat{\boldsymbol{\beta}}^\top\boldsymbol{x}_i)\Big]\boldsymbol{x}_i=\boldsymbol{0}
\]</span>
which ensures that <span class="math inline">\(\widehat{\boldsymbol{\beta}}\to_{\text{prob}}\boldsymbol{\beta}_0\)</span> as the number of observations <span class="math inline">\(n\to +\infty\)</span>. Only the correct specification of <span class="math inline">\(\mathbb{E}[N_i|\boldsymbol{x}_i]\)</span> is necessary to obtain consistent estimators.</p>
<p>Specification error does not affect the consistency of the estimator in large samples. However, specification error requires a modification of the calculation of the asymptotic variance, which is now given by <span class="math inline">\(\boldsymbol{H}^{-1}\mathcal{I}\boldsymbol{H}\)</span>, where
<span class="math display">\[
\boldsymbol{H}=\sum_{i=1}^n\boldsymbol{x}_i\boldsymbol{x}_i^\topd_i\exp(\boldsymbol{\beta}_0^\top\boldsymbol{x}_i)
\text{ and }
\mathcal{I}=\sum_{i=1}^n\boldsymbol{x}_i\boldsymbol{x}_i^\top\mathbb{V}[N_i|\boldsymbol{x}_i].
\]</span>
In practice, the asymptotic variance-covariance matrix will be estimated as <span class="math inline">\(\widehat{\boldsymbol{H}}^{-1}\widehat{\mathcal{I}}\widehat{\boldsymbol{H}}\)</span> where
<span class="math display">\[
\widehat{\boldsymbol{H}}=\sum_{i=1}^n\boldsymbol{x}_i\boldsymbol{x}_i^\top d_i\exp(\widehat{\boldsymbol{\beta}}^\top\boldsymbol{x}_i)
\text{ and }
\widehat{\mathcal{I}}=\sum_{i=1}^n\boldsymbol{x}_i\boldsymbol{x}_i^\top(n_i-\lambda_i)^2.
\]</span></p>
<p>Overdispersion (indicating specification error) has little effect on point estimates in large samples. However, overdispersion leads to an underestimation of the variances of the estimators, resulting in overly narrow confidence intervals and overestimation of chi-squared statistics used to test the nullity of regression coefficients. Consequently, a variable deemed relevant in the Poisson model may no longer be relevant after overdispersion has been taken into account.</p>
</div>
<div id="negative-binomial-regression-model" class="section level4 hasAnchor" number="10.11.8.6">
<h4><span class="header-section-number">10.11.8.6</span> Negative Binomial Regression Model<a href="chap9.html#negative-binomial-regression-model" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>A simple and effective technique to account for overdispersion is to introduce a random error term into the linear predictor (recognizing the heterogeneity of insureds within each tariff class: even though they are identical for the company, they still have relatively different risk profiles). If we denote <span class="math inline">\(\epsilon_i\)</span> as this error term, we assume that
<span class="math display">\[
[N_i|\boldsymbol{x}_i,\epsilon_i]\sim\mathcal{P}oi\big(d_i\exp(\boldsymbol{\beta}^\top\boldsymbol{x}_i+\epsilon_i)\big).
\]</span>
Let <span class="math inline">\(\Theta_i=\exp(\epsilon_i)\)</span> and require <span class="math inline">\(\mathbb{E}[\Theta_i]=1\)</span>. Thus,
<span class="math display">\[
\mathbb{E}[N_i|\boldsymbol{x}_i]=d_i\exp(\boldsymbol{\beta}^\top\boldsymbol{x}_i)\mathbb{E}[\Theta_i]=d_i\exp(\boldsymbol{\beta}^\top\boldsymbol{x}_i)=\lambda_i;
\]</span>
the addition of the error term <span class="math inline">\(\epsilon_i\)</span> to the linear predictor does not change the prior mean of <span class="math inline">\(N_i\)</span>, conditioned on the observables <span class="math inline">\(\boldsymbol{x}_i\)</span>. This property ensures that the tariff is correct on average, and that the premium collected for a tariff class is sufficient to compensate for claims. As
<span class="math display">\[\begin{eqnarray*}
\mathbb{V}[N_i|\boldsymbol{x}_i]&amp;=&amp;\mathbb{E}\Big[\mathbb{V}[N_i|\boldsymbol{x}_i,\Theta_i]\Big]+\mathbb{V}\Big[\mathbb{E}[N_i|\boldsymbol{x}_i,\Theta_i]\Big] \\
&amp; = &amp; \mathbb{E}[\lambda_i\Theta_i]+\mathbb{V}[\lambda_i\Theta_i]=\lambda_i\big\{1+\sigma^2\lambda_i\big\}&gt;\lambda_i=\mathbb{E}[N_i|\boldsymbol{x}_i]
\end{eqnarray*}\]</span>
where we have set <span class="math inline">\(\sigma^2=\mathbb{V}[\Theta_i]\)</span>, the introduction of a random error term to the linear predictor automatically leads to data overdispersion.</p>
</div>
<div id="justification-for-introducing-the-error-epsilon" class="section level4 hasAnchor" number="10.11.8.7">
<h4><span class="header-section-number">10.11.8.7</span> Justification for Introducing the Error <span class="math inline">\(\epsilon\)</span><a href="chap9.html#justification-for-introducing-the-error-epsilon" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Many relevant explanatory variables cannot be observed by the insurer (for legal or economic reasons). Of course, some hidden variables could be correlated with the observable variables <span class="math inline">\(\boldsymbol{X}\)</span>. For example, annual mileage is a hidden variable for the company, but it is likely to be correlated with the use of the vehicle (private-professional), which is an observable variable for the company. The idea is to represent the residual effect of hidden variables by an error term <span class="math inline">\(\epsilon\)</span> that would be superimposed on the score. Technically, <span class="math inline">\(\epsilon\)</span> is assumed to be independent of the vector <span class="math inline">\(\boldsymbol{X}\)</span> of observable characteristics of the insured.</p>
<p>For an individual such that <span class="math inline">\(\boldsymbol{X}=\boldsymbol{x}\)</span>, the annual number of claims <span class="math inline">\(N\)</span> follows a Poisson distribution with mean <span class="math inline">\(\exp(\boldsymbol{\beta}^\top\boldsymbol{x}+\epsilon)\)</span>, where <span class="math inline">\(\epsilon\)</span> is a random variable representing the influence of omitted variables in the tariff. <span class="math inline">\(\epsilon\)</span> is called a random effect and represents the residual heterogeneity of the portfolio. The annual number of claims <span class="math inline">\(N\)</span> becomes a mixture of Poisson distributions, reflecting the fact that each risk class (defined by <span class="math inline">\(\boldsymbol{X}=\boldsymbol{x}\)</span>) contains a mixture of insureds with different risk profiles on unobservable factors.</p>
<p>If we denote <span class="math inline">\(f_\Theta\)</span> as the probability density function of <span class="math inline">\(\Theta_i\)</span>, we obtain
<span class="math display">\[\begin{eqnarray*}
\Pr[N_i=k|\boldsymbol{x}_i]&amp;=&amp;\int_{\theta\in{\mathbb{R}}^+}\Pr[N_i=k|\boldsymbol{x}_i,\Theta_i=\theta]f_\Theta(\theta)d\theta\\
&amp;=&amp;\int_{\theta\in{\mathbb{R}}^+}\exp(-\lambda_i\theta)\frac{(\lambda_i\theta)^k}{k!}f_\Theta(\theta)d\theta.
\end{eqnarray*}\]</span>
For most choices of <span class="math inline">\(f_\Theta\)</span>, this integral does not have an explicit expression. A notable exception is the density associated with the Gamma distribution, for which we obtain the Negative Binomial distribution for <span class="math inline">\([N_i|\boldsymbol{x}_i]\)</span>. Considering that the random effects <span class="math inline">\(\Theta_i=\exp(\epsilon_i)\)</span>, <span class="math inline">\(i=1,2,\ldots,n\)</span>, are independent and identically distributed as Gamma with mean 1 and variance <span class="math inline">\(1/a\)</span>, the density of <span class="math inline">\(\Theta_i\)</span> is given by
<span class="math display">\[
f_\Theta(\theta)=\frac{1}{\Gamma(a)}a^a\theta^{a-1}\exp(-a\theta),\hspace{2mm}\theta\in{\mathbb{R}}^+.
\]</span>
The choice of the density <span class="math inline">\(f_\Theta\)</span> is justified by purely analytical considerations (the Gamma distribution being the conjugate prior to the Poisson distribution). Conditionally on the observable variables <span class="math inline">\(\boldsymbol{x}_i\)</span> and the random effect <span class="math inline">\(\Theta_i\)</span>, the discrete probability density of <span class="math inline">\(N_i\)</span> is given by
<span class="math display">\[
\Pr[N_i=n_i|\boldsymbol{x}_{i},\Theta_i=\theta_i]=\exp\Big(-\theta_id_i\exp(\eta_i)\Big)\frac{\big\{\theta_id_i\exp(\eta_i)\big\}^{n_i}}{n_i!}.
\]</span>
Conditionally on the observables, <span class="math inline">\(N_i\)</span> follows a Poisson distribution with mean <span class="math inline">\(d_i\exp(\eta_i)\Theta_i\)</span>. Unconditionally, <span class="math inline">\(N_i\)</span> follows a Negative Binomial distribution, and the probabilities are
\[2mm]
<span class="math inline">\(\Pr[N_i=n_i|\boldsymbol{x}_i]\)</span>
<span class="math display">\[\begin{eqnarray*}
%&amp;=&amp;
%\int_{\theta\in{\Reel}^+}\Pr[N_i=n_i|\xvec_i,\Theta_i=\theta]f_\Theta(\theta)d\theta\\
&amp;=&amp;\left(\begin{array}{c}a+n_i-1 \\n_i\end{array}\right)\left(\frac{d_i\exp(\eta_i)}{a+d_i\exp(\eta_i)}\right)^{n_i}
\left(\frac{a}{a+d_i\exp(\eta_i)}\right)^a,
\end{eqnarray*}\]</span>
for <span class="math inline">\(n_i\in{\mathbb{N}}\)</span>.</p>
<p>While the maximum likelihood estimator <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> in the Poisson model does not coincide with that in the model with random effects, the estimates obtained can be considered the results of the generalized method of moments. Indeed, equation <span class="math inline">\(\eqref{EqVraisPois984}\)</span>, for which the maximum likelihood estimator <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> is a solution, can be viewed as the empirical counterpart of the equation
<span class="math display">\[\begin{equation}
\label{EmpMom}
\mathbb{E}\left[\sum_{i=1}^n\big\{N_i-\lambda_i\big\}\boldsymbol{x}_i\right]=\boldsymbol{0},
\end{equation}\]</span>
which is valid in both the simple Poisson model and the model with random effects. Therefore, if one is willing to abandon the maximum likelihood method in favor of the generalized method of moments, the estimates obtained in the model without random effects can be retained.</p>
<p>It is relatively simple to obtain an estimation of <span class="math inline">\(\sigma^2=\mathbb{V}[\Theta_i]\)</span> using the method of moments. To do this, note that
<span class="math display">\[\begin{eqnarray}
\mathbb{V}[N_{i}] &amp; = &amp; \mathbb{E}\Big[\mathbb{V}[N_{i}|\Theta_i]\Big]+\mathbb{V}\Big[\mathbb{E}[N_{i}|\Theta_i]\Big]\nonumber\\
  &amp; = &amp; \mathbb{E}\Big[\Theta_id_{i}\exp(\eta_{i})\Big]+\mathbb{V}\Big[\Theta_id_{i}\exp(\eta_{i})\Big]\nonumber\\
   &amp; = &amp; d_{i}\exp(\eta_{i})+\Big\{d_{i}\exp(\eta_{i})\Big\}^2\sigma^2\nonumber\\
   &amp;=&amp;\mathbb{E}[N_{i}]+\Big\{d_{i}\exp(\eta_{i})\Big\}^2\sigma^2.\label{FormParamJF}
\end{eqnarray}\]</span>
Let’s write a similar relation to <span class="math inline">\(\eqref{EmpMom}\)</span> for the variance, which is
<span class="math display">\[\begin{equation}
\label{EmpVar}
\sum_{i=1}^n\left\{\Big(n_{i}-d_{i}\exp(\eta_{i})\Big)^2-n_{i}
-\Big(d_{i}\exp(\eta_{i})\Big)^2\sigma^2\right\}=0.
\end{equation}\]</span>
The estimators <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> and <span class="math inline">\(\widehat{\sigma}\)</span> are solutions of the system formed by <span class="math inline">\(\eqref{EmpMom}\)</span> and <span class="math inline">\(\eqref{EmpVar}\)</span>; they converge in the model with random effects. Therefore, the maximum likelihood estimator <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> of <span class="math inline">\(\boldsymbol{\beta}\)</span>, a solution of <span class="math inline">\(\eqref{EmpMom}\)</span>, converges in the model with random effects. The estimator of <span class="math inline">\(\sigma^2\)</span> is given by
<span class="math display">\[
\widehat{\sigma}^2=\frac{\sum_{i=1}^n\left\{\Big(n_{i}-d_{i}
\exp(\widehat{\eta}_{i})\Big)^2-n_{i}\right\}}
{\sum_{i=1}^n\Big\{d_{i}\exp(\widehat{\eta}_{i})\Big\}^2}
\]</span>
where <span class="math inline">\(\widehat{\eta}_{i}=\widehat{\boldsymbol{\beta}}^\top\boldsymbol{x}_{i}\)</span>, and <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> is the maximum likelihood estimator of <span class="math inline">\(\boldsymbol{\beta}\)</span> in the model without random effects.</p>
<p>Instead of first estimating the parameters in a Poisson regression model <span class="math inline">\(\eqref{RegPoiss1}\)</span>-<span class="math inline">\(\eqref{RegPoiss2}\)</span> and then adding a random effect without questioning the estimation of <span class="math inline">\(\boldsymbol{\beta}\)</span>, one could also work directly in a Negative Binomial regression model.</p>
<p>If the Gamma distribution is chosen for <span class="math inline">\(\Theta_i\)</span>, the likelihood is given by
<span class="math display">\[\begin{eqnarray*}
\mathcal{L}(\boldsymbol{\beta}|\boldsymbol{n})&amp;=&amp;\prod_{i=1}^n\frac{\lambda_{i}^{n_{i}}}{n_i!}\left(\frac{a}{a+\lambda_{i}}\right)^a
\left(a+\lambda_{i}\right)^{-n_{i}}\frac{\Gamma\left(a+n_{i}\right)}{\Gamma(a)}.
\end{eqnarray*}\]</span></p>
<p>The maximum likelihood estimators of the parameters <span class="math inline">\(\boldsymbol{\beta}\)</span> and <span class="math inline">\(a\)</span> are solutions of the system
<span class="math display">\[\begin{equation}
\label{PerpGen}
\sum_{i=1}^n\boldsymbol{x}_{i}\left(n_{i}-\lambda_{i}\frac{a+n_{i}}
{a+\lambda_{i}}\right)=\boldsymbol{0}.
\end{equation}\]</span>
To interpret this relation, note that if a constant <span class="math inline">\(\beta_0\)</span>
is introduced into the score, the first equation of <span class="math inline">\(\eqref{PerpGen}\)</span>
guarantees that
<span class="math display">\[
\sum_{i=1}^nn_i=\sum_{i=1}^n\lambda_i\frac{a+n_{i}}{a+\lambda_{i}}.
\]</span>
This means that the model reproduces the number of claims by incorporating the information provided by them. Indeed, <span class="math inline">\(\lambda_i\frac{a+n_{i}}{a+\lambda_{i}}\)</span>
is the expected number of claims in the following period for an insured with characteristics <span class="math inline">\(\boldsymbol{x}_i\)</span> who has reported <span class="math inline">\(n_i\)</span> claims during the current insurance period.
If the first variable is 1 if the insured is male and 0 otherwise, then
<span class="math display">\[
\sum_{\text{males}}n_i=\sum_{\text{males}}\lambda_i\frac{a+n_{i}}{a+\lambda_{i}}.
\]</span>
One can see this last relation as a guarantee of non-subsidization of males by females, and vice versa.</p>
<p>The likelihood equations <span class="math inline">\(\eqref{PerpGen}\)</span> do not have an explicit solution; therefore, numerical solution methods will be used, using the estimates provided by the method of moments as initial values.</p>
<p>% LaTeX Document without preamble</p>
<p>The adjustment of the negative binomial regression model is given in Tables <span class="math inline">\(\ref{ResBN1}\)</span>-<span class="math inline">\(\ref{ResBN2}\)</span>. As expected, the point estimates are similar to those obtained by Poisson regression, but the confidence intervals are wider in the negative binomial model.</p>
<table>
<caption><span id="tab:ResBN1">Table 10.16: </span>Adjustment of the negative binomial regression model, final model</caption>
<colgroup>
<col width="26%" />
<col width="4%" />
<col width="10%" />
<col width="8%" />
<col width="28%" />
<col width="12%" />
<col width="12%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Parameter</th>
<th align="center">DF</th>
<th align="center">Estimate</th>
<th align="center">Error</th>
<th align="center">Wald 95% Confidence Limits</th>
<th align="center">Chi-Square</th>
<th align="center">Pr &gt; ChiSq</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Intercept</td>
<td align="center">1</td>
<td align="center">-1.9555</td>
<td align="center">0.0182</td>
<td align="center">(-1.9913, -1.9198)</td>
<td align="center">11486.30</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="even">
<td align="center">CARB Diesel</td>
<td align="center">1</td>
<td align="center">0.1867</td>
<td align="center">0.0164</td>
<td align="center">(0.1545, 0.2189)</td>
<td align="center">129.24</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="odd">
<td align="center">CARB Gasoline</td>
<td align="center">0</td>
<td align="center">0.0000</td>
<td align="center">0.0000</td>
<td align="center">(0, 0)</td>
<td align="center">0.00</td>
<td align="center">.</td>
</tr>
<tr class="even">
<td align="center">FRAC Annual</td>
<td align="center">1</td>
<td align="center">-0.2757</td>
<td align="center">0.0152</td>
<td align="center">(-0.3055, -0.2458)</td>
<td align="center">326.86</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="odd">
<td align="center">FRAC Fractional</td>
<td align="center">0</td>
<td align="center">0.0000</td>
<td align="center">0.0000</td>
<td align="center">(0, 0)</td>
<td align="center">0.00</td>
<td align="center">.</td>
</tr>
<tr class="even">
<td align="center">GARACCESS RC+Accessories</td>
<td align="center">1</td>
<td align="center">-0.1341</td>
<td align="center">0.0155</td>
<td align="center">(-0.1644, -0.1038)</td>
<td align="center">75.07</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="odd">
<td align="center">GARACCESS RC Only</td>
<td align="center">0</td>
<td align="center">0.0000</td>
<td align="center">0.0000</td>
<td align="center">(0, 0)</td>
<td align="center">0.00</td>
<td align="center">.</td>
</tr>
<tr class="even">
<td align="center">AGGLOM Urban</td>
<td align="center">1</td>
<td align="center">0.2375</td>
<td align="center">0.0158</td>
<td align="center">(0.2065, 0.2685)</td>
<td align="center">225.05</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="odd">
<td align="center">AGGLOM Rural</td>
<td align="center">0</td>
<td align="center">0.0000</td>
<td align="center">0.0000</td>
<td align="center">(0, 0)</td>
<td align="center">0.00</td>
<td align="center">.</td>
</tr>
<tr class="even">
<td align="center">SEXE Female</td>
<td align="center">1</td>
<td align="center">0.0712</td>
<td align="center">0.0171</td>
<td align="center">(0.0377, 0.1048)</td>
<td align="center">17.36</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="odd">
<td align="center">SEXE Male</td>
<td align="center">0</td>
<td align="center">0.0000</td>
<td align="center">0.0000</td>
<td align="center">(0, 0)</td>
<td align="center">0.00</td>
<td align="center">.</td>
</tr>
<tr class="even">
<td align="center">KW Large Displacement</td>
<td align="center">1</td>
<td align="center">0.1297</td>
<td align="center">0.0206</td>
<td align="center">(0.0894, 0.1701)</td>
<td align="center">39.68</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="odd">
<td align="center">KW Small Displacement</td>
<td align="center">1</td>
<td align="center">-0.0852</td>
<td align="center">0.0192</td>
<td align="center">(-0.1228, -0.0475)</td>
<td align="center">19.66</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="even">
<td align="center">KW Medium Displacement</td>
<td align="center">0</td>
<td align="center">0.0000</td>
<td align="center">0.0000</td>
<td align="center">(0, 0)</td>
<td align="center">0.00</td>
<td align="center">.</td>
</tr>
<tr class="odd">
<td align="center">AGESGRP Beginner</td>
<td align="center">1</td>
<td align="center">0.8006</td>
<td align="center">0.0621</td>
<td align="center">(0.6788, 0.9224)</td>
<td align="center">166.02</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="even">
<td align="center">AGESGRP Young</td>
<td align="center">1</td>
<td align="center">0.3906</td>
<td align="center">0.0193</td>
<td align="center">(0.3528, 0.4284)</td>
<td align="center">410.41</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="odd">
<td align="center">AGESGRP Senior</td>
<td align="center">1</td>
<td align="center">-0.2213</td>
<td align="center">0.0191</td>
<td align="center">(-0.2587, -0.1839)</td>
<td align="center">134.30</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="even">
<td align="center">AGESGRP Experienced</td>
<td align="center">0</td>
<td align="center">0.0000</td>
<td align="center">0.0000</td>
<td align="center">(0, 0)</td>
<td align="center">0.00</td>
<td align="center">.</td>
</tr>
<tr class="odd">
<td align="center">Dispersion</td>
<td align="center">1</td>
<td align="center">0.5431</td>
<td align="center">0.0358</td>
<td align="center">(0.4773, 0.6178)</td>
<td align="center">NA</td>
<td align="center">.</td>
</tr>
</tbody>
</table>
<table>
<caption><span id="tab:ResBN2">Table 10.17: </span>Likelihood ratio statistics for Type 3 analysis, negative binomial model</caption>
<thead>
<tr class="header">
<th align="center">Source</th>
<th align="center">DF</th>
<th align="center">Chi.Square</th>
<th align="center">Pr…ChiSq</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">CARB</td>
<td align="center">1</td>
<td align="center">127.51</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="even">
<td align="center">FRAC</td>
<td align="center">1</td>
<td align="center">329.10</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="odd">
<td align="center">GARACCESS</td>
<td align="center">1</td>
<td align="center">75.58</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="even">
<td align="center">AGGLOM</td>
<td align="center">1</td>
<td align="center">219.49</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="odd">
<td align="center">SEXE</td>
<td align="center">1</td>
<td align="center">17.22</td>
<td align="center">$&lt;.0001</td>
</tr>
<tr class="even">
<td align="center">KW</td>
<td align="center">2</td>
<td align="center">NA</td>
<td align="center">.</td>
</tr>
<tr class="odd">
<td align="center">AGESGRP</td>
<td align="center">3</td>
<td align="center">813.99</td>
<td align="center">$&lt;.0001</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="analysis-of-claim-costs" class="section level3 hasAnchor" number="10.11.9">
<h3><span class="header-section-number">10.11.9</span> Analysis of Claim Costs<a href="chap9.html#analysis-of-claim-costs" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="challenges" class="section level4 hasAnchor" number="10.11.9.1">
<h4><span class="header-section-number">10.11.9.1</span> Challenges<a href="chap9.html#challenges" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Before proceeding, let’s explain why the analysis of claim costs is significantly more complicated than that of claim counts.</p>
<p>While all policies in the portfolio can be used to estimate the annual claim frequency, it is clear that only policies with claims can be used to study the distribution of claim amounts. Therefore, the actuary has a limited number of observations to work with when fitting a model to claim amounts. Furthermore, claim amounts are much more challenging to model than claim counts due to the increased complexity of the phenomenon.</p>
<p>Often, claims of some severity require relatively long periods to be closed. Consider, for example, an automobile liability claim with bodily injury, where it is necessary to wait for the victim’s condition to stabilize before determining the amount of compensation. For this reason, the company often has only cost projections in its records, making the analysis uncertain.</p>
<p>According to some authors, considering claim amounts in automobile liability insurance pricing is questionable. Indeed, the amounts paid by the insurer are intended to compensate third parties for their losses. Thus, an insured who injures a pedestrian would expose their insurer to very different expenses depending on whether the pedestrian is an elderly, sick person with no family or a young, dynamic executive with two young children. It is difficult to see how the characteristics of the insured could explain this phenomenon, even though it is understandable that the insured’s profile could explain the probability of causing an accident with bodily injury. In the same vein, an insured who damages another vehicle would force their insurer to pay very different amounts depending on whether the other vehicle is a limousine or a low-end car. However, there is no doubt that claim amounts should be considered in related coverages such as “property damage” coverage, for example.</p>
</div>
<div id="severe-claims" class="section level4 hasAnchor" number="10.11.9.2">
<h4><span class="header-section-number">10.11.9.2</span> Severe Claims<a href="chap9.html#severe-claims" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Often, less than 20% of claims account for more than 80% of the company’s expenses. This requires special treatment of these “severe claims,” which are usually not segmented or only slightly segmented.</p>
<p>Different representations of claim costs by policy were examined in Section 6.2.2. Here, we will adopt the formalism of Example 6.2.1. Specifically, the total claim cost incurred by a policy is written as follows:
<span class="math display">\[
S = \sum_{k=1}^N C_k + IL
\]</span>
where</p>
<ul>
<li><span class="math inline">\(N\)</span> is the number of standard claims, assumed to follow a Poisson distribution.</li>
<li><span class="math inline">\(C_k\)</span> is the cost of the <span class="math inline">\(k\)</span>th standard claim.</li>
<li><span class="math inline">\(I\)</span> indicates whether the policy generated at least one severe claim.</li>
<li><span class="math inline">\(L\)</span> is the cumulative cost of these severe claims, if any.</li>
</ul>
<p>The insurer will segment their premium based on <span class="math inline">\(\mathbb{E}[N]\)</span> and <span class="math inline">\(\mathbb{E}[C_k]\)</span>, possibly on <span class="math inline">\(\mathbb{E}[I]\)</span>, but generally not on <span class="math inline">\(\mathbb{E}[L]\)</span>. Severe claims are too few to allow for customization of amounts.</p>
<div class="remark">
<p><span id="unlabeled-div-43" class="remark"><em>Remark</em>. </span>Since severe claims are the most expensive (they represent more than 80% of the amounts paid by the insurer) and do not lend themselves well to segmentation, the highly segmented part of the premium should represent only 20% of the amount paid by the insured. Therefore, since insureds all seem equal in the face of severe claims (with few exceptions), the rate competition observed in commercial premiums sometimes seems questionable.</p>
</div>
<p>In our portfolio, 17,785 policyholders reported at least one claim in the year 1997. The empirical pure premium amounts to 203.38 Euros. Some descriptive statistics can be found in Table <span class="math inline">\(\ref{StatDescCout}\)</span>. The level at which a claim is classified as “severe” is obtained using extreme value theory (presented in Chapter <span class="math inline">\(\ref{ChapExtremes}\)</span>).</p>
<table>
<caption><span id="tab:StatDescCout">Table 10.18: </span>Descriptive statistics of pure premium amount</caption>
<thead>
<tr class="header">
<th align="center">Statistic</th>
<th align="center">Value</th>
<th align="center">Percentile</th>
<th align="center">Percentile.Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Mean</td>
<td align="center">1638.51000</td>
<td align="center">100%</td>
<td align="center">1.98957E+06</td>
</tr>
<tr class="even">
<td align="center">Median</td>
<td align="center">522.58400</td>
<td align="center">99%</td>
<td align="center">1.75898E+04</td>
</tr>
<tr class="odd">
<td align="center">Mode</td>
<td align="center">1426.37900</td>
<td align="center">95%</td>
<td align="center">3.59446E+03</td>
</tr>
<tr class="even">
<td align="center">Std Deviation</td>
<td align="center">17132.44560</td>
<td align="center">90%</td>
<td align="center">2.58231E+03</td>
</tr>
<tr class="odd">
<td align="center">Skewness</td>
<td align="center">91.79986</td>
<td align="center">75% Q3</td>
<td align="center">1.42638E+03</td>
</tr>
<tr class="even">
<td align="center">Coeff Variation</td>
<td align="center">1045.61099</td>
<td align="center">50% Median</td>
<td align="center">5.22584E+02</td>
</tr>
<tr class="odd">
<td align="center">Interquartile Range</td>
<td align="center">1284.00000</td>
<td align="center">25% Q1</td>
<td align="center">1.42192E+02</td>
</tr>
</tbody>
</table>
</div>
<div id="logistic-regression-for-analyzing-the-occurrence-of-severe-claims" class="section level4 hasAnchor" number="10.11.9.3">
<h4><span class="header-section-number">10.11.9.3</span> Logistic Regression for Analyzing the Occurrence of Severe Claims<a href="chap9.html#logistic-regression-for-analyzing-the-occurrence-of-severe-claims" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Logistic regression is used to explain the occurrence of severe claims. The backward method for selecting explanatory variables leads to the successive exclusion of the following variables: , , , , , , and . This finally results in the following results (after grouping some categories of the variable), as shown in Table <span class="math inline">\(\ref{TabSinGrave}\)</span>.</p>
<table>
<caption><span id="tab:TabSinGrave">Table 10.19: </span>Probabilities of not causing severe claims based on the insured’s characteristics</caption>
<colgroup>
<col width="13%" />
<col width="27%" />
<col width="58%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">CARB</th>
<th align="center">AGESGROUP</th>
<th align="center">Probability.of.Not.Causing.a.Severe.Claim</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Gasoline</td>
<td align="center">Beginner-Young</td>
<td align="center">0.9533</td>
</tr>
<tr class="even">
<td align="center">Gasoline</td>
<td align="center">Senior-Experienced</td>
<td align="center">0.9620</td>
</tr>
<tr class="odd">
<td align="center">Diesel</td>
<td align="center">Beginner-Young</td>
<td align="center">0.9435</td>
</tr>
<tr class="even">
<td align="center">Diesel</td>
<td align="center">Senior-Experienced</td>
<td align="center">0.9540</td>
</tr>
</tbody>
</table>
</div>
<div id="analysis-of-standard-claim-costs" class="section level4 hasAnchor" number="10.11.9.4">
<h4><span class="header-section-number">10.11.9.4</span> Analysis of Standard Claim Costs<a href="chap9.html#analysis-of-standard-claim-costs" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>Gamma Regression Model</strong></p>
<p>We will assume that the claim costs <span class="math inline">\(C_{i1},C_{i2},\ldots\)</span> incurred by insured <span class="math inline">\(i\)</span> are independent and identically distributed as Gamma variables with mean
<span class="math display">\[
\mu_i = \mathbb{E}[C_{ik}|\mathbf{x}_i] = \exp(\boldsymbol{\beta}^\top\mathbf{x}_i)
\]</span>
and variance
<span class="math display">\[
\text{Var}[C_{ik}|\mathbf{x}_i] = \frac{\{\exp(\boldsymbol{\beta}^\top\mathbf{x}_i)\}^2}{\nu}.
\]</span></p>
<div class="remark">
<p><span id="unlabeled-div-44" class="remark"><em>Remark</em>. </span>Often, only the total cost <span class="math inline">\(C_{i\bullet} = \sum_{k=1}^{n_i}C_{ik}\)</span> is available, not the details of the various components of the sum <span class="math inline">\(C_{ik}\)</span>. In this case, we will work with the average cost <span class="math inline">\(\overline{C}_{i\bullet} = C_{i\bullet}/n_i\)</span>. In this case, we can let the parameter <span class="math inline">\(\nu\)</span> vary from insured to insured by specifying <span class="math inline">\(\nu_i = \nu\omega_i\)</span>, where the weight <span class="math inline">\(\omega_i\)</span> is the number of claims <span class="math inline">\(n_i\)</span>.</p>
</div>
<p>Neglecting severe claims, the pure premium for insured <span class="math inline">\(i\)</span> is given by
<span class="math display">\[\begin{eqnarray*}
\mathbb{E}\left[\sum_{k=1}^{N_i}C_{ik}\right] &amp;=&amp; \mathbb{E}[N_i]\mathbb{E}[C_{i1}] = \exp\left((\boldsymbol{\beta}_{\text{freq}} + \boldsymbol{\beta}_{\text{cost}})^\top\mathbf{x}_i\right),
\end{eqnarray*}\]</span>
which provides a multiplicative rate when all explanatory variables are binary. The pure premium then becomes
<span class="math display">\[
\exp\left((\boldsymbol{\beta}_{\text{freq}} + \boldsymbol{\beta}_{\text{cost}})^\top\mathbf{x}_i\right) + q_i\mathbb{E}[L],
\]</span>
where <span class="math inline">\(q_i = \mathbb{E}[I_i]\)</span> is the probability that insured <span class="math inline">\(i\)</span> causes at least one severe claim.</p>
<p><strong>Model Fitting</strong></p>
<p>The backward method for selecting explanatory variables led to the exclusion of several explanatory variables. Table <span class="math inline">\(\ref{ModelRegCout}\)</span> describes the final model.</p>
<table>
<caption><span id="tab:ModelRegCout">Table 10.20: </span>Parameter estimation of the gamma regression model for claim costs.</caption>
<thead>
<tr class="header">
<th align="center">Parameter</th>
<th align="center">Estimate</th>
<th align="center">Wald.95..Confidence.Limits</th>
<th align="center">Pr…ChiSq</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Intercept</td>
<td align="center">6.6389</td>
<td align="center">6.5908 - 6.6873</td>
<td align="center">&lt;.0001</td>
</tr>
<tr class="even">
<td align="center">FRAC</td>
<td align="center">-0.0402</td>
<td align="center">-0.0764 - -0.0039</td>
<td align="center">0.0296</td>
</tr>
<tr class="odd">
<td align="center">FRAC</td>
<td align="center">0.0000</td>
<td align="center">0 - 0</td>
<td align="center">.</td>
</tr>
<tr class="even">
<td align="center">GARACCESS</td>
<td align="center">0.0702</td>
<td align="center">0.0342 - 0.1062</td>
<td align="center">0.0001</td>
</tr>
<tr class="odd">
<td align="center">GARACCESS</td>
<td align="center">0.0000</td>
<td align="center">0 - 0</td>
<td align="center">.</td>
</tr>
<tr class="even">
<td align="center">AGESGRP</td>
<td align="center">-0.0657</td>
<td align="center">-0.1095 - -0.0222</td>
<td align="center">0.0032</td>
</tr>
<tr class="odd">
<td align="center">AGESGRP</td>
<td align="center">0.0000</td>
<td align="center">0 - 0</td>
<td align="center">.</td>
</tr>
<tr class="even">
<td align="center">AGGLOM</td>
<td align="center">0.0826</td>
<td align="center">0.0314 - 0.1345</td>
<td align="center">0.0017</td>
</tr>
<tr class="odd">
<td align="center">AGGLOM</td>
<td align="center">0.0000</td>
<td align="center">0 - 0</td>
<td align="center">.</td>
</tr>
<tr class="even">
<td align="center">Scale</td>
<td align="center">0.7294</td>
<td align="center">0.7155 - 0.7435</td>
<td align="center">.</td>
</tr>
</tbody>
</table>
<div class="remark">
<p><span id="unlabeled-div-45" class="remark"><em>Remark</em>. </span>Gamma regression is not the only way to model claim costs. In the log-normal model, it is assumed that the natural logarithm of claim amounts follows a normal distribution with a mean given by a linear predictor <span class="math inline">\(\boldsymbol{\beta}^\top\mathbf{x}_{it}\)</span> and a constant variance <span class="math inline">\(\sigma^2\)</span>, i.e.,
<span class="math display">\[
\ln C_{ik} \sim \mathcal{N}or(\boldsymbol{\beta}^\top\mathbf{x}_i, \sigma^2).
\]</span>
The likelihood equation for estimating the parameter <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> is given by
<span class="math display">\[
\sum_{i\text{ s.t. }n_i&gt;0}\sum_{k=1}^{n_i}\left(\ln c_{ik} - \boldsymbol{\beta}^\top\mathbf{x}_i\right)\mathbf{x}_i = \boldsymbol{0} \Leftrightarrow \sum_{i\text{ s.t. }n_i&gt;0}lcres_i\mathbf{x}_i = \boldsymbol{0},
\]</span>
where <span class="math inline">\(lcres_i\)</span> is the estimation residual defined for insureds who reported at least one claim as
<span class="math display">\[
lcres_i = \sum_{k=1}^{n_i}\left\{\ln c_{ik} - \boldsymbol{\beta}^\top\mathbf{x}_i\right\} = \sum_{k=1}^{n_i}\ln c_{ik} - n_i\boldsymbol{\beta}^\top\mathbf{x}_i.
\]</span>
This equation also expresses an orthogonality relationship between explanatory variables and estimation residuals.</p>
<p>This approach is not without drawbacks: since the dependent variable is the natural logarithm of claim amounts, inferential conclusions will relate to the transformed claim amounts, and it is not always easy to revert to the original data.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-46" class="remark"><em>Remark</em> (Another Application of Gamma Regression: Individual Reserving Model). </span>An interesting application of the analysis of claim costs is to determine individual reserving rules. As soon as a claim is reported, the company must set aside an amount corresponding to the probable cost of that event to provide shareholders, the market, and regulatory authorities with an accurate picture of its financial position.</p>
<p>An automated approach is to explain the amount to be reserved based on the characteristics of claims reported in the past (in addition to a priori variables, the insurer will also use information related to the circumstances of the claims, such as the time of day it occurred, the presence of bodily injury, etc.).</p>
</div>
</div>
</div>
</div>
<div id="panel-data-rating" class="section level2 hasAnchor" number="10.12">
<h2><span class="header-section-number">10.12</span> Panel Data Rating<a href="chap9.html#panel-data-rating" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This section is based on <span class="citation">(<a href="#ref-denuit2003tarification" role="doc-biblioref">Denuit, Walhin, and Pitrebois 2003</a>)</span> and only addresses
the number of claims.</p>
<div id="rating-based-on-panel-data" class="section level3 hasAnchor" number="10.12.1">
<h3><span class="header-section-number">10.12.1</span> Rating Based on Panel Data<a href="chap9.html#rating-based-on-panel-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Often, actuaries use several years of observations to construct their
rates (to increase the size of the database and to avoid giving too much importance to events in a particular year).
This has the consequence that some of the data will no longer be independent.
Indeed, observations made on the same insured over different periods are likely correlated (which is the reason for {} rating, which will be discussed in the following chapters).
We are thus dealing with panel data.</p>
<p>In the context of {} rating, the dependence between observations related to the same policy
is considered a nuisance: at this stage, the actuary wants to determine the impact of observable factors on the insured risk,
and the correlations that exist between the data prevent the use of classical statistical techniques
(most of which are based on the assumption of independence). Here, we will show how to take this dependence into account to improve the quality of estimates using techniques proposed by <span class="citation">(<a href="#ref-liang1986longitudinal" role="doc-biblioref">Liang and Zeger 1986</a>)</span> and <span class="citation">(<a href="#ref-zeger1988models" role="doc-biblioref">Zeger, Liang, and Albert 1988</a>)</span>.</p>
<p>The estimators of claim frequencies obtained under the assumption of independence of individual data over different periods are convergent (meaning they will tend
in probability to population values as the sample size increases).
Therefore, it is reasonable to expect that for large automobile portfolios, the impact of the simplifying assumption of independence
on point estimates will be minimal.
Indeed, this is what we will demonstrate in the empirical part of our study.</p>
</div>
<div id="notation" class="section level3 hasAnchor" number="10.12.2">
<h3><span class="header-section-number">10.12.2</span> Notation<a href="chap9.html#notation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As explained above, insurance companies often use
multiple observation periods to build their rates.
Individual observations are therefore doubly indexed, by policy <span class="math inline">\(i\)</span>
and period <span class="math inline">\(t\)</span>. From now on, <span class="math inline">\(N_{it}\)</span> represents the number of claims reported by insured <span class="math inline">\(i\)</span>
during period <span class="math inline">\(t\)</span>, <span class="math inline">\(i=1,2,\ldots,n\)</span>, <span class="math inline">\(t=1,2,\ldots,T_i\)</span>, where <span class="math inline">\(T_i\)</span>
denotes the number of observation periods for insured <span class="math inline">\(i\)</span>.
We will denote <span class="math inline">\(d_{it}\)</span> as the duration of the <span class="math inline">\(t\)</span>-th observation period for individual
<span class="math inline">\(i\)</span>. When there is a change in observable variables, a new interval begins,
so <span class="math inline">\(d_{it}\)</span> may be different from 1. We assume that
we also have other variables <span class="math inline">\(\mathbf{x}_{it}\)</span> known at the beginning of the
period <span class="math inline">\(t\)</span>, which can serve as explanatory factors for the insured’s claims experience.
In addition to explanatory variables, we can introduce calendar time as a regression component
to account for specific events or possible
trends in claims frequency, following the approach of <span class="citation">(<a href="#ref-besson1992trend" role="doc-biblioref">Besson and Patrat 1992</a>)</span>.</p>
<p>Typically, we are dealing with panel data: the same variable
is measured on a large number <span class="math inline">\(n\)</span> of individuals over time, with a relatively small number <span class="math inline">\(\max_{1\leq i\leq n} T_i\)</span>
of repetitions. Asymptotics will be done here by letting <span class="math inline">\(n\)</span> tend to infinity,
rather than the number of observations made on the same individual (as is typically the case in time series analysis).</p>
</div>
<div id="presentation-of-the-dataset" class="section level3 hasAnchor" number="10.12.3">
<h3><span class="header-section-number">10.12.3</span> Presentation of the Dataset<a href="chap9.html#presentation-of-the-dataset" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We illustrate our points on a Belgian insurance portfolio consisting of 20,354 policies observed over a period of 3 years.
Figure <span class="math inline">\(\ref{DUR}\)</span> provides an idea of the exposure duration for the policies in the portfolio.
A little over 34% of insureds remained in the portfolio for all three years. For each
policy and each year, the number of claims and certain characteristics of the insured are recorded:
the gender of the driver (male-female), the age of the driver (three age classes:
<span class="math inline">\(18-22\)</span> years, <span class="math inline">\(23-30\)</span> years, and <span class="math inline">\(&gt;30\)</span> years), the vehicle power (three power classes:
<span class="math inline">\(&lt;66\)</span>kW, <span class="math inline">\(66-110\)</span>kW, and <span class="math inline">\(&gt;110\)</span>kW), the size of the driver’s city of residence
(large, medium, or small based on the number of inhabitants), and the color of the vehicle (red
or other). Across the entire portfolio, the average annual frequency is 18.4% (which is well above
the European average).</p>
<p>Figures~<span class="math inline">\(\ref{hist1}\)</span> to~<span class="math inline">\(\ref{hist5}\)</span> show histograms describing,
for each explanatory variable, the distribution of the portfolio among different levels
of the variable and, for each of these levels, the average frequency (in
<span class="math inline">\(\%\)</span>) of claims.</p>
<p>FIGURES</p>
<p>These histograms prompt the following comments. Figure <span class="math inline">\(\ref{hist1}\)</span> shows a slight
underperformance for women (<span class="math inline">\(17.7\%\)</span> compared to <span class="math inline">\(18.8\%\)</span>), who represent <span class="math inline">\(36\%\)</span>
of the insured in the portfolio. The overperformance of young drivers is evident from Figure <span class="math inline">\(\ref{hist2}\)</span> (although
they are underrepresented in the portfolio). Claim frequencies seem
to decrease with age, going from <span class="math inline">\(30.8\%\)</span> to <span class="math inline">\(20.8\%\)</span> and finally to <span class="math inline">\(16.3\%\)</span>. Regarding the
vehicle power, Figure <span class="math inline">\(\ref{hist3}\)</span> shows an underperformance for high-powered vehicles. Examination
of Figure <span class="math inline">\(\ref{hist4}\)</span> reveals that claim frequency is higher in large urban areas. Claims
frequency appears to decrease with the size of the urban area. Finally, Figure <span class="math inline">\(\ref{hist5}\)</span>
shows that the red color of the vehicle does not seem to be an aggravating factor.</p>
</div>
<div id="poisson-regression-assuming-temporal-independence" class="section level3 hasAnchor" number="10.12.4">
<h3><span class="header-section-number">10.12.4</span> Poisson Regression Assuming Temporal Independence<a href="chap9.html#poisson-regression-assuming-temporal-independence" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="model" class="section level4 hasAnchor" number="10.12.4.1">
<h4><span class="header-section-number">10.12.4.1</span> Model<a href="chap9.html#model" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>As a first approximation, we will assume that the <span class="math inline">\(N_{it}\)</span> are independent for different values of
<span class="math inline">\(i\)</span> and <span class="math inline">\(t\)</span>. This is, of course, a strong simplifying assumption that we will evaluate the impact of
by comparing the results obtained with those provided by different methods to account for the serial dependence that exists between the <span class="math inline">\(N_{it}\)</span> at a fixed <span class="math inline">\(i\)</span>.</p>
<p>We assume that the conditional distribution of <span class="math inline">\(N_{it}\)</span> given <span class="math inline">\(\mathbf{x}_{it}\)</span>
is Poisson and specify a mean of the form of a linear exponential, i.e.,
<span class="math display">\[\begin{equation}
\label{ModRegPoiss}
N_{it}\sim\mathcal{P}ois\big(d_{it}\exp(\eta_{it})\big),\hspace{2mm}
i=1,2,\ldots,n,\hspace{2mm}t=1,2,\ldots,T_i.
\end{equation}\]</span>
The claim frequency for individual <span class="math inline">\(i\)</span> during period <span class="math inline">\(t\)</span> is <span class="math inline">\(\lambda_{it}=d_{it}\exp(\eta_{it})\)</span>.</p>
</div>
<div id="parameter-estimation-2" class="section level4 hasAnchor" number="10.12.4.2">
<h4><span class="header-section-number">10.12.4.2</span> Parameter Estimation<a href="chap9.html#parameter-estimation-2" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Let <span class="math inline">\(n_{it}\)</span> be the number of claims reported by insured <span class="math inline">\(i\)</span> during period <span class="math inline">\(t\)</span>.
The likelihood associated with these observations is then
<span class="math display">\[
\mathcal{L}(\boldsymbol{\beta}|\mathbf{n})=\prod_{i=1}^n\prod_{t=1}^{T_i}
\exp\{-\lambda_{it}\}\frac{\{\lambda_{it}\}^{n_{it}}}{n_{it}!};
\]</span>
this is the probability of obtaining the observations made within the portfolio
in the considered model (note that <span class="math inline">\(\mathcal{L}\)</span> is a function of the parameters <span class="math inline">\(\boldsymbol{\beta}\)</span>,
with the observations assumed to be known).</p>
<p>The maximum likelihood estimation of <span class="math inline">\(\boldsymbol{\beta}\)</span> consists of determining <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span>
by maximizing <span class="math inline">\(\mathcal{L}(\boldsymbol{\beta}|\mathbf{n})\)</span>: <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> is therefore the value of the parameter
making the observations collected by the actuary most probable.
To facilitate the attainment of the maximum, we often switch to
the log-likelihood, which is given by
<span class="math display">\[
L(\boldsymbol{\beta}|\mathbf{n})=\ln \mathcal{L}(\boldsymbol{\beta}|\mathbf{n})=\sum_{i=1}^n\sum_{t=1}^{T_i}
\Big\{-\ln n_{it}!+n_{it}(\eta_{it}+\ln d_{it})-\lambda_{it}\Big\}.
\]</span>
Therefore, <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> is the solution of the system
<span class="math display">\[\begin{equation}
\label{EqVrais1}
\frac{\partial}{\partial \beta_0}L(\boldsymbol{\beta}|\mathbf{n})=0
\Leftrightarrow \sum_{i=1}^n\sum_{t=1}^{T_i}n_{it}=\sum_{i=1}^n\sum_{t=1}^{T_i}\lambda_{it}
\end{equation}\]</span>
and for <span class="math inline">\(j=1,2,\ldots,p\)</span>,
<span class="math display">\[\begin{eqnarray}
\label{EqVraisPois}
\frac{\partial}{\partial \beta_j}L(\boldsymbol{\beta}|\mathbf{n})=0
&amp;\Leftrightarrow &amp;\sum_{i=1}^n\sum_{t=1}^{T_i}x_{itj}\Big\{n_{it}-\lambda_{it}\Big\}=0.
%&amp;\Leftrightarrow &amp;\sum_{i=1}^n\sum_{t=1}^{T_i}x_{itj}\Big\{n_{it}-\mathbb{E}[N_{it}|\mathbf{x}_{it}]\Big\}=0.
\end{eqnarray}\]</span></p>
<p>Unsurprisingly, we can interpret the likelihood equations <span class="math inline">\(\eqref{EqVraisPois}\)</span>
as a relation
of orthogonality between the explanatory variables <span class="math inline">\(\mathbf{x}_{it}\)</span> and the
estimation residuals.</p>
<p>The variance-covariance matrix <span class="math inline">\(\boldsymbol{\Sigma}\)</span> of the maximum likelihood estimator <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> of the parameter <span class="math inline">\(\boldsymbol{\beta}\)</span>
is the inverse of the Fisher information matrix <span class="math inline">\(\mathcal{I}\)</span>.
It can be estimated by
<span class="math display">\[
\widehat{\boldsymbol{\Sigma}}=\left\{\sum_{i=1}^n\sum_{t=1}^{T_i}\mathbf{x}_{it}\mathbf{x}_{it}^\top
\widehat{\lambda}_{it}\right\}^{-1}.
\]</span></p>
<p>Under the asymptotic theory of the maximum likelihood method,
<span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> is approximately normally distributed with a mean
equal to the true parameter value and a variance-covariance matrix <span class="math inline">\(\widehat{\boldsymbol{\Sigma}}\)</span>.
This allows for obtaining confidence intervals and regions for the parameters.</p>
</div>
<div id="numerical-illustration" class="section level4 hasAnchor" number="10.12.4.3">
<h4><span class="header-section-number">10.12.4.3</span> Numerical Illustration<a href="chap9.html#numerical-illustration" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The procedure in SAS allows for performing Poisson regression of the number of
claims on the <span class="math inline">\(5\)</span> explanatory variables presented in Section 1.7.
The explanatory variable “vehicle color” is not significant.
After removing it from the model, we group the
power levels “<span class="math inline">\(66-110\)</span>kW” and “<span class="math inline">\(&gt;110\)</span>kW” into a single class.
This leads us to the selected model, which is described in Table~<span class="math inline">\(\ref{genfinal}\)</span>.</p>
<table>
<caption><span id="tab:genfinal">Table 10.21: </span>Parameter estimation of the gamma regression model for claim costs.</caption>
<thead>
<tr class="header">
<th align="center">Parameter</th>
<th align="center">Estimate</th>
<th align="center">Wald.95..Confidence.Limits</th>
<th align="center">Pr…ChiSq</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Intercept</td>
<td align="center">6.6389</td>
<td align="center">6.5908 - 6.6873</td>
<td align="center">&lt;.0001</td>
</tr>
<tr class="even">
<td align="center">FRAC</td>
<td align="center">-0.0402</td>
<td align="center">-0.0764 - -0.0039</td>
<td align="center">0.0296</td>
</tr>
<tr class="odd">
<td align="center">FRAC</td>
<td align="center">0.0000</td>
<td align="center">0 - 0</td>
<td align="center">.</td>
</tr>
<tr class="even">
<td align="center">GARACCESS</td>
<td align="center">0.0702</td>
<td align="center">0.0342 - 0.1062</td>
<td align="center">0.0001</td>
</tr>
<tr class="odd">
<td align="center">GARACCESS</td>
<td align="center">0.0000</td>
<td align="center">0 - 0</td>
<td align="center">.</td>
</tr>
<tr class="even">
<td align="center">AGESGRP</td>
<td align="center">-0.0657</td>
<td align="center">-0.1095 - -0.0222</td>
<td align="center">0.0032</td>
</tr>
<tr class="odd">
<td align="center">AGESGRP</td>
<td align="center">0.0000</td>
<td align="center">0 - 0</td>
<td align="center">.</td>
</tr>
<tr class="even">
<td align="center">AGGLOM</td>
<td align="center">0.0826</td>
<td align="center">0.0314 - 0.1345</td>
<td align="center">0.0017</td>
</tr>
<tr class="odd">
<td align="center">AGGLOM</td>
<td align="center">0.0000</td>
<td align="center">0 - 0</td>
<td align="center">.</td>
</tr>
<tr class="even">
<td align="center">Scale</td>
<td align="center">0.7294</td>
<td align="center">0.7155 - 0.7435</td>
<td align="center">.</td>
</tr>
</tbody>
</table>
<p>The log-likelihood is -19,283.2, and the Type 3 analysis provides the results
shown in Table~<span class="math inline">\(\ref{type3_final}\)</span>. Except for the power variable, all variables are
statistically significant, and the omission of any of them significantly worsens
the model (at the 5% level).
However, we decide to keep the power variable due to its importance in insurance pricing
and its slight exceedance of the threshold (only 0.93%).
The log-likelihood of the final model is only slightly worse than that of the unconstrained model (-19,282.6).</p>
<table>
<caption>(#tab:type3_final)Results of the Type 3 analysis for the final model assuming serial independence</caption>
<thead>
<tr class="header">
<th align="center">Source</th>
<th align="center">DF</th>
<th align="center">ChiSquare</th>
<th align="center">Pr…ChiSq</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Sex</td>
<td align="center">1</td>
<td align="center">4.74</td>
<td align="center">0.0294</td>
</tr>
<tr class="even">
<td align="center">Age</td>
<td align="center">2</td>
<td align="center">176.07</td>
<td align="center">&lt;.0001</td>
</tr>
<tr class="odd">
<td align="center">Age</td>
<td align="center">0</td>
<td align="center">0.00</td>
<td align="center">.</td>
</tr>
<tr class="even">
<td align="center">Power</td>
<td align="center">1</td>
<td align="center">3.56</td>
<td align="center">0.0593</td>
</tr>
<tr class="odd">
<td align="center">City</td>
<td align="center">2</td>
<td align="center">73.82</td>
<td align="center">&lt;.0001</td>
</tr>
<tr class="even">
<td align="center">City</td>
<td align="center">0</td>
<td align="center">0.00</td>
<td align="center">.</td>
</tr>
</tbody>
</table>
</div>
<div id="residual-analysis-1" class="section level4 hasAnchor" number="10.12.4.4">
<h4><span class="header-section-number">10.12.4.4</span> Residual Analysis<a href="chap9.html#residual-analysis-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Figure <span class="math inline">\(\ref{res_pred}\)</span> shows individual deviance residuals. One can observe
the structure reflecting the few observed values for <span class="math inline">\(N_{it}\)</span>.
Therefore, the quality of the model cannot be judged based on Figure <span class="math inline">\(\ref{res_pred}\)</span>.
If we recalculate the residuals
by classes, we obtain Figure <span class="math inline">\(\ref{res_classes}\)</span>. No particular structure can be observed, but
some residuals have relatively high values, which question the accuracy of the model.</p>
</div>
<div id="overdispersion-1" class="section level4 hasAnchor" number="10.12.4.5">
<h4><span class="header-section-number">10.12.4.5</span> Overdispersion<a href="chap9.html#overdispersion-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>To detect possible overdispersion, we calculate, for each risk class,
the mean and empirical variance of the number of claims, <span class="math inline">\(\widehat{m}_k\)</span>
and <span class="math inline">\(\widehat{\sigma}_k^2\)</span>, respectively, and plot the points
<span class="math inline">\(\{(\widehat{m}_k, \widehat{\sigma}_k^2),\hspace{2mm}k=1,2,\ldots\}\)</span> in a graph.
The result is visible in Figure~<span class="math inline">\(\ref{VerifSurdisp}\)</span>.
Strong overdispersion can be observed for all categories of insureds.
The points <span class="math inline">\((\widehat{m}_k, \widehat{\sigma}_k^2)\)</span> are indeed located above the
first bisector of the grid.
This also leads us to consider that the model of Poisson with
temporal independence is not suitable.</p>
<div class="remark">
<p><span id="unlabeled-div-47" class="remark"><em>Remark</em>. </span>It is possible to account for the observed overdispersion without acknowledging the potential serial dependence. To do this, either a Poisson mixture model or a quasi-likelihood approach is used by specifying
<span class="math display">\[\begin{equation}
\label{RelQuasiV}
\mathbb{V}[N_{it}|\boldsymbol{x}_{it}]=\phi\mathbb{E}[N_{it}|\boldsymbol{x}_{it}]=\phi\lambda_{it}.
\end{equation}\]</span>
To visually test the validity of this relationship, we fit the point cloud
from Figure~<span class="math inline">\(\ref{VerifSurdisp}\)</span> using a line passing through the origin (i.e., with equation <span class="math inline">\(y=\phi x\)</span>).
This yields an estimated dispersion parameter <span class="math inline">\(\phi\)</span> of 1.9122 and a coefficient of determination
<span class="math inline">\(R^2=86.17\%\)</span> (indicating that the line explains over 86% of the variability in the point cloud).
For comparison, if we had attempted to fit a second-degree curve (such as <span class="math inline">\(y=x+\gamma x^2\)</span>, typical of the mean-variance relationship in a Poisson mixture model), we would have obtained
<span class="math inline">\(y=x+2.9545 x^2\)</span> with <span class="math inline">\(R^2=90.90\%\)</span>. Therefore, a Poisson mixture (such as the negative binomial distribution) could also have been considered.
However, in this section, we prioritize a quasi-likelihood approach.
This involves determining <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> by solving the system <span class="math inline">\(\eqref{EqVrais1}\)</span>-<span class="math inline">\(\eqref{EqVraisPois}\)</span>.
Then, <span class="math inline">\(\widehat{\phi}\)</span> is obtained by dividing either the deviance or the Pearson statistic by the
number of degrees of freedom. The estimated value of <span class="math inline">\(\phi\)</span> on our data is 1.35, which reflects
the overdispersion of the data.</p>
<p>The introduction of the overdispersion parameter <span class="math inline">\(\phi\)</span> inflates the variances and covariances of the <span class="math inline">\(\widehat{\beta}_j\)</span>
(which are multiplied by <span class="math inline">\(\widehat{\phi}\)</span>). This has the effect of reducing the values of the test statistics used to assess the nullity of <span class="math inline">\(\beta_j\)</span> or the relevance of including certain variables in the model. Accounting for overdispersion can therefore lead to the exclusion of tariff variables that would have been retained in the pure Poisson model. We observe such an effect in our dataset, as the p-value of the power variable in the Type 3 analysis increases to 10.44%.</p>
</div>
</div>
</div>
<div id="accounting-for-temporal-dependence" class="section level3 hasAnchor" number="10.12.5">
<h3><span class="header-section-number">10.12.5</span> Accounting for Temporal Dependence<a href="chap9.html#accounting-for-temporal-dependence" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="detection-of-serial-dependence" class="section level4 hasAnchor" number="10.12.5.1">
<h4><span class="header-section-number">10.12.5.1</span> Detection of Serial Dependence<a href="chap9.html#detection-of-serial-dependence" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>To get a first idea of the type of dependence between <span class="math inline">\(N_{it}\)</span>, one can consider the observations <span class="math inline">\(N_{it}\)</span>, <span class="math inline">\(t=2,\ldots,T_i\)</span>, <span class="math inline">\(i=1,\ldots,n\)</span>, and perform a regression of these on the corresponding explanatory variables <span class="math inline">\(\boldsymbol{x}_{it}\)</span> as well as the number <span class="math inline">\(N_{i,t-1}\)</span> of claims observed during the previous coverage period. This will also allow us to see the effect of including past values of the variable of interest in the explanatory variables.</p>
<p>To highlight this dependence, we work with the observations of the last two years available to us. Therefore, we consider the observations <span class="math inline">\(N_{it}\)</span>, <span class="math inline">\(t=2,3\)</span>, <span class="math inline">\(i=1,\ldots,n\)</span>, and perform a regression of these on the explanatory variables <span class="math inline">\(\boldsymbol{x}_{it}\)</span>, to which we add the variable <span class="math inline">\(N_{i,t-1}\)</span>, i.e., the number of claims observed during the previous period. We start with a model containing the 5 explanatory variables already presented and refine it step by step, using Type 3 analysis. We begin by removing the “vehicle color” variable, which has a p-value of <span class="math inline">\(27.37\%\)</span>, and in a second step, we remove the “driver’s gender” variable, whose p-value has become <span class="math inline">\(21.10\%\)</span>. We then obtain the model whose results are presented in Tables~<span class="math inline">\(\ref{gen_sinprec}\)</span> and~<span class="math inline">\(\ref{type3_sinprec}\)</span>. The regression coefficient obtained for the past number of claims is highly significant, indicating serial dependence.</p>
<table>
<caption>(#tab:gen_sinprec)Results of the regression for the model accounting for past claims</caption>
<colgroup>
<col width="12%" />
<col width="12%" />
<col width="14%" />
<col width="12%" />
<col width="24%" />
<col width="10%" />
<col width="11%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Variable</th>
<th align="center">Level</th>
<th align="center">Coeff.beta</th>
<th align="center">Std.Error</th>
<th align="center">Wald.95..Conf.Limit</th>
<th align="center">Chi.Sq</th>
<th align="center">Pr.ChiSq</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Intercept</td>
<td align="center"></td>
<td align="center">-2.0405</td>
<td align="center">0.0370</td>
<td align="center">(-2.1131, -1.9680)</td>
<td align="center">3041.80</td>
<td align="center">&lt;.0001</td>
</tr>
<tr class="even">
<td align="center">Age</td>
<td align="center">17-22</td>
<td align="center">0.5841</td>
<td align="center">0.0983</td>
<td align="center">(0.3914, 0.7767)</td>
<td align="center">35.31</td>
<td align="center">&lt;.0001</td>
</tr>
<tr class="odd">
<td align="center">Age</td>
<td align="center">23-30</td>
<td align="center">0.1822</td>
<td align="center">0.0348</td>
<td align="center">(0.1140, 0.2503)</td>
<td align="center">27.41</td>
<td align="center">&lt;.0001</td>
</tr>
<tr class="even">
<td align="center">Age</td>
<td align="center">&gt;30</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">(0, 0)</td>
<td align="center">.</td>
<td align="center">.</td>
</tr>
<tr class="odd">
<td align="center">Power</td>
<td align="center">&gt;110 kW</td>
<td align="center">-0.0745</td>
<td align="center">0.1035</td>
<td align="center">(-2.2773, 0.1283)</td>
<td align="center">0.52</td>
<td align="center">0.4716</td>
</tr>
<tr class="even">
<td align="center">Power</td>
<td align="center">66-110 kW</td>
<td align="center">0.0933</td>
<td align="center">0.0357</td>
<td align="center">(0.0233, 0.1633)</td>
<td align="center">6.83</td>
<td align="center">0.0090</td>
</tr>
<tr class="odd">
<td align="center">Power</td>
<td align="center">&lt;66 kW</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">(0, 0)</td>
<td align="center">.</td>
<td align="center">.</td>
</tr>
<tr class="even">
<td align="center">City</td>
<td align="center">Large</td>
<td align="center">0.2201</td>
<td align="center">0.0412</td>
<td align="center">(0.1394, 0.3009)</td>
<td align="center">28.54</td>
<td align="center">&lt;.0001</td>
</tr>
<tr class="odd">
<td align="center">City</td>
<td align="center">Medium</td>
<td align="center">0.1050</td>
<td align="center">0.0413</td>
<td align="center">(0.0242, 0.1859)</td>
<td align="center">6.48</td>
<td align="center">0.0109</td>
</tr>
<tr class="even">
<td align="center">City</td>
<td align="center">Small</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">(0, 0)</td>
<td align="center">.</td>
<td align="center">.</td>
</tr>
<tr class="odd">
<td align="center">N_{t-1}</td>
<td align="center"></td>
<td align="center">0.3113</td>
<td align="center">0.0371</td>
<td align="center">(0.2387, 0.3839)</td>
<td align="center">70.59</td>
<td align="center">&lt;.0001</td>
</tr>
</tbody>
</table>
<table>
<caption>(#tab:type3_sinprec)Results of the Type 3 analysis for the model accounting for past claims</caption>
<thead>
<tr class="header">
<th align="center">Source</th>
<th align="center">DF</th>
<th align="center">ChiSquare</th>
<th align="center">Pr…ChiSq</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Age</td>
<td align="center">2</td>
<td align="center">50.58</td>
<td align="center">&lt;.0001</td>
</tr>
<tr class="even">
<td align="center">Age</td>
<td align="center">2</td>
<td align="center">50.58</td>
<td align="center">&lt;.0001</td>
</tr>
<tr class="odd">
<td align="center">Age</td>
<td align="center">2</td>
<td align="center">50.58</td>
<td align="center">&lt;.0001</td>
</tr>
<tr class="even">
<td align="center">Power</td>
<td align="center">2</td>
<td align="center">7.94</td>
<td align="center">0.0188</td>
</tr>
<tr class="odd">
<td align="center">Power</td>
<td align="center">2</td>
<td align="center">7.94</td>
<td align="center">0.0188</td>
</tr>
<tr class="even">
<td align="center">Power</td>
<td align="center">2</td>
<td align="center">7.94</td>
<td align="center">0.0188</td>
</tr>
<tr class="odd">
<td align="center">City</td>
<td align="center">2</td>
<td align="center">28.68</td>
<td align="center">&lt;.0001</td>
</tr>
<tr class="even">
<td align="center">City</td>
<td align="center">2</td>
<td align="center">28.68</td>
<td align="center">&lt;.0001</td>
</tr>
<tr class="odd">
<td align="center">City</td>
<td align="center">2</td>
<td align="center">28.68</td>
<td align="center">&lt;.0001</td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(N_{t-1}\)</span></td>
<td align="center">1</td>
<td align="center">63.38</td>
<td align="center">&lt;.0001</td>
</tr>
</tbody>
</table>
<p>In a second approach, we start from the frequency obtained under the assumption
of independence and without adding the number of claims from the previous year as an explanatory variable. This premium is then corrected by a multiplicative factor obtained from a Poisson regression on the single variable “number of claims from the previous year” (with the frequency premium obtained under the assumption of independence as an offset). The results of this regression can be found in Tables~<span class="math inline">\(\ref{gen_sinprecmult}\)</span> and~<span class="math inline">\(\ref{type3_sinprecmult}\)</span>.</p>
<table>
<caption>(#tab:gen_sinprecmult)Results of the regression for the model accounting for past claims while fixing the influence of explanatory variables</caption>
<colgroup>
<col width="14%" />
<col width="16%" />
<col width="14%" />
<col width="28%" />
<col width="10%" />
<col width="16%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Variable</th>
<th align="center">Coeff.beta</th>
<th align="center">Std.Error</th>
<th align="center">Wald.95..Conf.Limit</th>
<th align="center">Chi.Sq</th>
<th align="center">Pr…ChiSq</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Intercept</td>
<td align="center">-0.1147</td>
<td align="center">0.0180</td>
<td align="center">(-0.1500, -0.0793)</td>
<td align="center">40.42</td>
<td align="center">&lt;.0001</td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(N_{t-1}\)</span></td>
<td align="center">0.3040</td>
<td align="center">0.0370</td>
<td align="center">(0.2316, 0.3765)</td>
<td align="center">67.65</td>
<td align="center">&lt;.0001</td>
</tr>
</tbody>
</table>
<table>
<caption>(#tab:type3_sinprecmult)Results of the Type 3 analysis for the model accounting for past claims while fixing the influence of explanatory variables</caption>
<thead>
<tr class="header">
<th align="center">Source</th>
<th align="center">DF</th>
<th align="center">ChiSquare</th>
<th align="center">Pr…ChiSq</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(N_{t-1}\)</span></td>
<td align="center">1</td>
<td align="center">60.84</td>
<td align="center">&lt;.0001</td>
</tr>
</tbody>
</table>
<div class="remark">
<p><span id="unlabeled-div-48" class="remark"><em>Remark</em>. </span>It is interesting to note that this approach immediately provides “French-style” bonus-malus coefficients. Indeed, Table~<span class="math inline">\(\ref{gen_sinprecmult}\)</span> informs us that policyholders who have not reported any claims in the year will see their premium multiplied by <span class="math inline">\(\exp(-0.1147)=0.8916\)</span>, while those who have reported <span class="math inline">\(k\)</span> claims will face a premium increase of <span class="math inline">\(\exp(-0.1147+k\times 0.3040)=0.8916\times(1.3553)^k\)</span>. It is always interesting to compare these coefficients to those produced by a more orthodox model formulated in terms of latent variables.</p>
</div>
<p>The data, therefore, suggest serial dependence. This invalidates the results obtained in the previous section, which are based on the assumption that <span class="math inline">\(N_{it}\)</span> is independent for different values of <span class="math inline">\(i\)</span> and <span class="math inline">\(t\)</span>. Theoretically, however, it can be shown that the maximum likelihood estimator <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> calculated under the assumption of serial independence (i.e., with a specification error) is convergent. Therefore, if the portfolio size is sufficiently large, we expect little impact on the point estimates of the different <span class="math inline">\(\beta_j\)</span>. However, the variance of <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> cannot be calculated as described above and is affected by serial dependence.</p>
</div>
<div id="parameter-estimation-using-gee-technique" class="section level4 hasAnchor" number="10.12.5.2">
<h4><span class="header-section-number">10.12.5.2</span> Parameter Estimation Using GEE Technique<a href="chap9.html#parameter-estimation-using-gee-technique" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In the presence of serial dependence, one might consider keeping the maximum likelihood estimator in the Poisson model with temporal independence (i.e., the solution of <span class="math inline">\(\eqref{EqVrais1}\)</span>-<span class="math inline">\(\eqref{EqVraisPois}\)</span>), which is justified by its convergence property. As shown by <span class="citation">(<a href="#ref-liang1986longitudinal" role="doc-biblioref">Liang and Zeger 1986</a>)</span>, it is possible to improve this approach (i.e., obtain estimators whose asymptotic variance will be lower than that of those we have just described). This is the Generalized Estimating Equation (GEE) method proposed by <span class="citation">(<a href="#ref-liang1986longitudinal" role="doc-biblioref">Liang and Zeger 1986</a>)</span>. The estimators provided by this method are convergent; therefore, it is hoped that the estimates obtained in this way will be of good quality given the large number of observations generally available to actuaries.</p>
<p>The idea is simple: retaining the maximum likelihood estimator <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span>, a solution of <span class="math inline">\(\eqref{EqVrais1}\)</span>-<span class="math inline">\(\eqref{EqVraisPois}\)</span>, to estimate <span class="math inline">\(\boldsymbol{\beta}\)</span> in the model with random effects is certainly not optimal since it does not take into account the correlation structure of <span class="math inline">\(N_{it}\)</span>. Let us rewrite the system <span class="math inline">\(\eqref{EqVrais1}\)</span>-<span class="math inline">\(\eqref{EqVraisPois}\)</span> in vector form:
<span class="math display">\[\begin{equation}
\label{SystIndT}
\sum_{i=1}^n\boldsymbol{X}_i^\top(\boldsymbol{n}_i-\mathbb{E}[\boldsymbol{N}_i])=\boldsymbol{0}\text{ where }\boldsymbol{X}_i=(\boldsymbol{x}_{i1},\ldots,\boldsymbol{x}_{iT_i})^\top.
\end{equation}\]</span>
The covariance matrix of <span class="math inline">\(N_{it}\)</span> in the Poisson model with serial independence is
<span class="math display">\[
\boldsymbol{A}_i=\left(
\begin{array}{cccc}
\lambda_{i1} &amp; 0 &amp; \cdots &amp; 0\\
0 &amp; \lambda_{i2} &amp; \cdots &amp; 0\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
0 &amp; 0 &amp; \cdots &amp; \lambda_{iT_i}
\end{array}
\right).
\]</span>
This matrix does not account for overdispersion or serial dependence present in the data.
If we explicitly introduce the matrix <span class="math inline">\(\boldsymbol{A}_i\)</span> into <span class="math inline">\(\eqref{SystIndT}\)</span>, we obtain
<span class="math display">\[\begin{equation}
\label{Etape}
\sum_{i=1}^n\left(\frac{\partial}{\partial\boldsymbol{\beta}}
\mathbb{E}[\boldsymbol{N}_i]\right)^\top\boldsymbol{A}_i^{-1}(\boldsymbol{n}_i-\mathbb{E}[\boldsymbol{N}_i])
=\boldsymbol{0}
\end{equation}\]</span>
since
<span class="math display">\[
\frac{\partial}{\partial\boldsymbol{\beta}}\mathbb{E}[\boldsymbol{N}_i]=\boldsymbol{A}_i\boldsymbol{X}_i.
\]</span></p>
<p>The principle of GEE is to replace <span class="math inline">\(\boldsymbol{A}_i\)</span> in <span class="math inline">\(\eqref{Etape}\)</span> with a more reasonable candidate for the variance-covariance matrix of <span class="math inline">\(\boldsymbol{N}_i\)</span>, where “reasonable” means accounting for overdispersion and temporal correlation. Let us specify a plausible form for the covariance matrix of <span class="math inline">\(\boldsymbol{N}_i\)</span>: we could consider
<span class="math display">\[
\boldsymbol{V}_i=\phi\boldsymbol{A}_i^{1/2}\boldsymbol{R}_i(\boldsymbol{\alpha})\boldsymbol{A}_i^{1/2}
\]</span>
where the correlation matrix <span class="math inline">\(\boldsymbol{R}_i(\boldsymbol{\alpha})\)</span> accounts for the serial dependence between the components of <span class="math inline">\(\boldsymbol{N}_i\)</span> and depends on a set of parameters <span class="math inline">\(\boldsymbol{\alpha}\)</span>. The matrix <span class="math inline">\(\boldsymbol{R}_i\)</span> is a square sub-matrix of dimension <span class="math inline">\(T_i\times T_i\)</span> of a matrix <span class="math inline">\(\boldsymbol{R}\)</span> of dimension <span class="math inline">\(T_{\max}\times T_{\max}\)</span> whose elements do not depend on the characteristics <span class="math inline">\(\boldsymbol{x}_{it}\)</span> of individual <span class="math inline">\(i\)</span>. Overdispersion is accounted for since <span class="math inline">\(\mathbb{V}[N_{it}]=\phi\lambda_{it}\)</span>. Note that the matrix <span class="math inline">\(\boldsymbol{V}_i\)</span> defined in this way is the covariance matrix of <span class="math inline">\(\boldsymbol{N}_i\)</span> only if <span class="math inline">\(\boldsymbol{R}_i(\boldsymbol{\alpha})\)</span> is the correlation matrix of <span class="math inline">\(\boldsymbol{N}_i\)</span>, which is not necessarily the case.</p>
<p>As mentioned above, the idea is to substitute the matrix <span class="math inline">\(\boldsymbol{V}_i\)</span> for <span class="math inline">\(\boldsymbol{A}_i\)</span> in <span class="math inline">\(\eqref{Etape}\)</span>, and to retain as the estimate of <span class="math inline">\(\boldsymbol{\beta}\)</span> the solution of
<span class="math display">\[\begin{equation}
\label{GEE}
\sum_{i=1}^n\left(\frac{\partial}{\partial\boldsymbol{\beta}}\mathbb{E}[\boldsymbol{N}_i]\right)^\top
\boldsymbol{V}_i^{-1}(\boldsymbol{n}_i-\mathbb{E}[\boldsymbol{N}_i])
=\boldsymbol{0}
\end{equation}\]</span>
This last equation also expresses orthogonality between the regression residuals and the explanatory variables.
The estimators obtained in this way are convergent regardless of the choice of the matrix <span class="math inline">\(\boldsymbol{R}_i(\boldsymbol{\alpha})\)</span>. It is evident that they will be more precise the closer <span class="math inline">\(\boldsymbol{R}_i(\boldsymbol{\alpha})\)</span> is to the true correlation matrix of <span class="math inline">\(\boldsymbol{N}_i\)</span>.</p>
</div>
</div>
<div id="modeling-dependence-using-the-working-correlation-matrix" class="section level3 hasAnchor" number="10.12.6">
<h3><span class="header-section-number">10.12.6</span> Modeling Dependence Using the “Working Correlation Matrix”<a href="chap9.html#modeling-dependence-using-the-working-correlation-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As we understood from the above, it is the correlation matrix <span class="math inline">\(\boldsymbol{R}_i\)</span> that takes into account the dependence between observations related to the same insured individual. This matrix of dimension <span class="math inline">\(T_i\times T_i\)</span> is called the “working correlation matrix.” It is a correlation matrix of a specified form depending on a number of parameters contained in the vector <span class="math inline">\(\boldsymbol{\alpha}\)</span>.</p>
<p>If <span class="math inline">\(\boldsymbol{R}_i(\boldsymbol{\alpha})=\)</span>Identity, <span class="math inline">\(\eqref{GEE}\)</span> exactly yields the likelihood equations <span class="math inline">\(\eqref{SystIndT}\)</span> under the assumption of independence.</p>
<p>In general, in the context of prior-rate making, one specifies a matrix <span class="math inline">\(\boldsymbol{R}_i(\boldsymbol{\alpha})\)</span> that reflects an autoregressive-type structure. Thus, the diagonal elements of <span class="math inline">\(\boldsymbol{R}_i\)</span> are 1, and off-diagonal, the element <span class="math inline">\(jk\)</span> is <span class="math inline">\(\alpha_{|j-k|}\)</span> for <span class="math inline">\(|j-k|\leq m\)</span> and 0 for <span class="math inline">\(|j-k|&gt;m\)</span>. We take <span class="math inline">\(m=T_{\max}-1\)</span>. The components of the vector <span class="math inline">\(\boldsymbol{\alpha}\)</span> that parameterize the matrix <span class="math inline">\(\boldsymbol{R}_i(\alpha)\)</span> describing the type of dependence between the data are estimated based on observations.</p>
</div>
<div id="obtaining-estimates" class="section level3 hasAnchor" number="10.12.7">
<h3><span class="header-section-number">10.12.7</span> Obtaining Estimates<a href="chap9.html#obtaining-estimates" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Equation <span class="math inline">\(\eqref{GEE}\)</span> is generally solved using a modified Fisher score method for <span class="math inline">\(\boldsymbol{\beta}\)</span> and a moment estimation for <span class="math inline">\(\boldsymbol{\alpha}\)</span> (we refer the reader to <span class="citation">(<a href="#ref-liang1986longitudinal" role="doc-biblioref">Liang and Zeger 1986</a>)</span> for a complete description of the method). Specifically, starting with an initial value <span class="math inline">\(\widehat{\boldsymbol{\beta}}^{(0)}\)</span> that solves the system <span class="math inline">\(\eqref{EqVrais1}\)</span>-<span class="math inline">\(\eqref{EqVraisPois}\)</span>, we calculate
<span class="math display">\[\begin{eqnarray*}
\widehat{\boldsymbol{\beta}}^{(j+1)}&amp;=&amp;\widehat{\boldsymbol{\beta}}^{(j)}+\left\{\sum_{i=1}^n\boldsymbol{D}_i^\top(\widehat{\boldsymbol{\beta}}^{(j)})
\boldsymbol{V}_i^{-1}(\widehat{\boldsymbol{\beta}}^{(j)},\boldsymbol{\alpha}(\widehat{\boldsymbol{\beta}}^{(j)}))\boldsymbol{D}_i(\widehat{\boldsymbol{\beta}}^{(j)})\right\}^{-1}\\
&amp;&amp;\hspace{20mm}\left\{\sum_{i=1}^n\boldsymbol{D}_i^\top(\widehat{\boldsymbol{\beta}}^{(j)})
\boldsymbol{V}_i^{-1}(\widehat{\boldsymbol{\beta}}^{(j)},\boldsymbol{\alpha}(\widehat{\boldsymbol{\beta}}^{(j)}))\boldsymbol{S}_i(\widehat{\boldsymbol{\beta}}^{(j)})\right\}
\end{eqnarray*}\]</span>
where <span class="math inline">\(\boldsymbol{D}_i(\boldsymbol{\beta})=\frac{\partial}{\partial\boldsymbol{\beta}}\mathbb{E}[\boldsymbol{N}_i]\)</span> and <span class="math inline">\(\boldsymbol{S}_i(\boldsymbol{\beta})=\boldsymbol{N}_i-\mathbb{E}[\boldsymbol{N}_i]\)</span>. At each step, <span class="math inline">\(\phi\)</span> and <span class="math inline">\(\boldsymbol{\alpha}\)</span> are re-estimated from the Pearson residuals <span class="math inline">\(r_{it}^P=\frac{n_{it}-\lambda_{it}}{\sqrt{\lambda_{it}}}\)</span> using the formulas
<span class="math display">\[
\widehat{\phi}=\frac{1}{\sum_{i=1}^nT_i-p}\sum_{i=1}^n\sum_{t=1}^{T_i}\{r_{it}^P\}^2
\]</span>
and
<span class="math display">\[
\widehat{\alpha}_\tau=\frac{1}{\widehat{\phi}\left(\sum_{i|T_i&gt;\tau}(T_i-\tau)-p\right)}
\sum_{i|T_i&gt;\tau}\sum_{t=1}^{T_i-\tau}r_{it}^Pr_{it+\tau}^P.
\]</span></p>
</div>
<div id="numerical-illustration-1" class="section level3 hasAnchor" number="10.12.8">
<h3><span class="header-section-number">10.12.8</span> Numerical Illustration<a href="chap9.html#numerical-illustration-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The serial dependence of <span class="math inline">\(N_{it}\)</span> for a fixed <span class="math inline">\(i\)</span> has clearly been demonstrated in Section 3.1, and it is important to assess the impact of the independence assumption on frequency estimation. The GEE approach can be implemented using the SAS procedure. A variable selection, based as previously on Type 3 analysis, leads us to select the same variables as in the model assuming independence. The results are shown in Tables~<span class="math inline">\(\ref{gee_unstr}\)</span> and~<span class="math inline">\(\ref{type3_gee_unstr}\)</span>. The estimation of the “working correlation matrix” of a second-order autoregressive structure (i.e., order <span class="math inline">\(T_{\max}-1\)</span>) gives
<span class="math display">\[
\left(
\begin{array}{ccc}
1 &amp; 0.0493 &amp; 0.0462\\
0.0493 &amp; 1 &amp; 0.0493\\
0.0462 &amp; 0.0493 &amp; 1\\
\end{array}
\right)
\]</span>
and <span class="math inline">\(\widehat{\phi}=1.3437\)</span>.</p>
<p>TBLEAU</p>
</div>
</div>
<div id="technical-justifications-for-segmentation" class="section level2 hasAnchor" number="10.13">
<h2><span class="header-section-number">10.13</span> Technical Justifications for Segmentation<a href="chap9.html#technical-justifications-for-segmentation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="technical-rate-and-commercial-rate" class="section level3 hasAnchor" number="10.13.1">
<h3><span class="header-section-number">10.13.1</span> Technical Rate and Commercial Rate<a href="chap9.html#technical-rate-and-commercial-rate" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>One of the fundamental tasks of an actuary is to evaluate, on the one hand, the pure premium, which is the contribution of each policyholder that enables the insurer to compensate for claims, and, on the other hand, to assess the security loading that ensures the stability of the company’s results. The concept of pure premium has been discussed in Chapter 3, while the determination of security loadings was studied in Chapter 4. In this regard, the actuary will conduct as detailed a technical analysis as possible of the risk to be covered and will construct the technical rate. The technical rate provides the cost price of the coverage granted by the insurer, based on the risk profile of the insured.</p>
<p>The technical rate is for purely internal use. The company will apply the commercial rate to policyholders, which can significantly differ from the technical rate. While the commercial rate may be based on the technical rate, regulatory considerations or factors related to the company’s market positioning, as well as a desire to simplify the rate grid, can lead to commercial premiums that are sometimes very different from the technical rate. Two rate grids coexist: one purely technical and for internal use only, resulting from a detailed analysis carried out by the actuary, and the other commercial, describing the amounts that will actually be charged to policyholders.</p>
</div>
<div id="segmentation-of-technical-and-commercial-rates" class="section level3 hasAnchor" number="10.13.2">
<h3><span class="header-section-number">10.13.2</span> Segmentation of Technical and Commercial Rates<a href="chap9.html#segmentation-of-technical-and-commercial-rates" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>At the technical level, the actuary can either finely segment the portfolio or use mixture models to account for the heterogeneity resulting from the absence of segmentation. Let us now turn our attention to the commercial rate, where certain imperfections in the market may require the actuary to segment.</p>
<p>Over the last decade, commercial rates that were once nearly uniform have gradually been differentiated based on the profile of policyholders. This phenomenon is often referred to as segmentation (see Section 3.8). Although segmentation is not limited to rate differentiation, it also includes risk selection that the insurer performs when concluding the contract (acceptance) or during the contract (termination).</p>
<p>The purpose of this section is not to justify or criticize the principle of segmentation at the commercial level. In this matter, contractual freedom should prevail, and it is up to the state to regulate the market if necessary. We only want to show that most rate choices made by companies result from commercial or competitive considerations and are not imposed by actuarial science. Only adverse selection from policyholders can compel an insurer to reflect rate differentiation among policyholders. This criterion is then found both in the technical rate and in the commercial rate (but perhaps not in the same way).</p>
<p>The reader should keep in mind that rate differentiation has no impact on claims experience. Therefore, regardless of the company’s pricing policy, the pure premium for the portfolio must remain the same (since it corresponds to the expected cost of claims). Hence, any reduction in premium for one category of policyholders necessarily leads to a corresponding increase in premiums paid by other categories of policyholders.</p>
<p>Excessive rate differentiation can have disastrous effects on an insurance market. Indeed, such differentiation significantly limits the insurability of risks: by decreasing (often only moderately) the premium for certain categories of policyholders, it increases the premium for others, sometimes to the point of excluding them from the insurance market. The example of automobile liability insurance is instructive in this regard.</p>
<p>The increasing level of premium differentiation observed in the market is mainly explained by the spiral of segmentation: it is competition that drives companies to differentiate premiums more and more. Indeed, an insurer can hardly maintain a uniform rate in a market where competitors differentiate risks. Theoretically, as soon as a market participant introduces a new criterion for rate differentiation, its competitors are obliged to implicitly or explicitly recognize it in their rates, as we explained in detail in Volume I. This is why we have witnessed increasingly fine rate differentiation in recent years, particularly in automobile liability insurance. Therefore, it is the spiral of segmentation induced by fierce competition among market participants that leads to excessive segmentation, and not any actuarial prescription.</p>
</div>
<div id="adverse-selection-and-segmentation" class="section level3 hasAnchor" number="10.13.3">
<h3><span class="header-section-number">10.13.3</span> Adverse Selection and Segmentation<a href="chap9.html#adverse-selection-and-segmentation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>One characteristic of private insurance lies in the freedom to contract for both parties involved: the policyholder and the insurer. Therefore, the prospective policyholder will choose the insurance contract that they consider most attractive. Furthermore, the prospective policyholder has a certain advantage in terms of information compared to an insurer that does little segmentation. The prospective policyholder knows their own situation very precisely, whereas the insurer has no knowledge of certain risk-aggravating factors that may be present. This asymmetric information leads to adverse selection from policyholders: policyholders whose profile is riskier tend to subscribe to the policies offered by the company in large numbers, thereby deteriorating the insurer’s claims experience.</p>
<p>Adverse selection from policyholders has the effect of limiting access to insurance. To understand this, consider, for example, coverage against a dreaded disease, M. If the population includes individuals predisposed to this disease and others who are not, without any of them knowing their predisposition, the insurer may cover them all based on an average premium. The policy taken out can then be seen as a multi-guarantee contract, which first covers the risk of being predisposed to disease M and then covers the cost of treating it when it occurs.</p>
<p>Suppose, for example, that 10% of the population is predisposed to disease M. Specifically, in the case of predisposition, the probability of developing disease M is 2%, while it is only 0.1% in the absence of predisposition. This predisposition can be detected through a genetic test, but its use is prohibited for the insurer and too costly for the insured. The cost of treating disease M is €10,000. The pure premium for coverage against this disease is:</p>
<p><span class="math display">\[
10\%\times 2\% \times €10,000 + 90\% \times 0.1\% \times €10,000 = €20 + €9 = €29.
\]</span></p>
<p>Upon closer examination of this example, it can be seen that uniform pricing actually covers two distinct risks: first, the risk of being predisposed to disease M, and second, the risk of developing the disease. Formally, each policyholder pays a premium of €10 as if they were not predisposed, and adds to that a premium of €19 that covers the risk of being predisposed to disease M.</p>
<p>It is also evident that this approach allows for risk coverage at financially acceptable conditions for all individuals, whether predisposed or not. This perspective, equating the policies offered by an insurer that does not segment with multi-guarantee contracts, was developed by insurance economists, including <span class="citation">(<a href="#ref-chiappori1997risque" role="doc-biblioref">Chiappori 1997</a>)</span>. It justifies the higher premium for “good” risks who are unaware of their status by offering broader coverage.</p>
<p>Consider a portfolio of 10,000 policyholders whose composition is similar to that of the population. Individuals are unaware of their potential predisposition. This portfolio therefore includes, on average, 1,000 policyholders predisposed to disease M and 9,000 who are not. Among the former, an average of 20 will develop disease M, resulting in a cost of €200,000 for the insurer. Among the latter, an average of 9 will develop disease M, resulting in a cost of €90,000 for the insurer. The total cost of €290,000 on average will be offset by the collection of 10,000 premiums of €29 each.</p>
</div>
<div id="inequity-of-prior-pricing" class="section level3 hasAnchor" number="10.13.4">
<h3><span class="header-section-number">10.13.4</span> Inequity of Prior Pricing<a href="chap9.html#inequity-of-prior-pricing" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>An insurer who wishes to use a segmentation criterion must be able to demonstrate, with statistical evidence, the causal link between this criterion and the variations in claims that it is supposed to induce. Apart from the fact that establishing a causal link resembles the quest for the Holy Grail for a statistician (except in the case of an experiment plan with controlled parameters), this requirement discredits many segmentation criteria currently used. Who could believe that cohabitation or having multiple children improves the quality of driving? It is more likely that marital life or family responsibilities encourage caution and a refusal to take unnecessary risks, resulting in less recklessness behind the wheel, which in turn reduces the risk in automobile liability insurance. There is no causal relationship between these pricing variables and automobile claims. The relevant risk factors are hidden: aggressiveness on the road, adherence to traffic rules, alcohol consumption, annual mileage, etc., and therefore cannot be incorporated into the commercial rate.</p>
<p>However, there is a viable solution in automobile insurance: the PAYD (Pay As You Drive) system developed on both sides of the Atlantic by Norwich Union in the UK and Progressive in the USA. This is a decidedly innovative approach to automobile insurance, made possible by the latest technologies. A “black box” is installed in the insured vehicle, recording speed, accelerations and decelerations, kilometers traveled, and the type of road taken. Privacy is perfectly protected because the data is filtered before reaching the insurer, making it impossible for the insurer to reconstruct the journeys based on the summary provided. However, this summary allows the insurer to very accurately assess the vehicle’s driving quality: by comparing recorded speeds to the maximum speeds allowed depending on the type of road, the insurer can assess how well policyholders adhere to the speed limits imposed by traffic regulations, and accelerations and decelerations indicate the nervousness of the driving, and so on. The insurer thus has the relevant information to price the contract accurately.</p>
<p>This approach also promotes road safety and would undoubtedly help reduce the heavy social cost of road accidents. In addition to calculating the premium, the system offers additional benefits, such as the possible location of the vehicle by the police in case of theft, alerting emergency services in case of an accident, and more.</p>
<p>The cost of the system is reasonable (around €500 for the first year, covering the acquisition of the equipment and its installation, and limited maintenance costs in subsequent years). The system is currently being tested by 5,000 drivers in the UK and another 5,000 in the USA. Other experiments are ongoing, including in Italy.
Note that in Singapore, a similar system is used to bill monthly tolls based on the hours during which motorists used the roads and the types of roads taken.</p>
</div>
</div>
<div id="bibliographical-notes-7" class="section level2 hasAnchor" number="10.14">
<h2><span class="header-section-number">10.14</span> Bibliographical Notes<a href="chap9.html#bibliographical-notes-7" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Regarding scores, the reader can refer to <span class="citation">(<a href="#ref-gourieroux1992courbes" role="doc-biblioref">Gourieroux 1992</a>)</span> or <span class="citation">(<a href="#ref-gourieroux1999statistique" role="doc-biblioref">Gouriéroux 1999</a>)</span> and <span class="citation">(<a href="#ref-bardos2001analyse" role="doc-biblioref">Bardos 2001</a>)</span>. Standard approaches to data (multivariate) analysis are presented in <span class="citation">(<a href="#ref-lebart1995statistique" role="doc-biblioref">Lebart, Morineau, and Piron 1995</a>)</span>. Methods of classification are presented in a clear and pedagogical manner in <span class="citation">(<a href="#ref-nakache2004approche" role="doc-biblioref">Nakache and Confais 2004</a>)</span>. The theory of generalized linear models dates back to <span class="citation">(<a href="#ref-nelder1972generalized" role="doc-biblioref">Nelder and Wedderburn 1972</a>)</span> and has been thoroughly covered by <span class="citation">(<a href="#ref-mccullagh1989generalized" role="doc-biblioref">McCullagh and Nelder 1989</a>)</span>. An excellent introduction is provided by <span class="citation">(<a href="#ref-dobson2001introduction" role="doc-biblioref">Dobson 2001</a>)</span>.</p>
<p>We also recommend readers to consult <span class="citation">(<a href="#ref-bailey1963insurance" role="doc-biblioref">Bailey 1963</a>)</span> and <span class="citation">(<a href="#ref-bailey1960two" role="doc-biblioref">Bailey and Simon 1960</a>)</span> regarding the origins of risk classification, as well as <span class="citation">(<a href="#ref-anderson2004practitioner" role="doc-biblioref">Anderson et al. 2004</a>)</span> regarding the practice of generalized linear models in pricing. Notable pioneers of regression models in pricing include <span class="citation">(<a href="#ref-ter1980loglinear" role="doc-biblioref">Ter Berg 1980</a>)</span>, <span class="citation">(<a href="#ref-ter1980two" role="doc-biblioref">Berg 1980</a>)</span>, <span class="citation">(<a href="#ref-albrecht1983parametric" role="doc-biblioref">Albrecht 1983</a>)</span>, and especially <span class="citation">(<a href="#ref-renshaw1994modelling" role="doc-biblioref">Renshaw 1994</a>)</span>.</p>
<p>The treatment of severe claims can be done using extreme value theory, which is discussed later in this book. You can read <span class="citation">(<a href="#ref-cebrian2003generalized" role="doc-biblioref">Cebrián, Denuit, and Lambert 2003</a>)</span> for a practical case.</p>
<p>We haven’t discussed the problem of geographical zone pricing here. An accessible introduction, along with a practical case, can be found in <span class="citation">(<a href="#ref-brouhns2002ratemaking" role="doc-biblioref">Brouhns, Denuit, and Masuy 2002</a>)</span>. <span class="citation">(<a href="#ref-denuit2004non" role="doc-biblioref">Denuit and Lang 2004</a>)</span> review the various approaches used in a priori pricing and propose an integrated pricing model based on generalized additive models in the Bayesian paradigm. All rating factors, whether categorical, continuous, spatial, or temporal, are treated in a unified manner.</p>
<p>Empirical studies are relatively scarce in the literature. In addition to those mentioned earlier, you can refer to <span class="citation">(<a href="#ref-ramlau1988solvency" role="doc-biblioref">Ramlau-Hansen 1988</a>)</span> and <span class="citation">(<a href="#ref-beirlant1992statistical" role="doc-biblioref">Beirlant et al. 1992</a>)</span>. For other regression models not covered in this chapter, see <span class="citation">(<a href="#ref-beirlant1998burr" role="doc-biblioref">Beirlant et al. 1998</a>)</span>, <span class="citation">(<a href="#ref-cummins1990applications" role="doc-biblioref">Cummins et al. 1990</a>)</span>, <span class="citation">(<a href="#ref-ter1996loglinear" role="doc-biblioref">Ter Berg 1996</a>)</span>, and <span class="citation">(<a href="#ref-keiding1998cox" role="doc-biblioref">Keiding, Andersen, and Fledelius 1998</a>)</span>.</p>
<p>Finally, note that the models described in this chapter are also very useful in life insurance for constructing mortality tables. See <span class="citation">(<a href="#ref-delwarde2005construction" role="doc-biblioref">Delwarde and Denuit 2005</a>)</span> for more details.</p>
</div>
<div id="exercises-5" class="section level2 hasAnchor" number="10.15">
<h2><span class="header-section-number">10.15</span> Exercises<a href="chap9.html#exercises-5" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="exercise">
<p><span id="exr:unlabeled-div-49" class="exercise"><strong>Exercise 10.1  </strong></span>Given independent and identically distributed observations <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span>, let <span class="math inline">\(X_{(1)} \leq X_{(2)} \leq \ldots \leq X_{(n)}\)</span> be the observations arranged in ascending order.</p>
<ol style="list-style-type: decimal">
<li>Define the event
<span class="math display">\[
A(i_1, i_2) = \left[X_{(i_1)} &lt; q_p &lt; X_{(i_2)}\right],
\]</span>
which means that at least <span class="math inline">\(i_1\)</span> observations are less than the <span class="math inline">\(p\)</span>th quantile <span class="math inline">\(q_p\)</span>, and at least <span class="math inline">\(n - i_2 + 1\)</span> observations are greater than it. Show that
<span class="math display">\[\begin{eqnarray*}
\Pr[A(i_1, i_2)] &amp;=&amp; \sum_{j=i_1}^{i_2-1}\binom{n}{j}p^j(1-p)^{n-j}.
\end{eqnarray*}\]</span></li>
<li>Show that a confidence interval at the <span class="math inline">\(1-\alpha\)</span> level for <span class="math inline">\(q_p\)</span> is <span class="math inline">\(]x_{(i_1)}, x_{(i_2)}[\)</span>, where the integers <span class="math inline">\(i_1\)</span> and <span class="math inline">\(i_2\)</span> satisfy the relation
<span class="math display">\[
1 - \alpha = \Pr[i_1 \leq \text{Binomial}(n, p) &lt; i_2].
\]</span></li>
<li>When <span class="math inline">\(n\)</span> is large, show that <span class="math inline">\(i_1\)</span> and <span class="math inline">\(i_2\)</span> are approximately given by
<span class="math display">\[
i_1 = \lfloor -z_{\alpha/2}\sqrt{np(1-p)} + np \rfloor
\]</span>
and
<span class="math display">\[
i_2 = \lceil z_{\alpha/2}\sqrt{np(1-p)} + np \rceil,
\]</span>
where <span class="math inline">\(z_{\alpha/2}\)</span> is such that <span class="math inline">\(\Phi(z_{\alpha/2}) = 1 - \alpha/2\)</span>, and <span class="math inline">\(\lfloor x\rfloor\)</span> (resp. <span class="math inline">\(\lceil x\rceil\)</span>) represents the floor (integer part) and ceiling (smallest integer greater than or equal to) of the real number <span class="math inline">\(x\)</span>, respectively.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-50" class="exercise"><strong>Exercise 10.2  </strong></span>Show that for the Poisson distribution with mean <span class="math inline">\(\lambda\)</span>,
<span class="math display">\[\begin{eqnarray*}
\mathcal{I}(\widetilde{\lambda}|\lambda) &amp;=&amp; \widetilde{\lambda}\left(\ln\frac{\widetilde{\lambda}}{\lambda} - 1\right) + \lambda.
\end{eqnarray*}\]</span></p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-51" class="exercise"><strong>Exercise 10.3  </strong></span>Show that for the gamma distribution,
<span class="math display">\[\begin{eqnarray*}
\mathcal{I}(\widetilde{\lambda}|\lambda) &amp;=&amp; \frac{3(\widetilde{\lambda}^{1/3} - \lambda^{1/3})}{\lambda^{1/3}}.
\end{eqnarray*}\]</span></p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-52" class="exercise"><strong>Exercise 10.4  </strong></span>Consider the simple linear model <span class="math inline">\(Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i\)</span>, denoted as <span class="math inline">\((1)\)</span>, where <span class="math inline">\(\varepsilon_i\)</span> are independent and identically distributed with <span class="math inline">\(\mathcal{N}or(0,\sigma^2)\)</span>.</p>
<ol style="list-style-type: decimal">
<li>Let <span class="math inline">\(Z_i = Y_i - X_i\)</span>, <span class="math inline">\(\delta_0 = \beta_0\)</span>, and <span class="math inline">\(\delta_1 = \beta_1 - 1\)</span>, so <span class="math inline">\(Z_i = \delta_0 + \delta_1 X_i + \varepsilon_i\)</span>, denoted as <span class="math inline">\((2)\)</span>, is equivalent to model <span class="math inline">\((1)\)</span>. Compare the <span class="math inline">\(R^2\)</span> obtained in both models.</li>
<li>Conclude that by regressing <span class="math inline">\(Y\)</span> not on <span class="math inline">\(X\)</span> but on <span class="math inline">\(Y-X\)</span>, it is sometimes possible to artificially increase the value of <span class="math inline">\(R^2\)</span>.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-53" class="exercise"><strong>Exercise 10.5  </strong></span></p>
<ol style="list-style-type: decimal">
<li>Company A has 8,000 experienced drivers and 2,000 novice drivers in its portfolio. They have reported 400 and 200 claims, respectively. Estimate (using maximum likelihood in a Poisson regression model) the scores associated with these two categories of insureds.</li>
<li>Two companies share the market: Company A and a competitor, Company B, which does not segment its tariff a priori. Explain what should happen in the market assuming rational insureds.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-54" class="exercise"><strong>Exercise 10.6  </strong></span>In order to explain the annual claims frequency of insured individuals based on their observable characteristics, the actuary in charge of pricing Motor Third Party Liability insurance at a major insurance company performed a Poisson regression. The number <span class="math inline">\(N_i\)</span> of claims caused by insured individual <span class="math inline">\(i\)</span> is assumed to follow a Poisson distribution with a mean of <span class="math inline">\(d_i\exp(\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\beta_3x_{i3})\)</span>, where <span class="math inline">\(d_i\)</span> is the coverage duration and
<span class="math display">\[
x_{i1}=\left\{
\begin{array}{l}
1,\text{ if insured individual $i$ is female},\\
0,\text{ otherwise},
\end{array}
\right.
\]</span>
<span class="math display">\[
x_{i2}=\left\{
\begin{array}{l}
1,\text{ if insured individual $i$ pays their premium in installments},\\
0,\text{ if they pay it annually},
\end{array}
\right.
\]</span>
and
<span class="math display">\[
x_{i3}=\left\{
\begin{array}{l}
1,\text{ if insured individual $i$ drives a diesel vehicle},\\
0,\text{ if they drive a gasoline vehicle}.
\end{array}
\right.
\]</span></p>
<p>The maximum likelihood estimates obtained are described in the following table:</p>
<p>In this table, you can see the point estimates of the regression coefficients and their corresponding standard errors.</p>
<ol style="list-style-type: decimal">
<li>What equations are <span class="math inline">\(\widehat{\beta}_0\)</span>, <span class="math inline">\(\widehat{\beta}_1\)</span>, <span class="math inline">\(\widehat{\beta}_2\)</span>, and <span class="math inline">\(\widehat{\beta}_3\)</span> solutions to? What do they guarantee in terms of the distribution of claims between different categories of insureds?</li>
<li>Do all risk factors significantly influence claims frequency?</li>
<li>What is the annual claims frequency of the reference insured individual? Provide a point estimate and a 95% confidence interval.</li>
<li>What is the estimated annual claims frequency of a female policyholder paying her premium annually and driving a gasoline vehicle? Provide a 95% confidence interval if the coefficient of linear correlation between <span class="math inline">\(\widehat{\beta}_0\)</span> and <span class="math inline">\(\widehat{\beta}_1\)</span> is estimated to be 0.1.</li>
<li>What is the estimated annual claims frequency of a female policyholder who pays her premium in installments and drives a gasoline vehicle?</li>
<li>To detect possible overdispersion, a graph is plotted showing the mean and empirical variance of the number of claims observed within each class. What curve should the scatterplot be fitted to in order to support the mixed Poisson distribution hypothesis? Explain and justify.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-55" class="exercise"><strong>Exercise 10.7  </strong></span>In order to explain the annual claims frequency of insured individuals based on their observable characteristics, the actuary in charge of Motor Third Party Liability pricing at a major insurance company performed a Poisson regression on a dataset of 100,000 policies observed over one year, using driving experience (novice/experienced) and vehicle power (small/medium/large engine) as factors. The number <span class="math inline">\(N_i\)</span> of claims caused by insured individual <span class="math inline">\(i\)</span> is assumed to follow a Poisson distribution with a mean of <span class="math inline">\(d_i\exp(\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\beta_3x_{i3})\)</span>, where <span class="math inline">\(d_i\)</span> is the coverage duration and
<span class="math display">\[
x_{i1}=\left\{
\begin{array}{l}
1,\text{ if insured individual $i$ is a novice driver},\\
0,\text{ otherwise},
\end{array}
\right.
\]</span>
<span class="math display">\[
x_{i2}=\left\{
\begin{array}{l}
1,\text{ if insured individual $i$ drives a vehicle with a small engine},\\
0,\text{ otherwise},
\end{array}
\right.
\]</span>
and
<span class="math display">\[
x_{i3}=\left\{
\begin{array}{l}
1,\text{ if insured individual $i$ drives a vehicle with a large engine},\\
0,\text{ otherwise}.
\end{array}
\right.
\]</span></p>
<p>The maximum likelihood estimates obtained are as follows:
<span class="math display">\[
\widehat{\beta}_0=-2.10, \quad \widehat{\beta}_1=0.06, \quad \widehat{\beta}_2=-0.03, \quad \text{and} \quad \widehat{\beta}_3=0.05.
\]</span>
The standard errors are also provided:
<span class="math display">\[
\widehat{\sigma}\big(\widehat{\beta}_0\big)=0.03, \quad \widehat{\sigma}\big(\widehat{\beta}_1\big)=0.02, \quad \widehat{\sigma}\big(\widehat{\beta}_2\big)=0.01, \quad \text{and} \quad \widehat{\sigma}\big(\widehat{\beta}_3\big)=0.01.
\]</span></p>
<ol style="list-style-type: decimal">
<li>What equations are the point estimates provided above solutions to? Provide an actuarial interpretation.</li>
<li>Test the hypotheses <span class="math inline">\(\beta_j=0\)</span> for <span class="math inline">\(j=0,1,2,3\)</span>, and interpret the results.</li>
<li>Estimate the annual claims frequency of the reference insured individual and provide a confidence interval.</li>
<li>Do the same for the annual claims frequency of a novice driver with a medium-engine vehicle if <span class="math inline">\(\widehat{\mathbb{C}[\widehat{\beta}_0,\widehat{\beta}_1]}=0.0001\)</span>.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-56" class="exercise"><strong>Exercise 10.8  </strong></span>The cost of claims caused by insured individual <span class="math inline">\(i\)</span> can be expressed as
<span class="math display">\[
S_i=\sum_{k=1}^{N_i}C_{ik},
\]</span>
where <span class="math inline">\(N_i\)</span> follows a Poisson distribution with mean <span class="math inline">\(\lambda_i\)</span>, and <span class="math inline">\(C_{ik}\)</span> follows a Gamma distribution
with mean <span class="math inline">\(\mu_i\)</span> and variance <span class="math inline">\(\mu_i^2/\nu\)</span>. All random variables involved are independent.</p>
<p>The actuary in charge of pricing uses gender (male/female) and the insured’s location (rural/urban), coded using two binary variables, with “male” and “rural” as the reference levels. The actuary incorporates this information as follows:
<span class="math display">\[
\lambda_i=d_i\exp(\beta_0^{\text{freq}}+\beta_1^{\text{freq}}x_{i1}+\beta_2^{\text{freq}}x_{i2}),
\]</span>
with <span class="math inline">\(d_i\)</span> as the exposure duration, and
<span class="math display">\[
\mu_i=\exp(\beta_0^{\text{coût}}+\beta_1^{\text{coût}}x_{i1}+\beta_2^{\text{coût}}x_{i2}).
\]</span>
The point estimates of these parameters obtained by maximum likelihood estimation based on observations from a large portfolio are as follows:
For frequencies:
<span class="math display">\[
\widehat{\beta}_0^{\text{freq}}=-1.95, \quad \widehat{\beta}_1^{\text{freq}}=0.03, \quad \text{and} \quad \widehat{\beta}_2^{\text{freq}}=0.05,
\]</span>
And for costs:
<span class="math display">\[
\widehat{\beta}_0^{\text{coût}}=6.64, \quad \widehat{\beta}_1^{\text{coût}}=-0.05, \quad \text{and} \quad \widehat{\beta}_2^{\text{coût}}=-0.07,
\]</span>
with <span class="math inline">\(\widehat{\nu}=0.75\)</span>. Standard errors are also provided:
For frequencies:
<span class="math display">\[
\widehat{\sigma}\big(\widehat{\beta}_0^{\text{freq}}\big)=0.01, \quad \widehat{\sigma}\big(\widehat{\beta}_1^{\text{freq}}\big)=0.01,
\quad \text{and} \quad \widehat{\sigma}\big(\widehat{\beta}_2^{\text{freq}}\big)=0.02,
\]</span>
And for costs:
<span class="math display">\[
\widehat{\sigma}\big(\widehat{\beta}_0^{\text{coût}}\big)=0.02, \quad \widehat{\sigma}\big(\widehat{\beta}_1^{\text{coût}}\big)=0.01,
\quad \text{and} \quad \widehat{\sigma}\big(\widehat{\beta}_2^{\text{coût}}\big)=0.02,
\]</span>
for costs.</p>
<ol style="list-style-type: decimal">
<li>Describe the iterative algorithm that was used to obtain the point estimates <span class="math inline">\(\widehat{\beta}_0^{\text{freq}}\)</span>, <span class="math inline">\(\widehat{\beta}_1^{\text{freq}}\)</span>, and <span class="math inline">\(\widehat{\beta}_2^{\text{freq}}\)</span> provided above.</li>
<li>What equations are the point estimates <span class="math inline">\(\widehat{\beta}_0^{\text{coût}}\)</span>, <span class="math inline">\(\widehat{\beta}_1^{\text{coût}}\)</span>, and <span class="math inline">\(\widehat{\beta}_2^{\text{coût}}\)</span> solutions to? Provide an actuarial interpretation.</li>
<li>Estimate the pure premium <span class="math inline">\(\mathbb{E}[S_i]\)</span> and the variance <span class="math inline">\(\mathbb{V}[S_i]\)</span> for the different rate classes.</li>
</ol>
</div>

</div>
</div>
<h3>Postface<a href="postface.html#postface" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-albrecht1983parametric" class="csl-entry">
Albrecht, Peter. 1983. <span>“Parametric Multiple Regression Risk Models: Connections with Tariffication, Especially in Motor Insurance.”</span> <em>Insurance: Mathematics and Economics</em> 2 (2): 113–17.
</div>
<div id="ref-anderson2004practitioner" class="csl-entry">
Anderson, Duncan, Sholom Feldblum, Claudine Modlin, Doris Schirmacher, Ernesto Schirmacher, and Neeza Thandi. 2004. <span>“A Practitioner’s Guide to Generalized Linear Models.”</span> <em>Casualty Actuarial Society Discussion Paper Program</em> 11 (3): 1–116.
</div>
<div id="ref-bailey1963insurance" class="csl-entry">
Bailey, Robert A. 1963. <span>“Insurance Rates with Minimum Bias.”</span> In <em>Proceedings of the Casualty Actuarial Society</em>, 50:4–14. 93.
</div>
<div id="ref-bailey1960two" class="csl-entry">
Bailey, Robert A, and LeRoy J Simon. 1960. <span>“Two Studies in Automobile Insurance Ratemaking.”</span> <em>ASTIN Bulletin: The Journal of the IAA</em> 1 (4): 192–217.
</div>
<div id="ref-bardos2001analyse" class="csl-entry">
Bardos, Mireille. 2001. <em>Analyse Discriminante: Application Au Risque Et Scoring Financier</em>. Dunod.
</div>
<div id="ref-beirlant1992statistical" class="csl-entry">
Beirlant, Jan, V Derveaux, Anna Maria De Meyer, MJ Goovaerts, E Labie, and B Maenhoudt. 1992. <span>“Statistical Risk Evaluation Applied to (Belgian) Car Insurance.”</span> <em>Insurance: Mathematics and Economics</em> 10 (4): 289–302.
</div>
<div id="ref-beirlant1998burr" class="csl-entry">
Beirlant, Jan, Yuri Goegebeur, Robert Verlaak, and Petra Vynckier. 1998. <span>“Burr Regression and Portfolio Segmentation.”</span> <em>Insurance: Mathematics and Economics</em> 23 (3): 231–50.
</div>
<div id="ref-ter1980two" class="csl-entry">
Berg, Peter ter. 1980. <span>“Two Pragmatic Approaches to Loglinear Claim Cost Analysis.”</span> <em>ASTIN Bulletin: The Journal of the IAA</em> 11 (2): 77–90.
</div>
<div id="ref-besson1992trend" class="csl-entry">
Besson, Jean-Luc, and C. Patrat. 1992. <span>“Trend Et Syst<span>è</span>mes de Bonus-Malus.”</span> <em>ASTIN Bulletin: The Journal of the IAA</em> 22 (1): 11–31.
</div>
<div id="ref-brouhns2002ratemaking" class="csl-entry">
Brouhns, Natacha, Michel Denuit, and Bernard Masuy. 2002. <span>“Ratemaking by Geographical Area: A Case Study Using the Boskov and Verrall Model.”</span> <em>Publications of the Institut de Statistique, Louvain-La-Neuve</em>, 1–26.
</div>
<div id="ref-cebrian2003generalized" class="csl-entry">
Cebrián, Ana C, Michel Denuit, and Philippe Lambert. 2003. <span>“Generalized Pareto Fit to the Society of Actuaries’ Large Claims Database.”</span> <em>North American Actuarial Journal</em> 7 (3): 18–36.
</div>
<div id="ref-celeux1994analyse" class="csl-entry">
Celeux, Gilles, and Jean-Pierre Nakache. 1994. <em>Analyse Discriminante Sur Variables Qualitatives</em>. Polytechnica.
</div>
<div id="ref-chiappori1997risque" class="csl-entry">
Chiappori, Pierre-André. 1997. <em>Risque Et Assurance</em>. Flammarion.
</div>
<div id="ref-cleveland1979robust" class="csl-entry">
Cleveland, William S. 1979. <span>“Robust Locally Weighted Regression and Smoothing Scatterplots.”</span> <em>Journal of the American Statistical Association</em> 74 (368): 829–36.
</div>
<div id="ref-cleveland1988locally" class="csl-entry">
Cleveland, William S, and Susan J Devlin. 1988. <span>“Locally Weighted Regression: An Approach to Regression Analysis by Local Fitting.”</span> <em>Journal of the American Statistical Association</em> 83 (403): 596–610.
</div>
<div id="ref-cleveland1991computational" class="csl-entry">
Cleveland, William S, and Eric Grosse. 1991. <span>“Computational Methods for Local Regression.”</span> <em>Statistics and Computing</em> 1: 47–62.
</div>
<div id="ref-cornillon2007regression" class="csl-entry">
Cornillon, Pierre-André, and Eric Matzner-Løber. 2007. <span>“La r<span>é</span>gression Lin<span>é</span>aire Simple.”</span> <em>R<span>é</span>gression: Th<span>é</span>orie Et Applications</em>, 1–32.
</div>
<div id="ref-cummins1990applications" class="csl-entry">
Cummins, J David, Georges Dionne, James B McDonald, and B Michael Pritchett. 1990. <span>“Applications of the GB2 Family of Distributions in Modeling Insurance Loss Processes.”</span> <em>Insurance: Mathematics and Economics</em> 9 (4): 257–72.
</div>
<div id="ref-delwarde2005construction" class="csl-entry">
Delwarde, Antoine, and Michel Denuit. 2005. <em>Construction de Tables de Mortalit<span>é</span> p<span>é</span>riodiques Et Prospectives</em>. Economica.
</div>
<div id="ref-denuit2004non" class="csl-entry">
Denuit, Michel, and Stefan Lang. 2004. <span>“Non-Life Rate-Making with Bayesian GAMs.”</span> <em>Insurance: Mathematics and Economics</em> 35 (3): 627–47.
</div>
<div id="ref-denuit2003tarification" class="csl-entry">
Denuit, Michel, Jean-François Walhin, and Sandra Pitrebois. 2003. <span>“Tarification Automobile Sur Donn<span>é</span>es de Panel.”</span> <em>Bulletin of the Swiss Association of Actuaries</em>, 51.
</div>
<div id="ref-dobson2001introduction" class="csl-entry">
Dobson, Annette J. 2001. <em>An Introduction to Generalized Linear Models</em>. CRC press.
</div>
<div id="ref-fahrmeir1994multivariate" class="csl-entry">
Fahrmeir, Ludwig, Gerhard Tutz, Wolfgang Hennevogl, and Eliane Salem. 1994. <em>Multivariate Statistical Modelling Based on Generalized Linear Models</em>. Vol. 425. Springer.
</div>
<div id="ref-gourieroux1992courbes" class="csl-entry">
Gourieroux, Christian. 1992. <span>“Courbes de Performance, de s<span>é</span>lecton Et de Discrimination.”</span> <em>Annales d’<span>é</span>conomie Et de Statistique</em>, 107–23.
</div>
<div id="ref-gourieroux1984pseudo" class="csl-entry">
Gourieroux, Christian, Alain Monfort, and Alain Trognon. 1984. <span>“Pseudo Maximum Likelihood Methods: Theory.”</span> <em>Econometrica</em>, 681–700.
</div>
<div id="ref-gourieroux1999statistique" class="csl-entry">
Gouriéroux, Christian. 1999. <span>“Statistique de l’assurance.”</span> <em>Economica, Paris</em>.
</div>
<div id="ref-hastie1990generalized" class="csl-entry">
Hastie, Trevor, and Robert Tibshirani. 1990. <em>Generalized Additive Models</em>. Chapman; Hall.
</div>
<div id="ref-hurvich1998smoothing" class="csl-entry">
Hurvich, Clifford M, Jeffrey S Simonoff, and Chih-Ling Tsai. 1998. <span>“Smoothing Parameter Selection in Nonparametric Regression Using an Improved Akaike Information Criterion.”</span> <em>Journal of the Royal Statistical Society Series B: Statistical Methodology</em> 60 (2): 271–93.
</div>
<div id="ref-ingenbleek1988sports" class="csl-entry">
Ingenbleek, Jean-Francois, and Jean Lemaire. 1988. <span>“What Is a Sports Car?”</span> <em>ASTIN Bulletin: The Journal of the IAA</em> 18 (2): 175–87.
</div>
<div id="ref-jorgensen1994fitting" class="csl-entry">
Jørgensen, Bent, and Marta C Paes De Souza. 1994. <span>“Fitting Tweedie’s Compound Poisson Model to Insurance Claims Data.”</span> <em>Scandinavian Actuarial Journal</em> 1994 (1): 69–93.
</div>
<div id="ref-keiding1998cox" class="csl-entry">
Keiding, Niels, Christian Andersen, and Peter Fledelius. 1998. <span>“The Cox Regression Model for Claims Data m Non-Life Insurance.”</span> <em>ASTIN Bulletin: The Journal of the IAA</em> 28 (1): 95–118.
</div>
<div id="ref-lebart1995statistique" class="csl-entry">
Lebart, Ludovic, Alain Morineau, and Marie Piron. 1995. <em>Statistique Exploratoire Multidimensionnelle</em>. Dunod Paris.
</div>
<div id="ref-liang1986longitudinal" class="csl-entry">
Liang, Kung-Yee, and Scott L Zeger. 1986. <span>“Longitudinal Data Analysis Using Generalized Linear Models.”</span> <em>Biometrika</em> 73 (1): 13–22.
</div>
<div id="ref-mccullagh1989generalized" class="csl-entry">
McCullagh, Peter, and J. A. Nelder. 1989. <em>Generalized Linear Models</em>. Chapman &amp; Hall.
</div>
<div id="ref-monfort1982cours" class="csl-entry">
Monfort, Alain. 1982. <em>Cours de Statistique Math<span>é</span>matique</em>. Economica.
</div>
<div id="ref-nakache2004approche" class="csl-entry">
Nakache, Jean-Pierre, and Josiane Confais. 2004. <em>Approche Pragmatique de La Classification: Arbres Hi<span>é</span>rarchiques, Partitionnements</em>. Editions Technip.
</div>
<div id="ref-nelder1972generalized" class="csl-entry">
Nelder, John Ashworth, and Robert WM Wedderburn. 1972. <span>“Generalized Linear Models.”</span> <em>Journal of the Royal Statistical Society Series A: Statistics in Society</em> 135 (3): 370–84.
</div>
<div id="ref-ramlau1988solvency" class="csl-entry">
Ramlau-Hansen, Henrik. 1988. <span>“A Solvency Study in Non-Life Insurance: Part 1. Analyses of Fire, Windstorm, and Glass Claims.”</span> <em>Scandinavian Actuarial Journal</em> 1988 (1-3): 3–34.
</div>
<div id="ref-renshaw1994modelling" class="csl-entry">
Renshaw, Arthur E. 1994. <span>“Modelling the Claims Process in the Presence of Covariates.”</span> <em>ASTIN Bulletin: The Journal of the IAA</em> 24 (2): 265–85.
</div>
<div id="ref-ter1980loglinear" class="csl-entry">
Ter Berg, Peter. 1980. <span>“On the Loglinear Poisson and Gamma Model.”</span> <em>ASTIN Bulletin: The Journal of the IAA</em> 11 (1): 35–40.
</div>
<div id="ref-ter1996loglinear" class="csl-entry">
———. 1996. <span>“A Loglinear Lagrangian Poisson Model.”</span> <em>ASTIN Bulletin: The Journal of the IAA</em> 26 (1): 123–29.
</div>
<div id="ref-zeger1988models" class="csl-entry">
Zeger, Scott L, Kung-Yee Liang, and Paul S Albert. 1988. <span>“Models for Longitudinal Data: A Generalized Estimating Equation Approach.”</span> <em>Biometrics</em>, 1049–60.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="chap8.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="chap10.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/10-A-Priori.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
