<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9 Multiple Risks | Non Life Insurance Mathematics</title>
  <meta name="description" content="Chapter 9 Multiple Risks | Non Life Insurance Mathematics" />
  <meta name="generator" content="bookdown 0.35 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 9 Multiple Risks | Non Life Insurance Mathematics" />
  <meta property="og:type" content="book" />
  
  
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 Multiple Risks | Non Life Insurance Mathematics" />
  
  
  

<meta name="author" content="Arthur Charpentier and Michel Denuit" />


<meta name="date" content="2023-09-04" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="chap7.html"/>
<link rel="next" href="chap9.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Non Life Insurance Mathematics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#general-context"><i class="fa fa-check"></i><b>1.1</b> General Context</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#the-risk-and-its-contractual-coverage"><i class="fa fa-check"></i><b>1.2</b> The risk and its contractual coverage</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#actuarial-risk-modeling"><i class="fa fa-check"></i><b>1.3</b> Actuarial risk modeling</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#pure-premium"><i class="fa fa-check"></i><b>1.4</b> Pure premium</a></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#from-pure-to-net-premium"><i class="fa fa-check"></i><b>1.5</b> From pure to net Premium</a></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#measurement-and-comparison-of-risks"><i class="fa fa-check"></i><b>1.6</b> Measurement and comparison of risks</a></li>
<li class="chapter" data-level="1.7" data-path="intro.html"><a href="intro.html#collective-model"><i class="fa fa-check"></i><b>1.7</b> Collective model</a></li>
<li class="chapter" data-level="1.8" data-path="intro.html"><a href="intro.html#solvency"><i class="fa fa-check"></i><b>1.8</b> Solvency</a></li>
<li class="chapter" data-level="1.9" data-path="intro.html"><a href="intro.html#multiple-risks"><i class="fa fa-check"></i><b>1.9</b> Multiple risks</a></li>
<li class="chapter" data-level="1.10" data-path="intro.html"><a href="intro.html#prior-ratemaking"><i class="fa fa-check"></i><b>1.10</b> Prior ratemaking</a></li>
<li class="chapter" data-level="1.11" data-path="intro.html"><a href="intro.html#credibility"><i class="fa fa-check"></i><b>1.11</b> Credibility</a></li>
<li class="chapter" data-level="1.12" data-path="intro.html"><a href="intro.html#bonus-malus"><i class="fa fa-check"></i><b>1.12</b> Bonus-Malus</a></li>
<li class="chapter" data-level="1.13" data-path="intro.html"><a href="intro.html#economics-of-insurance"><i class="fa fa-check"></i><b>1.13</b> Economics of insurance</a></li>
<li class="chapter" data-level="1.14" data-path="intro.html"><a href="intro.html#claims-reserving"><i class="fa fa-check"></i><b>1.14</b> Claims reserving</a></li>
<li class="chapter" data-level="1.15" data-path="intro.html"><a href="intro.html#large-risks-and-extreme-value"><i class="fa fa-check"></i><b>1.15</b> Large risks and extreme value</a></li>
<li class="chapter" data-level="1.16" data-path="intro.html"><a href="intro.html#monte-carlo"><i class="fa fa-check"></i><b>1.16</b> Monte Carlo</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chap1.html"><a href="chap1.html"><i class="fa fa-check"></i><b>2</b> The risk and its contractual coverage</a>
<ul>
<li class="chapter" data-level="2.1" data-path="chap1.html"><a href="chap1.html#risk"><i class="fa fa-check"></i><b>2.1</b> Risk</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="chap1.html"><a href="chap1.html#did-you-say-risk"><i class="fa fa-check"></i><b>2.1.1</b> Did you say risk?</a></li>
<li class="chapter" data-level="2.1.2" data-path="chap1.html"><a href="chap1.html#the-reason-for-insurance-risquophobia"><i class="fa fa-check"></i><b>2.1.2</b> The reason for insurance: risquophobia</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="chap1.html"><a href="chap1.html#risk-management-methods"><i class="fa fa-check"></i><b>2.2</b> Risk management methods</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="chap1.html"><a href="chap1.html#caution-and-self-insurance"><i class="fa fa-check"></i><b>2.2.1</b> Caution and self-insurance</a></li>
<li class="chapter" data-level="2.2.2" data-path="chap1.html"><a href="chap1.html#risk-management-vehicles"><i class="fa fa-check"></i><b>2.2.2</b> Risk management vehicles</a></li>
<li class="chapter" data-level="2.2.3" data-path="chap1.html"><a href="chap1.html#risks-assumed-by-insurers"><i class="fa fa-check"></i><b>2.2.3</b> Risks assumed by insurers</a></li>
<li class="chapter" data-level="2.2.4" data-path="chap1.html"><a href="chap1.html#risk-management-by-the-insurer"><i class="fa fa-check"></i><b>2.2.4</b> Risk management by the insurer</a></li>
<li class="chapter" data-level="2.2.5" data-path="chap1.html"><a href="chap1.html#transfer-of-risks"><i class="fa fa-check"></i><b>2.2.5</b> Transfer of risks</a></li>
<li class="chapter" data-level="2.2.6" data-path="chap1.html"><a href="chap1.html#insurance-and-financial-risks"><i class="fa fa-check"></i><b>2.2.6</b> Insurance and financial risks</a></li>
<li class="chapter" data-level="2.2.7" data-path="chap1.html"><a href="chap1.html#common-point-risk"><i class="fa fa-check"></i><b>2.2.7</b> Common point: risk</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="chap1.html"><a href="chap1.html#alea-iacta-est"><i class="fa fa-check"></i><b>2.3</b> <em>Alea iacta est…</em></a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="chap1.html"><a href="chap1.html#insolvency-of-the-insurer"><i class="fa fa-check"></i><b>2.3.1</b> Insolvency of the insurer</a></li>
<li class="chapter" data-level="2.3.2" data-path="chap1.html"><a href="chap1.html#settlements"><i class="fa fa-check"></i><b>2.3.2</b> Settlements</a></li>
<li class="chapter" data-level="2.3.3" data-path="chap1.html"><a href="chap1.html#necessity-of-reserving"><i class="fa fa-check"></i><b>2.3.3</b> Necessity of reserving</a></li>
<li class="chapter" data-level="2.3.4" data-path="chap1.html"><a href="chap1.html#inversion-of-production-cycle"><i class="fa fa-check"></i><b>2.3.4</b> Inversion of production cycle</a></li>
<li class="chapter" data-level="2.3.5" data-path="chap1.html"><a href="chap1.html#liabilities-a-reflection-of-the-insurers-business"><i class="fa fa-check"></i><b>2.3.5</b> Liabilities: a reflection of the insurer’s business</a></li>
<li class="chapter" data-level="2.3.6" data-path="chap1.html"><a href="chap1.html#liabilities-of-an-insurance-company"><i class="fa fa-check"></i><b>2.3.6</b> Liabilities of an insurance company</a></li>
<li class="chapter" data-level="2.3.7" data-path="chap1.html"><a href="chap1.html#the-assets-of-an-insurance-company"><i class="fa fa-check"></i><b>2.3.7</b> The assets of an insurance company</a></li>
<li class="chapter" data-level="2.3.8" data-path="chap1.html"><a href="chap1.html#premiums-written-premiums-earned"><i class="fa fa-check"></i><b>2.3.8</b> Premiums written, premiums earned</a></li>
<li class="chapter" data-level="2.3.9" data-path="chap1.html"><a href="chap1.html#insurers-institutional-investors"><i class="fa fa-check"></i><b>2.3.9</b> Insurers, institutional investors</a></li>
<li class="chapter" data-level="2.3.10" data-path="chap1.html"><a href="chap1.html#asset-and-liability-management"><i class="fa fa-check"></i><b>2.3.10</b> Asset and liability management</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="chap1.html"><a href="chap1.html#the-insurance-contract"><i class="fa fa-check"></i><b>2.4</b> The insurance contract</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="chap1.html"><a href="chap1.html#the-origins-the-marine-insurance-contract"><i class="fa fa-check"></i><b>2.4.1</b> The origins: the marine insurance contract</a></li>
<li class="chapter" data-level="2.4.2" data-path="chap1.html"><a href="chap1.html#the-birth-of-terrestrial-insurance-the-great-fire-of-london"><i class="fa fa-check"></i><b>2.4.2</b> The birth of terrestrial insurance: the great fire of London</a></li>
<li class="chapter" data-level="2.4.3" data-path="chap1.html"><a href="chap1.html#from-informal-solidarity-to-insurance"><i class="fa fa-check"></i><b>2.4.3</b> From informal solidarity to insurance</a></li>
<li class="chapter" data-level="2.4.4" data-path="chap1.html"><a href="chap1.html#contract-and-policy"><i class="fa fa-check"></i><b>2.4.4</b> Contract and policy</a></li>
<li class="chapter" data-level="2.4.5" data-path="chap1.html"><a href="chap1.html#insured-policyholder-and-beneficiary"><i class="fa fa-check"></i><b>2.4.5</b> Insured, policyholder and beneficiary</a></li>
<li class="chapter" data-level="2.4.6" data-path="chap1.html"><a href="chap1.html#what-about-technology"><i class="fa fa-check"></i><b>2.4.6</b> What about technology?</a></li>
<li class="chapter" data-level="2.4.7" data-path="chap1.html"><a href="chap1.html#party-representations"><i class="fa fa-check"></i><b>2.4.7</b> Party representations</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="chap1.html"><a href="chap1.html#bibliographical-notes"><i class="fa fa-check"></i><b>2.5</b> Bibliographical notes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chap2.html"><a href="chap2.html"><i class="fa fa-check"></i><b>3</b> Actuarial risk modeling</a>
<ul>
<li class="chapter" data-level="3.1" data-path="chap2.html"><a href="chap2.html#introduction"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="chap2.html"><a href="chap2.html#probabilistic-description-of-risk"><i class="fa fa-check"></i><b>3.2</b> Probabilistic description of risk</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="chap2.html"><a href="chap2.html#events"><i class="fa fa-check"></i><b>3.2.1</b> Events</a></li>
<li class="chapter" data-level="3.2.2" data-path="chap2.html"><a href="chap2.html#elementary-events"><i class="fa fa-check"></i><b>3.2.2</b> Elementary events</a></li>
<li class="chapter" data-level="3.2.3" data-path="chap2.html"><a href="chap2.html#set-formalism"><i class="fa fa-check"></i><b>3.2.3</b> Set formalism</a></li>
<li class="chapter" data-level="3.2.4" data-path="chap2.html"><a href="chap2.html#properties-satisfied-by-the-set-of-events"><i class="fa fa-check"></i><b>3.2.4</b> Properties satisfied by the set of events</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="chap2.html"><a href="chap2.html#probability-calculation-and-lack-of-arbitrage-opportunity"><i class="fa fa-check"></i><b>3.3</b> Probability calculation and lack of arbitrage opportunity</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="chap2.html"><a href="chap2.html#the-notion-of-probability"><i class="fa fa-check"></i><b>3.3.1</b> The notion of probability</a></li>
<li class="chapter" data-level="3.3.2" data-path="chap2.html"><a href="chap2.html#risk-and-uncertainty"><i class="fa fa-check"></i><b>3.3.2</b> Risk and uncertainty</a></li>
<li class="chapter" data-level="3.3.3" data-path="chap2.html"><a href="chap2.html#probability-and-insurance-premium"><i class="fa fa-check"></i><b>3.3.3</b> Probability and insurance premium</a></li>
<li class="chapter" data-level="3.3.4" data-path="chap2.html"><a href="chap2.html#absence-of-arbitrage-opportunity"><i class="fa fa-check"></i><b>3.3.4</b> Absence of arbitrage opportunity</a></li>
<li class="chapter" data-level="3.3.5" data-path="chap2.html"><a href="chap2.html#property-of-additivity-for-incompatible-events"><i class="fa fa-check"></i><b>3.3.5</b> Property of additivity for incompatible events</a></li>
<li class="chapter" data-level="3.3.6" data-path="chap2.html"><a href="chap2.html#premiums-grow-with-risk"><i class="fa fa-check"></i><b>3.3.6</b> Premiums grow with risk</a></li>
<li class="chapter" data-level="3.3.7" data-path="chap2.html"><a href="chap2.html#fairness-property"><i class="fa fa-check"></i><b>3.3.7</b> Fairness property</a></li>
<li class="chapter" data-level="3.3.8" data-path="chap2.html"><a href="chap2.html#subadditivity-property"><i class="fa fa-check"></i><b>3.3.8</b> Subadditivity property</a></li>
<li class="chapter" data-level="3.3.9" data-path="chap2.html"><a href="chap2.html#poincaré-equality"><i class="fa fa-check"></i><b>3.3.9</b> Poincaré equality</a></li>
<li class="chapter" data-level="3.3.10" data-path="chap2.html"><a href="chap2.html#conditional-probability"><i class="fa fa-check"></i><b>3.3.10</b> Conditional probability</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="chap2.html"><a href="chap2.html#independent-events"><i class="fa fa-check"></i><b>3.4</b> Independent events</a></li>
<li class="chapter" data-level="3.5" data-path="chap2.html"><a href="chap2.html#multiplication-rule-bayes"><i class="fa fa-check"></i><b>3.5</b> Multiplication rule (Bayes)</a></li>
<li class="chapter" data-level="3.6" data-path="chap2.html"><a href="chap2.html#conditionally-independent-events"><i class="fa fa-check"></i><b>3.6</b> Conditionally independent events</a></li>
<li class="chapter" data-level="3.7" data-path="chap2.html"><a href="chap2.html#total-probability-theorem"><i class="fa fa-check"></i><b>3.7</b> Total probability theorem</a></li>
<li class="chapter" data-level="3.8" data-path="chap2.html"><a href="chap2.html#bayes-theorem"><i class="fa fa-check"></i><b>3.8</b> Bayes’ theorem</a></li>
<li class="chapter" data-level="3.9" data-path="chap2.html"><a href="chap2.html#random-variables"><i class="fa fa-check"></i><b>3.9</b> Random variables</a>
<ul>
<li class="chapter" data-level="3.9.1" data-path="chap2.html"><a href="chap2.html#definition"><i class="fa fa-check"></i><b>3.9.1</b> Definition</a></li>
<li class="chapter" data-level="3.9.2" data-path="chap2.html"><a href="chap2.html#distribution-function"><i class="fa fa-check"></i><b>3.9.2</b> Distribution function</a></li>
<li class="chapter" data-level="3.9.3" data-path="chap2.html"><a href="chap2.html#support-of-a-random-variable"><i class="fa fa-check"></i><b>3.9.3</b> Support of a random variable</a></li>
<li class="chapter" data-level="3.9.4" data-path="chap2.html"><a href="chap2.html#tail-or-survival-function"><i class="fa fa-check"></i><b>3.9.4</b> Tail (or Survival) function</a></li>
<li class="chapter" data-level="3.9.5" data-path="chap2.html"><a href="chap2.html#equality-in-distribution"><i class="fa fa-check"></i><b>3.9.5</b> Equality in distribution</a></li>
<li class="chapter" data-level="3.9.6" data-path="chap2.html"><a href="chap2.html#quantiles-and-generalized-inverses"><i class="fa fa-check"></i><b>3.9.6</b> Quantiles and generalized inverses</a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="chap2.html"><a href="chap2.html#discrete-random-variables-and-counts"><i class="fa fa-check"></i><b>3.10</b> Discrete random variables and counts</a>
<ul>
<li class="chapter" data-level="3.10.1" data-path="chap2.html"><a href="chap2.html#notion"><i class="fa fa-check"></i><b>3.10.1</b> Notion</a></li>
<li class="chapter" data-level="3.10.2" data-path="chap2.html"><a href="chap2.html#uniform-discrete-variable"><i class="fa fa-check"></i><b>3.10.2</b> Uniform discrete variable</a></li>
<li class="chapter" data-level="3.10.3" data-path="chap2.html"><a href="chap2.html#bernoulli-variables"><i class="fa fa-check"></i><b>3.10.3</b> Bernoulli variables</a></li>
<li class="chapter" data-level="3.10.4" data-path="chap2.html"><a href="chap2.html#binomial-variable"><i class="fa fa-check"></i><b>3.10.4</b> Binomial variable</a></li>
<li class="chapter" data-level="3.10.5" data-path="chap2.html"><a href="chap2.html#geometric-variable"><i class="fa fa-check"></i><b>3.10.5</b> Geometric variable</a></li>
<li class="chapter" data-level="3.10.6" data-path="chap2.html"><a href="chap2.html#negative-binomial-variable"><i class="fa fa-check"></i><b>3.10.6</b> Negative binomial variable</a></li>
<li class="chapter" data-level="3.10.7" data-path="chap2.html"><a href="chap2.html#poissons-distribution"><i class="fa fa-check"></i><b>3.10.7</b> Poisson’s distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.11" data-path="chap2.html"><a href="chap2.html#continuous-random-variables"><i class="fa fa-check"></i><b>3.11</b> Continuous random variables</a>
<ul>
<li class="chapter" data-level="3.11.1" data-path="chap2.html"><a href="chap2.html#notion-1"><i class="fa fa-check"></i><b>3.11.1</b> Notion</a></li>
<li class="chapter" data-level="3.11.2" data-path="chap2.html"><a href="chap2.html#continuous-uniform-distribution"><i class="fa fa-check"></i><b>3.11.2</b> Continuous uniform distribution</a></li>
<li class="chapter" data-level="3.11.3" data-path="chap2.html"><a href="chap2.html#beta-distribution"><i class="fa fa-check"></i><b>3.11.3</b> Beta Distribution</a></li>
<li class="chapter" data-level="3.11.4" data-path="chap2.html"><a href="chap2.html#normal-or-gaussian-distribution"><i class="fa fa-check"></i><b>3.11.4</b> Normal (or Gaussian) distribution</a></li>
<li class="chapter" data-level="3.11.5" data-path="chap2.html"><a href="chap2.html#log-normal-variable"><i class="fa fa-check"></i><b>3.11.5</b> Log-normal variable</a></li>
<li class="chapter" data-level="3.11.6" data-path="chap2.html"><a href="chap2.html#negative-exponential-distribution"><i class="fa fa-check"></i><b>3.11.6</b> (Negative) exponential distribution</a></li>
<li class="chapter" data-level="3.11.7" data-path="chap2.html"><a href="chap2.html#gamma-distribution"><i class="fa fa-check"></i><b>3.11.7</b> Gamma distribution</a></li>
<li class="chapter" data-level="3.11.8" data-path="chap2.html"><a href="chap2.html#pareto-distribution"><i class="fa fa-check"></i><b>3.11.8</b> Pareto distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.12" data-path="chap2.html"><a href="chap2.html#random-vector"><i class="fa fa-check"></i><b>3.12</b> Random vector</a>
<ul>
<li class="chapter" data-level="3.12.1" data-path="chap2.html"><a href="chap2.html#definition-1"><i class="fa fa-check"></i><b>3.12.1</b> Definition</a></li>
<li class="chapter" data-level="3.12.2" data-path="chap2.html"><a href="chap2.html#distribution-function-1"><i class="fa fa-check"></i><b>3.12.2</b> Distribution function</a></li>
<li class="chapter" data-level="3.12.3" data-path="chap2.html"><a href="chap2.html#support-of-vector-xvec"><i class="fa fa-check"></i><b>3.12.3</b> Support of vector <span class="math inline">\(\Xvec\)</span></a></li>
<li class="chapter" data-level="3.12.4" data-path="chap2.html"><a href="chap2.html#independence"><i class="fa fa-check"></i><b>3.12.4</b> Independence</a></li>
<li class="chapter" data-level="3.12.5" data-path="chap2.html"><a href="chap2.html#gaussian-vector"><i class="fa fa-check"></i><b>3.12.5</b> Gaussian vector</a></li>
<li class="chapter" data-level="3.12.6" data-path="chap2.html"><a href="chap2.html#ellipticpart"><i class="fa fa-check"></i><b>3.12.6</b> Elliptical vectors</a></li>
<li class="chapter" data-level="3.12.7" data-path="chap2.html"><a href="chap2.html#definition-2"><i class="fa fa-check"></i><b>3.12.7</b> Definition</a></li>
<li class="chapter" data-level="3.12.8" data-path="chap2.html"><a href="chap2.html#multinomial-vector"><i class="fa fa-check"></i><b>3.12.8</b> Multinomial vector</a></li>
</ul></li>
<li class="chapter" data-level="3.13" data-path="chap2.html"><a href="chap2.html#conditional-variables"><i class="fa fa-check"></i><b>3.13</b> Conditional Variables</a>
<ul>
<li class="chapter" data-level="3.13.1" data-path="chap2.html"><a href="chap2.html#the-case-of-counting-variables"><i class="fa fa-check"></i><b>3.13.1</b> The case of counting variables</a></li>
<li class="chapter" data-level="3.13.2" data-path="chap2.html"><a href="chap2.html#the-case-of-continuous-variables"><i class="fa fa-check"></i><b>3.13.2</b> The case of continuous variables</a></li>
<li class="chapter" data-level="3.13.3" data-path="chap2.html"><a href="chap2.html#the-mixed-case-one-counting-variable-and-another-continuous"><i class="fa fa-check"></i><b>3.13.3</b> The mixed case: one counting variable and another continuous</a></li>
<li class="chapter" data-level="3.13.4" data-path="chap2.html"><a href="chap2.html#conditional-independence"><i class="fa fa-check"></i><b>3.13.4</b> Conditional independence</a></li>
</ul></li>
<li class="chapter" data-level="3.14" data-path="chap2.html"><a href="chap2.html#compound-distributions"><i class="fa fa-check"></i><b>3.14</b> Compound distributions</a>
<ul>
<li class="chapter" data-level="3.14.1" data-path="chap2.html"><a href="chap2.html#definition-4"><i class="fa fa-check"></i><b>3.14.1</b> Definition</a></li>
<li class="chapter" data-level="3.14.2" data-path="chap2.html"><a href="chap2.html#SecProdConv"><i class="fa fa-check"></i><b>3.14.2</b> Convolution product</a></li>
<li class="chapter" data-level="3.14.3" data-path="chap2.html"><a href="chap2.html#distribution-function-associated-with-a-compound-distribution"><i class="fa fa-check"></i><b>3.14.3</b> Distribution function associated with a compound distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.15" data-path="chap2.html"><a href="chap2.html#RiskTransformations"><i class="fa fa-check"></i><b>3.15</b> Transformations of risks and conventional damage clauses</a>
<ul>
<li class="chapter" data-level="3.15.1" data-path="chap2.html"><a href="chap2.html#concept"><i class="fa fa-check"></i><b>3.15.1</b> Concept</a></li>
<li class="chapter" data-level="3.15.2" data-path="chap2.html"><a href="chap2.html#DecOblig"><i class="fa fa-check"></i><b>3.15.2</b> The compulsory overdraft</a></li>
<li class="chapter" data-level="3.15.3" data-path="chap2.html"><a href="chap2.html#the-deductible"><i class="fa fa-check"></i><b>3.15.3</b> The deductible</a></li>
<li class="chapter" data-level="3.15.4" data-path="chap2.html"><a href="chap2.html#upper-limit-of-indemnity"><i class="fa fa-check"></i><b>3.15.4</b> (Upper) limit of indemnity</a></li>
<li class="chapter" data-level="3.15.5" data-path="chap2.html"><a href="chap2.html#technical-consequence-censored-data"><i class="fa fa-check"></i><b>3.15.5</b> Technical consequence: censored data</a></li>
<li class="chapter" data-level="3.15.6" data-path="chap2.html"><a href="chap2.html#poissons-distribution-and-damage-clauses"><i class="fa fa-check"></i><b>3.15.6</b> Poisson’s distribution and damage clauses</a></li>
<li class="chapter" data-level="3.15.7" data-path="chap2.html"><a href="chap2.html#perverse-effects-of-contractual-damage-clauses"><i class="fa fa-check"></i><b>3.15.7</b> Perverse effects of contractual damage clauses</a></li>
</ul></li>
<li class="chapter" data-level="3.16" data-path="chap2.html"><a href="chap2.html#exercises"><i class="fa fa-check"></i><b>3.16</b> Exercises</a></li>
<li class="chapter" data-level="3.17" data-path="chap2.html"><a href="chap2.html#bibliographical-notes-1"><i class="fa fa-check"></i><b>3.17</b> Bibliographical notes</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="chap3.html"><a href="chap3.html"><i class="fa fa-check"></i><b>4</b> Pure Premium</a>
<ul>
<li class="chapter" data-level="4.1" data-path="chap3.html"><a href="chap3.html#introduction-1"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="chap3.html"><a href="chap3.html#pure-premium-and-mathematical-expectation"><i class="fa fa-check"></i><b>4.2</b> Pure Premium and Mathematical Expectation</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="chap3.html"><a href="chap3.html#mathematical-expectation"><i class="fa fa-check"></i><b>4.2.1</b> Mathematical Expectation</a></li>
<li class="chapter" data-level="4.2.2" data-path="chap3.html"><a href="chap3.html#probabilities-and-expectations-of-indicators"><i class="fa fa-check"></i><b>4.2.2</b> Probabilities and Expectations of Indicators</a></li>
<li class="chapter" data-level="4.2.3" data-path="chap3.html"><a href="chap3.html#determination-of-the-pure-premium"><i class="fa fa-check"></i><b>4.2.3</b> Determination of the Pure Premium</a></li>
<li class="chapter" data-level="4.2.4" data-path="chap3.html"><a href="chap3.html#mean-squared-error-is-it-a-must"><i class="fa fa-check"></i><b>4.2.4</b> Mean Squared Error, Is It a Must?</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="chap3.html"><a href="chap3.html#variance"><i class="fa fa-check"></i><b>4.3</b> Variance</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="chap3.html"><a href="chap3.html#definition-5"><i class="fa fa-check"></i><b>4.3.1</b> Definition</a></li>
<li class="chapter" data-level="4.3.2" data-path="chap3.html"><a href="chap3.html#actuarial-interpretation"><i class="fa fa-check"></i><b>4.3.2</b> Actuarial Interpretation</a></li>
<li class="chapter" data-level="4.3.3" data-path="chap3.html"><a href="chap3.html#some-examples"><i class="fa fa-check"></i><b>4.3.3</b> Some Examples</a></li>
<li class="chapter" data-level="4.3.4" data-path="chap3.html"><a href="chap3.html#properties"><i class="fa fa-check"></i><b>4.3.4</b> Properties</a></li>
<li class="chapter" data-level="4.3.5" data-path="chap3.html"><a href="chap3.html#variance-of-common-distributions"><i class="fa fa-check"></i><b>4.3.5</b> Variance of Common Distributions</a></li>
<li class="chapter" data-level="4.3.6" data-path="chap3.html"><a href="chap3.html#variance-of-composite-distributions"><i class="fa fa-check"></i><b>4.3.6</b> Variance of Composite Distributions</a></li>
<li class="chapter" data-level="4.3.7" data-path="chap3.html"><a href="chap3.html#coefficient-of-variation-and-risk-pooling"><i class="fa fa-check"></i><b>4.3.7</b> Coefficient of Variation and Risk Pooling</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="chap3.html"><a href="chap3.html#insurance-and-bienaymé-chebyshev-inequality"><i class="fa fa-check"></i><b>4.4</b> Insurance and Bienaymé-Chebyshev Inequality</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="chap3.html"><a href="chap3.html#markovs-inequality"><i class="fa fa-check"></i><b>4.4.1</b> Markov’s Inequality</a></li>
<li class="chapter" data-level="4.4.2" data-path="chap3.html"><a href="chap3.html#bienaymé-chebyshev-inequality"><i class="fa fa-check"></i><b>4.4.2</b> Bienaymé-Chebyshev Inequality</a></li>
<li class="chapter" data-level="4.4.3" data-path="chap3.html"><a href="chap3.html#actuarial-interpretation-of-the-bienaymé-chebyshev-inequality"><i class="fa fa-check"></i><b>4.4.3</b> Actuarial Interpretation of the Bienaymé-Chebyshev Inequality</a></li>
<li class="chapter" data-level="4.4.4" data-path="chap3.html"><a href="chap3.html#conservative-nature-of-the-bienaymé-chebyshev-inequality"><i class="fa fa-check"></i><b>4.4.4</b> Conservative Nature of the Bienaymé-Chebyshev Inequality</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="chap3.html"><a href="chap3.html#insurance-and-law-of-large-numbers"><i class="fa fa-check"></i><b>4.5</b> Insurance and Law of Large Numbers</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="chap3.html"><a href="chap3.html#convergence-in-probability"><i class="fa fa-check"></i><b>4.5.1</b> Convergence in Probability</a></li>
<li class="chapter" data-level="4.5.2" data-path="chap3.html"><a href="chap3.html#convergence-of-average-claim-amount-per-policy-to-the-pure-premium"><i class="fa fa-check"></i><b>4.5.2</b> Convergence of Average Claim Amount per Policy to the Pure Premium</a></li>
<li class="chapter" data-level="4.5.3" data-path="chap3.html"><a href="chap3.html#case-of-flat-indemnity"><i class="fa fa-check"></i><b>4.5.3</b> Case of Flat Indemnity</a></li>
<li class="chapter" data-level="4.5.4" data-path="chap3.html"><a href="chap3.html#case-of-indemnity-compensation"><i class="fa fa-check"></i><b>4.5.4</b> Case of Indemnity Compensation</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="chap3.html"><a href="chap3.html#characteristic-functions"><i class="fa fa-check"></i><b>4.6</b> Characteristic Functions</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="chap3.html"><a href="chap3.html#probability-generating-function"><i class="fa fa-check"></i><b>4.6.1</b> Probability Generating Function</a></li>
<li class="chapter" data-level="4.6.2" data-path="chap3.html"><a href="chap3.html#laplace-transform"><i class="fa fa-check"></i><b>4.6.2</b> Laplace Transform</a></li>
<li class="chapter" data-level="4.6.3" data-path="chap3.html"><a href="chap3.html#moment-generating-function"><i class="fa fa-check"></i><b>4.6.3</b> Moment Generating Function</a></li>
<li class="chapter" data-level="4.6.4" data-path="chap3.html"><a href="chap3.html#hazard-rate"><i class="fa fa-check"></i><b>4.6.4</b> Hazard Rate</a></li>
<li class="chapter" data-level="4.6.5" data-path="chap3.html"><a href="chap3.html#stop-loss-premiums"><i class="fa fa-check"></i><b>4.6.5</b> Stop-Loss Premiums</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="chap3.html"><a href="chap3.html#Hetero"><i class="fa fa-check"></i><b>4.7</b> Heterogeneity and Mixtures</a></li>
<li class="chapter" data-level="4.8" data-path="chap3.html"><a href="chap3.html#context"><i class="fa fa-check"></i><b>4.8</b> Context</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="chap3.html"><a href="chap3.html#a-simple-example"><i class="fa fa-check"></i><b>4.8.1</b> A Simple Example…</a></li>
<li class="chapter" data-level="4.8.2" data-path="chap3.html"><a href="chap3.html#poisson-mixtures"><i class="fa fa-check"></i><b>4.8.2</b> Poisson Mixtures</a></li>
<li class="chapter" data-level="4.8.3" data-path="chap3.html"><a href="chap3.html#shakeds-theorem"><i class="fa fa-check"></i><b>4.8.3</b> Shaked’s Theorem</a></li>
<li class="chapter" data-level="4.8.4" data-path="chap3.html"><a href="chap3.html#composite-mixed-poisson-distributions"><i class="fa fa-check"></i><b>4.8.4</b> Composite Mixed Poisson Distributions</a></li>
<li class="chapter" data-level="4.8.5" data-path="chap3.html"><a href="chap3.html#exponential-mixtures"><i class="fa fa-check"></i><b>4.8.5</b> Exponential Mixtures</a></li>
<li class="chapter" data-level="4.8.6" data-path="chap3.html"><a href="chap3.html#identifiability-of-exponential-mixtures"><i class="fa fa-check"></i><b>4.8.6</b> Identifiability of Exponential Mixtures</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="chap3.html"><a href="chap3.html#pure-premium-in-segmented-universe"><i class="fa fa-check"></i><b>4.9</b> Pure Premium in Segmented Universe</a>
<ul>
<li class="chapter" data-level="4.9.1" data-path="chap3.html"><a href="chap3.html#segmentation-techniques"><i class="fa fa-check"></i><b>4.9.1</b> Segmentation Techniques</a></li>
<li class="chapter" data-level="4.9.2" data-path="chap3.html"><a href="chap3.html#conditional-expectation"><i class="fa fa-check"></i><b>4.9.2</b> Conditional Expectation</a></li>
<li class="chapter" data-level="4.9.3" data-path="chap3.html"><a href="chap3.html#customization-of-premiums"><i class="fa fa-check"></i><b>4.9.3</b> Customization of Premiums</a></li>
<li class="chapter" data-level="4.9.4" data-path="chap3.html"><a href="chap3.html#segmentation-pooling-and-solidarity"><i class="fa fa-check"></i><b>4.9.4</b> Segmentation, Pooling, and Solidarity</a></li>
<li class="chapter" data-level="4.9.5" data-path="chap3.html"><a href="chap3.html#DeWit"><i class="fa fa-check"></i><b>4.9.5</b> Formalization of the Segmentation Concept</a></li>
<li class="chapter" data-level="4.9.6" data-path="chap3.html"><a href="chap3.html#drawbacks-resulting-from-extensive-segmentation"><i class="fa fa-check"></i><b>4.9.6</b> Drawbacks Resulting from Extensive Segmentation</a></li>
<li class="chapter" data-level="4.9.7" data-path="chap3.html"><a href="chap3.html#segmentation-and-information-asymmetry"><i class="fa fa-check"></i><b>4.9.7</b> Segmentation and Information Asymmetry</a></li>
</ul></li>
<li class="chapter" data-level="4.10" data-path="chap3.html"><a href="chap3.html#exercises-1"><i class="fa fa-check"></i><b>4.10</b> Exercises</a></li>
<li class="chapter" data-level="4.11" data-path="chap3.html"><a href="chap3.html#bibliographical-notes-2"><i class="fa fa-check"></i><b>4.11</b> Bibliographical notes</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="chap4.html"><a href="chap4.html"><i class="fa fa-check"></i><b>5</b> From Pure to Net Premium</a>
<ul>
<li class="chapter" data-level="5.1" data-path="chap4.html"><a href="chap4.html#introduction-2"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="chap4.html"><a href="chap4.html#insurance-and-the-central-limit-theorem"><i class="fa fa-check"></i><b>5.2</b> Insurance and the Central Limit Theorem</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="chap4.html"><a href="chap4.html#central-limit-theorem"><i class="fa fa-check"></i><b>5.2.1</b> Central Limit Theorem</a></li>
<li class="chapter" data-level="5.2.2" data-path="chap4.html"><a href="chap4.html#quality-of-the-approximation-based-on-the-central-limit-theorem"><i class="fa fa-check"></i><b>5.2.2</b> Quality of the Approximation Based on the Central Limit Theorem</a></li>
<li class="chapter" data-level="5.2.3" data-path="chap4.html"><a href="chap4.html#central-limit-theorem-and-law-of-large-numbers"><i class="fa fa-check"></i><b>5.2.3</b> Central Limit Theorem and Law of Large Numbers</a></li>
<li class="chapter" data-level="5.2.4" data-path="chap4.html"><a href="chap4.html#central-limit-theorem-for-the-compound-poisson-distribution"><i class="fa fa-check"></i><b>5.2.4</b> Central Limit Theorem for the Compound Poisson Distribution</a></li>
<li class="chapter" data-level="5.2.5" data-path="chap4.html"><a href="chap4.html#tail-function-approximation-in-the-case-of-fixed-forfaiture-policy"><i class="fa fa-check"></i><b>5.2.5</b> Tail Function Approximation in the Case of Fixed-Forfaiture Policy</a></li>
<li class="chapter" data-level="5.2.6" data-path="chap4.html"><a href="chap4.html#tail-function-approximation-in-the-case-of-indemnity-policy"><i class="fa fa-check"></i><b>5.2.6</b> Tail Function Approximation in the Case of Indemnity Policy</a></li>
<li class="chapter" data-level="5.2.7" data-path="chap4.html"><a href="chap4.html#pure-premium-as-the-minimum-price-for-risk"><i class="fa fa-check"></i><b>5.2.7</b> Pure Premium as the Minimum Price for Risk</a></li>
<li class="chapter" data-level="5.2.8" data-path="chap4.html"><a href="chap4.html#sensitivity-of-results-to-possible-dependence"><i class="fa fa-check"></i><b>5.2.8</b> Sensitivity of Results to Possible Dependence</a></li>
<li class="chapter" data-level="5.2.9" data-path="chap4.html"><a href="chap4.html#stable-distributions"><i class="fa fa-check"></i><b>5.2.9</b> Stable Distributions</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="chap4.html"><a href="chap4.html#probability-of-ruin-over-a-period"><i class="fa fa-check"></i><b>5.3</b> Probability of Ruin over a Period</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="chap4.html"><a href="chap4.html#definition-14"><i class="fa fa-check"></i><b>5.3.1</b> Definition</a></li>
<li class="chapter" data-level="5.3.2" data-path="chap4.html"><a href="chap4.html#approximation-based-on-the-central-limit-theorem"><i class="fa fa-check"></i><b>5.3.2</b> Approximation based on the central limit theorem</a></li>
<li class="chapter" data-level="5.3.3" data-path="chap4.html"><a href="chap4.html#the-case-of-the-flat-rate-deductible"><i class="fa fa-check"></i><b>5.3.3</b> The case of the flat-rate deductible</a></li>
<li class="chapter" data-level="5.3.4" data-path="chap4.html"><a href="chap4.html#the-case-of-indemnity-based-claims"><i class="fa fa-check"></i><b>5.3.4</b> The case of indemnity-based claims</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="chap4.html"><a href="chap4.html#security-loading"><i class="fa fa-check"></i><b>5.4</b> Security Loading</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="chap4.html"><a href="chap4.html#concept-1"><i class="fa fa-check"></i><b>5.4.1</b> Concept</a></li>
<li class="chapter" data-level="5.4.2" data-path="chap4.html"><a href="chap4.html#determining-the-security-loading-based-on-the-central-limit-theorem"><i class="fa fa-check"></i><b>5.4.2</b> Determining the security loading Based on the Central Limit Theorem</a></li>
<li class="chapter" data-level="5.4.3" data-path="chap4.html"><a href="chap4.html#the-absolute-necessity-of-security-load"><i class="fa fa-check"></i><b>5.4.3</b> The Absolute Necessity of Security Load</a></li>
<li class="chapter" data-level="5.4.4" data-path="chap4.html"><a href="chap4.html#PrincCalculPrim"><i class="fa fa-check"></i><b>5.4.4</b> Premium Calculation Principle</a></li>
<li class="chapter" data-level="5.4.5" data-path="chap4.html"><a href="chap4.html#comments"><i class="fa fa-check"></i><b>5.4.5</b> Comments</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="chap4.html"><a href="chap4.html#security-coefficient"><i class="fa fa-check"></i><b>5.5</b> Security Coefficient</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="chap4.html"><a href="chap4.html#companys-technical-result"><i class="fa fa-check"></i><b>5.5.1</b> Company’s Technical Result</a></li>
<li class="chapter" data-level="5.5.2" data-path="chap4.html"><a href="chap4.html#determining-the-security-coefficient"><i class="fa fa-check"></i><b>5.5.2</b> Determining the Security Coefficient</a></li>
<li class="chapter" data-level="5.5.3" data-path="chap4.html"><a href="chap4.html#determining-the-safety-loading-based-on-the-bienaymé-chebyshev-inequality"><i class="fa fa-check"></i><b>5.5.3</b> Determining the Safety Loading Based on the Bienaymé-Chebyshev Inequality</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="chap4.html"><a href="chap4.html#the-normal-power-np-approximation"><i class="fa fa-check"></i><b>5.6</b> The Normal-Power (NP) Approximation</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="chap4.html"><a href="chap4.html#skewness-coefficient"><i class="fa fa-check"></i><b>5.6.1</b> Skewness Coefficient</a></li>
<li class="chapter" data-level="5.6.2" data-path="chap4.html"><a href="chap4.html#edgeworth-expansion"><i class="fa fa-check"></i><b>5.6.2</b> Edgeworth Expansion</a></li>
<li class="chapter" data-level="5.6.3" data-path="chap4.html"><a href="chap4.html#esscher-approximation"><i class="fa fa-check"></i><b>5.6.3</b> Esscher Approximation</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="chap4.html"><a href="chap4.html#using-the-np-approximation-to-determine-the-safety-loading-rate-rho-in-the-expected-value-principle"><i class="fa fa-check"></i><b>5.7</b> Using the NP Approximation to Determine the Safety Loading Rate <span class="math inline">\(\rho\)</span> in the Expected Value Principle</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="chap4.html"><a href="chap4.html#concluding-remarks-on-esscher-and-np-approximations"><i class="fa fa-check"></i><b>5.7.1</b> Concluding Remarks on Esscher and NP Approximations</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="chap4.html"><a href="chap4.html#the-premium"><i class="fa fa-check"></i><b>5.8</b> The Premium</a></li>
<li class="chapter" data-level="5.9" data-path="chap4.html"><a href="chap4.html#calculation-of-the-commercial-premium"><i class="fa fa-check"></i><b>5.9</b> Calculation of the Commercial Premium</a></li>
<li class="chapter" data-level="5.10" data-path="chap4.html"><a href="chap4.html#exercises-2"><i class="fa fa-check"></i><b>5.10</b> Exercises</a></li>
<li class="chapter" data-level="5.11" data-path="chap4.html"><a href="chap4.html#bibliographical-notes-3"><i class="fa fa-check"></i><b>5.11</b> Bibliographical Notes</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="chap5.html"><a href="chap5.html"><i class="fa fa-check"></i><b>6</b> Measurement and Comparison of Risks</a>
<ul>
<li class="chapter" data-level="6.1" data-path="chap5.html"><a href="chap5.html#introduction-3"><i class="fa fa-check"></i><b>6.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="chap5.html"><a href="chap5.html#measuring-risk-an-essential-task-for-actuaries"><i class="fa fa-check"></i><b>6.1.1</b> Measuring Risk: An Essential Task for Actuaries</a></li>
<li class="chapter" data-level="6.1.2" data-path="chap5.html"><a href="chap5.html#comparing-risks-another-specialty-of-actuaries"><i class="fa fa-check"></i><b>6.1.2</b> Comparing Risks: Another Specialty of Actuaries</a></li>
<li class="chapter" data-level="6.1.3" data-path="chap5.html"><a href="chap5.html#measuring-and-then-comparing-risks-two-related-tasks"><i class="fa fa-check"></i><b>6.1.3</b> Measuring and then Comparing Risks, Two Related Tasks</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="chap5.html"><a href="chap5.html#risk-measures"><i class="fa fa-check"></i><b>6.2</b> Risk Measures</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="chap5.html"><a href="chap5.html#definition-17"><i class="fa fa-check"></i><b>6.2.1</b> Definition</a></li>
<li class="chapter" data-level="6.2.2" data-path="chap5.html"><a href="chap5.html#coherence"><i class="fa fa-check"></i><b>6.2.2</b> Coherence</a></li>
<li class="chapter" data-level="6.2.3" data-path="chap5.html"><a href="chap5.html#value-at-risk"><i class="fa fa-check"></i><b>6.2.3</b> Value-at-Risk</a></li>
<li class="chapter" data-level="6.2.4" data-path="chap5.html"><a href="chap5.html#tail-var-and-related-measures"><i class="fa fa-check"></i><b>6.2.4</b> Tail-VaR and Related Measures</a></li>
<li class="chapter" data-level="6.2.5" data-path="chap5.html"><a href="chap5.html#esscher-risk-measure"><i class="fa fa-check"></i><b>6.2.5</b> Esscher Risk Measure</a></li>
<li class="chapter" data-level="6.2.6" data-path="chap5.html"><a href="chap5.html#wang-risk-measures"><i class="fa fa-check"></i><b>6.2.6</b> Wang Risk Measures</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="chap5.html"><a href="chap5.html#main-properties-of-wangs-risk-measures"><i class="fa fa-check"></i><b>6.3</b> Main Properties of Wang’s Risk Measures</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="chap5.html"><a href="chap5.html#concavity-of-the-distortion-function"><i class="fa fa-check"></i><b>6.3.1</b> Concavity of the Distortion Function</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="chap5.html"><a href="chap5.html#uniform-comparison-of-var"><i class="fa fa-check"></i><b>6.4</b> Uniform Comparison of VaR</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="chap5.html"><a href="chap5.html#definition-22"><i class="fa fa-check"></i><b>6.4.1</b> Definition</a></li>
<li class="chapter" data-level="6.4.2" data-path="chap5.html"><a href="chap5.html#equivalent-conditions"><i class="fa fa-check"></i><b>6.4.2</b> Equivalent Conditions</a></li>
<li class="chapter" data-level="6.4.3" data-path="chap5.html"><a href="chap5.html#properties-3"><i class="fa fa-check"></i><b>6.4.3</b> Properties</a></li>
<li class="chapter" data-level="6.4.4" data-path="chap5.html"><a href="chap5.html#hazard-rate-and-ph-risk-measure"><i class="fa fa-check"></i><b>6.4.4</b> Hazard Rate and PH Risk Measure</a></li>
<li class="chapter" data-level="6.4.5" data-path="chap5.html"><a href="chap5.html#likelihood-ratio-and-esscher-principle"><i class="fa fa-check"></i><b>6.4.5</b> Likelihood Ratio and Esscher Principle</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="chap5.html"><a href="chap5.html#uniform-comparison-of-tvars"><i class="fa fa-check"></i><b>6.5</b> Uniform Comparison of TVaRs</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="chap5.html"><a href="chap5.html#definition-24"><i class="fa fa-check"></i><b>6.5.1</b> Definition</a></li>
<li class="chapter" data-level="6.5.2" data-path="chap5.html"><a href="chap5.html#equivalent-conditions-1"><i class="fa fa-check"></i><b>6.5.2</b> Equivalent Conditions</a></li>
<li class="chapter" data-level="6.5.3" data-path="chap5.html"><a href="chap5.html#sufficient-condition"><i class="fa fa-check"></i><b>6.5.3</b> Sufficient Condition</a></li>
<li class="chapter" data-level="6.5.4" data-path="chap5.html"><a href="chap5.html#properties-4"><i class="fa fa-check"></i><b>6.5.4</b> Properties</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="chap5.html"><a href="chap5.html#optimal-form-of-risk-transfer"><i class="fa fa-check"></i><b>6.6</b> Optimal Form of Risk Transfer</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="chap5.html"><a href="chap5.html#the-problem"><i class="fa fa-check"></i><b>6.6.1</b> The Problem</a></li>
<li class="chapter" data-level="6.6.2" data-path="chap5.html"><a href="chap5.html#admissible-indemnity-functions"><i class="fa fa-check"></i><b>6.6.2</b> Admissible Indemnity Functions</a></li>
<li class="chapter" data-level="6.6.3" data-path="chap5.html"><a href="chap5.html#ordering-of-contracts"><i class="fa fa-check"></i><b>6.6.3</b> Ordering of Contracts</a></li>
<li class="chapter" data-level="6.6.4" data-path="chap5.html"><a href="chap5.html#optimality-of-the-stop-loss-contract"><i class="fa fa-check"></i><b>6.6.4</b> Optimality of the Stop-Loss Contract</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="chap5.html"><a href="chap5.html#incomplete-information"><i class="fa fa-check"></i><b>6.7</b> Incomplete Information</a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="chap5.html"><a href="chap5.html#context-2"><i class="fa fa-check"></i><b>6.7.1</b> Context</a></li>
<li class="chapter" data-level="6.7.2" data-path="chap5.html"><a href="chap5.html#known-mean-and-support"><i class="fa fa-check"></i><b>6.7.2</b> Known Mean and Support</a></li>
<li class="chapter" data-level="6.7.3" data-path="chap5.html"><a href="chap5.html#application-to-calculating-a-grouped-data-stop-loss-premium"><i class="fa fa-check"></i><b>6.7.3</b> Application to Calculating a Grouped Data Stop-Loss Premium</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="chap5.html"><a href="chap5.html#exercises-3"><i class="fa fa-check"></i><b>6.8</b> Exercises</a></li>
<li class="chapter" data-level="6.9" data-path="chap5.html"><a href="chap5.html#bibliographical-notes-4"><i class="fa fa-check"></i><b>6.9</b> Bibliographical notes</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="chap6.html"><a href="chap6.html"><i class="fa fa-check"></i><b>7</b> Collective Model</a>
<ul>
<li class="chapter" data-level="7.1" data-path="chap6.html"><a href="chap6.html#introduction-4"><i class="fa fa-check"></i><b>7.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="chap6.html"><a href="chap6.html#different-levels-of-analysis"><i class="fa fa-check"></i><b>7.1.1</b> Different Levels of Analysis</a></li>
<li class="chapter" data-level="7.1.2" data-path="chap6.html"><a href="chap6.html#the-individual-model"><i class="fa fa-check"></i><b>7.1.2</b> The Individual Model</a></li>
<li class="chapter" data-level="7.1.3" data-path="chap6.html"><a href="chap6.html#total-claims-in-the-individual-model"><i class="fa fa-check"></i><b>7.1.3</b> Total Claims in the Individual Model</a></li>
<li class="chapter" data-level="7.1.4" data-path="chap6.html"><a href="chap6.html#difficulty-of-calculations-in-the-individual-model"><i class="fa fa-check"></i><b>7.1.4</b> Difficulty of Calculations in the Individual Model</a></li>
<li class="chapter" data-level="7.1.5" data-path="chap6.html"><a href="chap6.html#the-collective-model"><i class="fa fa-check"></i><b>7.1.5</b> The Collective Model</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="chap6.html"><a href="chap6.html#approximation-of-the-individual-model"><i class="fa fa-check"></i><b>7.2</b> Approximation of the Individual Model</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="chap6.html"><a href="chap6.html#formalization-of-the-individual-model"><i class="fa fa-check"></i><b>7.2.1</b> Formalization of the Individual Model</a></li>
<li class="chapter" data-level="7.2.2" data-path="chap6.html"><a href="chap6.html#representation-of-the-total-claims-amount-in-the-individual-model"><i class="fa fa-check"></i><b>7.2.2</b> Representation of the Total Claims Amount in the Individual Model</a></li>
<li class="chapter" data-level="7.2.3" data-path="chap6.html"><a href="chap6.html#justification-of-approximating-the-individual-model-with-the"><i class="fa fa-check"></i><b>7.2.3</b> Justification of Approximating the Individual Model with the</a></li>
<li class="chapter" data-level="7.2.4" data-path="chap6.html"><a href="chap6.html#transition-from-the-individual-model-to-the-collective-model"><i class="fa fa-check"></i><b>7.2.4</b> Transition from the Individual Model to the Collective Model</a></li>
<li class="chapter" data-level="7.2.5" data-path="chap6.html"><a href="chap6.html#choice-of-parameters-for-the-collective-model"><i class="fa fa-check"></i><b>7.2.5</b> Choice of Parameters for the Collective Model</a></li>
<li class="chapter" data-level="7.2.6" data-path="chap6.html"><a href="chap6.html#bounds-on-the-approximation-error-cumulative-distribution-function"><i class="fa fa-check"></i><b>7.2.6</b> Bounds on the Approximation Error: Cumulative Distribution Function</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="chap6.html"><a href="chap6.html#ApproxPoisson"><i class="fa fa-check"></i><b>7.3</b> Analysis of Proposition</a></li>
<li class="chapter" data-level="7.4" data-path="chap6.html"><a href="chap6.html#numerical-analysis"><i class="fa fa-check"></i><b>7.4</b> Numerical Analysis</a></li>
<li class="chapter" data-level="7.5" data-path="chap6.html"><a href="chap6.html#approximating-the-individual-model-by-a-collective-model"><i class="fa fa-check"></i><b>7.5</b> Approximating the Individual Model by a Collective Model</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="chap6.html"><a href="chap6.html#bounds-on-approximation-error-stop-loss-premiums"><i class="fa fa-check"></i><b>7.5.1</b> Bounds on Approximation Error: Stop-Loss Premiums</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="chap6.html"><a href="chap6.html#discretization-of-claim-amounts"><i class="fa fa-check"></i><b>7.6</b> Discretization of Claim Amounts</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="chap6.html"><a href="chap6.html#necessity-of-discretization"><i class="fa fa-check"></i><b>7.6.1</b> Necessity of Discretization</a></li>
<li class="chapter" data-level="7.6.2" data-path="chap6.html"><a href="chap6.html#discretization-in-accordance-with-var"><i class="fa fa-check"></i><b>7.6.2</b> Discretization in Accordance with VaR</a></li>
<li class="chapter" data-level="7.6.3" data-path="chap6.html"><a href="chap6.html#discretization-in-accordance-with-tvar"><i class="fa fa-check"></i><b>7.6.3</b> Discretization in Accordance with TVaR</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="chap6.html"><a href="chap6.html#panjers-algorithm"><i class="fa fa-check"></i><b>7.7</b> Panjer’s Algorithm</a>
<ul>
<li class="chapter" data-level="7.7.1" data-path="chap6.html"><a href="chap6.html#the-challenge-of-a-direct-approach"><i class="fa fa-check"></i><b>7.7.1</b> The Challenge of a Direct Approach</a></li>
<li class="chapter" data-level="7.7.2" data-path="chap6.html"><a href="chap6.html#panjer-family"><i class="fa fa-check"></i><b>7.7.2</b> Panjer Family</a></li>
<li class="chapter" data-level="7.7.3" data-path="chap6.html"><a href="chap6.html#panjers-algorithm-for-positive-claim-costs"><i class="fa fa-check"></i><b>7.7.3</b> Panjer’s Algorithm for Positive Claim Costs</a></li>
<li class="chapter" data-level="7.7.4" data-path="chap6.html"><a href="chap6.html#panjers-algorithm-for-non-negative-claim-costs"><i class="fa fa-check"></i><b>7.7.4</b> Panjer’s Algorithm for Non-Negative Claim Costs</a></li>
<li class="chapter" data-level="7.7.5" data-path="chap6.html"><a href="chap6.html#evaluating-ruin-probabilities-over-a-period"><i class="fa fa-check"></i><b>7.7.5</b> Evaluating Ruin Probabilities over a Period</a></li>
<li class="chapter" data-level="7.7.6" data-path="chap6.html"><a href="chap6.html#evaluation-of-var-and-solvency-margin"><i class="fa fa-check"></i><b>7.7.6</b> Evaluation of VaR and Solvency Margin</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="chap6.html"><a href="chap6.html#evaluation-of-stop-loss-premiums"><i class="fa fa-check"></i><b>7.8</b> Evaluation of Stop-Loss Premiums</a>
<ul>
<li class="chapter" data-level="7.8.1" data-path="chap6.html"><a href="chap6.html#iterative-scheme-for-calculating-stop-loss-premiums"><i class="fa fa-check"></i><b>7.8.1</b> Iterative Scheme for Calculating Stop-Loss Premiums</a></li>
<li class="chapter" data-level="7.8.2" data-path="chap6.html"><a href="chap6.html#error-due-to-discretization"><i class="fa fa-check"></i><b>7.8.2</b> Error Due to Discretization</a></li>
</ul></li>
<li class="chapter" data-level="7.9" data-path="chap6.html"><a href="chap6.html#exercises-4"><i class="fa fa-check"></i><b>7.9</b> Exercises</a></li>
<li class="chapter" data-level="7.10" data-path="chap6.html"><a href="chap6.html#bibliographical-notes-5"><i class="fa fa-check"></i><b>7.10</b> Bibliographical Notes</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="chap7.html"><a href="chap7.html"><i class="fa fa-check"></i><b>8</b> Solvency</a></li>
<li class="chapter" data-level="9" data-path="chap8.html"><a href="chap8.html"><i class="fa fa-check"></i><b>9</b> Multiple Risks</a>
<ul>
<li class="chapter" data-level="9.1" data-path="chap8.html"><a href="chap8.html#introduction-5"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="chap8.html"><a href="chap8.html#comonotonicity-and-antimonotonicity"><i class="fa fa-check"></i><b>9.2</b> Comonotonicity and Antimonotonicity</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="chap8.html"><a href="chap8.html#fréchet-classes"><i class="fa fa-check"></i><b>9.2.1</b> Fréchet Classes</a></li>
<li class="chapter" data-level="9.2.2" data-path="chap8.html"><a href="chap8.html#fréchet-bounds"><i class="fa fa-check"></i><b>9.2.2</b> Fréchet Bounds</a></li>
<li class="chapter" data-level="9.2.3" data-path="chap8.html"><a href="chap8.html#perfect-dependence-comonotonicity-and-antimonotonicity"><i class="fa fa-check"></i><b>9.2.3</b> Perfect Dependence: Comonotonicity and Antimonotonicity</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="chap8.html"><a href="chap8.html#measures-of-dependence"><i class="fa fa-check"></i><b>9.3</b> Measures of Dependence</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="chap8.html"><a href="chap8.html#concept-2"><i class="fa fa-check"></i><b>9.3.1</b> Concept</a></li>
<li class="chapter" data-level="9.3.2" data-path="chap8.html"><a href="chap8.html#linear-correlation-or-pearson-correlation"><i class="fa fa-check"></i><b>9.3.2</b> Linear Correlation or Pearson Correlation</a></li>
<li class="chapter" data-level="9.3.3" data-path="chap8.html"><a href="chap8.html#values-of-the-linear-correlation-coefficient"><i class="fa fa-check"></i><b>9.3.3</b> Values of the Linear Correlation Coefficient</a></li>
<li class="chapter" data-level="9.3.4" data-path="chap8.html"><a href="chap8.html#kendalls-rank-correlation-coefficient"><i class="fa fa-check"></i><b>9.3.4</b> Kendall’s Rank Correlation Coefficient</a></li>
<li class="chapter" data-level="9.3.5" data-path="chap8.html"><a href="chap8.html#spearmans-rank-correlation-coefficient"><i class="fa fa-check"></i><b>9.3.5</b> Spearman’s Rank Correlation Coefficient</a></li>
<li class="chapter" data-level="9.3.6" data-path="chap8.html"><a href="chap8.html#functional-stability-of-supermodular-comparisons"><i class="fa fa-check"></i><b>9.3.6</b> Functional Stability of Supermodular Comparisons</a></li>
<li class="chapter" data-level="9.3.7" data-path="chap8.html"><a href="chap8.html#supermodular-comparison-and-fréchet-space"><i class="fa fa-check"></i><b>9.3.7</b> Supermodular Comparison and Fréchet Space</a></li>
<li class="chapter" data-level="9.3.8" data-path="chap8.html"><a href="chap8.html#supermodular-comparison-and-joint-distributiontail-functions"><i class="fa fa-check"></i><b>9.3.8</b> Supermodular Comparison and Joint Distribution/Tail Functions</a></li>
<li class="chapter" data-level="9.3.9" data-path="chap8.html"><a href="chap8.html#extreme-structures-of-supermodular-dependence"><i class="fa fa-check"></i><b>9.3.9</b> Extreme Structures of Supermodular Dependence</a></li>
<li class="chapter" data-level="9.3.10" data-path="chap8.html"><a href="chap8.html#supermodular-comparison-and-correlation-coefficients"><i class="fa fa-check"></i><b>9.3.10</b> Supermodular Comparison and Correlation Coefficients</a></li>
<li class="chapter" data-level="9.3.11" data-path="chap8.html"><a href="chap8.html#association"><i class="fa fa-check"></i><b>9.3.11</b> Association</a></li>
<li class="chapter" data-level="9.3.12" data-path="chap8.html"><a href="chap8.html#conditional-growth"><i class="fa fa-check"></i><b>9.3.12</b> Conditional Growth</a></li>
<li class="chapter" data-level="9.4.5" data-path="chap8.html"><a href="chap8.html#dependencemeasures"><i class="fa fa-check"></i><b>9.4.5</b> Dependence Measures and Copulas</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="chap8.html"><a href="chap8.html#archimedean-copulas"><i class="fa fa-check"></i><b>9.5</b> Archimedean Copulas</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="chap8.html"><a href="chap8.html#definition-32"><i class="fa fa-check"></i><b>9.5.1</b> Definition</a></li>
<li class="chapter" data-level="9.5.2" data-path="chap8.html"><a href="chap8.html#frailty-models-and-archimedean-copulas"><i class="fa fa-check"></i><b>9.5.2</b> Frailty Models and Archimedean Copulas</a></li>
<li class="chapter" data-level="9.5.3" data-path="chap8.html"><a href="chap8.html#survival-function"><i class="fa fa-check"></i><b>9.5.3</b> Survival Function</a></li>
<li class="chapter" data-level="9.5.4" data-path="chap8.html"><a href="chap8.html#regression-function"><i class="fa fa-check"></i><b>9.5.4</b> Regression Function</a></li>
<li class="chapter" data-level="9.5.5" data-path="chap8.html"><a href="chap8.html#bivariate-integral-transformation"><i class="fa fa-check"></i><b>9.5.5</b> Bivariate Integral Transformation</a></li>
<li class="chapter" data-level="9.5.6" data-path="chap8.html"><a href="chap8.html#order-relations-for-archimedean-copulas"><i class="fa fa-check"></i><b>9.5.6</b> Order Relations for Archimedean Copulas</a></li>
<li class="chapter" data-level="9.5.7" data-path="chap8.html"><a href="chap8.html#study-of-a-function-of-two-correlated-risks"><i class="fa fa-check"></i><b>9.5.7</b> Study of a Function of Two Correlated Risks</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="chap8.html"><a href="chap8.html#bibliographical-notes-6"><i class="fa fa-check"></i><b>9.8</b> Bibliographical Notes</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="chap9.html"><a href="chap9.html"><i class="fa fa-check"></i><b>10</b> Prior Ratemaking</a>
<ul>
<li class="chapter" data-level="10.1" data-path="chap9.html"><a href="chap9.html#introduction-6"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="chap9.html"><a href="chap9.html#Sec965New"><i class="fa fa-check"></i><b>10.2</b> Rating Variables</a></li>
<li class="chapter" data-level="10.3" data-path="chap9.html"><a href="chap9.html#basic-principles-of-statistics"><i class="fa fa-check"></i><b>10.3</b> Basic Principles of Statistics</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="chap9.html"><a href="chap9.html#empirical-cumulative-distribution-function"><i class="fa fa-check"></i><b>10.3.1</b> Empirical Cumulative Distribution Function</a></li>
<li class="chapter" data-level="10.3.2" data-path="chap9.html"><a href="chap9.html#the-parametric-approach"><i class="fa fa-check"></i><b>10.3.2</b> The Parametric Approach</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="chap9.html"><a href="chap9.html#fishers-information"><i class="fa fa-check"></i><b>10.4</b> Fisher’s Information</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="chap9.html"><a href="chap9.html#conditions-on-the-statistical-model"><i class="fa fa-check"></i><b>10.4.1</b> Conditions on the Statistical Model</a></li>
<li class="chapter" data-level="10.4.2" data-path="chap9.html"><a href="chap9.html#definition-34"><i class="fa fa-check"></i><b>10.4.2</b> Definition</a></li>
<li class="chapter" data-level="10.4.3" data-path="chap9.html"><a href="chap9.html#parameter-estimation-using-maximum-likelihood-method"><i class="fa fa-check"></i><b>10.4.3</b> Parameter Estimation Using Maximum Likelihood Method</a></li>
<li class="chapter" data-level="10.4.4" data-path="chap9.html"><a href="chap9.html#likelihood-ratio-test"><i class="fa fa-check"></i><b>10.4.4</b> Likelihood Ratio Test</a></li>
<li class="chapter" data-level="10.4.5" data-path="chap9.html"><a href="chap9.html#other-estimation-methods"><i class="fa fa-check"></i><b>10.4.5</b> Other Estimation Methods</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="chap9.html"><a href="chap9.html#SecPCA"><i class="fa fa-check"></i><b>10.5</b> Data Analysis</a>
<ul>
<li class="chapter" data-level="10.5.1" data-path="chap9.html"><a href="chap9.html#principle-3"><i class="fa fa-check"></i><b>10.5.1</b> Principle</a></li>
<li class="chapter" data-level="10.5.2" data-path="chap9.html"><a href="chap9.html#principal-component-analysis-pca"><i class="fa fa-check"></i><b>10.5.2</b> Principal Component Analysis (PCA)</a></li>
<li class="chapter" data-level="10.5.3" data-path="chap9.html"><a href="chap9.html#multiple-correspondence-analysis-mca"><i class="fa fa-check"></i><b>10.5.3</b> Multiple Correspondence Analysis (MCA)</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="chap9.html"><a href="chap9.html#Sec923MC"><i class="fa fa-check"></i><b>10.6</b> Scoring Methods</a>
<ul>
<li class="chapter" data-level="10.6.1" data-path="chap9.html"><a href="chap9.html#classification-methods"><i class="fa fa-check"></i><b>10.6.1</b> Classification Methods</a></li>
<li class="chapter" data-level="10.6.2" data-path="chap9.html"><a href="chap9.html#definition-of-a-score"><i class="fa fa-check"></i><b>10.6.2</b> Definition of a Score</a></li>
<li class="chapter" data-level="10.6.3" data-path="chap9.html"><a href="chap9.html#principle-of-scoring"><i class="fa fa-check"></i><b>10.6.3</b> Principle of Scoring</a></li>
<li class="chapter" data-level="10.6.4" data-path="chap9.html"><a href="chap9.html#optimal-classification-and-threshold-selection"><i class="fa fa-check"></i><b>10.6.4</b> Optimal Classification and Threshold Selection</a></li>
<li class="chapter" data-level="10.6.5" data-path="chap9.html"><a href="chap9.html#practical-construction-of-a-score"><i class="fa fa-check"></i><b>10.6.5</b> Practical Construction of a Score</a></li>
<li class="chapter" data-level="10.6.6" data-path="chap9.html"><a href="chap9.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>10.6.6</b> (Linear) Discriminant Analysis</a></li>
<li class="chapter" data-level="10.6.7" data-path="chap9.html"><a href="chap9.html#discriminant-analysis"><i class="fa fa-check"></i><b>10.6.7</b> Discriminant Analysis</a></li>
<li class="chapter" data-level="10.6.8" data-path="chap9.html"><a href="chap9.html#the-disqual-method"><i class="fa fa-check"></i><b>10.6.8</b> The DISQUAL Method</a></li>
<li class="chapter" data-level="10.6.9" data-path="chap9.html"><a href="chap9.html#the-probit-model"><i class="fa fa-check"></i><b>10.6.9</b> The Probit Model</a></li>
<li class="chapter" data-level="10.6.10" data-path="chap9.html"><a href="chap9.html#the-logit-model"><i class="fa fa-check"></i><b>10.6.10</b> The Logit Model</a></li>
<li class="chapter" data-level="10.6.11" data-path="chap9.html"><a href="chap9.html#duality-of-approaches"><i class="fa fa-check"></i><b>10.6.11</b> Duality of Approaches</a></li>
<li class="chapter" data-level="10.6.12" data-path="chap9.html"><a href="chap9.html#performance-and-selection-curves"><i class="fa fa-check"></i><b>10.6.12</b> Performance and Selection Curves</a></li>
<li class="chapter" data-level="10.6.13" data-path="chap9.html"><a href="chap9.html#desirable-properties-of-a-score"><i class="fa fa-check"></i><b>10.6.13</b> Desirable Properties of a Score</a></li>
<li class="chapter" data-level="10.6.14" data-path="chap9.html"><a href="chap9.html#score-comparison"><i class="fa fa-check"></i><b>10.6.14</b> Score Comparison</a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="chap9.html"><a href="chap9.html#Sec93MLMC"><i class="fa fa-check"></i><b>10.7</b> Linear Model</a>
<ul>
<li class="chapter" data-level="10.7.1" data-path="chap9.html"><a href="chap9.html#definition-35"><i class="fa fa-check"></i><b>10.7.1</b> Definition</a></li>
<li class="chapter" data-level="10.7.2" data-path="chap9.html"><a href="chap9.html#matrix-formalism"><i class="fa fa-check"></i><b>10.7.2</b> Matrix Formalism</a></li>
<li class="chapter" data-level="10.7.3" data-path="chap9.html"><a href="chap9.html#parameter-estimation"><i class="fa fa-check"></i><b>10.7.3</b> Parameter Estimation</a></li>
<li class="chapter" data-level="10.7.4" data-path="chap9.html"><a href="chap9.html#prediction-matrix"><i class="fa fa-check"></i><b>10.7.4</b> Prediction Matrix</a></li>
<li class="chapter" data-level="10.7.5" data-path="chap9.html"><a href="chap9.html#estimation-of-means-and-variance"><i class="fa fa-check"></i><b>10.7.5</b> Estimation of Means and Variance</a></li>
<li class="chapter" data-level="10.7.6" data-path="chap9.html"><a href="chap9.html#measurement-of-fit-quality-the-coefficient-of-determination"><i class="fa fa-check"></i><b>10.7.6</b> Measurement of Fit Quality: The Coefficient of Determination</a></li>
<li class="chapter" data-level="10.7.7" data-path="chap9.html"><a href="chap9.html#standardized-residuals"><i class="fa fa-check"></i><b>10.7.7</b> Standardized Residuals</a></li>
<li class="chapter" data-level="10.7.8" data-path="chap9.html"><a href="chap9.html#inferential-results-for-parameters"><i class="fa fa-check"></i><b>10.7.8</b> Inferential Results for Parameters</a></li>
<li class="chapter" data-level="10.7.9" data-path="chap9.html"><a href="chap9.html#testing-a-simple-hypothesis"><i class="fa fa-check"></i><b>10.7.9</b> Testing a Simple Hypothesis</a></li>
<li class="chapter" data-level="10.7.10" data-path="chap9.html"><a href="chap9.html#comparison-of-nested-models"><i class="fa fa-check"></i><b>10.7.10</b> Comparison of Nested Models</a></li>
<li class="chapter" data-level="10.7.11" data-path="chap9.html"><a href="chap9.html#confidence-regions"><i class="fa fa-check"></i><b>10.7.11</b> Confidence Regions</a></li>
<li class="chapter" data-level="10.7.12" data-path="chap9.html"><a href="chap9.html#confidence-intervals"><i class="fa fa-check"></i><b>10.7.12</b> Confidence Intervals</a></li>
<li class="chapter" data-level="10.7.13" data-path="chap9.html"><a href="chap9.html#measures-of-influence"><i class="fa fa-check"></i><b>10.7.13</b> Measures of Influence</a></li>
<li class="chapter" data-level="10.7.14" data-path="chap9.html"><a href="chap9.html#weighted-least-squares"><i class="fa fa-check"></i><b>10.7.14</b> Weighted Least Squares</a></li>
</ul></li>
<li class="chapter" data-level="10.8" data-path="chap9.html"><a href="chap9.html#additive-models"><i class="fa fa-check"></i><b>10.8</b> Additive Models</a>
<ul>
<li class="chapter" data-level="10.8.1" data-path="chap9.html"><a href="chap9.html#principle-5"><i class="fa fa-check"></i><b>10.8.1</b> Principle</a></li>
<li class="chapter" data-level="10.8.2" data-path="chap9.html"><a href="chap9.html#single-regressor-case"><i class="fa fa-check"></i><b>10.8.2</b> Single Regressor Case</a></li>
<li class="chapter" data-level="10.8.3" data-path="chap9.html"><a href="chap9.html#estimation-with-multiple-regressors-backfitting"><i class="fa fa-check"></i><b>10.8.3</b> Estimation with Multiple Regressors: Backfitting</a></li>
<li class="chapter" data-level="10.8.4" data-path="chap9.html"><a href="chap9.html#comparison-of-different-approaches"><i class="fa fa-check"></i><b>10.8.4</b> Comparison of Different Approaches</a></li>
</ul></li>
<li class="chapter" data-level="10.9" data-path="chap9.html"><a href="chap9.html#Sec96GLM"><i class="fa fa-check"></i><b>10.9</b> Generalized Linear Models (GLM)</a>
<ul>
<li class="chapter" data-level="10.9.1" data-path="chap9.html"><a href="chap9.html#a-brief-history-of-actuarial-applications-of-regression-models"><i class="fa fa-check"></i><b>10.9.1</b> A Brief History of Actuarial Applications of Regression Models</a></li>
<li class="chapter" data-level="10.9.2" data-path="chap9.html"><a href="chap9.html#definition-37"><i class="fa fa-check"></i><b>10.9.2</b> Definition</a></li>
<li class="chapter" data-level="10.9.3" data-path="chap9.html"><a href="chap9.html#mean-and-variance"><i class="fa fa-check"></i><b>10.9.3</b> Mean and Variance</a></li>
<li class="chapter" data-level="10.9.4" data-path="chap9.html"><a href="chap9.html#regression-model"><i class="fa fa-check"></i><b>10.9.4</b> Regression Model</a></li>
<li class="chapter" data-level="10.9.5" data-path="chap9.html"><a href="chap9.html#canonical-link-function"><i class="fa fa-check"></i><b>10.9.5</b> Canonical Link Function</a></li>
<li class="chapter" data-level="10.9.6" data-path="chap9.html"><a href="chap9.html#likelihood-equations"><i class="fa fa-check"></i><b>10.9.6</b> Likelihood Equations</a></li>
<li class="chapter" data-level="10.9.7" data-path="chap9.html"><a href="chap9.html#solving-likelihood-equations"><i class="fa fa-check"></i><b>10.9.7</b> Solving Likelihood Equations</a></li>
<li class="chapter" data-level="10.9.8" data-path="chap9.html"><a href="chap9.html#fisher-information"><i class="fa fa-check"></i><b>10.9.8</b> Fisher Information</a></li>
<li class="chapter" data-level="10.9.9" data-path="chap9.html"><a href="chap9.html#confidence-interval-for-parameters"><i class="fa fa-check"></i><b>10.9.9</b> Confidence Interval for Parameters</a></li>
<li class="chapter" data-level="10.9.10" data-path="chap9.html"><a href="chap9.html#model-comparison"><i class="fa fa-check"></i><b>10.9.10</b> Model Comparison</a></li>
<li class="chapter" data-level="10.9.11" data-path="chap9.html"><a href="chap9.html#hypothesis-tests-on-parameters"><i class="fa fa-check"></i><b>10.9.11</b> Hypothesis Tests on Parameters</a></li>
<li class="chapter" data-level="10.9.12" data-path="chap9.html"><a href="chap9.html#estimation-of-the-dispersion-parameter"><i class="fa fa-check"></i><b>10.9.12</b> Estimation of the Dispersion Parameter</a></li>
<li class="chapter" data-level="10.9.13" data-path="chap9.html"><a href="chap9.html#residual-analysis"><i class="fa fa-check"></i><b>10.9.13</b> Residual Analysis</a></li>
<li class="chapter" data-level="10.9.14" data-path="chap9.html"><a href="chap9.html#the-practice-of-generalized-linear-models"><i class="fa fa-check"></i><b>10.9.14</b> The Practice of Generalized Linear Models</a></li>
</ul></li>
<li class="chapter" data-level="10.10" data-path="chap9.html"><a href="chap9.html#generalized-additive-models-gams"><i class="fa fa-check"></i><b>10.10</b> Generalized Additive Models (GAMs)</a>
<ul>
<li class="chapter" data-level="10.10.1" data-path="chap9.html"><a href="chap9.html#principle-6"><i class="fa fa-check"></i><b>10.10.1</b> Principle</a></li>
<li class="chapter" data-level="10.10.2" data-path="chap9.html"><a href="chap9.html#in-practice"><i class="fa fa-check"></i><b>10.10.2</b> In Practice…</a></li>
</ul></li>
<li class="chapter" data-level="10.11" data-path="chap9.html"><a href="chap9.html#practical-case-of-auto-insurance-pricing"><i class="fa fa-check"></i><b>10.11</b> Practical Case of Auto Insurance Pricing</a>
<ul>
<li class="chapter" data-level="10.11.1" data-path="chap9.html"><a href="chap9.html#portfolio-description"><i class="fa fa-check"></i><b>10.11.1</b> Portfolio Description</a></li>
<li class="chapter" data-level="10.11.2" data-path="chap9.html"><a href="chap9.html#variables-describing-claims"><i class="fa fa-check"></i><b>10.11.2</b> Variables Describing Claims</a></li>
<li class="chapter" data-level="10.11.3" data-path="chap9.html"><a href="chap9.html#measuring-exposure-to-risk-the-variable"><i class="fa fa-check"></i><b>10.11.3</b> Measuring Exposure to Risk: the Variable</a></li>
<li class="chapter" data-level="10.11.4" data-path="chap9.html"><a href="chap9.html#characteristics-of-the-policyholder"><i class="fa fa-check"></i><b>10.11.4</b> Characteristics of the Policyholder</a></li>
<li class="chapter" data-level="10.11.5" data-path="chap9.html"><a href="chap9.html#vehicle-characteristics"><i class="fa fa-check"></i><b>10.11.5</b> Vehicle Characteristics</a></li>
<li class="chapter" data-level="10.11.6" data-path="chap9.html"><a href="chap9.html#interaction-between-rating-variables-1"><i class="fa fa-check"></i><b>10.11.6</b> Interaction Between Rating Variables</a></li>
<li class="chapter" data-level="10.11.7" data-path="chap9.html"><a href="chap9.html#initial-screening-of-rating-variables"><i class="fa fa-check"></i><b>10.11.7</b> Initial Screening of Rating Variables</a></li>
<li class="chapter" data-level="10.11.8" data-path="chap9.html"><a href="chap9.html#analysis-of-claim-frequencies"><i class="fa fa-check"></i><b>10.11.8</b> Analysis of Claim Frequencies</a></li>
<li class="chapter" data-level="10.11.9" data-path="chap9.html"><a href="chap9.html#analysis-of-claim-costs"><i class="fa fa-check"></i><b>10.11.9</b> Analysis of Claim Costs</a></li>
</ul></li>
<li class="chapter" data-level="10.12" data-path="chap9.html"><a href="chap9.html#panel-data-rating"><i class="fa fa-check"></i><b>10.12</b> Panel Data Rating</a>
<ul>
<li class="chapter" data-level="10.12.1" data-path="chap9.html"><a href="chap9.html#rating-based-on-panel-data"><i class="fa fa-check"></i><b>10.12.1</b> Rating Based on Panel Data</a></li>
<li class="chapter" data-level="10.12.2" data-path="chap9.html"><a href="chap9.html#notation"><i class="fa fa-check"></i><b>10.12.2</b> Notation</a></li>
<li class="chapter" data-level="10.12.3" data-path="chap9.html"><a href="chap9.html#presentation-of-the-dataset"><i class="fa fa-check"></i><b>10.12.3</b> Presentation of the Dataset</a></li>
<li class="chapter" data-level="10.12.4" data-path="chap9.html"><a href="chap9.html#poisson-regression-assuming-temporal-independence"><i class="fa fa-check"></i><b>10.12.4</b> Poisson Regression Assuming Temporal Independence</a></li>
<li class="chapter" data-level="10.12.5" data-path="chap9.html"><a href="chap9.html#accounting-for-temporal-dependence"><i class="fa fa-check"></i><b>10.12.5</b> Accounting for Temporal Dependence</a></li>
<li class="chapter" data-level="10.12.6" data-path="chap9.html"><a href="chap9.html#modeling-dependence-using-the-working-correlation-matrix"><i class="fa fa-check"></i><b>10.12.6</b> Modeling Dependence Using the “Working Correlation Matrix”</a></li>
<li class="chapter" data-level="10.12.7" data-path="chap9.html"><a href="chap9.html#obtaining-estimates"><i class="fa fa-check"></i><b>10.12.7</b> Obtaining Estimates</a></li>
<li class="chapter" data-level="10.12.8" data-path="chap9.html"><a href="chap9.html#numerical-illustration-1"><i class="fa fa-check"></i><b>10.12.8</b> Numerical Illustration</a></li>
</ul></li>
<li class="chapter" data-level="10.13" data-path="chap9.html"><a href="chap9.html#technical-justifications-for-segmentation"><i class="fa fa-check"></i><b>10.13</b> Technical Justifications for Segmentation</a>
<ul>
<li class="chapter" data-level="10.13.1" data-path="chap9.html"><a href="chap9.html#technical-rate-and-commercial-rate"><i class="fa fa-check"></i><b>10.13.1</b> Technical Rate and Commercial Rate</a></li>
<li class="chapter" data-level="10.13.2" data-path="chap9.html"><a href="chap9.html#segmentation-of-technical-and-commercial-rates"><i class="fa fa-check"></i><b>10.13.2</b> Segmentation of Technical and Commercial Rates</a></li>
<li class="chapter" data-level="10.13.3" data-path="chap9.html"><a href="chap9.html#adverse-selection-and-segmentation"><i class="fa fa-check"></i><b>10.13.3</b> Adverse Selection and Segmentation</a></li>
<li class="chapter" data-level="10.13.4" data-path="chap9.html"><a href="chap9.html#inequity-of-prior-pricing"><i class="fa fa-check"></i><b>10.13.4</b> Inequity of Prior Pricing</a></li>
</ul></li>
<li class="chapter" data-level="10.14" data-path="chap9.html"><a href="chap9.html#bibliographical-notes-7"><i class="fa fa-check"></i><b>10.14</b> Bibliographical Notes</a></li>
<li class="chapter" data-level="10.15" data-path="chap9.html"><a href="chap9.html#exercises-5"><i class="fa fa-check"></i><b>10.15</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="chap10.html"><a href="chap10.html"><i class="fa fa-check"></i><b>11</b> Credibility</a>
<ul>
<li class="chapter" data-level="11.1" data-path="chap10.html"><a href="chap10.html#introduction-7"><i class="fa fa-check"></i><b>11.1</b> Introduction</a></li>
<li class="chapter" data-level="11.2" data-path="chap10.html"><a href="chap10.html#bayesian-credibility"><i class="fa fa-check"></i><b>11.2</b> Bayesian Credibility</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="chap10.html"><a href="chap10.html#introductory-example"><i class="fa fa-check"></i><b>11.2.1</b> Introductory Example</a></li>
<li class="chapter" data-level="11.2.2" data-path="chap10.html"><a href="chap10.html#bayesian-posterior-pricing-model"><i class="fa fa-check"></i><b>11.2.2</b> Bayesian Posterior Pricing Model</a></li>
<li class="chapter" data-level="11.2.3" data-path="chap10.html"><a href="chap10.html#frequentist-bayesian-credibility-without-a-priori-pricing"><i class="fa fa-check"></i><b>11.2.3</b> Frequentist Bayesian Credibility without A Priori Pricing</a></li>
<li class="chapter" data-level="11.2.4" data-path="chap10.html"><a href="chap10.html#bayesian-frequentist-credibility-with-prior-rate-making"><i class="fa fa-check"></i><b>11.2.4</b> Bayesian-Frequentist Credibility with Prior Rate Making</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="chap10.html"><a href="chap10.html#linear-credibility"><i class="fa fa-check"></i><b>11.3</b> Linear Credibility</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="chap10.html"><a href="chap10.html#bühlmann-model"><i class="fa fa-check"></i><b>11.3.1</b> Bühlmann Model</a></li>
<li class="chapter" data-level="11.3.2" data-path="chap10.html"><a href="chap10.html#bühlmann-straub-model"><i class="fa fa-check"></i><b>11.3.2</b> Bühlmann-Straub Model</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="chap10.html"><a href="chap10.html#SecCredTot"><i class="fa fa-check"></i><b>11.4</b> Total Credibility</a></li>
<li class="chapter" data-level="11.5" data-path="chap10.html"><a href="chap10.html#multivariate-credibility"><i class="fa fa-check"></i><b>11.5</b> Multivariate Credibility</a></li>
<li class="chapter" data-level="11.6" data-path="chap10.html"><a href="chap10.html#sec:matcorp"><i class="fa fa-check"></i><b>11.6</b> Modeling</a>
<ul>
<li class="chapter" data-level="11.6.1" data-path="chap10.html"><a href="chap10.html#linear-credibility-premium"><i class="fa fa-check"></i><b>11.6.1</b> Linear Credibility Premium</a></li>
<li class="chapter" data-level="11.6.2" data-path="chap10.html"><a href="chap10.html#an-approach-on-disaggregated-data"><i class="fa fa-check"></i><b>11.6.2</b> An Approach on Disaggregated Data</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="chap10.html"><a href="chap10.html#hierarchical-credibility"><i class="fa fa-check"></i><b>11.7</b> Hierarchical Credibility</a>
<ul>
<li class="chapter" data-level="11.7.1" data-path="chap10.html"><a href="chap10.html#motivation-fleet-insurance"><i class="fa fa-check"></i><b>11.7.1</b> Motivation: Fleet Insurance</a></li>
<li class="chapter" data-level="11.7.2" data-path="chap10.html"><a href="chap10.html#linear-credibility-1"><i class="fa fa-check"></i><b>11.7.2</b> Linear Credibility</a></li>
<li class="chapter" data-level="11.7.3" data-path="chap10.html"><a href="chap10.html#the-case-of-new-vehicles"><i class="fa fa-check"></i><b>11.7.3</b> The Case of New Vehicles</a></li>
<li class="chapter" data-level="11.7.4" data-path="chap10.html"><a href="chap10.html#the-case-of-existing-vehicles"><i class="fa fa-check"></i><b>11.7.4</b> The Case of Existing Vehicles</a></li>
<li class="chapter" data-level="11.7.5" data-path="chap10.html"><a href="chap10.html#open-fleets"><i class="fa fa-check"></i><b>11.7.5</b> Open Fleets</a></li>
</ul></li>
<li class="chapter" data-level="11.8" data-path="chap10.html"><a href="chap10.html#bibliographical-notes-8"><i class="fa fa-check"></i><b>11.8</b> Bibliographical Notes</a></li>
<li class="chapter" data-level="11.9" data-path="chap10.html"><a href="chap10.html#exercises-6"><i class="fa fa-check"></i><b>11.9</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="chap11.html"><a href="chap11.html"><i class="fa fa-check"></i><b>12</b> Bonus-Malus</a></li>
<li class="chapter" data-level="13" data-path="chap12.html"><a href="chap12.html"><i class="fa fa-check"></i><b>13</b> Economics of Insurance</a></li>
<li class="chapter" data-level="14" data-path="chap13.html"><a href="chap13.html"><i class="fa fa-check"></i><b>14</b> Claims Reserving</a></li>
<li class="chapter" data-level="15" data-path="chap14.html"><a href="chap14.html"><i class="fa fa-check"></i><b>15</b> Large Risks</a>
<ul>
<li class="chapter" data-level="15.1" data-path="chap14.html"><a href="chap14.html#introduction-8"><i class="fa fa-check"></i><b>15.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="15.1.1" data-path="chap14.html"><a href="chap14.html#the-concept-of-catastrophe"><i class="fa fa-check"></i><b>15.1.1</b> The Concept of Catastrophe</a></li>
<li class="chapter" data-level="15.1.2" data-path="chap14.html"><a href="chap14.html#why-manage-extreme-events-ex-ante"><i class="fa fa-check"></i><b>15.1.2</b> Why Manage Extreme Events <em>Ex Ante</em>?</a></li>
<li class="chapter" data-level="15.1.3" data-path="chap14.html"><a href="chap14.html#what-types-of-catastrophes"><i class="fa fa-check"></i><b>15.1.3</b> What Types of Catastrophes?</a></li>
<li class="chapter" data-level="15.1.4" data-path="chap14.html"><a href="chap14.html#data-presentation"><i class="fa fa-check"></i><b>15.1.4</b> Data Presentation</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="chap14.html"><a href="chap14.html#limit-law-of-maxima"><i class="fa fa-check"></i><b>15.2</b> Limit Law of Maxima</a>
<ul>
<li class="chapter" data-level="15.2.1" data-path="chap14.html"><a href="chap14.html#behavior-of-the-maximum-in-large-samples"><i class="fa fa-check"></i><b>15.2.1</b> Behavior of the Maximum in Large Samples</a></li>
<li class="chapter" data-level="15.2.2" data-path="chap14.html"><a href="chap14.html#large-deviations"><i class="fa fa-check"></i><b>15.2.2</b> Large Deviations</a></li>
<li class="chapter" data-level="15.2.3" data-path="chap14.html"><a href="chap14.html#estimation-of-the-maximum-distribution"><i class="fa fa-check"></i><b>15.2.3</b> Estimation of the Maximum Distribution</a></li>
<li class="chapter" data-level="15.2.4" data-path="chap14.html"><a href="chap14.html#the-n-th-largest-value"><i class="fa fa-check"></i><b>15.2.4</b> The <span class="math inline">\(n\)</span>-th Largest Value</a></li>
<li class="chapter" data-level="15.2.5" data-path="chap14.html"><a href="chap14.html#random-frequency-of-claims"><i class="fa fa-check"></i><b>15.2.5</b> Random Frequency of Claims</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="chap14.html"><a href="chap14.html#tail-thickness-of-distributions"><i class="fa fa-check"></i><b>15.3</b> Tail Thickness of Distributions</a>
<ul>
<li class="chapter" data-level="15.3.1" data-path="chap14.html"><a href="chap14.html#the-concept-of-regular-variation"><i class="fa fa-check"></i><b>15.3.1</b> The Concept of Regular Variation</a></li>
<li class="chapter" data-level="15.3.2" data-path="chap14.html"><a href="chap14.html#regular-variation-and-the-max-domain-of-attraction-of-the-fréchet-distribution"><i class="fa fa-check"></i><b>15.3.2</b> Regular Variation and the Max-Domain of Attraction of the Fréchet Distribution</a></li>
<li class="chapter" data-level="15.3.3" data-path="chap14.html"><a href="chap14.html#sum-maximum-and-subexponential-distribution"><i class="fa fa-check"></i><b>15.3.3</b> Sum, Maximum, and Subexponential Distribution</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="chap14.html"><a href="chap14.html#study-of-the-excess-distribution"><i class="fa fa-check"></i><b>15.4</b> Study of the Excess Distribution</a>
<ul>
<li class="chapter" data-level="15.4.1" data-path="chap14.html"><a href="chap14.html#generalized-pareto-distribution"><i class="fa fa-check"></i><b>15.4.1</b> Generalized Pareto Distribution</a></li>
<li class="chapter" data-level="15.4.2" data-path="chap14.html"><a href="chap14.html#pickands-balkema-de-haan-theorem"><i class="fa fa-check"></i><b>15.4.2</b> Pickands-Balkema-de Haan Theorem</a></li>
<li class="chapter" data-level="15.4.3" data-path="chap14.html"><a href="chap14.html#number-of-exceedances"><i class="fa fa-check"></i><b>15.4.3</b> Number of Exceedances</a></li>
<li class="chapter" data-level="15.4.4" data-path="chap14.html"><a href="chap14.html#sample-size-with-a-poisson-distribution"><i class="fa fa-check"></i><b>15.4.4</b> Sample Size with a Poisson Distribution</a></li>
<li class="chapter" data-level="15.4.5" data-path="chap14.html"><a href="chap14.html#sec-comportement-ex"><i class="fa fa-check"></i><b>15.4.5</b> Examples of Tail Behavior</a></li>
<li class="chapter" data-level="15.4.6" data-path="chap14.html"><a href="chap14.html#heavy-tailed-distributions-the-max-domain-of-attraction-of-the-fréchet-distribution"><i class="fa fa-check"></i><b>15.4.6</b> Heavy-Tailed Distributions: the Max-Domain of Attraction of the Fréchet Distribution</a></li>
<li class="chapter" data-level="15.4.7" data-path="chap14.html"><a href="chap14.html#thin-tailed-distributions-the-max-domain-of-attraction-of-the-gumbel-distribution"><i class="fa fa-check"></i><b>15.4.7</b> Thin-Tailed Distributions: the Max-Domain of Attraction of the Gumbel Distribution</a></li>
</ul></li>
<li class="chapter" data-level="15.5" data-path="chap14.html"><a href="chap14.html#estimation-of-extreme-quantiles"><i class="fa fa-check"></i><b>15.5</b> Estimation of Extreme Quantiles</a>
<ul>
<li class="chapter" data-level="15.5.1" data-path="chap14.html"><a href="chap14.html#estimation-of-the-tail-index"><i class="fa fa-check"></i><b>15.5.1</b> Estimation of the Tail Index</a></li>
<li class="chapter" data-level="15.5.2" data-path="chap14.html"><a href="chap14.html#time-and-return-period"><i class="fa fa-check"></i><b>15.5.2</b> Time and Return Period</a></li>
<li class="chapter" data-level="15.5.3" data-path="chap14.html"><a href="chap14.html#gpd-approximation-for-var"><i class="fa fa-check"></i><b>15.5.3</b> GPD Approximation for VaR</a></li>
<li class="chapter" data-level="15.5.4" data-path="chap14.html"><a href="chap14.html#hill-estimator-for-var"><i class="fa fa-check"></i><b>15.5.4</b> Hill Estimator for VaR</a></li>
<li class="chapter" data-level="15.5.5" data-path="chap14.html"><a href="chap14.html#cumulative-risk-extremes-and-compound-distributions"><i class="fa fa-check"></i><b>15.5.5</b> Cumulative Risk, Extremes, and Compound Distributions</a></li>
</ul></li>
<li class="chapter" data-level="15.6" data-path="chap14.html"><a href="chap14.html#multivariate-extreme-value-theory"><i class="fa fa-check"></i><b>15.6</b> Multivariate Extreme Value Theory</a>
<ul>
<li class="chapter" data-level="15.6.1" data-path="chap14.html"><a href="chap14.html#componentwise-maxima"><i class="fa fa-check"></i><b>15.6.1</b> Componentwise Maxima</a></li>
<li class="chapter" data-level="15.6.2" data-path="chap14.html"><a href="chap14.html#expression-of-limit-distributions"><i class="fa fa-check"></i><b>15.6.2</b> Expression of Limit Distributions</a></li>
<li class="chapter" data-level="15.6.3" data-path="chap14.html"><a href="chap14.html#representation-of-the-dependence-function-a"><i class="fa fa-check"></i><b>15.6.3</b> Representation of the Dependence Function <span class="math inline">\(A\)</span></a></li>
<li class="chapter" data-level="15.6.4" data-path="chap14.html"><a href="chap14.html#estimation-of-the-dependence-function"><i class="fa fa-check"></i><b>15.6.4</b> Estimation of the Dependence Function</a></li>
<li class="chapter" data-level="15.6.5" data-path="chap14.html"><a href="chap14.html#copulas-of-multivariate-extreme-value-distributions"><i class="fa fa-check"></i><b>15.6.5</b> Copulas of Multivariate Extreme Value Distributions</a></li>
<li class="chapter" data-level="15.6.6" data-path="chap14.html"><a href="chap14.html#correlation-coefficient"><i class="fa fa-check"></i><b>15.6.6</b> Correlation Coefficient</a></li>
<li class="chapter" data-level="15.6.7" data-path="chap14.html"><a href="chap14.html#comparison-of-dependence"><i class="fa fa-check"></i><b>15.6.7</b> Comparison of Dependence</a></li>
<li class="chapter" data-level="15.6.8" data-path="chap14.html"><a href="chap14.html#tail-dependence-coefficient"><i class="fa fa-check"></i><b>15.6.8</b> Tail Dependence Coefficient</a></li>
<li class="chapter" data-level="15.6.9" data-path="chap14.html"><a href="chap14.html#application-in-reinsurance-cost-vs.-expenses"><i class="fa fa-check"></i><b>15.6.9</b> Application in Reinsurance, Cost vs. Expenses</a></li>
</ul></li>
<li class="chapter" data-level="15.7" data-path="chap14.html"><a href="chap14.html#standard-reinsurance-treaties"><i class="fa fa-check"></i><b>15.7</b> Standard Reinsurance Treaties</a>
<ul>
<li class="chapter" data-level="15.7.1" data-path="chap14.html"><a href="chap14.html#reinsurance"><i class="fa fa-check"></i><b>15.7.1</b> Reinsurance</a></li>
<li class="chapter" data-level="15.7.2" data-path="chap14.html"><a href="chap14.html#proportional-reinsurance"><i class="fa fa-check"></i><b>15.7.2</b> Proportional Reinsurance</a></li>
<li class="chapter" data-level="15.7.3" data-path="chap14.html"><a href="chap14.html#Reass-NP"><i class="fa fa-check"></i><b>15.7.3</b> Non-Proportional Reinsurance</a></li>
<li class="chapter" data-level="15.7.4" data-path="chap14.html"><a href="chap14.html#pricing-of-non-proportional-treaties"><i class="fa fa-check"></i><b>15.7.4</b> Pricing of Non-Proportional Treaties</a></li>
<li class="chapter" data-level="15.7.5" data-path="chap14.html"><a href="chap14.html#quantile-quantile-q-q-plots"><i class="fa fa-check"></i><b>15.7.5</b> Quantile-Quantile (Q-Q) Plots</a></li>
<li class="chapter" data-level="15.7.6" data-path="chap14.html"><a href="chap14.html#mean-excess-function"><i class="fa fa-check"></i><b>15.7.6</b> Mean Excess Function</a></li>
<li class="chapter" data-level="15.7.7" data-path="chap14.html"><a href="chap14.html#the-lorenz-curve"><i class="fa fa-check"></i><b>15.7.7</b> The Lorenz Curve</a></li>
<li class="chapter" data-level="15.7.8" data-path="chap14.html"><a href="chap14.html#approximation-of-pure-premium"><i class="fa fa-check"></i><b>15.7.8</b> Approximation of Pure Premium</a></li>
<li class="chapter" data-level="15.7.9" data-path="chap14.html"><a href="chap14.html#approximation-of-a-wang-premium"><i class="fa fa-check"></i><b>15.7.9</b> Approximation of a Wang Premium</a></li>
<li class="chapter" data-level="15.7.10" data-path="chap14.html"><a href="chap14.html#estimation-of-tvar"><i class="fa fa-check"></i><b>15.7.10</b> Estimation of TVaR</a></li>
<li class="chapter" data-level="15.7.11" data-path="chap14.html"><a href="chap14.html#index-based-coverage-and-securitization"><i class="fa fa-check"></i><b>15.7.11</b> Index-Based Coverage and Securitization</a></li>
</ul></li>
<li class="chapter" data-level="15.8" data-path="chap14.html"><a href="chap14.html#insolvency-and-large-risks"><i class="fa fa-check"></i><b>15.8</b> Insolvency and Large Risks</a>
<ul>
<li class="chapter" data-level="15.8.1" data-path="chap14.html"><a href="chap14.html#probability-of-ruin-in-the-presence-of-large-losses-von-bahrs-approximation"><i class="fa fa-check"></i><b>15.8.1</b> Probability of Ruin in the Presence of Large Losses: von Bahr’s Approximation</a></li>
<li class="chapter" data-level="15.8.2" data-path="chap14.html"><a href="chap14.html#using-the-pollaczeck-khinchine-beekman-formula"><i class="fa fa-check"></i><b>15.8.2</b> Using the Pollaczeck-Khinchine-Beekman Formula</a></li>
</ul></li>
<li class="chapter" data-level="15.9" data-path="chap14.html"><a href="chap14.html#bibliographical-notes-9"><i class="fa fa-check"></i><b>15.9</b> Bibliographical Notes</a></li>
<li class="chapter" data-level="15.10" data-path="chap14.html"><a href="chap14.html#exercises-7"><i class="fa fa-check"></i><b>15.10</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="chap15.html"><a href="chap15.html"><i class="fa fa-check"></i><b>16</b> Monte Carlo</a></li>
<li class="chapter" data-level="17" data-path="chap16.html"><a href="chap16.html"><i class="fa fa-check"></i><b>17</b> Conclusion</a></li>
<li class="chapter" data-level="" data-path="postface.html"><a href="postface.html"><i class="fa fa-check"></i>Postface</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Non Life Insurance Mathematics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chap8" class="section level1 hasAnchor" number="9">
<h1><span class="header-section-number">Chapter 9</span> Multiple Risks<a href="chap8.html#chap8" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="introduction-5" class="section level2 hasAnchor" number="9.1">
<h2><span class="header-section-number">9.1</span> Introduction<a href="chap8.html#introduction-5" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Traditionally, actuarial developments are based on the assumption of independence among random variables present. A notable exception is the credibility theory and bonus-malus systems (which we will study in a later chapter) where the serial dependence between the annual number or amount of claims caused by an insured is exploited to revise the premium amount that the insured must pay to benefit from the insurer’s coverage.</p>
<p>The ever-increasing complexity of insurance products and the obligation to cover events that were once excluded from coverage (such as floods or earthquakes) have highlighted the importance of considering dependence. Therefore, it seemed pertinent to dedicate a (lengthy) chapter to the study of stochastic dependence, a topic traditionally ignored in risk theory literature.</p>
<p>While there is only one way for risks to be mutually independent, not influencing each other, mathematically expressed by the fact that their joint distribution factorizes into the product of their marginals, there are, of course, infinitely many ways to introduce dependence into an actuarial model. For a long time, the multivariate normal distribution was the only tool used to account for certain dependence, in statistics as well as in finance and actuarial science. It is only recently that techniques relying (implicitly or explicitly) on the multivariate normal distribution have been severely criticized, and more realistic alternatives have been developed to model the correlation between random variables with diverse distributions. The purpose of this chapter is precisely to introduce readers to the basic concepts underlying these new approaches.</p>
</div>
<div id="comonotonicity-and-antimonotonicity" class="section level2 hasAnchor" number="9.2">
<h2><span class="header-section-number">9.2</span> Comonotonicity and Antimonotonicity<a href="chap8.html#comonotonicity-and-antimonotonicity" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="fréchet-classes" class="section level3 hasAnchor" number="9.2.1">
<h3><span class="header-section-number">9.2.1</span> Fréchet Classes<a href="chap8.html#fréchet-classes" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Named in honor of the French mathematician Maurice Fréchet, these sets contain all probability distributions with fixed marginals. Fréchet classes are, therefore, the ideal framework for studying dependence, as two elements in the same class differ only in their correlation structure, not in their marginal behavior.</p>
<div class="definition">
<p><span id="def:unlabeled-div-1" class="definition"><strong>Definition 9.1  </strong></span>We denote <span class="math inline">\(\mathcal{F}\left( F_1, F_2 \right)\)</span> as the set of bivariate distribution functions with marginal distribution functions <span class="math inline">\(F_1\)</span> and <span class="math inline">\(F_2\)</span>, respectively, i.e.,
<span class="math display">\[\begin{eqnarray*}
\mathcal{F}\left( F_1, F_2 \right) &amp;=&amp; \Big\{ \text{distribution functions }F_{\boldsymbol{X}} \text{ such that } \\
&amp;&amp; \lim_{t \rightarrow \infty} F_{\boldsymbol{X}}\left( x_1, t \right) = F_1\left( x_1 \right), \text{ for any } x_1 \in \mathbb{R} \\
&amp;&amp; \lim_{t \rightarrow \infty} F_{\boldsymbol{X}}\left( t, x_2 \right) = F_2\left( x_2 \right), \text{ for any } x_2 \in \mathbb{R} \Big\}.
\end{eqnarray*}\]</span></p>
</div>
</div>
<div id="fréchet-bounds" class="section level3 hasAnchor" number="9.2.2">
<h3><span class="header-section-number">9.2.2</span> Fréchet Bounds<a href="chap8.html#fréchet-bounds" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="definition-25" class="section level4 hasAnchor" number="9.2.2.1">
<h4><span class="header-section-number">9.2.2.1</span> Definition<a href="chap8.html#definition-25" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Within each Fréchet class <span class="math inline">\(\mathcal{F}\left( F_1, F_2 \right)\)</span>, two elements play a very particular role: they are the upper and lower Fréchet bounds, defined as follows.</p>
<div class="definition">
<p><span id="def:unlabeled-div-2" class="definition"><strong>Definition 9.2  </strong></span>The distribution function <span class="math inline">\(W\)</span> defined as
<span class="math display">\[
W(x_1, x_2) = \min \left\{ F_{1}(x_{1}), F_2(x_2) \right\},
\hspace{2mm} \boldsymbol{x}\in \mathbb{R}^{2},
\]</span>
is called the upper Fréchet bound in <span class="math inline">\(\mathcal{F}\left( F_1, F_2 \right)\)</span>. Similarly, the distribution function <span class="math inline">\(M\)</span> defined as
<span class="math display">\[
M(x_1, x_2) = \max \left\{ F_{1}(x_{1}) + F_2(x_2) - 1, 0 \right\},
\hspace{2mm} \boldsymbol{x}\in \mathbb{R}^{2},
\]</span>
is called the lower Fréchet bound in <span class="math inline">\(\mathcal{F}\left( F_1, F_2 \right)\)</span>.</p>
</div>
<p>The term “Fréchet bound” comes from the following result, which shows that any distribution function <span class="math inline">\(F_{\boldsymbol{X}}\)</span> in <span class="math inline">\(\mathcal{F}\left( F_1, F_2 \right)\)</span> is bounded from below by <span class="math inline">\(M\)</span> and from above by <span class="math inline">\(W\)</span>. The Fréchet bounds thus delimit the set <span class="math inline">\(\mathcal{F}\left( F_1, F_2 \right)\)</span>.</p>
<div class="proposition">
<p><span id="prp:Prop6.2.3" class="proposition"><strong>(#prp:Prop6.2.3) </strong></span>The Fréchet class <span class="math inline">\(\mathcal{F}\left( F_1, F_2 \right)\)</span> is bounded in the sense that for all <span class="math inline">\(F_{\boldsymbol{X}} \in \mathcal{F}\left( F_1, F_2 \right)\)</span>,
<span class="math display">\[
M(\boldsymbol{x}) \leq F_{\boldsymbol{X}}(\boldsymbol{x}) \leq W(\boldsymbol{x}) \text{ for any } \boldsymbol{x}\in \mathbb{R}^2.
\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-3" class="proof"><em>Proof</em>. </span>The announced result comes from the following inequality for any random events <span class="math inline">\(A_1\)</span> and <span class="math inline">\(A_2\)</span>:
<span class="math display">\[\begin{eqnarray}
\label{IneEvent2}
\max\Big\{{\Pr}[A_1] + {\Pr}[A_2] - 1, 0\Big\} &amp;\leq&amp; {\Pr}[A_1 \cap A_2] \nonumber \\
&amp;\leq&amp;
\min\Big\{{\Pr}[A_1], {\Pr}[A_2]\Big\}.
\end{eqnarray}\]</span>
The first of these inequalities comes from
<span class="math display">\[
1 \geq {\Pr}[A_1 \cup A_2] = {\Pr}[A_1] + {\Pr}[A_2] - {\Pr}[A_1 \cap A_2],
\]</span>
while the second is explained by the fact that <span class="math inline">\((A_1 \cap A_2) \subseteq A_1\)</span> and <span class="math inline">\((A_1 \cap A_2) \subseteq A_2\)</span>. Applying <span class="math inline">\(\eqref{IneEvent2}\)</span> with <span class="math inline">\(A_i = \{X_i \leq x_i\}\)</span>, <span class="math inline">\(i=1,2\)</span>, yields the desired result.</p>
</div>
<div class="remark">
<p><span id="FrechetNonVide" class="remark"><em>Remark</em>. </span>Regardless of the marginal distribution functions, the Fréchet classes <span class="math inline">\(\mathcal{F}\left( F_1, F_2 \right)\)</span> are never empty. To see this, it suffices to note that <span class="math inline">\(\mathcal{F}\left( F_1, F_2 \right)\)</span> always contains the distribution functions <span class="math inline">\(M\)</span>, <span class="math inline">\(W\)</span>, and <span class="math inline">\(F_1F_2\)</span>, for example. In reality, <span class="math inline">\(\mathcal{F}\left( F_1, F_2 \right)\)</span> always contains many elements. Indeed, for any <span class="math inline">\(\theta \in \left[ 0, 1 \right]\)</span>, and distribution functions <span class="math inline">\(F_{\boldsymbol{X}}\)</span> and <span class="math inline">\(F_{\boldsymbol{Y}}\)</span> in <span class="math inline">\(\mathcal{F}\left( F_1, F_2 \right)\)</span>, the distribution function
<span class="math display">\[
F_{\boldsymbol{Z}}(\boldsymbol{x}) = \theta F_{\boldsymbol{X}}(\boldsymbol{x}) + (1-\theta) F_{\boldsymbol{Y}}(\boldsymbol{x})
\]</span>
belongs to <span class="math inline">\(\mathcal{F}\left( F_1, F_2 \right)\)</span>. Using the Fréchet bounds of <span class="math inline">\(\mathcal{F}\left( F_1, F_2 \right)\)</span>, we can define the family of distribution functions
<span class="math display">\[\begin{equation*}
F_\theta(\boldsymbol{x})  = \theta M\left( \boldsymbol{x}\right) + \left( 1-\theta \right) W\left( \boldsymbol{x}\right), \hspace{2mm} \theta \in \left[ 0, 1 \right],
\end{equation*}\]</span>
all within <span class="math inline">\(\mathcal{F}\left( F_1, F_2 \right)\)</span>.</p>
<p>The distribution function <span class="math inline">\(F_1F_2\)</span> corresponding to independence is not part of the family we just defined (i.e., there is no value of <span class="math inline">\(\theta\)</span> such that <span class="math inline">\(F_\theta = F_1F_2\)</span>). To include it, we can propose the family
<span class="math display">\[\begin{eqnarray*}
F_\theta(\boldsymbol{x}) &amp; =&amp; \frac{1}{2}\theta^{2}(1-\theta)M\left( \boldsymbol{x}\right) \\
&amp;&amp; + \left( 1-\theta^{2} \right) F_1(x_1)F_2(x_2) \\
&amp;&amp; + \frac{1}{2}\theta^{2}(1+\theta)W\left( \boldsymbol{x}\right), \hspace{2mm} \theta \in [-1, 1],
\end{eqnarray*}\]</span>
known as the Mardia family, which includes the Fréchet bounds and independence as special cases. Specifically, we have <span class="math inline">\(M\)</span> for <span class="math inline">\(\theta=1\)</span>, <span class="math inline">\(W\)</span> for <span class="math inline">\(\theta=-1\)</span>, and <span class="math inline">\(F_1F_2\)</span> for <span class="math inline">\(\theta=0\)</span>.</p>
</div>
<p>One may wonder to which random variable pairs the Fréchet bounds in <span class="math inline">\({\mathcal{F}}(F_{1},F_{2})\)</span> correspond. The following result provides an answer to this question.</p>
<div class="proposition">
<p><span id="prp:Prop3B3" class="proposition"><strong>Proposition 9.1  </strong></span>Let <span class="math inline">\(U\sim\mathcal{U}ni(0,1)\)</span>. In <span class="math inline">\({\mathcal{F}}(F_{1},F_{2})\)</span>,</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(W_2\)</span> is the distribution function of the pair <span class="math inline">\((F_{1}^{-1}(U),F_{2}^{-1}(U))\)</span>.</li>
<li><span class="math inline">\(M_2\)</span> is the distribution function of the pair <span class="math inline">\((F_{1}^{-1}(U),F_{2}^{-1}(1-U))\)</span>.</li>
</ol>
<p>\end{Proposition}
::: {.proof}
For any <span class="math inline">\(\boldsymbol{x}\in {\mathbb{R}}^2\)</span>, we have
<span class="math display">\[\begin{eqnarray*}
&amp;&amp;{\Pr}[F_{1}^{-1}(U)\leq x_1,F_{2}^{-1}(U)\leq x_2]\\
&amp; = &amp;
{\Pr}[U\leq\min\{F_1(x_1),F_2(x_2)\}]
\\
&amp;=&amp;\Pr[U\leq F_1(x_1),U\leq F_2(x_2)]\\
&amp; = &amp;W_2(x_1,x_2),
\end{eqnarray*}\]</span>
and
<span class="math display">\[\begin{eqnarray*}
&amp;&amp;{\Pr}[F_{1}^{-1}(U)\leq x_1,F_{2}^{-1}(1-U)\leq x_2] \\
&amp; = &amp;
{\Pr}[U\leq F_1(x_1),1-U\leq F_2(x_2)]
\\
&amp; = &amp; M_2(x_1,x_2).
\end{eqnarray*}\]</span>
The announced result is thus established.</p>
</div>
<p>Proposition <span class="math inline">\(\ref{Prop3B3}\)</span> teaches us, in particular, that the support of <span class="math inline">\(W\)</span> is a non-decreasing curve in <span class="math inline">\({\mathbb{R}}^2\)</span>, while that of <span class="math inline">\(M\)</span> is a non-increasing curve in <span class="math inline">\({\mathbb{R}}^2\)</span>.</p>
</div>
</div>
<div id="perfect-dependence-comonotonicity-and-antimonotonicity" class="section level3 hasAnchor" number="9.2.3">
<h3><span class="header-section-number">9.2.3</span> Perfect Dependence: Comonotonicity and Antimonotonicity<a href="chap8.html#perfect-dependence-comonotonicity-and-antimonotonicity" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="definition-26" class="section level4 hasAnchor" number="9.2.3.1">
<h4><span class="header-section-number">9.2.3.1</span> Definition<a href="chap8.html#definition-26" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Perfect dependence is said to occur when two risks can be written as increasing or decreasing functions of the same underlying random variable.</p>
<div class="definition">
<p><span id="def:DefComonot" class="definition"><strong>Definition 9.3  </strong></span></p>
<ol style="list-style-type: decimal">
<li>The pair <span class="math inline">\(\boldsymbol{X}=(X_1,X_2)\)</span> is called comonotone if there exist non-decreasing functions <span class="math inline">\(g_1\)</span> and <span class="math inline">\(g_2\)</span> and a random variable <span class="math inline">\(Z\)</span> such that
<span class="math display">\[
\boldsymbol{X}\stackrel{d}{=}(g_1(Z),g_2(Z)).
\]</span></li>
<li>The pair <span class="math inline">\(\boldsymbol{X}=(X_1,X_2)\)</span> is called antimonotone if there exist a non-decreasing function <span class="math inline">\(g_1\)</span>, a non-increasing function <span class="math inline">\(g_2\)</span>, and a random variable <span class="math inline">\(Z\)</span> such that
<span class="math display">\[
\boldsymbol{X}\stackrel{d}{=}(g_1(Z),g_2(Z)).
\]</span></li>
</ol>
<p>\end{Definition}</p>
<p>Now that we have defined the concepts of perfect dependence, let’s examine some situations where comonotonicity and antimonotonicity naturally come into play.</p>
<div class="example">
<p><span id="exm:unlabeled-div-4" class="example"><strong>Example 9.1  </strong></span>Very often, a risk <span class="math inline">\(X\)</span> is divided into tranches and covered by different economic agents (insured, insurer, reinsurer, etc.). The interval <span class="math inline">\((a,a+h]\)</span> of width <span class="math inline">\(h\)</span> for the risk <span class="math inline">\(X\)</span> is defined as
<span class="math display">\[
X_{(a,a+h]}=\left\{
\begin{array}{l}
0\text{ if }0\leq X&lt;a\\
X-a\text{ if }a\leq X&lt;a+h\\
h\text{ if }a+h\leq X
\end{array}
\right.
\]</span>
where <span class="math inline">\(a\)</span> is called the retention. The tail distribution function of <span class="math inline">\(X_{(a,a+h]}\)</span> is given by
<span class="math display">\[
\overline{F}_{X_{(a,a+h]}}=\left\{
\begin{array}{l}
\overline{F}_X(a+t)\text{ if }t&lt;h\\
0\text{ if }t\geq h
\end{array}
\right.
\]</span>
so the pure premium for the coverage of the interval is
<span class="math display">\[\begin{eqnarray*}
\mathbb{E}[X_{(a,a+h]}]&amp;=&amp;\int_{t=0}^{+\infty}\overline{F}_{X_{(a,a+h]}}(t)dt\\
&amp;=&amp;\int_{t=0}^h\overline{F}_X(a+t)dt\\
&amp;=&amp;\int_{t=a}^{a+h}\overline{F}_X(t)dt.
\end{eqnarray*}\]</span></p>
<p>Now consider two intervals <span class="math inline">\((a,a+h)\)</span> and <span class="math inline">\((b,b+h)\)</span> of the same risk <span class="math inline">\(X\)</span> (whose distribution function is assumed to be continuous, for simplicity). The costs <span class="math inline">\(X_{(a,a+h]}\)</span> and <span class="math inline">\(X_{(b,b+h]}\)</span> are comonotonic because both are non-decreasing functions of <span class="math inline">\(X\)</span> (which satisfies Definition <span class="math inline">\(\ref{DefComonot}\)</span>(i)). Indeed,
<span class="math display">\[\begin{eqnarray*}
X_{(a,a+h]}&amp;=&amp;\min\{X-a,h\}\mathbb{I}[X&gt;a]=g_1(X)\\
X_{(b,b+h]}&amp;=&amp;\min\{X-b,h\}\mathbb{I}[X&gt;b]=g_2(X)
\end{eqnarray*}\]</span>
where <span class="math inline">\(g_1\)</span> and <span class="math inline">\(g_2\)</span> are non-decreasing functions.</p>
</div>
<p>::: {.example}[Comonotonic Exchange of Risks]</p>
<p>Consider a risk <span class="math inline">\(X\)</span> divided into
<span class="math display">\[
X_1=\left\{\begin{array}{l}
X,\text{ if } X\leq d,\\
d,\text{ otherwise},
\end{array}
\right.
\]</span>
covered by the insurer, and
<span class="math display">\[
X_2=\left\{\begin{array}{l}
0,\text{ if } X\leq d,\\
X-d,\text{ otherwise},
\end{array}
\right.
\]</span>
covered by the reinsurer in a stop-loss treaty. Since <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are increasing functions of risk <span class="math inline">\(X\)</span>, <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are comonotonic according to Definition <span class="math inline">\(\ref{DefComonot}\)</span>(i).</p>
<p>This is the most commonly encountered situation, with most risk exchanges leading to comonotonic variables. This ensures that both partners participating in the exchange will see their financial exposure increase when the underlying risk increases. More precisely, if we denote <span class="math inline">\(I:{\mathbb{R}}^+\rightarrow {\mathbb{R}}^+\)</span> as the indemnity function (i.e., <span class="math inline">\(I(x)\)</span> is the amount the insurer (in the broad sense) will have to pay if a loss of amount <span class="math inline">\(x\)</span> occurs), we require that <span class="math inline">\(I\)</span> be non-decreasing. This ensures that <span class="math inline">\(X\)</span> and <span class="math inline">\(I(X)\)</span> are comonotonic. If we further require that <span class="math inline">\(I\)</span> grows less rapidly than the identity (which is equivalent to demanding <span class="math inline">\(I&#39;\leq 1\)</span> when <span class="math inline">\(I\)</span> is differentiable), then <span class="math inline">\(X-I(X)\)</span> and <span class="math inline">\(X\)</span> are also comonotonic.</p>
<p>There are many examples of insurance policies that satisfy these constraints. These include</p>
<ol style="list-style-type: decimal">
<li>compulsory excess: <span class="math inline">\(I(x)=(x-\delta)_+\)</span> for an excess <span class="math inline">\(\delta\geq 0\)</span>;</li>
<li>coinsurance: <span class="math inline">\(I(x)=\alpha x\)</span> for <span class="math inline">\(\alpha\in[0,1]\)</span>;</li>
<li>intervention cap: <span class="math inline">\(I(x)=\min\{x,\omega\}\)</span> for <span class="math inline">\(\omega\geq 0\)</span>;</li>
<li>coverages combining several of these mechanisms, such as <span class="math inline">\(I(x)=\min\{\alpha(x-d_1)_+,d_2\}\)</span>.</li>
</ol>
</div>
<p>::: {.example}[Derivative Products in Finance]</p>
<p>Beautiful examples of comonotonicity and antimonotonicity are provided by modern stochastic finance. Let <span class="math inline">\(Z\)</span> be the price of a stock at time <span class="math inline">\(t\)</span>, and consider call and put options with a strike price <span class="math inline">\(K\)</span> and maturity <span class="math inline">\(t\)</span> on this stock. In this case, the value of the call option is
<span class="math display">\[
V_{\text{call}}=(Z-K)_+
\]</span>
and the value of the put option is
<span class="math display">\[
V_{\text{put}}=(K-Z)_+.
\]</span>
Returning to Definition <span class="math inline">\(\ref{DefComonot}\)</span>, it is easy to see that <span class="math inline">\(V_{\text{call}}\)</span> and <span class="math inline">\(Z\)</span> are comonotonic, while <span class="math inline">\(V_{\text{put}}\)</span> and <span class="math inline">\(Z\)</span> are antimonotonic, as are <span class="math inline">\(V_{\text{call}}\)</span> and <span class="math inline">\(V_{\text{put}}\)</span>.</p>
<p>:::</p>
<p>This situation of perfect dependence corresponds precisely to Fréchet bounds, as shown by the following result, to be related to Proposition <span class="math inline">\(\ref{Prop3B3}\)</span>.</p>
<div class="proposition">
<p><span id="prp:FunctLink" class="proposition"><strong>Proposition 9.2  </strong></span></p>
<ol style="list-style-type: decimal">
<li>The pair <span class="math inline">\(\boldsymbol{X}=(X_1,X_2)\)</span> is comonotonic if, and only if, it has <span class="math inline">\(W\)</span> as its distribution function.</li>
<li>The pair <span class="math inline">\(\boldsymbol{X}=(X_1,X_2)\)</span> is antimonotonic if, and only if, it has <span class="math inline">\(M\)</span> as its distribution function.</li>
</ol>
</div>
<p>In the case where the marginal distribution functions <span class="math inline">\(F_1\)</span> and <span class="math inline">\(F_2\)</span> are continuous, this result can be further strengthened using Property <span class="math inline">\(\ref{NombrAl}\)</span>.</p>
<div class="proposition">
<p><span id="prp:PropDh1" class="proposition"><strong>Proposition 9.3  </strong></span>Suppose <span class="math inline">\(F_{1}\)</span> and <span class="math inline">\(F_{2}\)</span> are continuous. Then,</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\boldsymbol{X}\)</span> is comonotonic if, and only if,
<span class="math display">\[
(X_1,X_2)=_{\text{loi}}(X_1,F_2^{-1}(F_1(X_1)));
\]</span></li>
<li><span class="math inline">\(\boldsymbol{X}\)</span> is antimonotonic if, and only if,
<span class="math display">\[
(X_1,X_2)=_{\text{loi}}(X_1,F_2^{-1}(\overline{F}_1(X_1))).
\]</span></li>
</ol>
</div>
<div class="proof">
<p><span id="unlabeled-div-5" class="proof"><em>Proof</em>. </span>We only prove (1), the reasoning leading to (2) being similar. Property <span class="math inline">\(\ref{NombrAl}\)</span> allows us to write
<span class="math display">\[\begin{eqnarray*}
&amp; &amp; {\Pr}\Big[X_1\leq x_1,F_2^{-1}\Big(F_1(X_1)\Big)\leq x_2\Big]\\&amp;=&amp;
{\Pr}\Big[X_1\leq F_1^{-1}\Big(F_1(x_1)\Big),F_2^{-1}\Big(F_1(X_1)\Big)\leq x_2\Big]\\
&amp;=&amp;
{\Pr}\Big[F_1(X_1)\leq F_1(x_1),F_1(X_1)\leq F_2(x_2)\Big]\\&amp;=&amp;W(x_1,x_2)
\end{eqnarray*}\]</span>
which completes the proof thanks to Proposition <span class="math inline">\(\ref{FunctLink}\)</span>.</p>
</div>
<p>The following result indicates that VaRs are additive in the case of comonotonic risks.</p>
<div class="proposition">
<p><span id="prp:AdditivityInv" class="proposition"><strong>Proposition 9.4  </strong></span>Let <span class="math inline">\(\boldsymbol{X}\)</span> be comonotonic, with its joint distribution function belonging to <span class="math inline">\(\mathcal{F}(F_1,F_2)\)</span>, where <span class="math inline">\(F_1\)</span> and <span class="math inline">\(F_2\)</span> are continuous and increasing. Then, for any probability level <span class="math inline">\(\alpha\in[0,1]\)</span>, we have
<span class="math display">\[
{\text{VaR}}[X_1+X_2;\alpha]={\text{VaR}}[X_1;\alpha]+{\text{VaR}}[X_2;\alpha]
\]</span>
\end{Proposition}
::: {.proof}
If <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are comonotonic, then according to Propositions
<span class="math inline">\(\ref{Prop3B3}\)</span> and <span class="math inline">\(\ref{PropDh1}\)</span>
<span class="math display">\[
X_{1}+X_{2}=_{\text{loi}}\Psi \left( U\right)
\]</span>
with <span class="math inline">\(U\sim\mathcal{U}ni(0,1)\)</span>, and the function <span class="math inline">\(\Psi\)</span> is given by
<span class="math display">\[
\Psi \left( u\right) =F_1^{-1}(u)+F_2^{-1}(u),\qquad 0\leq u\leq 1.
\]</span>
The function <span class="math inline">\(\Psi\)</span> defined in this way is clearly non-decreasing. For <span class="math inline">\(0&lt;p&lt;1\)</span>, it is sufficient to invoke Lemma <span class="math inline">\(\ref{InvGQuant}\)</span> to write
<span class="math display">\[
F_{X_1+X_2}^{-1}(p)=F_{\Psi \left( U\right) }^{-1}(p)=\Psi
\left( F_{U}^{-1}(p)\right) =\Psi (p),
\]</span>
as stated. It remains to deal with the limit cases, i.e., to verify that the result still holds for <span class="math inline">\(p=0\)</span> and <span class="math inline">\(p=1\)</span>. Indeed,
<span class="math display">\[
F_{X_1+X_2}^{-1}(1)=F_1^{-1}(1)+F_2^{-1}(1)
\]</span>
is correct since <span class="math inline">\({X_1+X_2}\)</span> will reach its maximum value if and only if each of the two terms reaches its maximum value, both being non-decreasing functions of the same variable <span class="math inline">\(Z\)</span>). This completes the proof.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-6" class="example"><strong>Example 9.2  </strong></span>Suppose <span class="math inline">\(X_1\sim\mathcal{E}xp(1/b_1)\)</span> and <span class="math inline">\(X_2\sim\mathcal{E}xp(1/b_2)\)</span> with <span class="math inline">\(b_1&gt;0\)</span> and <span class="math inline">\(b_2&gt;0\)</span>. If <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are comonotones, then the inverse of the tail function of <span class="math inline">\(X_1+X_2\)</span> is
<span class="math display">\[
\overline{F}_{X_1+X_2}^{-1}(p)=-b_\bullet\ln p\text{ with }b_\bullet=b_1+b_2,
\]</span>
i.e., <span class="math inline">\(X_1+X_2\sim\mathcal{E}xp(1/b_\bullet)\)</span>. In other words, the sum of two exponentially distributed and comonotonic random variables is also exponentially distributed.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-7" class="example"><strong>Example 9.3  </strong></span>Suppose <span class="math inline">\(X_1\sim\mathcal{P}ar(\alpha,\theta_1)\)</span> and <span class="math inline">\(X_2\sim\mathcal{P}ar(\alpha,\theta_2)\)</span>. If <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are comonotonic, the inverse of the tail function of their sum <span class="math inline">\(X_1+X_2\)</span> is
<span class="math display">\[
\overline{F}_{X_1+X_2}^{-1}(p)=\theta_\bullet(p^{-1/\alpha}-1)\text{ with }\theta_\bullet=\theta_1+\theta_2,
\]</span>
i.e., <span class="math inline">\(X_1+X_2\sim\mathcal{P}ar(\alpha,\theta_\bullet)\)</span>.</p>
</div>
</div>
</div>
</div>
<div id="measures-of-dependence" class="section level2 hasAnchor" number="9.3">
<h2><span class="header-section-number">9.3</span> Measures of Dependence<a href="chap8.html#measures-of-dependence" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="concept-2" class="section level3 hasAnchor" number="9.3.1">
<h3><span class="header-section-number">9.3.1</span> Concept<a href="chap8.html#concept-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Although common in insurance and finance, the term “correlation” is often misunderstood. While correlation is just a specific measure of dependence in statistics (often called linear correlation or Pearson correlation), practitioners tend to refer to all concepts of dependence with the same term.</p>
<p>In this section, we will study some measures of dependence. We will see that the coefficient of linear correlation, a canonical measure of dependence in the Gaussian world, loses much of its relevance when departing from this domain. For continuous variables, we will find that the Spearman and Kendall correlation coefficients provide good measures of dependence.</p>
<p>Before we proceed, it is important to define what is meant by a “good measure of dependence.” Probabilists quickly wondered what properties a measure of dependence should possess to be practical. This led to the definition of a concordance measure.</p>
<div class="definition">
<p><span id="def:DefMesConc" class="definition"><strong>Definition 9.4  </strong></span>A measure of dependence <span class="math inline">\(\delta(.,.)\)</span> is a concordance measure if it has the following desirable properties:</p>
<ul>
<li><strong>P1</strong> (Symmetry) <span class="math inline">\(\delta(X_1,X_2)=\delta(X_2,X_1)\)</span>;</li>
<li><strong>P2</strong> (Normalization) <span class="math inline">\(-1\leq\delta(X_1,X_2)\leq 1\)</span>;</li>
<li><strong>P3</strong> <span class="math inline">\(\delta(X_1,X_2)=1\)</span> if, and only if, <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are comonotonic;</li>
<li><strong>P4</strong> <span class="math inline">\(\delta(X_1,X_2)=-1\)</span> if, and only if, <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are antimonotonic;</li>
<li><strong>P5</strong> for any strictly monotonic function <span class="math inline">\(g:{\mathbb{R}}\to{\mathbb{R}}\)</span>,
<span class="math display">\[
\delta(g(X_1),X_2)=\left\{
\begin{array}{l}
\delta(X_1,X_2)\text{ if $g$ is increasing}\\
-\delta(X_1,X_2)\text{ if $g$ is decreasing}.
\end{array}
\right.
\]</span></li>
</ul>
</div>
<p>Initially, one may consider other desirable properties, but they may not necessarily be compatible with P1-P5. Another seemingly interesting property is
<span class="math display">\[\begin{equation}
\label{DesDep}
\delta(X_1,X_2)=0\Leftrightarrow X_1\text{ and }X_2\text{ are independent}.
\end{equation}\]</span>
Unfortunately, this property contradicts P5, as shown in the following result.</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-8" class="proposition"><strong>Proposition 9.5  </strong></span>There is no concordance measure satisfying (<span class="math inline">\(\ref{DesDep}\)</span>).</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-9" class="proof"><em>Proof</em>. </span>Consider the pair <span class="math inline">\((X_1,X_2)\)</span> uniformly distributed along the unit circle in the <span class="math inline">\({\mathbb{R}}^2\)</span> plane, i.e.,
<span class="math display">\[
(X_1, X_2)=(\cos Z,\sin Z)\text{ where }Z\sim\mathcal{U}ni[0,2\pi].
\]</span>
Since <span class="math inline">\((-X_1,X_2)\stackrel{\text{law}}{=}(X_1,X_2)\)</span>, we have
<span class="math display">\[
\delta(-X_1,X_2)=\delta(X_1,X_2)=-\delta(X_1,X_2),
\]</span>
which implies that <span class="math inline">\(\delta(X_1,X_2)=0\)</span> even though <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are clearly dependent (but not comonotonic!).</p>
</div>
</div>
<div id="linear-correlation-or-pearson-correlation" class="section level3 hasAnchor" number="9.3.2">
<h3><span class="header-section-number">9.3.2</span> Linear Correlation or Pearson Correlation<a href="chap8.html#linear-correlation-or-pearson-correlation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="covariance" class="section level4 hasAnchor" number="9.3.2.1">
<h4><span class="header-section-number">9.3.2.1</span> Covariance<a href="chap8.html#covariance" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In general, the variance of a sum is not simply the sum of variances (this is only true if the random variables involved are independent, as we saw in Property <span class="math inline">\(\ref{VarSomme}\)</span>). Indeed, the variability of a sum of random variables can be more or less than the simple aggregation of the variabilities of the individual terms, depending on how they interact. Let’s illustrate this. To do so, let’s focus on the variance of a sum of two random variables <span class="math inline">\(X_1+X_2\)</span>:
<span class="math display">\[\begin{eqnarray*}
\mathbb{V}[X_1+X_2]&amp;=&amp;\mathbb{E}\Big[\big\{X_1-\mathbb{E}[X_1]+X_2-\mathbb{E}[X_2]\big\}^2\Big]\\
&amp;=&amp;\mathbb{E}\big[(X_1-\mathbb{E}[X_1])^2\big]+\mathbb{E}\big[(X_2-\mathbb{E}[X_2])^2\big]\\
&amp; &amp; +2\mathbb{E}\Big[\big\{X_1-\mathbb{E}[X_1]\big\}\big\{X_2-\mathbb{E}[X_2]\big\}\Big]\\
&amp;=&amp;\mathbb{V}[X_1]+\mathbb{V}[X_2]+2\mathbb{C}[X_1,X_2].
\end{eqnarray*}\]</span>
The variance of a sum is equal to the sum of the variances of the two terms plus the expectation of the product of the centered variables (which disappears when <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent). This latter expectation is called covariance (as it expresses how <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> “co-vary”) and is defined as follows.</p>
<div class="definition">
<p><span id="def:unlabeled-div-10" class="definition"><strong>Definition 9.5  </strong></span>The covariance between <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>, denoted as <span class="math inline">\(\mathbb{C}[X_1,X_2]\)</span>, is defined as
<span class="math display">\[\begin{eqnarray*}
\mathbb{C}[X_1,X_2] &amp;=&amp;\mathbb{E}\big[ \left( X_1-\mathbb{E}[X_1]\right) \left( X_2-\mathbb{E}[ X_2] \right) \big]\\
&amp;=&amp;\mathbb{E}[X_1X_2] -\mathbb{E}[ X_1] \mathbb{E}[X_2] .
%\\
%&amp;=&amp;\int \int \left[ F\left( x,y\right) -F^{\bot }\left( x,y\right)
%\right] dxdy
\end{eqnarray*}\]</span></p>
</div>
<p>Thus,
<span class="math display">\[\begin{equation}
\label{VarSom}
\mathbb{V}[X_1+X_2]=\mathbb{V}[X_1]+\mathbb{V}[X_2]+2\mathbb{C}[X_1,X_2]
\end{equation}\]</span>
and it is the covariance that quantifies the excess variability (or reduction of it) of the sum of two random variables compared to the sum of their variances.</p>
<div class="remark">
<p><span id="unlabeled-div-11" class="remark"><em>Remark</em>. </span>It is worth noting that <span class="math inline">\(\mathbb{V}[X]=\mathbb{C}[X,X]\)</span>, so the variance of a random variable measures how it “co-varies” with itself.</p>
</div>
<p>::: {.example}[Covariance between components of a normal vector]</p>
<p>If <span class="math inline">\(\boldsymbol{X}\sim\mathcal{N}or(\boldsymbol{\mu},\boldsymbol{\Sigma})\)</span>, then it can be easily verified that the off-diagonal elements <span class="math inline">\(\sigma_{ij}\)</span> of <span class="math inline">\(\boldsymbol{\Sigma}\)</span> are the covariances between pairs of components of <span class="math inline">\(\boldsymbol{X}\)</span>, i.e.,
<span class="math display">\[
\sigma_{ij}=\mathbb{C}[X_i,X_j]\text{ for }i\neq j.
\]</span>
:::</p>
<p>::: {.example}[Covariance between components of a multinomial vector]
Consider the vector <span class="math inline">\(\boldsymbol{N}\sim\mathcal{M}ult(m,p_1,\ldots,p_n)\)</span>. Since <span class="math inline">\(N_i\sim\mathcal{B}in(m,p_i)\)</span> and <span class="math inline">\(N_i+N_j\sim\mathcal{B}in(m,p_i+p_j)\)</span> for any <span class="math inline">\(i\neq j\)</span>, we can deduce from <span class="math inline">\(\eqref{VarSom}\)</span> that
<span class="math display">\[\begin{eqnarray*}
\mathbb{C}[N_i,N_j]&amp;=&amp;\frac{1}{2}\Big(\mathbb{V}[N_i+N_j]-\mathbb{V}[N_i]-\mathbb{V}[N_j]\Big)\\
&amp;=&amp;\frac{1}{2}\Big(m(p_i+p_j)(1-p_i-p_j)-mp_i(1-p_i)-mp_j(1-p_j)\Big)\\
&amp;=&amp;-mp_ip_j.
\end{eqnarray*}\]</span>
:::</p>
<p>If <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent, their covariance is zero. However, the converse is not true, as demonstrated in the following example.</p>
<div class="example">
<p><span id="exm:unlabeled-div-12" class="example"><strong>Example 9.4  </strong></span>
A zero covariance either indicates no dependence between <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> or a non-linear relationship. For example, if <span class="math inline">\(X\sim\mathcal{N}or(0,1)\)</span>,
<span class="math display">\[
\mathbb{C}[X,X^2]=\mathbb{E}[X^3]-\mathbb{E}[X]\mathbb{E}[X^2]=0
\]</span>
even though <span class="math inline">\(X\)</span> and <span class="math inline">\(X^2\)</span> are strongly dependent. Note that <span class="math inline">\(X\)</span> and <span class="math inline">\(X^2\)</span> are not comonotonic!</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-13" class="remark"><em>Remark</em>. </span>A null covariance is closely related to the concept of orthogonality. Indeed, the space <span class="math inline">\(L^2\)</span> of square-integrable random variables is a Hilbert space equipped with an inner product for which <span class="math inline">\(\mathbb{C}[X_1,X_2]=0\)</span> reflects orthogonality in <span class="math inline">\(L^2\)</span>.</p>
</div>
<p>Just as we defined conditional expectation and conditional variance, we can introduce the concept of conditional covariance.</p>
<div class="definition">
<p><span id="def:unlabeled-div-14" class="definition"><strong>Definition 9.6  </strong></span>For a random vector <span class="math inline">\(\boldsymbol{\Theta}\)</span> with <span class="math inline">\(m\)</span> dimensions, the conditional covariance of random variables <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> given <span class="math inline">\(\boldsymbol{\Theta}\)</span> is the random variable
<span class="math display">\[\begin{eqnarray*}
\mathbb{C}[X_1,X_2|\boldsymbol{\Theta}]&amp;=&amp;\mathbb{E}\Big[\big\{X_1-\mathbb{E}[X_1|\boldsymbol{\Theta}]\big\}
\big\{X_2-\mathbb{E}[X_2|\boldsymbol{\Theta}]\big\}\Big|\boldsymbol{\Theta}\Big]\\
&amp;=&amp;\mathbb{E}[X_1X_2|\boldsymbol{\Theta}]-\mathbb{E}[X_1|\boldsymbol{\Theta}]\mathbb{E}[X_2|\boldsymbol{\Theta}].
\end{eqnarray*}\]</span></p>
</div>
<p>Furthermore, conditional covariance enjoys the following properties.</p>
<div class="proof">
<p><span id="unlabeled-div-15" class="proof"><em>Proof</em>. </span>We will only prove (i). It suffices to write
<span class="math display">\[\begin{eqnarray*}
\mathbb{C}[X_1,X_2] &amp; = &amp; \mathbb{E}\Big[\mathbb{E}\big[(X_1-\mathbb{E}[X_1])(X_2-\mathbb{E}[X_2])\big|\boldsymbol{\Theta}\big]\Big]\\
&amp;=&amp;\mathbb{E}\Big[\mathbb{E}\big[(X_1-\mathbb{E}[X_1|\boldsymbol{\Theta}]+\mathbb{E}[X_1|\boldsymbol{\Theta}]-\mathbb{E}[X_1])\\
&amp;&amp;\hspace{10mm}
(X_2-\mathbb{E}[X_2|\boldsymbol{\Theta}]+\mathbb{E}[X_2|\boldsymbol{\Theta}]-\mathbb{E}[X_2])\big|\boldsymbol{\Theta}\big]\Big]\\
&amp;=&amp;\mathbb{E}\Big[\mathbb{E}\big[(X_1-\mathbb{E}[X_1|\boldsymbol{\Theta}])(X_2-\mathbb{E}[X_2|\boldsymbol{\Theta}])\big|\boldsymbol{\Theta}\big]\Big]\\
&amp;&amp;+\mathbb{E}\Big[\underbrace{\mathbb{E}\big[X_1-\mathbb{E}[X_1|\boldsymbol{\Theta}]|\boldsymbol{\Theta}]}_{=0}(\mathbb{E}[X_2|\boldsymbol{\Theta}]-\mathbb{E}[X_2])\Big]
\\
&amp;&amp;+\mathbb{E}\Big[\underbrace{\mathbb{E}\big[X_2-\mathbb{E}[X_2|\boldsymbol{\Theta}]|\boldsymbol{\Theta}]}_{=0}(\mathbb{E}[X_1|\boldsymbol{\Theta}]-\mathbb{E}[X_1])\Big]\\
&amp;&amp;+\mathbb{E}\Big[(\mathbb{E}[X_1|\boldsymbol{\Theta}]-\mathbb{E}[X_1])(\mathbb{E}[X_2|\boldsymbol{\Theta}]-\mathbb{E}[X_2])\Big]
\\
&amp;=&amp;\mathbb{E}\Big[\mathbb{C}[X_1,X_2|\boldsymbol{\Theta}]\Big]+
\mathbb{C}\Big[\mathbb{E}[X_1|\boldsymbol{\Theta}],\mathbb{E}[X_2|\boldsymbol{\Theta}]\Big].
\end{eqnarray*}\]</span></p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-16" class="example"><strong>Example 9.5  </strong></span>Suppose that, conditional on <span class="math inline">\(\Theta=\theta\)</span>, <span class="math inline">\(N_1\)</span> and <span class="math inline">\(N_2\)</span> are independently distributed with respective Poisson distributions <span class="math inline">\(\mathcal{P}oi(\lambda_1\theta)\)</span> and <span class="math inline">\(\mathcal{P}oi(\lambda_2\theta)\)</span>. Then, in accordance with Property <span class="math inline">\(\ref{PropCovCond}\)</span>(i), we have
<span class="math display">\[
\mathbb{C}[N_1,N_2]=\mathbb{E}\Big[\mathbb{C}[N_1,N_2|\Theta]\Big]
+\mathbb{C}\Big[\mathbb{E}[N_1|\Theta],\mathbb{E}[N_2|\Theta]\Big].
\]</span>
Now,
<span class="math display">\[
\mathbb{C}[N_1,N_2|\Theta]=0
\]</span>
due to Property <span class="math inline">\(\ref{PropCovCond}\)</span>(ii), and thus
<span class="math display">\[
\mathbb{C}[N_1,N_2]=\mathbb{C}[\lambda_1\Theta,\lambda_2\Theta]=\lambda_1\lambda_2\mathbb{V}[\Theta]
\]</span>
which, for example, reduces to <span class="math inline">\(\lambda_1\lambda_2/a\)</span> when <span class="math inline">\(\Theta\sim\mathcal{G}am(a,a)\)</span>.</p>
</div>
<p>Covariance, by its sign, informs us about the direction of “co-variation” between <span class="math inline">\(X_1\)</span> and $X_2,” but its absolute value does not tell us about the magnitude of this co-variation. Indeed, the value of covariance is sensitive to the inherent variability of both variables <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>. In particular, covariance can change drastically if we change the units of measurement for <span class="math inline">\(X_1\)</span> and/or <span class="math inline">\(X_2\)</span> (switching from thousands to millions of euros for <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> would divide the covariance by <span class="math inline">\(10^6\)</span>!).</p>
<p>Correlation aims to correct this drawback of covariance by providing an index that does not depend on units of measurement or the individual variability of the variables.</p>
<div class="definition">
<p><span id="def:unlabeled-div-17" class="definition"><strong>Definition 9.7  </strong></span>The linear correlation coefficient between <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>, denoted as <span class="math inline">\(r(X_1,X_2)\)</span>, is defined as
<span class="math display">\[
r\left(X_1,X_2\right) =\frac{\mathbb{C}[X_1,X_2]}{\sqrt{\mathbb{V}[ X_1] \mathbb{V}[X_2]}}.
\]</span></p>
</div>
<p>We observe that this is a dimensionless number that exists only when the variances and covariance that compose it are well-defined. The sign of the covariance coincides with that of the correlation. Therefore, what was mentioned earlier about the sign of the covariance is always applicable to the sign of the correlation.</p>
<div class="remark">
<p><span id="unlabeled-div-18" class="remark"><em>Remark</em>. </span>It is easy to see that <span class="math inline">\(r(X_1, X_2)\)</span> is, in fact, the covariance between the two centered and standardized variables, i.e.,
<span class="math display">\[
r(X_1, X_2) = \text{Cov}[V_1, V_2],
\]</span>
where
<span class="math display">\[
V_1 = \frac{X_1 - \mathbb{E}[X_1]}{\sqrt{\text{Var}[X_1]}} \text{ and } V_2 = \frac{X_2 - \mathbb{E}[X_2]}{\sqrt{\text{Var}[X_2]}}.
\]</span></p>
</div>
<p>For any random variables <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>, we always have
<span class="math display">\[
-1 \leq r(X_1, X_2) \leq 1
\]</span>
according to the Cauchy-Schwarz inequality. The bounds +1 and -1 are reached when <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are related by a linear relationship. More precisely, if <span class="math inline">\(X_2 \sim a + bX_1\)</span> with <span class="math inline">\(b \neq 0\)</span>, then the correlation is 1 in absolute value, and it has the sign of <span class="math inline">\(b\)</span>. The converse is also true.</p>
<p>Contrary to what one might think, there are many situations where the linear correlation coefficient cannot reach the bounds -1 and 1, as demonstrated in the following example.</p>
<div class="example">
<p><span id="exm:unlabeled-div-19" class="example"><strong>Example 9.6  </strong></span>Let <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> be two random variables with the same support in <span class="math inline">\({\mathbb{R}}^+\)</span>. Then, <span class="math inline">\(r(X_1, X_2) &gt; -1\)</span>. To establish this result, let us reason by contradiction and assume that <span class="math inline">\(r(X_1, X_2) = -1\)</span>, which implies
<span class="math display">\[
X_2 \sim aX_1 + b \text{ with } a &lt; 0, \hspace{2mm} b \in {\mathbb{R}}.
\]</span>
It follows then for any arbitrary <span class="math inline">\(x_2 &lt; 0\)</span>,
<span class="math display">\[\begin{align*}
F_2(x_2) = \Pr[aX_1 + b \leq x_2] &amp; = \Pr\left[X_1 \geq \frac{x_2 - b}{a}\right] \\
&amp; \geq \Pr\left[X_1 &gt; \frac{x_2 - b}{a}\right] \\
&amp; = \overline{F}_1\left(\frac{x_2 - b}{a}\right) &gt; 0,
\end{align*}\]</span>
which clearly contradicts the assumption <span class="math inline">\(F_2(0) = 0\)</span>.</p>
</div>
<p>The following technical lemma will be very useful in obtaining the possible values for the linear correlation coefficient. It can be seen as a two-dimensional generalization of Property <span class="math inline">\(\ref{RepEsp}\)</span>.</p>
<div class="lemma">
<p><span id="lem:TechLem" class="lemma"><strong>Lemma 9.1  </strong></span>For any random variables <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> with joint distribution function <span class="math inline">\(F_{\boldsymbol{X}}\)</span>, we have
<span class="math display">\[
\mathbb{E}[X_1X_2] = \int_{x_1=0}^{+\infty}\int_{x_2=0}^{+\infty} \overline{F}_{\boldsymbol{X}}(x_1,x_2)dx_1dx_2.
\]</span>
\end{Lemma}
::: {.proof}
Let us start by writing
<span class="math display">\[\begin{align*}
&amp;\int_{x_1=0}^{+\infty}\int_{x_2=0}^{+\infty} \Pr[X_1&gt;x_1,X_2&gt;x_2]dx_1dx_2\\
&amp;= \int_{x_1=0}^{+\infty}\int_{x_2=0}^{+\infty}\int_{y_1=x_1}^{+\infty}\int_{y_2=x_2}^{+\infty} dF_{\boldsymbol{X}}(y_1,y_2)dx_1dx_2.
\end{align*}\]</span>
Now, let’s invoke Fubini’s theorem to interchange the integrals, yielding
<span class="math display">\[\begin{align*}
&amp;\int_{x_1=0}^{+\infty}\int_{x_2=0}^{+\infty} \Pr[X_1&gt;x_1,X_2&gt;x_2]dx_1dx_2\\
&amp;= \int_{x_1=0}^{+\infty}\int_{x_2=0}^{+\infty}\int_{y_1=x_1}^{+\infty}\int_{y_2=x_2}^{+\infty} dF_{\boldsymbol{X}}(y_1,y_2)dx_1dx_2\\
&amp;= \int_{y_1=0}^{+\infty}\int_{y_2=0}^{+\infty}\int_{x_1=0}^{y_1}\int_{x_2=0}^{y_2} dx_1dx_2dF_{\boldsymbol{X}}(y_1,y_2)\\
&amp;= \int_{y_1=0}^{+\infty}\int_{y_2=0}^{+\infty}y_1y_2dF_{\boldsymbol{X}}(y_1,y_2) = \mathbb{E}[X_1X_2],
\end{align*}\]</span>
which completes the proof.</p>
</div>
<p>This representation of the covariance portrays it as a distance between the joint distribution function <span class="math inline">\(F_{\boldsymbol{X}}\)</span> of <span class="math inline">\(\boldsymbol{X}\)</span> and that of a pair with the same marginal distributions as <span class="math inline">\(\boldsymbol{X}\)</span> but with independent components.</p>
<p>We deduce from Corollary <span class="math inline">\(\ref{TchenIn}\)</span> that for any pair <span class="math inline">\(\boldsymbol{X}\)</span>, the following inequalities hold:
<span class="math display">\[\begin{align*}
&amp; \int_{x_1=0}^{+\infty}\int_{x_2=0}^{+\infty} \Big\{M(x_1,x_2)-F_1(x_1)F_2(x_2)\Big\}dx_1dx_2\\
&amp; = \mathbb{C}[F_1^{-1}(U),F_2^{-1}(1-U)]\\
&amp; \leq \mathbb{C}[X_1,X_2]\\
&amp; \leq \int_{x_1=0}^{+\infty}\int_{x_2=0}^{+\infty} \Big\{W(x_1,x_2)-F_1(x_1)F_2(x_2)\Big\}dx_1dx_2\\
&amp; = \mathbb{C}[F_1^{-1}(U),F_2^{-1}(U)].
\end{align*}\]</span></p>
</div>
</div>
<div id="values-of-the-linear-correlation-coefficient" class="section level3 hasAnchor" number="9.3.3">
<h3><span class="header-section-number">9.3.3</span> Values of the Linear Correlation Coefficient<a href="chap8.html#values-of-the-linear-correlation-coefficient" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If we define
<span class="math display">\[
r_{\min} = \frac{\mathbb{C}[F_1^{-1}(U),F_2^{-1}(1-U)]}
{\sqrt{\mathbb{V}[X_1]\mathbb{V}[X_2]}}
\]</span>
and
<span class="math display">\[
r_{\max} = \frac{\mathbb{C}[F_1^{-1}(U),F_2^{-1}(U)]}
{\sqrt{\mathbb{V}[X_1]\mathbb{V}[X_2]}}
\]</span>
then the linear correlation coefficient <span class="math inline">\(r(X_1,X_2)\)</span> satisfies the inequalities
<span class="math display">\[\begin{equation}
\label{InCoeffCorr}
r_{\min}\leq
r(X_1,X_2)
\leq r_{\max},
\end{equation}\]</span>
so that a value of <span class="math inline">\(\pm 1\)</span> is generally not admissible.</p>
<p>We show in the following result that the extreme values of the linear correlation coefficient characterize comonotonicity and antimonotonicity.</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-20" class="proposition"><strong>Proposition 9.6  </strong></span>Let <span class="math inline">\(\boldsymbol{X}\)</span> have a distribution function <span class="math inline">\(F_{\boldsymbol{X}}\)</span> in <span class="math inline">\(\mathcal{F}(F_1,F_2)\)</span>. Then,</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(r(X_1,X_2)=r_{\max}\)</span> if, and only if, <span class="math inline">\(\boldsymbol{X}\)</span> is comonotonic.</li>
<li><span class="math inline">\(r(X_1,X_2)=r_{\min}\)</span> if, and only if, <span class="math inline">\(\boldsymbol{X}\)</span> is antimonotonic.</li>
</ol>
</div>
<div class="proof">
<p><span id="unlabeled-div-21" class="proof"><em>Proof</em>. </span>We establish only (1), with the reasoning leading to (2) being similar. Corollary <span class="math inline">\(\ref{TchenIn}\)</span> shows that under the conditions of (1)
<span class="math display">\[
0=\int_{x_1=0}^{+\infty}\int_{x_2=0}^{+\infty}
\Big\{W(x_1,x_2)-F_{\boldsymbol{X}}(x_1,x_2)\Big\}dx_1dx_2.
\]</span>
Since the integrand is everywhere non-negative, the integral’s nullity implies <span class="math inline">\(F_{\boldsymbol{X}}=W\)</span>, from which the result follows by virtue of Proposition <span class="math inline">\(\ref{Prop3B3}\)</span>.</p>
</div>
<p>The following example illustrates the possible values of the linear correlation coefficient for a pair of log-normally distributed random variables.</p>
<div class="example">
<p><span id="exm:unlabeled-div-22" class="example"><strong>Example 9.7  </strong></span>
Let <span class="math inline">\(X_1\sim \mathcal{LN}or(0,1)\)</span> and <span class="math inline">\(X_2\sim \mathcal{LN}or (0,\sigma ^{2})\)</span>. The extreme values for the linear correlation coefficient are achieved when <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are perfectly dependent. Thus,
<span class="math display">\[\begin{align*}
r_{\max}(\sigma)&amp;=r(\exp (Z),\exp(\sigma Z))\\
&amp;=\frac{\exp(\sigma)-1}{\sqrt{\exp(\sigma^2)-1}\sqrt{e-1}}
\end{align*}\]</span>
where <span class="math inline">\(Z\sim \mathcal{N}or(0,1)\)</span>, and
<span class="math display">\[\begin{align*}
r_{\min}(\sigma)&amp;=r(\exp (Z),\exp (-\sigma Z))\\
&amp;=\frac{\exp(-\sigma)-1}{\sqrt{\exp(\sigma^2)-1}\sqrt{e-1}}.
\end{align*}\]</span>
These bounds are plotted as functions of <span class="math inline">\(\sigma\)</span> in Figure <span class="math inline">\(\ref{ExtPearson}\)</span>.
It can be seen that
<span class="math display">\[
\lim_{\sigma\to +\infty}r_{\max}(\sigma)=\lim_{\sigma\to +\infty}r_{\min}(\sigma)=0.
\]</span>
Therefore, it is possible to have a pair <span class="math inline">\(\boldsymbol{X}\)</span> with a nearly zero correlation coefficient even though its components are perfectly dependent. This clearly contradicts the intuition that low values of the correlation coefficient imply low dependence.</p>
</div>
<p>::: {.example}[Exchangeability and Correlation]</p>
<p>Random variables <span class="math inline">\(X_{1},...,X_{n}\)</span> are said to be exchangeable if, for any permutation $$ of ${
1,…,n}
$,%
<span class="math display">\[\begin{equation*}
\left( X_{1},...,X_{n}\right) =_{\text{law}}\left(X_{\pi \left( 1\right) },...,X_{\pi \left( n\right)
}\right).
\end{equation*}\]</span>%</p>
<p>It can be noted that if random variables <span class="math inline">\(X_{1},...,X_{n}\)</span> are independent and identically distributed, then they are exchangeable. However, the converse is false: if <span class="math inline">\(Y\)</span> is a symmetric variable, and if $( X_{1},X_{2})=(Y,-Y) $, then the pair $( X_{1},X_{2}) $ is exchangeable, but <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are not independent and identically distributed. The concept of exchangeability is very important for modeling homogeneous risks, for example.</p>
<p>Consider random variables <span class="math inline">\(X_{1},...,X_{n}\)</span> that are exchangeable.
Then, denoting <span class="math inline">\(\sigma ^{2}=\mathbb{V}[X_{i}]\)</span> and <span class="math inline">\(\rho =r(X_{i},X_{j})\)</span> for <span class="math inline">\(i\neq j\)</span>, we have
<span class="math display">\[\begin{equation*}
0\leq \mathbb{V}[ X_{1}+...+X_{n}] =n\sigma ^{2}+n\left(n-1\right) \rho \sigma ^{2}.
\end{equation*}\]</span>
As a result, the correlation $$ of a pair of exchangeable variables satisfies
<span class="math display">\[
\rho \geq \frac{-1}{ n-1}.
\]</span>
This bound can be achieved by considering Gaussian vectors.
:::</p>
<p>Despite its drawbacks, the correlation coefficient remains widely used in practice for three major reasons:</p>
<p>The linear correlation coefficient naturally appears as a parameter in the density of Gaussian vectors.</p>
<p>The coefficients <span class="math inline">\(a^*\)</span> and <span class="math inline">\(b^*\)</span> that minimize <span class="math inline">\(\mathbb{E}[(X_2-aX_1-b)^2]\)</span> are given by
<span class="math display">\[
a^*=\frac{\mathbb{C}[X_1,X_2]}{\mathbb{V}[X_1]}\text{ and }b^*=\mathbb{E}[X_2]-a^*\mathbb{E}[X_1].
\]</span>
The relation
<span class="math display">\[
r^2(X_1,X_2)=\frac{\mathbb{V}[X_2]-\min_{a,b}\mathbb{E}[(X_2-aX_1-b)^2]}{\mathbb{V}[X_2]}
\]</span>
shows that the square of the correlation coefficient represents the proportion of variability (measured by the sum of squares) in <span class="math inline">\(X_2\)</span> that can be explained by a linear function of <span class="math inline">\(X_1\)</span>.</p>
<p>The linear correlation coefficient appears as a fundamental factor in the portfolio theory introduced by Markowitz in 1952. Indeed, if we denote <span class="math inline">\(R_{M}\)</span> as the market return, <span class="math inline">\(R_{i}\)</span> as the return of the <span class="math inline">\(i\)</span>th risky asset, and <span class="math inline">\(R_{0}\)</span> as the risk-free rate, then the CAPM theory allows us to write
<span class="math display">\[\begin{equation*}
\mathbb{E}[R_{i}-R_0] =\beta _{i}.\mathbb{E}[R_{M}-R_0] \text{ where }\beta _{i}=\frac{\mathbb{C}[
R_{i},R_{M}]}{\mathbb{V}[R_{M}] }
\end{equation*}\]</span>
This coefficient <span class="math inline">\(\beta _{i}\)</span> is then directly related to the correlation between the return of asset <span class="math inline">\(i\)</span> and the average market return.</p>
</div>
<div id="kendalls-rank-correlation-coefficient" class="section level3 hasAnchor" number="9.3.4">
<h3><span class="header-section-number">9.3.4</span> Kendall’s Rank Correlation Coefficient<a href="chap8.html#kendalls-rank-correlation-coefficient" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="definition-27" class="section level4 hasAnchor" number="9.3.4.1">
<h4><span class="header-section-number">9.3.4.1</span> Definition<a href="chap8.html#definition-27" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>As reasonable and intuitive as it may seem, one must be cautious not to blindly trust the linear correlation coefficient <span class="math inline">\(r\)</span>. Indeed, while it is the canonical measure of dependence in the multivariate Gaussian world, we have seen above that it loses much of its relevance as soon as we leave this ideal world.</p>
<p>To address the issues encountered with the linear correlation coefficient, we can turn to rank correlation coefficients, such as Kendall’s tau and Spearman’s rho. These measures of dependence are based on the concordances and discordances observed among collected data. Given two pairs <span class="math inline">\((X_1,X_2)\)</span> and <span class="math inline">\((X_1&#39;,X_2&#39;)\)</span>, independently and identically distributed, Kendall’s tau is defined as the probability of “concordance” (i.e., the probability that we simultaneously have <span class="math inline">\(X_1&lt;X_1&#39;\)</span> and <span class="math inline">\(X_2&lt;X_2&#39;\)</span>, or <span class="math inline">\(X_1&gt;X_1&#39;\)</span> and <span class="math inline">\(X_2&gt;X_2&#39;\)</span>) minus the probability of “discordance” (i.e., the probability that we simultaneously have <span class="math inline">\(X_1&gt;X_1&#39;\)</span> and <span class="math inline">\(X_2&lt;X_2&#39;\)</span>, or <span class="math inline">\(X_1&gt;X_1&#39;\)</span> and <span class="math inline">\(X_2&lt;X_2&#39;\)</span>). This leads to the following definition.</p>
<div class="definition">
<p><span id="def:DefTau" class="definition"><strong>Definition 9.8  </strong></span>Kendall’s tau associated with the pair <span class="math inline">\((X_1, X_2)\)</span> of random variables with continuous marginal distribution functions is defined as follows:
<span class="math display">\[\begin{eqnarray*}
\tau(X_1, X_2) &amp; = &amp; \Pr\left[(X_1-X_1&#39;)(X_2-X_2&#39;) &gt; 0\right]\\
&amp;&amp; - \Pr\left[(X_1-X_1&#39;)(X_2-X_2&#39;) &lt; 0\right],
\end{eqnarray*}\]</span>
where <span class="math inline">\((X_1&#39;, X_2&#39;)\)</span> is independent of <span class="math inline">\((X_1, X_2)\)</span> and has the same distribution as the latter.</p>
</div>
<p>Kendall’s tau can also be expressed as follows:</p>
<div class="proof">
<p><span id="unlabeled-div-23" class="proof"><em>Proof</em>. </span>Starting from the Definition <span class="math inline">\(\ref{DefTau}\)</span> of Kendall’s tau, we can write:
<span class="math display">\[\begin{eqnarray*}
\tau(X_1, X_2) &amp; = &amp; 2\Pr\left[(X_1-X_1&#39;)(X_2-X_2&#39;) &gt; 0\right] - 1\\
&amp;=&amp; 2\left\{\Pr\left[X_1 &lt; X_1&#39;, X_2 &lt; X_2&#39;\right] + \Pr\left[X_1 &gt; X_1&#39;, X_2 &gt; X_2&#39;\right]\right\} - 1\\
&amp; = &amp; 2\left\{\mathbb{E}\left[F_{\boldsymbol{X}}(X_1&#39;, X_2&#39;)\right] + \mathbb{E}\left[F_{\boldsymbol{X}}(X_1, X_2)\right]\right\} - 1,
\end{eqnarray*}\]</span>
which completes the proof.</p>
</div>
<p>Kendall’s tau enjoys the property of functional invariance (as it is based on the ranks of observations rather than their values).</p>
<p>In particular, Kendall’s tau satisfies Property P5 of Definition <span class="math inline">\(\ref{DefMesConc}\)</span> of concordance measures. Indeed, when <span class="math inline">\(g\)</span> is increasing, this is an immediate consequence of <span class="math inline">\(\eqref{FunctInvTau}\)</span>. If <span class="math inline">\(g\)</span> is decreasing, we have:
<span class="math display">\[\begin{eqnarray*}
\tau(g(X_1), X_2) &amp; = &amp; 2\Pr\left[(g(X_1)-g(X_1&#39;))(X_2-X_2&#39;) &gt; 0\right] - 1\\
&amp;=&amp; 2\Pr\left[(X_1-X_1&#39;)(X_2-X_2&#39;) &lt; 0\right] - 1\\
&amp;=&amp; 2\left\{1 - \Pr\left[(X_1-X_1&#39;)(X_2-X_2&#39;) &gt; 0\right]\right\} - 1\\
&amp;=&amp; -\tau(X_1, X_2).
\end{eqnarray*}\]</span></p>
<p>It is easy to see that Kendall’s tau takes its values in the interval <span class="math inline">\([-1, 1]\)</span> and is defined regardless of the distributions involved (the restriction to finite variance distributions no longer applies to this measure of dependence). The values <span class="math inline">\(\pm 1\)</span> can be achieved regardless of the marginals, and they characterize perfect dependence (thus, Kendall’s tau satisfies Properties P3 and P4 of the Definition <span class="math inline">\(\ref{DefMesConc}\)</span> of concordance measures).</p>
<div class="proof">
<p><span id="unlabeled-div-24" class="proof"><em>Proof</em>. </span>In accordance with Proposition <span class="math inline">\(\ref{PropDh1}\)</span>(i), it suffices to show that the maximum value of 1 is achieved when <span class="math inline">\(X_2=g(X_1)\)</span> with <span class="math inline">\(g\)</span> non-decreasing (<span class="math inline">\(g\)</span> is the function <span class="math inline">\(F_2^{-1}\circ F_1\)</span>). This follows from:
<span class="math display">\[
\tau\left(X_1, g(X_1)\right) = 2\Pr\left[\left(X_1-X_1&#39;\right)\left(g(X_1)-g(X_1&#39;)\right) &gt; 0\right] - 1 = 1,
\]</span>
since <span class="math inline">\(X_1-X_1&#39;\)</span> and <span class="math inline">\(g(X_1)-g(X_1&#39;)\)</span> obviously always have the same sign. Similarly, the value -1 for Kendall’s tau is achieved when <span class="math inline">\(X_2=g(X_1)\)</span> with <span class="math inline">\(g\)</span> non-increasing.</p>
<p>Conversely, if <span class="math inline">\(\tau(X_1, X_2) = 1\)</span>, then the distribution function is <span class="math inline">\(W\)</span>. Indeed, the equality:
<span class="math display">\[
\tau(X_1, X_2) = \tau(U_1, U_2) \text{ where } U_1=F_1(X_1) \text{ and } U_2=F_2(X_2),
\]</span>
is valid, and we can, in accordance with Property <span class="math inline">\(\ref{NombrAl}\)</span>, assume without loss of generality that the marginals <span class="math inline">\(F_1\)</span> and <span class="math inline">\(F_2\)</span> are <span class="math inline">\(\mathcal{U}ni(0,1)\)</span>. We must therefore demonstrate the implication:
<span class="math display">\[
\tau(U_1, U_2) = 1 \Rightarrow (U_1, U_2) =_{\text{law}} (U, U) \text{ where } U\sim\mathcal{U}ni(0,1).
\]</span>
Let <span class="math inline">\(C\)</span> be the distribution function of <span class="math inline">\((U_1, U_2)\)</span> and <span class="math inline">\(C_U\)</span> those of <span class="math inline">\((U, U)\)</span>; Proposition <span class="math inline">\(\ref{Prop6.2.3}\)</span> ensures that <span class="math inline">\(C\leq C_U\)</span>. The following inequalities hold:
<span class="math display">\[
\mathbb{E}[C(U_1, U_2)]\leq \mathbb{E}[C_U(U_1, U_2)]\leq \mathbb{E}[C_U(U, U)] = \frac{1}{2},
\]</span>
since <span class="math inline">\(\min\{U_1, U_2\}\leq U_1\)</span>. We thus have the equivalence:
<span class="math display">\[
\tau(U_1, U_2) = \tau(U, U) \Leftrightarrow \mathbb{E}[C(U_1, U_2)] = \mathbb{E}[C_U(U, U)]
\]</span>
which in turn implies:
<span class="math display">\[
\mathbb{E}[C_U(U_1, U_2)] - \mathbb{E}[C(U_1, U_2)] = \int_{u_1=0}^1\int_{u_2=0}^1\left\{C_U(u_1, u_2) - C(u_1, u_2)\right\}dC(u_1, u_2) = 0,
\]</span>
allowing us to conclude that <span class="math inline">\(C=C_U\)</span>.</p>
<p>Similarly, let <span class="math inline">\(C_L\)</span> be the distribution function of the pair <span class="math inline">\((U, 1-U)\)</span>; Proposition <span class="math inline">\(\ref{Prop6.2.3}\)</span> ensures that <span class="math inline">\(C_L\leq C\)</span>. We can reason as above starting from the inequalities:
<span class="math display">\[
\mathbb{E}[C_L(U, 1-U)]\leq \mathbb{E}[C_L(U_1, U_2)]\leq \mathbb{E}[C(U_1, U_2)].
\]</span></p>
</div>
<p>So we have established that Kendall’s tau is indeed a measure of concordance. We can also note that if <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent, then <span class="math inline">\(\tau(X_1, X_2) = 0\)</span>. Indeed,
<span class="math display">\[\begin{eqnarray*}
\tau(X_1, X_2) &amp; = &amp; 2\Pr\left[(X_1-X_1&#39;)(X_2-X_2&#39;) &gt; 0\right] - 1
\\
&amp; = &amp; 2\left\{\Pr\left[X_1-X_1&#39; &gt; 0, X_2-X_2&#39; &gt; 0\right]
+ \Pr\left[X_1-X_1&#39; &lt; 0, X_2-X_2&#39; &lt; 0\right]\right\} - 1
\\
&amp; = &amp; 2\left\{\frac{1}{4} + \frac{1}{4}\right\} - 1 = 0.
\end{eqnarray*}\]</span></p>
</div>
</div>
<div id="spearmans-rank-correlation-coefficient" class="section level3 hasAnchor" number="9.3.5">
<h3><span class="header-section-number">9.3.5</span> Spearman’s Rank Correlation Coefficient<a href="chap8.html#spearmans-rank-correlation-coefficient" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="definition-28" class="section level4 hasAnchor" number="9.3.5.1">
<h4><span class="header-section-number">9.3.5.1</span> Definition<a href="chap8.html#definition-28" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Just like Kendall’s tau, Spearman’s rho is based on the concepts of concordance and discordance.</p>
<div class="definition">
<p><span id="def:unlabeled-div-25" class="definition"><strong>Definition 9.9  </strong></span>Consider the pair <span class="math inline">\(\boldsymbol{X}\)</span> of continuous marginal distribution functions <span class="math inline">\(F_1\)</span> and <span class="math inline">\(F_2\)</span> and define <span class="math inline">\(\boldsymbol{X}^\perp = (X_1^\perp, X_2^\perp)\)</span> as an independent version of <span class="math inline">\(\boldsymbol{X}\)</span> (i.e., <span class="math inline">\(\boldsymbol{X}^\perp\)</span> has the joint distribution function <span class="math inline">\(F_1F_2\)</span>). Spearman’s rho is then defined as three times the difference between the probabilities of concordance and discordance of <span class="math inline">\(\boldsymbol{X}\)</span> and <span class="math inline">\(\boldsymbol{X}^\perp\)</span>, i.e.,
<span class="math display">\[\begin{eqnarray*}
\rho(X_1, X_2) &amp; = &amp; 3\left\{\Pr\left[(X_1-X_1&#39;)(X_2-X_2&#39;) &gt; 0\right]
\right.
\\
&amp;&amp; \left. - \Pr\left[(X_1-X_1&#39;)(X_2-X_2&#39;) &lt; 0\right]\right\}.
\end{eqnarray*}\]</span></p>
</div>
<p>Spearman’s rank correlation coefficient addresses the shortcomings of Pearson’s linear correlation coefficient by considering not the original variables <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>, but the uniform versions <span class="math inline">\(F_1(X_1)\)</span> and <span class="math inline">\(F_2(X_2)\)</span>. When dealing with random variables having a linear relationship between them, extreme values <span class="math inline">\(\pm 1\)</span> can be achieved.</p>
<p>We are now able to define the supermodular order, which will allow us to compare the intensity of dependence between the components of random couples.</p>
<div class="definition">
<p><span id="def:DefSm" class="definition"><strong>Definition 9.10  </strong></span>The vector <span class="math inline">\({\boldsymbol{X}}\)</span> is said to be inferior to the vector <span class="math inline">\({\boldsymbol{Y}}\)</span> in the supermodular sense, which will now be denoted as <span class="math inline">\({\boldsymbol{X}}{\preceq_{\text{sm}}}{\boldsymbol{Y}}\)</span>, when the inequality <span class="math inline">\(\mathbb{E}[g({\boldsymbol{X}})]\leq \mathbb{E}[g({\boldsymbol{Y}})]\)</span> holds for all supermodular functions <span class="math inline">\(g:{\mathbb{R}}^2\to {\mathbb{R}}\)</span> such that expectations exist.</p>
</div>
<p>The stochastic inequality <span class="math inline">\({\boldsymbol{X}}{\preceq_{\text{sm}}}{\boldsymbol{Y}}\)</span> should be understood as “the components of <span class="math inline">\({\boldsymbol{X}}\)</span> are less dependent than those of <span class="math inline">\({\boldsymbol{Y}}\)</span>,” meaning that <span class="math inline">\(F_{\boldsymbol{Y}}\)</span>, on average, assigns more probability mass to points located on the upward diagonal of all rectangles in the plane than <span class="math inline">\(F_{\boldsymbol{X}}\)</span>.</p>
<p>Directly verifying the conditions of Definition <span class="math inline">\(\ref{DefSm}\)</span> is generally difficult. However, the following result, which we will accept without proof, allows us to restrict ourselves to regular supermodular functions, as defined in Property <span class="math inline">\(\ref{PropSM}\)</span>(i).</p>
</div>
</div>
<div id="functional-stability-of-supermodular-comparisons" class="section level3 hasAnchor" number="9.3.6">
<h3><span class="header-section-number">9.3.6</span> Functional Stability of Supermodular Comparisons<a href="chap8.html#functional-stability-of-supermodular-comparisons" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Having two random couples such that <span class="math inline">\(\boldsymbol{X}\preceq_{\text{sm}}\boldsymbol{Y}\)</span>, i.e., the components <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span> are more dependent than <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>, the same judgment holds for any monotonic transformation of the components of these couples, as shown by the following result.</p>
<div class="proof">
<p><span id="unlabeled-div-26" class="proof"><em>Proof</em>. </span>The result is easily obtained from Property <span class="math inline">\(\ref{PropSM}\)</span>(ii). Indeed, for any supermodular function <span class="math inline">\(g\)</span>, define the function
<span class="math display">\[
\Psi(x_1,x_2)=g\big(f_1(x_1),f_2(x_2)\big).
\]</span>
Then, we can write
<span class="math display">\[\begin{eqnarray*}
\mathbb{E}\Big[g\big(f_1(X_1),f_2(X_2)\big)\Big]&amp;=&amp;\mathbb{E}[\Psi(X_1,X_2)]\\
&amp;\leq &amp;\mathbb{E}[\Psi(X_1,X_2)]\\
&amp;&amp;\text{ because $\Psi$ is supermodular and $\boldsymbol{X}\preceq_{\text{sm}}\boldsymbol{Y}$}\\
&amp;=&amp;\mathbb{E}\Big[g\big(f_1(Y_1),f_2(Y_2)\big)\Big],
\end{eqnarray*}\]</span>
which completes the proof.</p>
</div>
</div>
<div id="supermodular-comparison-and-fréchet-space" class="section level3 hasAnchor" number="9.3.7">
<h3><span class="header-section-number">9.3.7</span> Supermodular Comparison and Fréchet Space<a href="chap8.html#supermodular-comparison-and-fréchet-space" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The functions <span class="math inline">\(g_1\)</span> and <span class="math inline">\(g_2\)</span> defined by
<span class="math display">\[
g_1(\boldsymbol{y})= \mathbb{I}[{\boldsymbol{y}}&gt;{\boldsymbol{x}}]\text{ and }
g_2(\boldsymbol{y})= \mathbb{I}[{\boldsymbol{y}}\le{\boldsymbol{x}}]
\]</span>
are both supermodular for any fixed <span class="math inline">\({\boldsymbol{x}}\)</span>, and it is easy to see that
<span class="math display">\[\begin{equation}
\label{eq9.A.16}
{\boldsymbol{X}}\preceq_{\text{sm}}{\boldsymbol{Y}}\Rightarrow\left\{
\begin{array}{l}
\mathbb{E}[g_1(\boldsymbol{X})]=\Pr[\boldsymbol{X}&gt;\boldsymbol{x}]\leq\mathbb{E}[g_1(\boldsymbol{Y})]=\Pr[\boldsymbol{Y}&gt;\boldsymbol{x}],\\
\hspace{10mm}\text{ for all }\boldsymbol{x}\in{\mathbb{R}}^2,\\
\mathbb{E}[g_2(\boldsymbol{X})]=\Pr[\boldsymbol{X}\leq\boldsymbol{x}]\leq\mathbb{E}[g_2(\boldsymbol{Y})]=\Pr[\boldsymbol{Y}\leq\boldsymbol{x}],\\
\hspace{10mm}\text{ for all }\boldsymbol{x}\in{\mathbb{R}}^2,\\
\end{array}
\right.
\end{equation}\]</span>
It also follows from <span class="math inline">\(\eqref{eq9.A.16}\)</span> that
<span class="math display">\[
\boldsymbol{X}\preceq_{\text{sm}}\boldsymbol{Y}\Rightarrow X_i=_{\text{law}}Y_i\text{ for }i=1,2.
\]</span>
So, the relation <span class="math inline">\(\preceq_{\text{sm}}\)</span> can only be used within the same Fréchet space.</p>
</div>
<div id="supermodular-comparison-and-joint-distributiontail-functions" class="section level3 hasAnchor" number="9.3.8">
<h3><span class="header-section-number">9.3.8</span> Supermodular Comparison and Joint Distribution/Tail Functions<a href="chap8.html#supermodular-comparison-and-joint-distributiontail-functions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The following result shows that in fact, <span class="math inline">\(\eqref{eq9.A.16}\)</span> characterizes <span class="math inline">\(\preceq_{\text{sm}}\)</span>.</p>
<p>\begin{Characterization}
Let <span class="math inline">\(\boldsymbol{X}=(X_1,X_2)\)</span> and <span class="math inline">\(\boldsymbol{Y}=(Y_1,Y_2)\)</span> whose distribution functions are part of the same Fréchet space <span class="math inline">\(\mathcal{F}(F_1,F_2)\)</span>. Then,
<span class="math display">\[\begin{equation}
\label{eq9.A.3}
\boldsymbol{X}\preceq_{\text{sm}}\boldsymbol{Y}\Leftrightarrow
F_{\boldsymbol{X}}(x_1,x_2)\le F_{\boldsymbol{Y}}(x_1,x_2),\text{ for all }\boldsymbol{x}\in{\mathbb{R}}^2,
\end{equation}\]</span>
or equivalently
<span class="math display">\[\begin{equation}
\label{eq9.A.4}
\boldsymbol{X}\preceq_{\text{sm}}\boldsymbol{Y}\Leftrightarrow
\overline{F}_{\boldsymbol{X}}(x_1,x_2)\le\overline{F}_{\boldsymbol{Y}}(x_1,x_2),\text{ for all }\boldsymbol{x}\in{\mathbb{R}}^2.
\end{equation}\]</span>
end{Characterization}
::: {.proof}
Let <span class="math inline">\(g:{\mathbb{R}}^2\to{\mathbb{R}}\)</span> satisfy <span class="math inline">\(\eqref{RegSuperm}\)</span>. Integration by parts then gives
<span class="math display">\[\begin{eqnarray*}
&amp;&amp;\mathbb{E}[g(\boldsymbol{Y})]-\mathbb{E}[g(\boldsymbol{X})]\\
&amp;=&amp;
\int\int_{\boldsymbol{x}\in{\mathbb{R}}^2}g(\boldsymbol{x})d\Big\{F_{\boldsymbol{Y}}(\boldsymbol{x})-F_{\boldsymbol{X}}(\boldsymbol{x})\Big\}\\
&amp;=&amp;
\int\int_{\boldsymbol{x}\in{\mathbb{R}}^2}\frac{\partial^2}{\partial x_1\partial x_2}g(\boldsymbol{x})
\Big\{F_{\boldsymbol{Y}}(\boldsymbol{x})-F_{\boldsymbol{X}}(\boldsymbol{x})\Big\}d\boldsymbol{x}
\end{eqnarray*}\]</span>
which completes the proof.
:::</p>
<div class="remark">
<p><span id="unlabeled-div-27" class="remark"><em>Remark</em>. </span>It is important at this stage to emphasize that Characterization <span class="math inline">\(\ref{CorrelSm}\)</span> is specific to dimension 2 (in the sense that the reverse of implication <span class="math inline">\(\eqref{eq9.A.16}\)</span> is not valid in dimension 3 and higher).</p>
</div>
</div>
<div id="extreme-structures-of-supermodular-dependence" class="section level3 hasAnchor" number="9.3.9">
<h3><span class="header-section-number">9.3.9</span> Extreme Structures of Supermodular Dependence<a href="chap8.html#extreme-structures-of-supermodular-dependence" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The Fréchet bounds correspond to the strongest supermodular dependence structures, as shown by the following result, which immediately follows from Characterization <span class="math inline">\(\ref{CorrelSm}\)</span>.</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-28" class="proposition"><strong>Proposition 9.7  </strong></span>In <span class="math inline">\(\mathcal{F}(F_1,F_2)\)</span>, we have
<span class="math display">\[
(F_1^{-1}(U),F_2^{-1}(1-U))\preceq_{\text{sm}}\boldsymbol{X}\preceq_{\text{sm}}(F_1^{-1}(U),F_2^{-1}(U))
\]</span>
where <span class="math inline">\(U\sim\mathcal{U}ni(0,1)\)</span>.</p>
</div>
</div>
<div id="supermodular-comparison-and-correlation-coefficients" class="section level3 hasAnchor" number="9.3.10">
<h3><span class="header-section-number">9.3.10</span> Supermodular Comparison and Correlation Coefficients<a href="chap8.html#supermodular-comparison-and-correlation-coefficients" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The following result shows that supermodular comparison of multivariate normal distributions boils down to comparing the covariance matrices.</p>
<p>::: {.example}[Gaussian Vectors]
Let <span class="math inline">\(\boldsymbol{X}\sim \mathcal{N}or(\boldsymbol{\mu}_{\boldsymbol{X}},\boldsymbol{\Sigma}_{\boldsymbol{X}})\)</span> and <span class="math inline">\(\boldsymbol{Y}\sim \mathcal{N}or(\boldsymbol{\mu}_{\boldsymbol{Y}},\boldsymbol{\Sigma}_{\boldsymbol{Y}})\)</span>. Then,
<span class="math display">\[
\boldsymbol{X}\preceq_{\text{sm}}\boldsymbol{Y}\Leftrightarrow \mathbb{C}[X_1,X_2]\leq\mathbb{C}[Y_1,Y_2].
\]</span>
This comes from the following representation valid for <span class="math inline">\(g\)</span> satisfying <span class="math inline">\(\eqref{RegSuperm}\)</span>:
<span class="math display">\[\begin{eqnarray}
\label{normaleq}
&amp;&amp;\mathbb{E}[g(\boldsymbol{Y})] - \mathbb{E}[g(\boldsymbol{X})]  \nonumber\\
&amp;=&amp;\int\limits_0^1 \!\int\int_{\boldsymbol{x}\in{\mathbb{R}}^2} \! \Big( (\boldsymbol{\mu}_{\boldsymbol{Y}} -
\boldsymbol{\mu}_{\boldsymbol{X}})^\top \nabla g(\boldsymbol{x}) \nonumber\\
&amp;&amp;\hspace{15mm}+ \frac{1}{2} tr\big((\boldsymbol{\Sigma}_{\boldsymbol{Y}} -
\boldsymbol{\Sigma}_{\boldsymbol{X}})\boldsymbol{H}_g(\boldsymbol{x})\big) \Big) \cdot \phi_\lambda(\boldsymbol{x}) \
d\boldsymbol{x}\ d\lambda,
\end{eqnarray}\]</span>bda,
\end{eqnarray}
where <span class="math inline">\(\nabla g\)</span> and <span class="math inline">\(\boldsymbol{H}_g\)</span> denote the gradient vector and Hessian matrix associated with the function <span class="math inline">\(g\)</span>, <span class="math inline">\(tr(\boldsymbol{A})\)</span> is the trace of matrix <span class="math inline">\(\boldsymbol{A}\)</span> (i.e., the sum of the diagonal elements of <span class="math inline">\(\boldsymbol{A}\)</span>), and <span class="math inline">\(\phi_\lambda\)</span> is the density associated with the distribution <span class="math inline">\(\mathcal{N}or(\lambda \boldsymbol{\mu}_{\boldsymbol{Y}} + (1-\lambda) \boldsymbol{\mu}_{\boldsymbol{X}} ,\lambda \boldsymbol{\Sigma}_{\boldsymbol{Y}} + (1-\lambda) \boldsymbol{\Sigma}_{\boldsymbol{X}}), \ 0 \le \lambda \le 1\)</span>.</p>
<p>If <span class="math inline">\(\boldsymbol{X}\preceq_{\text{sm}}\boldsymbol{Y}\)</span>, then the means and variances of the components of <span class="math inline">\(\boldsymbol{X}\)</span> and <span class="math inline">\(\boldsymbol{Y}\)</span> are the same, and <span class="math inline">\(\eqref{normaleq}\)</span> reduces to
<span class="math display">\[\begin{eqnarray*}
&amp;&amp;\mathbb{E}[g(\boldsymbol{Y})] - \mathbb{E}[g(\boldsymbol{X})]\\
&amp;=&amp;\int\limits_0^1 \!\int\int_{\boldsymbol{x}\in{\mathbb{R}}^2} \! \left(
\frac{1}{2} tr((\boldsymbol{\Sigma}_{\boldsymbol{Y}} -
\boldsymbol{\Sigma}_{\boldsymbol{X}})\boldsymbol{H}_g(\boldsymbol{x})) \right) \cdot \phi_\lambda(\boldsymbol{x}) \
d\boldsymbol{x}\ d\lambda,
\end{eqnarray*}\]</span>da,
\end{eqnarray*}
which proves the announced result.
:::</p>
<p>However, this result does not have general applicability. Aside from the Gaussian case and the case of binary variables (see Exercise <span class="math inline">\(\ref{DepBinaires}\)</span>), supermodular comparison generally requires more than just comparing covariances. Supermodular comparison, however, never contradicts the usual correlation coefficients, as shown by the following result.</p>
<div class="proof">
<p><span id="unlabeled-div-29" class="proof"><em>Proof</em>. </span>The stated result follows from
<span class="math display">\[\begin{eqnarray*}
&amp;&amp;\Pr[f_1(X_1)&gt;x_1,f_2(X_2)&gt;x_2] \\
&amp; = &amp;
\Pr[X_1&gt;f_1^{-1}(x_1),X_2&gt;f_2^{-1}(x_2)] \\
&amp; \geq &amp; \Pr[X_1&gt;f_1^{-1}(x_1)]\Pr[X_2&gt;f_2^{-1}(x_2)] \\
&amp; = &amp; \Pr[f_1(X_1)&gt;x_1]\Pr[f_2(X_2)&gt;x_2],
\end{eqnarray*}\]</span>
which completes the proof.</p>
</div>
<p>Positive quadrant dependence can be detected by a positive correlation coefficient, as shown in the following result, which can be easily deduced from Property <span class="math inline">\(\ref{SmCorrel}\)</span> (remembering the interpretation of positive quadrant dependence in terms of supermodular comparison).</p>
<p>We know that in general, non-correlation and independence are not equivalent (just take a look at Remark <span class="math inline">\(\ref{CorrelNIndep}\)</span> to see this). However, these two notions become equivalent under well-chosen dependence structures, such as the one discussed in this section.</p>
<div class="proof">
<p><span id="unlabeled-div-30" class="proof"><em>Proof</em>. </span>Let’s show the equivalence between (i) and (ii). The nullity of the linear correlation coefficient is equivalent to the nullity of the covariance, which allows us to write, thanks to Corollary <span class="math inline">\(\ref{TchenIn}\)</span>,
<span class="math display">\[
0=\int_{x_1=0}^{+\infty}\int_{x_2=0}^{+\infty}
\Big\{\overline{F}_{\boldsymbol{X}}(x_1,x_2)-\overline{F}_1(x_1)\overline{F}_2(x_2)\Big\}dx_1dx_2.
\]</span>
Since the integrand is everywhere non-negative when <span class="math inline">\(\boldsymbol{X}\)</span> has positive quadrant dependence, we deduce that <span class="math inline">\(\overline{F}_{\boldsymbol{X}}=\overline{F}_1\overline{F}_2\)</span>, which means that <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent.</p>
<p>Now, let’s move on to the equivalence between (i) and (iii). Since <span class="math inline">\(\boldsymbol{X}\)</span> is positively quadrant dependent, it dominates <span class="math inline">\(\boldsymbol{X}^\perp\)</span> in the <span class="math inline">\(\preceq_{\text{sm}}\)</span> sense. This allows us to write the chain of inequalities:
<span class="math display">\[
\mathbb{E}\big[F_1(X_1^\perp)F_2(X_2^\perp)\big]\leq
\mathbb{E}\big[F_{\boldsymbol{X}}(X_1^\perp,X_2^\perp)\big]\leq
\mathbb{E}\big[F_{\boldsymbol{X}}(X_1,X_2)\big].
\]</span>
The nullity of Kendall’s tau then guarantees
<span class="math display">\[
\mathbb{E}\big[F_1(X_1^\perp)F_2(X_2^\perp)\big]=
\mathbb{E}\big[F_{\boldsymbol{X}}(X_1,X_2)\big]
\]</span>
which in turn implies
<span class="math display">\[
0=\int_{x_1=0}^{+\infty}\int_{x_2=0}^{+\infty}
\Big\{{F}_{\boldsymbol{X}}(x_1,x_2)-{F}_1(x_1){F}_2(x_2)\Big\}dF_1(x_1)dF_2(x_2).
\]</span>
The desired result then follows directly from this last equality (the integrand is everywhere non-negative, and the integral is zero).</p>
<p>Similarly, we can show the equivalence between (i) and (iv). The nullity of Spearman’s rho also guarantees
<span class="math display">\[
0=\int_{x_1=0}^{+\infty}\int_{x_2=0}^{+\infty}
\Big\{{F}_{\boldsymbol{X}}(x_1,x_2)-{F}_1(x_1){F}_2(x_2)\Big\}dF_1(x_1)dF_2(x_2),
\]</span>
from which the desired result follows as above.</p>
</div>
<p>The following result is easily deduced from Properties <span class="math inline">\(\ref{PQDFuncInv}\)</span> and <span class="math inline">\(\ref{Prop1.1.11bis}\)</span>.</p>
<div class="proposition">
<p><span id="prp:PQDbis" class="proposition"><strong>Proposition 9.8  </strong></span>The pair <span class="math inline">\(\boldsymbol{X}\)</span> has positive quadrant dependence if, and only if, the inequality
<span class="math display">\[
\mathbb{C}[g_1(X_1),g_2(X_2)]\geq 0
\]</span>
holds for all non-decreasing functions <span class="math inline">\(g_1\)</span> and <span class="math inline">\(g_2\)</span> such that the covariance exists.</p>
</div>
<p>This result sheds light on the link between covariance and positive quadrant dependence: for <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> to have positive quadrant dependence, not only must the covariance between <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> be positive, but the same must hold for the covariance between any increasing transformation of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>.</p>
<p>As mentioned earlier, if <span class="math inline">\(\boldsymbol{X}\)</span> has positive quadrant dependence, it dominates <span class="math inline">\(\boldsymbol{X}^\perp\)</span> in the supermodular sense. Returning to Proposition <span class="math inline">\(\ref{SMDecideurs}\)</span>, it is not difficult to establish the following result.</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-31" class="proposition"><strong>Proposition 9.9  </strong></span>If risks <span class="math inline">\(X_{1}\)</span> and <span class="math inline">\(X_{2}\)</span> have positive quadrant dependence, then <span class="math inline">\(X_{1}^{\perp }+X_{2}^{\perp }\preceq_{\text{TVaR,=}}X_{1}\,+\,X_{2}\)</span>.</p>
</div>
<p>If the actuary neglects the dependence between the risks and calculates the stop-loss premium retention <span class="math inline">\(d\)</span> associated with the sum <span class="math inline">\(X_{1}\,+\,X_{2}\)</span> as if they were independent, they will underestimate this quantity. The same holds for any quantity that can be expressed in the form <span class="math inline">\(\mathbb{E}[g(\boldsymbol{X})]\)</span> with <span class="math inline">\(g\)</span> supermodular.</p>
<p>Returning to (<span class="math inline">\(\ref{JoeEq2.1}\)</span>) and (<span class="math inline">\(\ref{JoeEq2.2}\)</span>), it is easy to verify that when <span class="math inline">\(\boldsymbol{X}\)</span> has positive quadrant dependence, the random variables <span class="math inline">\(\min\{X_1,X_2\}\)</span>, <span class="math inline">\(\min\{X_1^\perp,X_2^\perp\}\)</span>, <span class="math inline">\(\max\{X_1,X_2\}\)</span>, and <span class="math inline">\(\max\{X_1^\perp,X_2^\perp\}\)</span> are comparable in terms of <span class="math inline">\(\preceq_{\text{VaR}}\)</span>.</p>
<div class="proof">
<p><span id="unlabeled-div-32" class="proof"><em>Proof</em>. </span>It suffices to note that
<span class="math display">\[\begin{eqnarray*}
\Pr[\min \{X_1,X_2\}&gt;t]&amp;=&amp;\overline{F}_{\boldsymbol{X}}(t,t)\\
&amp;\geq &amp;\overline{F}_1(t)\overline{F}_2(t)\\
&amp;=&amp;\Pr[\min \{X_1^\perp,X_2^\perp\}&gt;t].
\end{eqnarray*}\]</span>
Similarly,
<span class="math display">\[\begin{eqnarray*}
\Pr[\max\{X_1,X_2\}\leq t]&amp;=&amp;{F}_{\boldsymbol{X}}(t,t)\\
&amp;\geq &amp;{F}_1(t){F}_2(t)\\
&amp;=&amp;\Pr[\max \{X_1^\perp,X_2^\perp\}\leq t],
\end{eqnarray*}\]</span>
which completes the proof.</p>
</div>
</div>
<div id="association" class="section level3 hasAnchor" number="9.3.11">
<h3><span class="header-section-number">9.3.11</span> Association<a href="chap8.html#association" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="definition-29" class="section level4 hasAnchor" number="9.3.11.1">
<h4><span class="header-section-number">9.3.11.1</span> Definition<a href="chap8.html#definition-29" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In order to define a notion of positive dependence between two risks <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>, one might think of using <span class="math inline">\(\mathbb{C}[X_1,X_2]\geq 0\)</span> or <span class="math inline">\(\mathbb{C}[g_1(X_1),g_2(X_2)]\geq 0\)</span> for any pair of increasing functions <span class="math inline">\(g_1\)</span> and <span class="math inline">\(g_2\)</span>. In the latter case, we recover the concept of positive quadrant dependence. A more restrictive condition would be to consider covariances of the form <span class="math inline">\(\mathbb{C}[\Psi_1(X_1,X_2),\Psi_2(X_1,X_2)]\)</span> for non-decreasing functions <span class="math inline">\(\Psi_1\)</span> and <span class="math inline">\(\Psi_2\)</span>, and demand non-negativity of these covariances. This leads to the notion of association.</p>
<div class="definition">
<p><span id="def:DefAssoc" class="definition"><strong>Definition 9.11  </strong></span>Random variables <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are said to be associated when
<span class="math display">\[\begin{equation}
\label{assoc}
\mathbb{C}\big[\Psi_1(X_1,X_2),\Psi_2(X_1,X_2)\big]\geq 0
\end{equation}\]</span>
for any non-decreasing functions <span class="math inline">\(\Psi_1\)</span> and <span class="math inline">\(\Psi_2:{\mathbb{R}}^2\to{\mathbb{R}}\)</span> such that the covariance exists.</p>
</div>
<p>Returning to Proposition <span class="math inline">\(\ref{PQDbis}\)</span>, we see that association is a stronger notion than positive quadrant dependence, in the sense that
<span class="math display">\[
\boldsymbol{X}\text{ associated}\Rightarrow\boldsymbol{X}\text{ positively dependent by quadrant}.
\]</span>
Therefore, the results established for risks with positive quadrant dependence are still valid when they are associated.</p>
<p>Independence appears as a limiting case of association, as shown by the following result.</p>
<div class="proof">
<p><span id="unlabeled-div-33" class="proof"><em>Proof</em>. </span>We use Property <span class="math inline">\(\ref{PropCovCond}\)</span>(i) to write
<span class="math display">\[\begin{eqnarray*}
&amp;&amp;\mathbb{C}\big[\Psi_1(X_1,X_2),\Psi_2(X_1,X_2)\big]\\
&amp;=&amp;\mathbb{E}\Big[\mathbb{C}\big[\Psi_1(X_1,X_2),\Psi_2(X_1,X_2)\big|X_2\big]\Big]\\
&amp;&amp;\mathbb{C}\Big[\mathbb{E}\big[\Psi_1(X_1,X_2)\big|X_2\big],\mathbb{E}\big[\Psi_1(X_1,X_2)\big|X_2\big]\Big].
\end{eqnarray*}\]</span>
The first term is positive due to Example <span class="math inline">\(\ref{PQDXX}\)</span> and Proposition <span class="math inline">\(\ref{PQDbis}\)</span>. The functions
<span class="math display">\[
x_2\mapsto \mathbb{E}\big[\Psi_i(X_1,X_2)\big|X_2=x_2\big],\hspace{2mm}i=1,2,
\]</span>
are both non-decreasing. Therefore, the random variables <span class="math inline">\(\mathbb{E}\big[\Psi_1(X_1,X_2)\big|X_2\big]\)</span> and <span class="math inline">\(\mathbb{E}\big[\Psi_1(X_1,X_2)\big|X_2\big]\)</span> are comonotone, so the second term is also positive, which completes the proof, as shown in Example <span class="math inline">\(\ref{PQDXX}\)</span>.</p>
</div>
<p>::: {.example}[Ambagaspitiya Counting Distribution]
Consider the random vector <span class="math inline">\(\boldsymbol{N}\)</span> defined based on the random vector <span class="math inline">\(\boldsymbol{M}\)</span> as follows:
<span class="math display">\[
\left(
\begin{array}{c}
N_1\\
N_2\\
\end{array}\right)=
\left(
\begin{array}{cccc}
a_{11} &amp; a_{12} \\
a_{21} &amp; a_{22} \\
\end{array}\right)
\left(
\begin{array}{c}
M_1\\
M_2
\\
\end{array}\right),
\]</span>
where <span class="math inline">\(a_{ij}\in \mathbb{N}\)</span> for all <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>, and <span class="math inline">\(M_1\)</span> and <span class="math inline">\(M_2\)</span> are independent. Such a vector is associated because for any non-decreasing functions <span class="math inline">\(\Psi_1\)</span> and <span class="math inline">\(\Psi_2:\mathbb{R}^2\to \mathbb{R}\)</span>, there exist functions <span class="math inline">\(\widetilde{\Psi}_1\)</span> and <span class="math inline">\(\widetilde{\Psi}_2:\mathbb{R}^2\to \mathbb{R}\)</span> such that
<span class="math display">\[
\mathbb{C}[\Psi_1({\boldsymbol{N}}),\Psi_2({\boldsymbol{N}})]=\mathbb{C}[\widetilde{\Psi}_1({\boldsymbol{M}}),\widetilde{\Psi}_2({\boldsymbol{M}})].
\]</span>
This latter covariance is non-negative since <span class="math inline">\(\boldsymbol{M}\)</span> is associated according to Property <span class="math inline">\(\ref{IndAssoc}\)</span>.
:::</p>
<p>Example <span class="math inline">\(\ref{Ambagaspitiya}\)</span> can be further generalized as follows.</p>
<p>We have seen in Property <span class="math inline">\(\ref{PQDNor}\)</span> that when <span class="math inline">\(\boldsymbol{X}\)</span> follows a bivariate normal distribution, positive correlation implies positive quadrant dependence. In fact, positive correlation is synonymous with association in this case, as shown by the following result.</p>
<div class="proof">
<p><span id="unlabeled-div-34" class="proof"><em>Proof</em>. </span>Without loss of generality, we can assume <span class="math inline">\(\boldsymbol{\mu}=\boldsymbol{0}\)</span>. Let <span class="math inline">\(\boldsymbol{X}&#39;\)</span> be an independently sampled random vector with the same distribution as <span class="math inline">\(\boldsymbol{X}\)</span>. For <span class="math inline">\(\lambda\in[0,1]\)</span>, define the random vector
<span class="math display">\[
\boldsymbol{Y}(\lambda)=\lambda\boldsymbol{X}+\sqrt{1-\lambda^2}\boldsymbol{X}&#39;.
\]</span>
For a fixed <span class="math inline">\(\lambda\)</span>, <span class="math inline">\(\boldsymbol{Y}(\lambda)\sim\mathcal{N}or(\boldsymbol{0},\boldsymbol{\Sigma})\)</span>, and
<span class="math display">\[\begin{eqnarray*}
\mathbb{C}[X_i,Y_j(\lambda)]&amp;=&amp;\mathbb{C}[X_i,\lambda X_j+\sqrt{1-\lambda^2}X_j&#39;]\\
&amp;=&amp;\mathbb{C}[X_i,\lambda X_j]+\mathbb{C}[X_i,\sqrt{1-\lambda^2}X_j&#39;]\\
&amp;=&amp;\lambda\sigma_{ij}.
\end{eqnarray*}\]</span>
Define the function
<span class="math display">\[
\psi(\lambda)=\mathbb{E}[\Psi_1(\boldsymbol{X}),\Psi_2(\boldsymbol{Y}(\lambda))]
\]</span>
where <span class="math inline">\(\Psi_1\)</span> and <span class="math inline">\(\Psi_2\)</span> are non-decreasing and differentiable functions. The function <span class="math inline">\(\psi\)</span> is continuous in <span class="math inline">\(\lambda\)</span>, and
<span class="math display">\[
\psi(0)=\mathbb{E}[\Psi_1(\boldsymbol{X})]\mathbb{E}[\Psi_2(\boldsymbol{X})]
\]</span>
while
<span class="math display">\[
\psi(1)=\mathbb{E}[\Psi_1(\boldsymbol{X})\Psi_2(\boldsymbol{X})].
\]</span>
Therefore, it suffices to show that the derivative <span class="math inline">\(\psi^{(1)}\)</span> exists and is non-negative for <span class="math inline">\(0\leq\lambda&lt;1\)</span>. This ensures
<span class="math display">\[
\psi(1)\geq\psi(0)\Leftrightarrow\boldsymbol{X}\text{ is associated}.
\]</span>
Let <span class="math inline">\(\boldsymbol{C}=\{c_{ij}\}\)</span> be the inverse of <span class="math inline">\(\boldsymbol{\Sigma}\)</span>. The density of <span class="math inline">\(\boldsymbol{X}\)</span> can be written as
<span class="math display">\[
f_{\boldsymbol{X}}(\boldsymbol{x})=\frac{1}{(2\pi)^{-1/2}}\{|\boldsymbol{C}|\}^{-1/2}\exp\left\{-\frac{1}{2}
\sum_{i,j=1}^nc_{ij}x_ix_j\right\}.
\]</span>
Conditioned on <span class="math inline">\(\boldsymbol{X}=\boldsymbol{x}\)</span>, <span class="math inline">\(Y(\lambda)\)</span> has the same distribution as <span class="math inline">\(\lambda\boldsymbol{x}+\sqrt{1-\lambda^2}\boldsymbol{X}&#39;\)</span> and has the conditional density
<span class="math display">\[
f_{\boldsymbol{Y}(\lambda)|\boldsymbol{X}}(\boldsymbol{y}|\boldsymbol{x})=\frac{1}{1-\lambda^2}
f_{\boldsymbol{X}}\big(\sqrt{1-\lambda^2}(\lambda\boldsymbol{x}-\boldsymbol{y})\big).
\]</span>
Then,
<span class="math display">\[
\psi(\lambda)=\int\int_{\boldsymbol{x}\in\mathbb{R}^2}f_{\boldsymbol{X}}(\boldsymbol{x})\Psi_1(\boldsymbol{x})
\left\{\int\int_{\boldsymbol{y}\in\mathbb{R}^2}f_{\boldsymbol{Y}(\lambda)|\boldsymbol{X}}(\boldsymbol{y}|\boldsymbol{x})\Psi_2(\boldsymbol{y})d\boldsymbol{y}\right\}
d\boldsymbol{x}.
\]</span>
A difficult and tedious calculation would show that the derivative is then given by
<span class="math display">\[
\psi^{(1)}(\lambda)=\frac{1}{\lambda}\int\int_{\boldsymbol{x}\in\mathbb{R}^2}f_{\boldsymbol{X}}(\boldsymbol{x})
\left\{\sum_{i,j=1}^2\sigma_{ij}\frac{\partial}{\partial x_i}\Psi_1(\boldsymbol{x})
\frac{\partial}{\partial x_j}g(\lambda,\boldsymbol{x})\right\}d\boldsymbol{x}
\]</span>
where
<span class="math display">\[
g(\lambda,\boldsymbol{x})=\int\int_{\boldsymbol{y}\in\mathbb{R}^2}\Psi_2(\lambda\boldsymbol{x}-\boldsymbol{y})\frac{f_{\boldsymbol{X}}(\boldsymbol{y}/\sqrt{1-\lambda^2})}
{1-\lambda^2}d\boldsymbol{y}
\]</span>
is such that
<span class="math display">\[
\frac{\partial}{\partial x_1}g(\lambda,\boldsymbol{x})\geq 0\mbox{ and }\frac{\partial}{\partial x_2}g(\lambda,\boldsymbol{x})\geq 0,
\]</span>
which completes the verification.</p>
</div>
</div>
</div>
<div id="conditional-growth" class="section level3 hasAnchor" number="9.3.12">
<h3><span class="header-section-number">9.3.12</span> Conditional Growth<a href="chap8.html#conditional-growth" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="definition-30" class="section level4 hasAnchor" number="9.3.12.1">
<h4><span class="header-section-number">9.3.12.1</span> Definition<a href="chap8.html#definition-30" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The abstract definition of association using inequality (<span class="math inline">\(\ref{assoc}\)</span>) makes it a challenging concept to use and establish in a concrete situation. This is why a notion of dependence stronger than association and easily established would be quite useful. Conditional growth is one such concept.</p>
<div class="definition">
<p><span id="def:DefCIS" class="definition"><strong>Definition 9.12  </strong></span>The random vector <span class="math inline">\(\boldsymbol{X}\)</span> is said to be conditionally increasing when:</p>
<ol style="list-style-type: decimal">
<li>For any <span class="math inline">\(x_1\leq y_1\)</span> in the support of <span class="math inline">\(X_1\)</span>,
<span class="math display">\[
\Pr[X_2&gt;t|X_1=x_1]\leq\Pr[X_2&gt;t|X_1=y_1]
\]</span>
for all <span class="math inline">\(t\in{\mathbb{R}}\)</span>;</li>
<li>For any <span class="math inline">\(x_2\leq y_2\)</span> in the support of <span class="math inline">\(X_2\)</span>,
<span class="math display">\[
\Pr[X_1&gt;t|X_2=x_2]\leq\Pr[X_1&gt;t|X_2=y_2]
\]</span>
for all <span class="math inline">\(t\in{\mathbb{R}}\)</span>.</li>
</ol>
</div>
<p>Definition <span class="math inline">\(\ref{DefCIS}\)</span> can be interpreted using the concept of comonotonicity. When <span class="math inline">\(\boldsymbol{X}\)</span> is conditionally increasing, an increase in one of the two components makes the other component riskier in terms of comonotonicity.</p>
<p>It is easy to see that independence is a limit case of conditional growth (the inequalities in (i) and (ii) of Definition <span class="math inline">\(\ref{DefCIS}\)</span> become equalities in this case), and this concept is invariant under increasing transformations, i.e.,
<span class="math display">\[
\boldsymbol{X}\text{ is conditionally increasing}
\]</span>
<span class="math display">\[
\Rightarrow(f_1(X_1),f_2(X_2))\text{ is conditionally increasing}
\]</span>
for all continuous and increasing functions <span class="math inline">\(f_1\)</span> and <span class="math inline">\(f_2\)</span>.</p>
<p>Let’s show that conditional growth is indeed a stronger notion of dependence than association (and therefore positive quadrant dependence).</p>
<div class="proof">
<p><span id="unlabeled-div-38" class="proof"><em>Proof</em>. </span>Consider non-decreasing functions <span class="math inline">\(\Psi_1\)</span> and <span class="math inline">\(\Psi_2:{\mathbb{R}}^2\to{\mathbb{R}}\)</span>.
Property <span class="math inline">\(\ref{PropCovCond}\)</span>(i) allows us to write
<span class="math display">\[\begin{eqnarray*}
&amp;&amp;\mathbb{C}[\Psi_1(X_1,X_2),\Psi_2(X_1,X_2)] \\
&amp; = &amp;
\mathbb{E}\Big[\mathbb{C}[\Psi_1(X_1,X_2),\Psi_2(X_1,X_2)|X_1]\Big]\\
&amp;  &amp; +\mathbb{C}\Big[\mathbb{E}[\Psi_1(X_1,X_2)|X_1], \mathbb{E}[\Psi_2(X_1,X_2)|X_1]\Big].
\end{eqnarray*}\]</span>
First, note that
<span class="math display">\[
\mathbb{C}[\Psi_1(x_1,X_2),\Psi_2(x_1,X_2)|X_1=x_1]\geq 0
\]</span>
which implies
<span class="math display">\[
  \mathbb{E}\Big[\mathbb{C}[\Psi_1(X_1,X_2),\Psi_2(X_1,X_2)|X_1]\Big]\geq 0.
\]</span>
The first term is therefore positive. Let’s show that the second term is also positive. Since <span class="math inline">\(\boldsymbol{X}\)</span> is conditionally increasing, the functions
<span class="math display">\[
x_1\mapsto\mathbb{E}[\Psi_i(X_1,X_2)|X_1=x_1],\hspace{2mm}i=1,2,
\]</span>
are both non-decreasing. Therefore, the random variables <span class="math inline">\(\mathbb{E}[\Psi_1(X_1,X_2)|X_1]\)</span> and <span class="math inline">\(\mathbb{E}[\Psi_2(X_1,X_2)|X_1]\)</span> are comonotonic, and
<span class="math display">\[
\mathbb{C}\Big[\mathbb{E}[\phi_1(X_1,X_2)|X_1],\mathbb{E}[\phi_2(X_1,X_2)|X_1]\Big]\geq 0.
\]</span>
:::</p>
<div id="introduction-to-copula-theory" class="section level2 hasAnchor" number="9.4">
<h2><span class="header-section-number">9.4</span> Introduction to Copula Theory<a href="chap8.html#introduction-to-copula-theory" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="principle-2" class="section level3 hasAnchor" number="9.4.1">
<h3><span class="header-section-number">9.4.1</span> Principle<a href="chap8.html#principle-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Copulas (or <em>copulae</em>) were introduced by <span class="citation">(<a href="#ref-sklar1959fonctions" role="doc-biblioref">Sklar 1959</a>)</span>. The copula is also referred to as “<em>dependence function</em>” by <span class="citation">(<a href="#ref-deheuvels1979proprietes" role="doc-biblioref">Deheuvels 1979</a>)</span>, or uniform representation by <span class="citation">(<a href="#ref-kimeldorf1975uniform" role="doc-biblioref">Kimeldorf and Sampson 1975</a>)</span>. Kruskal also introduced this function as early as 1958 to define notions of association, <span class="citation">(<a href="#ref-kruskal1958ordinal" role="doc-biblioref">Kruskal 1958</a>)</span>.</p>
<p>The idea behind the concept of copula can be presented as follows. Let’s start with a couple <span class="math inline">\((Z_1,Z_2)\sim\mathcal{N}or(\boldsymbol{0},\boldsymbol{\Sigma})\)</span> where
<span class="math display">\[
\boldsymbol{\Sigma}=\left(\begin{array}{cc}
1&amp;\alpha\\
\alpha&amp;1\end{array}\right)
\]</span>
and define the couple <span class="math inline">\((\Phi(Z_1),\Phi(Z_2))\)</span>. This couple has uniform margins <span class="math inline">\(\mathcal{U}ni(0,1)\)</span> and a joint cumulative distribution function of the form
<span class="math display">\[\begin{eqnarray}
\label{CopNorm}
&amp;&amp;C_\alpha(u_1,u_2)\\
&amp;=&amp;\frac{1}{2\pi\sqrt{1-\alpha^2}}\int_{\xi_1=-\infty}^{\Phi^{-1}(u_1)}
\int_{\xi_2=-\infty}^{\Phi^{-1}(u_2)}\exp\left\{\frac{-(\xi_1^2-2\alpha\xi_1\xi_2+\xi_2^2)}
{2(1-\alpha^2)}\right\}d\xi_1d\xi_2.\nonumber
\end{eqnarray}\]</span>
The corresponding probability density function is
<span class="math display">\[\begin{eqnarray*}
c_\alpha(u_1,u_2)&amp;=&amp;\frac{\partial^2}{\partial u_1\partial u_2}C_\alpha(u_1,u_2)\\
&amp;=&amp;\frac{1}{2\pi\sqrt{1-\alpha^2}}\exp\left\{\frac{-(\zeta_1^2-2\alpha \zeta_1\zeta_2+\zeta_2^2)}
{2(1-\alpha^2)}\right\}\\
&amp;&amp;\frac{d}{du_1}\Phi^{-1}(u_1)\frac{d}{du_2}\Phi^{-1}(u_2)
\end{eqnarray*}\]</span>
where <span class="math inline">\(\zeta_i=\Phi^{-1}(u_i)\)</span>, <span class="math inline">\(i=1,2\)</span>. As
<span class="math display">\[
\frac{d}{du_i}\Phi^{-1}(u_i)=\frac{1}{\phi\big(\Phi^{-1}(u_i)\big)}=\sqrt{2\pi}\exp(-\zeta_i^2/2)
\]</span>
we finally obtain
<span class="math display">\[\begin{equation}
\label{DensCopNorm}
c_\alpha(u_1,u_2)
=\frac{1}{\sqrt{1-\alpha^2}}\exp\left\{\frac{-(\zeta_1^2-2\alpha \zeta_1\zeta_2+\zeta_2^2)}
{2(1-\alpha^2)}\right\}\exp\left\{\frac{\zeta_1^2+\zeta_2^2}{2}\right\}.
\end{equation}\]</span>
You can see in Figure <span class="math inline">\(\ref{pdfNorm1}\)</span> graphs of this density for different values of the Kendall’s tau coefficient
<span class="math display">\[
\tau(Z_1,Z_2)=\tau\big(\Phi(Z_1),\Phi(Z_2)\big).
\]</span></p>
<p>At this point, starting from the bivariate normal distribution, we have constructed a density <span class="math inline">\(\eqref{DensCopNorm}\)</span> with marginal distributions that have been “uniformized.” Thanks to Property <span class="math inline">\(\ref{NombrAl}\)</span>, we can now move on to a density with the desired marginal distributions for the actuary (such as Pareto, Lognormal, or Gamma, for example). Specifically, let <span class="math inline">\(F_1\)</span> and <span class="math inline">\(F_2\)</span> be the desired marginal distributions. If we construct the couple
<span class="math display">\[
\boldsymbol{X}=\Big(F_1^{-1}\big(\Phi(Z_1)\big), F_2^{-1}\big(\Phi(Z_2)\big)\Big),
\]</span>
it is easy to see that the cumulative distribution function of <span class="math inline">\(\boldsymbol{X}\)</span> is part of <span class="math inline">\(\mathcal{F}(F_1,F_2)\)</span>, as desired. Moreover, the dependence structure is induced by that of the bivariate normal distribution. The cumulative distribution function of <span class="math inline">\(\boldsymbol{X}\)</span> is then given by
<span class="math display">\[\begin{eqnarray*}
F_{\boldsymbol{X}}(x_1,x_2)&amp;=&amp;\Pr\Big[F_1^{-1}\big(\Phi(Z_1)\big)\leq x_1,F_2^{-1}\big(\Phi(Z_2)\big)\leq x_2\Big]\\
&amp;=&amp;\Pr\Big[\Phi(Z_1)\leq F_1(x_1),\Phi(Z_2)\leq F_2(x_2)\Big]\\
&amp;=&amp;C_\alpha\big(F_1(x_1),F_2(x_2)\big)
\end{eqnarray*}\]</span>
where <span class="math inline">\(C_\alpha\)</span> is given by <span class="math inline">\(\eqref{CopNorm}\)</span>. The density of <span class="math inline">\(\boldsymbol{X}\)</span> is then
<span class="math display">\[\begin{eqnarray*}
f_{\boldsymbol{X}}(x_1,x_2)&amp;=&amp;\frac{\partial^2}{\partial x_1\partial x_2}F_{\boldsymbol{X}}(x_1,x_2)\\
&amp;=&amp;f_1(x_1)f_2(x_2)c_\alpha\big(F_1(x_1),F_2(x_2)\big)
\end{eqnarray*}\]</span>
where <span class="math inline">\(f_1\)</span> and <span class="math inline">\(f_2\)</span> are the densities corresponding to <span class="math inline">\(F_1\)</span> and <span class="math inline">\(F_2\)</span>, and where <span class="math inline">\(c_\alpha\)</span> is the density <span class="math inline">\(\eqref{DensCopNorm}\)</span>. You can see in Figure <span class="math inline">\(\ref{pdfNorm3}\)</span> bivariate densities with <span class="math inline">\(\mathcal{G}am(3,1)\)</span> marginals obtained in this way.</p>
</div>
<div id="definition-31" class="section level3 hasAnchor" number="9.4.2">
<h3><span class="header-section-number">9.4.2</span> Definition<a href="chap8.html#definition-31" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s now provide a precise definition of the concept of a copula.</p>
<p><strong>Definition:</strong>
A two-dimensional copula is a mapping <span class="math inline">\(C\)</span> from $$ into the interval $$ satisfying:
1. <span class="math inline">\(C\left( u,0\right) =C\left( 0,u\right) =0\)</span> and <span class="math inline">\(C\left( u,1\right) =C\left( 1,u\right) =u\)</span> for all <span class="math inline">\(0\leq u\leq1\)</span>.
2. <span class="math inline">\(C\)</span> is supermodular.</p>
</div>
<div id="sklars-theorem" class="section level3 hasAnchor" number="9.4.3">
<h3><span class="header-section-number">9.4.3</span> Sklar’s Theorem<a href="chap8.html#sklars-theorem" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This result generalizes the construction based on the bivariate normal distribution that we presented as an introduction. It plays a fundamental role in copula theory, showing how the dependence structure can be separated from the marginal distributions.</p>
<p><strong>Theorem (Sklar’s Theorem):</strong>
Let <span class="math inline">\(\boldsymbol{X}\)</span> be a couple whose cumulative distribution function <span class="math inline">\(F_{\boldsymbol{X}}\)</span> belongs to <span class="math inline">\(\mathcal{F}(F_1,F_2)\)</span> with continuous <span class="math inline">\(F_1\)</span> and <span class="math inline">\(F_2\)</span>. Then, there exists a unique copula <span class="math inline">\(C\)</span> such that for all <span class="math inline">\(\boldsymbol{x}\in{\mathbb{R}}^2\)</span>:
<span class="math display">\[
F_{\boldsymbol{X}}(x_1,x_2)=C\left(F_1(x_1),F_2(x_2)\right).
\]</span>
Conversely, if <span class="math inline">\(C\)</span> is a copula and <span class="math inline">\(F_1\)</span> and <span class="math inline">\(F_2\)</span> are univariate cumulative distribution functions, the function <span class="math inline">\(F_{\boldsymbol{X}}\)</span> defined by the equation above is a bivariate cumulative distribution function in <span class="math inline">\(\mathcal{F}(F_1,F_2)\)</span>.</p>
<p>In passing, note that we have obtained an explicit formula for the copula involved in the equation above when the marginals are continuous, specifically:
$$
C() = F_{}(F_1<sup>{-1}(u_1),F_2</sup>{-1}(u_2)), ^2.</p>
<div class="remark">
<p><span id="unlabeled-div-35" class="remark"><em>Remark</em>. </span>However, Sklar’s Theorem loses much of its interest when one of the two marginal cumulative distribution functions is not continuous. In this case, the copula <span class="math inline">\(C\)</span> involved in the representation <span class="math inline">\(\eqref{SklarMain}\)</span> is no longer unique on the unit square <span class="math inline">\([0,1]^2\)</span> but only on the Cartesian product of the images of <span class="math inline">\(F_1\)</span> and <span class="math inline">\(F_2\)</span>. This multiplicity of copulas makes the decomposition <span class="math inline">\(\eqref{SklarMain}\)</span> less useful in practice.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-36" class="remark"><em>Remark</em>. </span>The use of uniform laws may seem natural, but the normal law remains the reference in statistics. As noted by <span class="citation">(<a href="#ref-hutchinson1991engineering" role="doc-biblioref">Hutchinson and Lai 1991</a>)</span>, the basic idea behind copulas is to decouple marginal behavior and dependence structure by transforming the margins. There are indeed several advantages to reducing to uniform marginal laws, such as:
1. Independence generally does not have a simple geometric interpretation. Few laws have a “simple” density when the components are independent, except for the case where the margins are <span class="math inline">\(\mathcal{U}ni(0,1)\)</span>, in which case the density is constant and equal to 1 on the square <span class="math inline">\([0,1] \times [0,1]\)</span>. However, it should be noted that the case where the components are Gaussian and have the same distribution, for example, $or( 0,1) $, is also simple: the level curves of the density are then circular.
2. Common measures of dependence, especially Kendall’s tau and Spearman’s rho, were developed for the case where the marginal distributions are uniform.</p>
</div>
<p>We have seen the normal copula in the introduction of this section (see formulas <span class="math inline">\(\eqref{CopNorm}\)</span> and <span class="math inline">\(\eqref{DensCopNorm}\)</span> for the cumulative distribution function and probability density function, respectively). Below, we review some other examples of copulas.</p>
<p><strong>Copula of the upper Fréchet bound:</strong></p>
<p>The copula corresponding to the upper Fréchet bound <span class="math inline">\(W\)</span>, denoted as <span class="math inline">\(C_W\)</span>, is given by:
<span class="math display">\[
C_W(u_1,u_2)=\min\{u_1,u_2\},
\hspace{3mm}(u_1,u_2)\in[0,1]\times[0,1].
\]</span>
This is the cumulative distribution function of the couple <span class="math inline">\((U,U)\)</span>, where <span class="math inline">\(U\sim\mathcal{U}ni(0,1)\)</span>.
The graph of the function <span class="math inline">\(C_W\)</span> is shown in Figure <span class="math inline">\(\ref{GraphCU}\)</span>.</p>
<p>The copula corresponding to the lower Fréchet bound <span class="math inline">\(M\)</span>, denoted as <span class="math inline">\(C_M\)</span>, is given by
<span class="math display">\[
C_M(u_1,u_2)=\max\{0,u_1+u_2-1\},
\hspace{3mm}(u_1,u_2)\in[0,1]\times[0,1].
\]</span>
This is the distribution function of the pair <span class="math inline">\((U,1-U)\)</span>, where <span class="math inline">\(U\sim\mathcal{U}ni(0,1)\)</span>. The graph of <span class="math inline">\(C_M\)</span> is shown in Figure <span class="math inline">\(\ref{GraphCL}\)</span>.</p>
<p>The independence copula, denoted as <span class="math inline">\(C_I\)</span>, is given by
<span class="math display">\[
C_I(u_1,u_2)=u_1u_2,\hspace{3mm}(u_1,u_2)\in[0,1]\times[0,1].
\]</span>
The graph of <span class="math inline">\(C_I\)</span> is shown in Figure <span class="math inline">\(\ref{GraphCI}\)</span>.</p>
<p>Introduced by <span class="citation">(<a href="#ref-gumbel1960multivariate" role="doc-biblioref">Gumbel 1960</a>)</span>, this copula is given by
<span class="math display">\[
C_\alpha(u_1,u_2)=\exp\left(-\left\{(-\ln u_1)^{\alpha}
+(-\ln u_2)^{\alpha}\right\}^{1/\alpha}\right),
\hspace{2mm}\alpha\geq 1.
\]</span>
The Kendall’s tau is given by
<span class="math display">\[
\tau=1-\alpha^{-1}.
\]</span>
The probability density associated with <span class="math inline">\(C_\alpha\)</span> is
<span class="math display">\[
c_\alpha(\boldsymbol{u})=\frac{C_\alpha(\boldsymbol{u})}{u_1u_2}\frac{(\ln u_1\ln u_2)^{\alpha-1}}{(-\ln u_1-\ln u_2)^{2-\frac{1}{\alpha}}}
\Big\{\big((-\ln u_1)^\alpha+(-\ln u_2)^\alpha\big)^{1/\alpha}+\alpha-1\Big\}.
\]</span>
The Gumbel family contains the independence copula <span class="math inline">\(C_I\)</span> and the upper Fréchet bound copula <span class="math inline">\(C_W\)</span>, specifically,
<span class="math display">\[
C_1=C_I\text{ and }\lim_{\alpha\to +\infty}C_\alpha=C_W.
\]</span></p>
<p>This family of copulas was introduced by <span class="citation">(<a href="#ref-kimeldorf1978monotone" role="doc-biblioref">Kimeldorf and Sampson 1978</a>)</span> and <span class="citation">(<a href="#ref-clayton1978model" role="doc-biblioref">Clayton 1978</a>)</span>: for <span class="math inline">\(\alpha &gt; 0\)</span>,
<span class="math display">\[\begin{equation}
\label{clinton}
C_\alpha(u_1,u_2)=\left(u_1^{-\alpha}+u_2^{-\alpha}-1\right)^{-1/\alpha}.
\end{equation}\]</span>
The Kendall’s tau is <span class="math inline">\(\alpha/(\alpha +2)\)</span>. The associated density is
<span class="math display">\[
c_\alpha(\boldsymbol{u})=
\frac{1+\alpha}{(u_1u_2)^{\alpha+1}}\big(u_1^{-\alpha}+u_2^{-\alpha}-1\big)^{-2-\frac{1}{\alpha}}.
\]</span>
The Clayton family contains the upper Fréchet bound and independence copulas, specifically,
<span class="math display">\[
\lim_{\alpha\to +\infty}C_\alpha(u_1,u_2)=\min\{u_1,u_2\}=C_W(\boldsymbol{u})
\]</span>
and
<span class="math display">\[
\lim_{\alpha\to 0}C_\alpha(u_1,u_2)=u_1u_2=C_I(\boldsymbol{u}).
\]</span></p>
<p>The density <span class="math inline">\(c_\alpha(\boldsymbol{u})\)</span> is depicted in Figure <span class="math inline">\(\ref{pdfClayton1}\)</span> for various values of <span class="math inline">\(\tau\)</span>. Figure <span class="math inline">\(\ref{pdfClayton2}\)</span> displays the probability density of a pair of random variables with <span class="math inline">\(\mathcal{N}or(0,1)\)</span> distribution, where the dependence structure is described by the Clayton copula. The corresponding contour plots are shown in Figure <span class="math inline">\(\ref{pdfClayton3}\)</span>. It’s evident that the characteristic ellipses of bivariate normal distribution have been altered. Lastly, Figure <span class="math inline">\(\ref{pdfClayton4}\)</span> illustrates the density associated with a pair of random variables with <span class="math inline">\(\mathcal{G}am(3,1)\)</span> distribution, where the dependence structure is given by the Clayton copula, for <span class="math inline">\(\tau=0.25\)</span>, 0.5, and 0.75.</p>
<p><span class="citation">(<a href="#ref-frank1979simultaneous" role="doc-biblioref">M. J. Frank 1979</a>)</span> introduced a family of copulas obtained by solving a functional equation. This family has the distribution function:
<span class="math display">\[
C_\alpha(u_1,u_2)=-\frac{1}{\alpha}\ln\left(1+\frac{(\exp(-\alpha u_1)-1)
(\exp(-\alpha u_2)-1)}{\exp(-\alpha)-1}\right),
\]</span>
where <span class="math inline">\(\alpha\neq 0\)</span>. Its density is given by:
<span class="math display">\[
c_\alpha(\boldsymbol{u})=\frac{\alpha\exp\big(-\alpha(u_1+u_2)\big)\big(1-\exp(-\alpha)\big)}{\Big\{\exp\big(-\alpha(u_1+u_2)\big)
-\exp(-\alpha u_1)-\exp(-\alpha u_2)+\exp(-\alpha)\Big\}^2}.
\]</span></p>
<p>The probability density associated with the Frank copula is shown in Figure <span class="math inline">\(\ref{pdfFrank1}\)</span> for <span class="math inline">\(\tau=0.1\Leftrightarrow \alpha=0.91\)</span>, <span class="math inline">\(\tau=0.4\Leftrightarrow \alpha=4.16\)</span>, <span class="math inline">\(\tau=0.7\Leftrightarrow \alpha=11.4\)</span>, <span class="math inline">\(\tau=0.9\Leftrightarrow \alpha=20.9\)</span>. For <span class="math inline">\(\tau=0.1\)</span>, 0.4, 0.7, and 0.9, Figure <span class="math inline">\(\ref{pdfFrank2}\)</span> displays the probability density of a pair of random variables with the Frank copula and marginal distributions <span class="math inline">\(\mathcal{N}or(0,1)\)</span>. As shown in Figure <span class="math inline">\(\ref{pdfFrank3}\)</span>, the Frank copula is not limited to positive dependence: it depicts the density <span class="math inline">\(c_\alpha(\boldsymbol{u})\)</span> for <span class="math inline">\(\tau=-0.4\)</span> and <span class="math inline">\(\tau=-0.7\)</span>. Figure <span class="math inline">\(\ref{pdfFrank4}\)</span> illustrates the density for <span class="math inline">\(\tau=-0.4\)</span> and <span class="math inline">\(\tau=-0.7\)</span> with marginal distributions <span class="math inline">\(\mathcal{N}or(0,1)\)</span>.</p>
<p>The Frank family contains the copulas <span class="math inline">\(C_M\)</span>, <span class="math inline">\(C_W\)</span>, and <span class="math inline">\(C_I\)</span> as special cases; specifically,
<span class="math display">\[
\lim_{\alpha\to -\infty}C_\alpha=C_M,\hspace{2mm}
\lim_{\alpha\to +\infty}C_\alpha=C_W\text{ and }
\lim_{\alpha\to 0}C_\alpha=C_I.
\]</span></p>
</div>
<div id="properties-of-copulas" class="section level3 hasAnchor" number="9.4.4">
<h3><span class="header-section-number">9.4.4</span> Properties of Copulas<a href="chap8.html#properties-of-copulas" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="survival-copulas" class="section level4 hasAnchor" number="9.4.4.1">
<h4><span class="header-section-number">9.4.4.1</span> Survival Copulas<a href="chap8.html#survival-copulas" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>If <span class="math inline">\(C\)</span> is a copula, the same applies to <span class="math inline">\(\overline{C}\)</span> defined as
<span class="math display">\[
\overline{C}(u_1,u_2)=C(1-u_1,1-u_2)+u_1+u_2-1;
\]</span>
<span class="math inline">\(\overline{C}\)</span> is called the survival copula associated with copula <span class="math inline">\(C\)</span>. The representation of the survival function <span class="math inline">\(\overline{F}_{\boldsymbol{X}}\)</span> of <span class="math inline">\(\boldsymbol{X}\)</span> in terms of <span class="math inline">\(\overline{F}_1\)</span> and <span class="math inline">\(\overline{F}_2\)</span> is given by
<span class="math display">\[\begin{eqnarray*}
\overline{F}_{\boldsymbol{X}}(\boldsymbol{x})&amp;=&amp;1-F_1(x_1)-F_2(x_2)+F_{\boldsymbol{X}}(\boldsymbol{x})\\
&amp;=&amp;
\overline{C}(\overline{F}_1(x_1),\overline{F}_2(x_2));
\end{eqnarray*}\]</span>
thus, <span class="math inline">\(\overline{C}\)</span> couples the univariate survival functions just as <span class="math inline">\(C\)</span> coupled the univariate distribution functions in <span class="math inline">\(\eqref{SklarMain}\)</span>.</p>
<div class="remark">
<p><span id="unlabeled-div-37" class="remark"><em>Remark</em>. </span>
It is important not to confuse the survival copula <span class="math inline">\(\overline{C}\)</span> with the survival function of a pair of uniform random variables <span class="math inline">\((U_1,U_2)\)</span> whose joint cumulative distribution function is <span class="math inline">\(C\)</span>, which is given by
<span class="math display">\[
\Pr[U_1&gt;u_1,U_2&gt;u_2]=1-u_1-u_2+C(u_1,u_2)\neq\overline{C}(u_1,u_2).
\]</span>
The copula
<span class="math display">\[
C^*(u_1,u_2)=1-u_1-u_2+C(u_1,u_2)
\]</span>
is called the dual copula of <span class="math inline">\(C\)</span>.</p>
</div>
<p>::: {.example}[Pareto Copula]
Consider the joint cumulative distribution function <span class="math inline">\(\eqref{CdfBivPar}\)</span>, and the underlying copula is given by
<span class="math display">\[
C(u_1,u_2)=u_1+u_2-1+\Big((1-u_1)^{-1/\alpha}+(1-u_2)^{-1/\alpha}-1\Big)^{-\alpha}.
\]</span>
This function is also called the Pareto copula (as it corresponds to the bivariate Pareto distribution). The survival copula is
<span class="math display">\[
\overline{C}(\boldsymbol{u})=\Big(u_1^{-1/\alpha}+u_2^{-1/\alpha}-1\Big)^{-\alpha}-1,
\]</span>
where we recognize the Clayton copula. The dual copula is given by
<span class="math display">\[
{C}^*(\boldsymbol{u})=\Big((1-u_1)^{-1/\alpha}+(1-u_2)^{-1/\alpha}-1\Big)^\alpha.
\]</span></p>
</div>
</div>
</div>
</div>
<p>The utility of copulas comes from the fact that they are not affected by monotonically increasing transformations of the components of the vectors involved, or vary predictably when these transformations are monotonic but not increasing.</p>
<div class="proposition">
<p><span id="prp:Prop521" class="proposition"><strong>Proposition 9.10  </strong></span>Let <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> be two random variables with cumulative distribution functions <span class="math inline">\(F_1\)</span> and <span class="math inline">\(F_2\)</span>, both continuous, and their dependence structure described by copula <span class="math inline">\(C\)</span>. Let <span class="math inline">\(g_1\)</span> and <span class="math inline">\(g_2\)</span> be monotonic functions,</p>
<ol style="list-style-type: decimal">
<li>if <span class="math inline">\(g_1\)</span> and <span class="math inline">\(g_2\)</span> are increasing, then <span class="math inline">\(\big(g_1(X_1),g_2(X_2)\big)\)</span> also has <span class="math inline">\(C\)</span> as its copula.</li>
<li>if <span class="math inline">\(g_1\)</span> is increasing and <span class="math inline">\(g_2\)</span> is decreasing, then <span class="math inline">\(\big(g_1(X_1),g_2(X_2)\big)\)</span> has the copula
<span class="math inline">\(u_1-C(u_1,1-u_2)\)</span>.</li>
<li>if <span class="math inline">\(g_1\)</span> is decreasing and <span class="math inline">\(g_2\)</span> is increasing, then <span class="math inline">\(\big(g_1(X_1),g_2(X_2)\big)\)</span> has the copula <span class="math inline">\(u_2-C(1-u_1,u_2)\)</span>.</li>
<li>if <span class="math inline">\(g_1\)</span> and <span class="math inline">\(g_2\)</span> are decreasing, then <span class="math inline">\(\big(g_1(X_1),g_2(X_2)\big)\)</span> has the copula <span class="math inline">\(u_1+u_2-1+C(1-u_1,1-u_2)\)</span>.</li>
</ol>
</div>
<div class="proof">
<p><span id="unlabeled-div-39" class="proof"><em>Proof</em>. </span></p>
<ol style="list-style-type: lower-roman">
<li><p>The cumulative distribution function of <span class="math inline">\(g_i(X_i)\)</span>, <span class="math inline">\(i=1,2\)</span>, is
<span class="math display">\[\begin{eqnarray*}
G_i(x)&amp;=&amp;\Pr[g_i(X_i)\leq x]\\
&amp;=&amp;\Pr[X_i\leq g_i^{-1}(x)]\\
&amp;=&amp;F_i(g_i^{-1}(x)),\hspace{2mm}i=1,2.
\end{eqnarray*}\]</span>
Denoting <span class="math inline">\(\widetilde{C}\)</span> as the copula for <span class="math inline">\((g_1(X_1),g_2(X_2))\)</span>, we have
<span class="math display">\[\begin{eqnarray*}
\widetilde{C}\Big(G_1(x_1),G_2(x_2)\Big)&amp;=&amp;\Pr[g_1(X_1)\leq x_1,g_2(X_2)\leq x_2]\\
&amp;=&amp;\Pr[X_1\leq g_1^{-1}(x_1),X_2\leq g_2^{-1}(x_2)]\\
&amp;=&amp;C\Big(F_1\big(g_1^{-1}(x_1)\big),F_2\big(g_2^{-1}(x_2)\big)\\
&amp;=&amp;C\Big(G_1(x_1),G_2(x_2)\Big)
\end{eqnarray*}\]</span>
which allows us to conclude that <span class="math inline">\(\widetilde{C}\equiv C\)</span>.</p></li>
<li><p>The cumulative distribution function of <span class="math inline">\(g_1(X_1)\)</span> is <span class="math inline">\(G_1\)</span> as defined in (i), but that of <span class="math inline">\(g_2(X_2)\)</span> is now given by
<span class="math display">\[\begin{eqnarray*}
G_2(x)&amp;=&amp;\Pr[g_2(X_2)\leq x]\\
&amp;=&amp;\Pr[X_2\geq g_i^{-1}(x)]\\
&amp;=&amp;1-F_2(g_2^{-1}(x)).
\end{eqnarray*}\]</span>
Therefore, denoting <span class="math inline">\(\widetilde{C}\)</span> as the copula of <span class="math inline">\((g_1(X_1),g_2(X_2))\)</span>, we have
<span class="math display">\[\begin{eqnarray*}
\widetilde{C}\Big(G_1(x_1),G_2(x_2)\Big)&amp;=&amp;\Pr[g_1(X_1)\leq x_1,g_2(X_2)\leq x_2]\\
&amp;=&amp;\Pr[X_1\leq g_1^{-1}(x_1),X_2\geq g_2^{-1}(x_2)]\\
&amp;=&amp;\Pr[X_1\leq g_1^{-1}(x_1)]\\
&amp;&amp;-\Pr[X_1\leq g_1^{-1}(x_1),X_2\leq g_2^{-1}(x_2)]\\
&amp;=&amp;G_1(x_1)-C\Big(G_1(x_1),1-G_2(x_2)\Big)
\end{eqnarray*}\]</span>
which concludes the proof of (ii). The proof of (iii)-(iv) is entirely similar and is left as an exercise to the reader.</p></li>
</ol>
</div>
<p>It is worth noting that the form of the copula for the pair <span class="math inline">\(\big(g_1(X_1),g_2(X_2)\big)\)</span> does not explicitly depend on the functions <span class="math inline">\(g_1\)</span> and <span class="math inline">\(g_2\)</span> but only on whether they are increasing or decreasing. This indicates that the copula perfectly summarizes the dependence structure between the variables at hand without being influenced by the scale of each of them.</p>
<p>Let <span class="math inline">\((U_1,U_2)\)</span> be a pair of random variables with copula <span class="math inline">\(C\)</span> as their joint cumulative distribution function. The conditional cumulative distribution function of <span class="math inline">\(U_2\)</span> given <span class="math inline">\(U_1=u_1\)</span> is given by
<span class="math display">\[\begin{eqnarray*}
C_{2|1}(u_2|u_1)&amp;=&amp;\Pr[U_2\leq u_2|U_1=u_1]\\
&amp;=&amp;\lim_{\Delta u_1\to 0}
\frac{\Pr[u_1\leq U_1\leq u_1+\Delta u_1, U_2\leq u_2]}{\Pr[u_1\leq U_1\leq u_1+\Delta u_1]}\\
&amp;=&amp;\lim_{\Delta u_1\to 0}
\frac{C(u_1+\Delta u_1,u_2)-C(u_1,u_2)}{\Delta u_1}\\
&amp;=&amp;\frac{\partial}{\partial u_1}
C(u_1,u_2).
\end{eqnarray*}\]</span>
If <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> have cumulative distribution functions <span class="math inline">\(F_1\)</span> and <span class="math inline">\(F_2\)</span>, both continuous, and copula <span class="math inline">\(C\)</span>, the conditional cumulative distribution function of <span class="math inline">\(X_2\)</span> given <span class="math inline">\(X_1=x_1\)</span> is given by
<span class="math display">\[
\Pr[X_2\leq x_2|X_1=x_1]=F_{2|1}(x_2|x_1)=C_{2|1}(F_1(x_1)|F_2(x_2)).
\]</span>
By symmetry, the conditional cumulative distribution function of <span class="math inline">\(X_1\)</span> given <span class="math inline">\(X_2=x_2\)</span> is
<span class="math display">\[
\Pr[X_1\leq x_1|X_2=x_2]=F_{1|2}(x_1|x_2)=C_{1|2}(F_1(x_1)|F_2(x_2))
\]</span>
where
<span class="math display">\[
C_{1|2}(u_1|u_2)=\frac{\partial}{\partial u_2}C(u_1,u_2).
\]</span></p>
<p>::: {.example}[Clayton Copula]
Consider a pair <span class="math inline">\((U_1,U_2)\)</span> of uniform marginals <span class="math inline">\(\mathcal{U}ni(0,1)\)</span> with the Clayton copula <span class="math inline">\(C_\alpha\)</span> as their joint cumulative distribution function. The conditional distribution of <span class="math inline">\(U_2\)</span> given <span class="math inline">\(U_1=u_1\)</span> is described by
<span class="math display">\[
C_{2|1}(u_2|u_1)=\frac{\partial}{\partial u_1}C_\alpha(u_1,u_2)=\big(1+u_1^\alpha(u_2^{-\alpha}-1)\big)^{-1-\frac{1}{\alpha}}.
\]</span>
If <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> have cumulative distribution functions <span class="math inline">\(F_1\)</span> and <span class="math inline">\(F_2\)</span>, both continuous, and the copula <span class="math inline">\(C_\alpha\)</span>, then
<span class="math display">\[
\Pr[X_2\leq x_2|X_1=x_1]=\Big(1+\big(F_1(x_1)\big)^\alpha\Big(\big(F_2(x_2)\big)^\alpha-1\Big)\Big)^{-1-\frac{1}{\alpha}}.
\]</span></p>
<p>:::</p>
<p>The comparison of dependence using <span class="math inline">\(\preceq_{\text{sm}}\)</span> relies exclusively on the copulas of the pairs involved when their common marginals are continuous. Indeed, <span class="math inline">\(\preceq_{\text{sm}}\)</span> can only be used within <span class="math inline">\(\mathcal{F}(F_1,F_2)\)</span>, and Property <span class="math inline">\(\ref{SmFonc}\)</span> allows us to write
<span class="math display">\[\begin{eqnarray*}
\boldsymbol{X}\preceq_{\text{sm}}\boldsymbol{Y}&amp;\Leftrightarrow &amp; \big(F_1(X_1),F_2(X_2)\big)\preceq_{\text{sm}}\big(F_1(Y_1),F_2(Y_2)\big)\\
&amp;\Leftrightarrow &amp;C_{\boldsymbol{X}}(u_1,u_2)\leq C_{\boldsymbol{Y}}(u_1,u_2)\text{ for all }u_1,u_2\in [0,1],
\end{eqnarray*}\]</span>
where <span class="math inline">\(C_{\boldsymbol{X}}\)</span> and <span class="math inline">\(C_{\boldsymbol{Y}}\)</span> denote the copulas of the respective pairs <span class="math inline">\(\boldsymbol{X}\)</span> and <span class="math inline">\(\boldsymbol{Y}\)</span>.</p>
<div class="example">
<p><span id="exm:unlabeled-div-40" class="example"><strong>Example 9.8  </strong></span>The parameter <span class="math inline">\(\alpha\)</span> controls the degree of dependence expressed by most of the parametric copula families presented earlier. Specifically, we have
<span class="math display">\[
C_\alpha(\boldsymbol{u})\leq C_{\alpha &#39;}(\boldsymbol{u})\text{ for all }\boldsymbol{u}\in[0,1]^2
\]</span>
when <span class="math inline">\(\alpha\leq \alpha &#39;\)</span> for the Gumbel copula, Clayton copula, and Frank copula.</p>
</div>
<p>Property <span class="math inline">\(\ref{PQDFuncInv}\)</span> shows that positive quadrant dependence is induced by the copula, without reference to the marginal distributions. Indeed, if <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> have cumulative distribution functions <span class="math inline">\(F_1\)</span> and <span class="math inline">\(F_2\)</span>, both continuous, and copula <span class="math inline">\(C\)</span>,
<span class="math display">\[\begin{eqnarray*}
&amp;&amp;\boldsymbol{X}\text{ is positively quadrant dependent }\\
&amp;\Leftrightarrow &amp; \big(F_1(X_1),F_2(X_2)\big)
\text{ is positively quadrant dependent }\\
&amp;\Leftrightarrow &amp;C(u_1,u_2)\geq C_I(u_1,u_2)=u_1u_2\text{ for all }u_1,u_2\in [0,1].
\end{eqnarray*}\]</span>
Thus, a pair is positively quadrant dependent if and only if the copula of that pair dominates the independence copula everywhere in the unit square.</p>
<div class="example">
<p><span id="exm:unlabeled-div-41" class="example"><strong>Example 9.9  </strong></span>The copula <span class="math inline">\(C_W\)</span> expresses positive quadrant dependence. The Gumbel copula and the Clayton copula both express positive quadrant dependence for all values of the parameter <span class="math inline">\(\alpha\)</span>. These copulas, therefore, only express positive dependence (and are not suitable for modeling situations where negative dependence between the variables is suspected).</p>
<p>The Frank copulas express positive quadrant dependence when <span class="math inline">\(\alpha\geq 0\)</span>.</p>
</div>
<p>Similarly, if <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> have cumulative distribution functions <span class="math inline">\(F_1\)</span> and <span class="math inline">\(F_2\)</span>, both continuous, and copula <span class="math inline">\(C\)</span>,
<span class="math display">\[\begin{eqnarray*}
&amp;&amp;\boldsymbol{X}\text{ is associated}\\
&amp; \Rightarrow &amp; \big(F_1(X_1),F_2(X_2)\big)\text{ is associated}\\
&amp; \Rightarrow &amp; \text{ the copula }C\text{ is associated}.
\end{eqnarray*}\]</span>
However, there is no simple condition to check if a given copula is associated. For conditional monotonicity, we have
<span class="math display">\[\begin{eqnarray*}
&amp;&amp;\boldsymbol{X}\text{ is conditionally increasing} \\
&amp; \Rightarrow &amp; \big(F_1(X_1),F_2(X_2)\big)\text{ is conditionally increasing}\\
&amp; \Rightarrow &amp; \text{ the copula }C\text{ is conditionally increasing.}
\end{eqnarray*}\]</span>
In the case of conditional monotonicity, we can state the following result.</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-42" class="proposition"><strong>Proposition 9.11  </strong></span>The copula <span class="math inline">\(C\)</span> is conditionally increasing if and only if the following two conditions are simultaneously satisfied:</p>
<ol style="list-style-type: decimal">
<li>for all <span class="math inline">\(0\leq u_2\leq 1\)</span>,
<span class="math display">\[
u_1\longmapsto \frac{\partial}{\partial u_1} C\left(u_1,u_2\right)
\]</span>
is strictly increasing, i.e., $u_1C( u_1,u_2) $ is a concave function.</li>
<li>for all <span class="math inline">\(0\leq u_1\leq 1\)</span>,
<span class="math display">\[
u_2\longmapsto \frac{\partial}{\partial u_2} C\left(u_1,u_2\right)
\]</span>
is strictly increasing, i.e., $u_2C( u_1,u_2) $ is a concave function.</li>
</ol>
</div>
</div>
</div>
<div id="dependencemeasures" class="section level3 hasAnchor" number="9.4.5">
<h3><span class="header-section-number">9.4.5</span> Dependence Measures and Copulas<a href="chap8.html#dependencemeasures" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="pearsons-correlation-coefficient" class="section level4 hasAnchor" number="9.4.5.1">
<h4><span class="header-section-number">9.4.5.1</span> Pearson’s Correlation Coefficient<a href="chap8.html#pearsons-correlation-coefficient" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We can express Pearson’s linear correlation coefficient in the following form:
<span class="math display">\[\begin{eqnarray*}
r(X_1,X_2)&amp;=&amp;\frac{1}{\sqrt{\mathbb{V}[X_1]\mathbb{V}[X_2]}}\\
&amp;&amp;\int\int_{(u_1,u_2)\in[0,1]^2}\big\{C(u_1,u_2)-u_1u_2\big\}
dF_1^{-1}(u_1)dF_2^{-1}(u_1).
\end{eqnarray*}\]</span>
Thus, we see that the linear correlation coefficient <span class="math inline">\(r\)</span> depends not only on the copula but also on the marginal distributions. This explains some of the drawbacks of this dependence measure.</p>
<p>Kendall’s tau and Spearman’s rho coefficients are based on the same concept: concordance. We can then generalize Kendall’s tau and Spearman’s rho as follows.</p>
<div class="definition">
<p><span id="def:unlabeled-div-43" class="definition"><strong>Definition 9.13  </strong></span>Let $( X_{1},X_{2}) $ and $( Y_{1},Y_{2}) $ be two independent pairs, with copulas <span class="math inline">\(C_{1}\)</span> and <span class="math inline">\(C_{2}\)</span>, and their cumulative distribution functions <span class="math inline">\(F_{\boldsymbol{X}}\)</span> and <span class="math inline">\(F_{\boldsymbol{Y}}\)</span> are in <span class="math inline">\(\mathcal{F}(F_1,F_2)\)</span>. The concordance measure <span class="math inline">\(Q\)</span> is defined as the difference between the probabilities of concordance and discordance of the vectors <span class="math inline">\(\boldsymbol{X}\)</span> and <span class="math inline">\(\boldsymbol{Y}\)</span>, i.e.,
<span class="math display">\[\begin{eqnarray*}
Q\left( C_{1},C_{2}\right)&amp; =&amp;{\Pr}\big[ \left(X_{1}-Y_{1}\right) \left( X_{2}-Y_{2}\right) &gt;0\big]\\
&amp;&amp;-{\Pr}\big[ \left(X_{1}-Y_{1}\right) \left( X_{2}-Y_{2}\right) &lt;0\big]\\
&amp;=&amp;4\int\int_{\left[ 0,1\right]^{2}}C_{2}\left( u_1,u_2\right) dC_{1}\left( u_1,u_2\right) -1.
\end{eqnarray*}\]</span>%</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-44" class="example"><strong>Example 9.10  </strong></span>Taking the copulas of the Fréchet bounds <span class="math inline">\(C_M\)</span> and <span class="math inline">\(C_W\)</span> and the independence copula <span class="math inline">\(C_I\)</span>, we have the following table:
<span class="math display">\[\begin{equation*}
\begin{tabular}{|c|ccc|}
\hline
$Q\left( C_{\cdot },C_{\cdot }\right) $ &amp; $C_M$ &amp; $C_I$ &amp; $C_W$ \\
\hline
$C_M$ &amp; $-1$ &amp; $-1/3$ &amp; $0$ \\
$C_I$ &amp; $-1/3$ &amp; $0$ &amp; $1/3$ \\
$C_W$ &amp; $0$ &amp; $1/3$ &amp; $1$ \\ \hline
\end{tabular}%
\end{equation*}\]</span>%</p>
<p>In general, for any copula <span class="math inline">\(C\)</span>, the following inequalities hold:
<span class="math display">\[\begin{eqnarray*}
0&amp;\leq&amp; Q\left(C,C_W\right) \leq 1,\\
-1&amp;\leq&amp; Q\left( C,C_M\right) \leq 0,\\
-1/3&amp;\leq&amp; Q\left( C,C_I\right) \leq 1/3.
\end{eqnarray*}\]</span></p>
</div>
<p>Kendall’s tau can be expressed using the concordance measure <span class="math inline">\(Q\)</span> introduced above, as shown in the following result.</p>
<p>In cases where the derivative exists, Kendall’s tau can also be expressed as
<span class="math display">\[\begin{eqnarray*}
\tau \left( X_1,X_2\right)&amp;=&amp;4\int\int_{\left[ 0,1\right] ^{2}}\frac{\partial ^{2}}{%
\partial u_1\partial u_2}C\left( u_1,u_2\right) du_1du_2-1\\
&amp;=&amp;1-4\int\int_{\left[ 0,1\right]
^{2}}\frac{\partial }{\partial u_1}C\left( u_1,u_2\right) \frac{\partial }{%
\partial u_2}C\left( u_1,u_2\right) du_1du_2.
\end{eqnarray*}\]</span></p>
<p>Now let’s move on to Spearman’s rho. The latter can be expressed as follows using the concordance measure <span class="math inline">\(Q\)</span>.</p>
<p>In Figure <span class="math inline">\(\ref{FIG-KENDALL-SPEARMAN-2}\)</span>, the values of $$ and $$ are represented for common families of one-parameter copulas. It can be noted that for most copulas (Clayton, Gumbel, Gaussian, Frank, etc.), a large part of the region of admissibility of Corollary <span class="math inline">\(\ref{InegaliteRhoTau}\)</span> represented in Figure <span class="math inline">\(\ref{FIG-KENDALL-SPEARMAN-1}\)</span> is not reached.</p>
<div class="remark">
<p><span id="unlabeled-div-45" class="remark"><em>Remark</em>. </span>It is entirely possible to have both <span class="math inline">\(\rho =0\)</span> and <span class="math inline">\(\tau =0\)</span> without having independence between the random variables. To see this, consider the “cubic” copula, with the following expression:
<span class="math display">\[
C\left( u_1,u_2\right) =u_1u_2+\theta \big( u_1\left( u_1-1\right) \left( 2u_1-1\right) %
\big) \big( u_2\left( u_2-1\right) \left( 2u_2-1\right) \big)
\]</span>
where <span class="math inline">\(\theta \in \left[ -1,2\right]\)</span>. The shape of this copula is shown in Figure <span class="math inline">\(\ref{copula-cubique1}\)</span> for extreme values <span class="math inline">\(\theta=-1\)</span> and <span class="math inline">\(\theta=2\)</span>. For any <span class="math inline">\(\theta \neq 0\)</span>, <span class="math inline">\(\rho =0\)</span> and <span class="math inline">\(\tau =0\)</span> without having independence.</p>
</div>
<p>In economic literature, the Gini coefficient is used to study income differences, for example, between two populations.</p>
<div class="definition">
<p><span id="def:unlabeled-div-46" class="definition"><strong>Definition 9.14  </strong></span>Let <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> be two random variables with cumulative distribution functions <span class="math inline">\(F_1\)</span> and <span class="math inline">\(F_2\)</span>, both continuous, and copula <span class="math inline">\(C\)</span>. The Gini coefficient <span class="math inline">\(\gamma\)</span> of the pair <span class="math inline">\(\left(X_1,X_2\right)\)</span> is given by
<span class="math display">\[\begin{eqnarray*}
\gamma \left( X_1,X_2\right)&amp; =&amp;2\mathbb{E}\Big[ \big| F_1(X_1)+F_2(X_2)-1\big|-\big|
F_1(X_1)+F_2(X_2)\big| \Big] \\
&amp;=&amp;2\int\int_{\left[ 0,1\right] ^{2}}\big(\left| u_1+u_2-1\right| -\left| u_1+u_2\right| \big) dC\left( u_1,u_2\right).
\end{eqnarray*}\]</span></p>
</div>
<p>The Gini coefficient can be expressed in various forms, including
<span class="math display">\[\begin{eqnarray}
\gamma \left( X_1,X_2\right) &amp;=&amp;4 \int_{0}^{1}C\left( u,1-u\right) du\nonumber\\
&amp;&amp;-4\int_{0}^{1}\left\{ u-C\left( u,u\right) \right\} du  \label{gini2} \\
&amp;=&amp;4\int\int_{\left[ 0,1\right] ^{2}}C\left( u_1,u_2\right) dC_W\left(u_1,u_2\right)\nonumber\\
&amp;&amp;+4\int\int_{\left[ 0,1\right] ^{2}}C\left( u_1,u_2\right)dC_M\left( u_1,u_2\right) -2, \nonumber
\end{eqnarray}\]</span>%
where, in the right term of <span class="math inline">\(\eqref{gini2}\)</span>, the second integral corresponds to the distance between the diagonal of the upper Fréchet bound and that of the copula <span class="math inline">\(C\)</span>.</p>
<p>As was the case for Kendall’s tau and Spearman’s rho, certain specific values of the Gini coefficient characterize the copulas <span class="math inline">\(C_M\)</span>, <span class="math inline">\(C_W\)</span>, and, under positive quadrant dependence, <span class="math inline">\(C_I\)</span>.</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-47" class="proposition"><strong>Proposition 9.12  </strong></span>Consider <span class="math inline">\(\boldsymbol{X}\)</span> with cumulative distribution function <span class="math inline">\(F_{\boldsymbol{X}}\)</span> in <span class="math inline">\(\mathcal{F}(F_1,F_2)\)</span>. Then,</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\gamma \left( X_1,X_2\right) =-1\)</span> if and only if <span class="math inline">\(\boldsymbol{X}\)</span> is antimonotone;</li>
<li><span class="math inline">\(\gamma \left( X_1,X_2\right) =1\)</span> if and only if <span class="math inline">\(\boldsymbol{X}\)</span> is comonotone;</li>
<li>if <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are positively dependent in the quadrant, then <span class="math inline">\(\gamma \left(X_1,X_2\right) =0\)</span> if and only if <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent.</li>
</ol>
</div>
<div class="proof">
<p><span id="unlabeled-div-48" class="proof"><em>Proof</em>. </span>We prove (2); the proofs for (1) and (3) are similar and are therefore omitted. If <span class="math inline">\(\boldsymbol{X}\)</span> is comonotone, then trivially, <span class="math inline">\(\gamma =1\)</span>. Conversely, suppose <span class="math inline">\(\gamma =1\)</span>. Then, using $( <span class="math inline">\(\ref{gini2}\)</span>) $, we can write
<span class="math display">\[
\gamma \left(U_1,U_2\right)=\gamma \left( U,U\right) =1,
\]</span>
which in turn gives
<span class="math display">\[
\int\int_{\left[ 0,1\right] ^{2}}\underset{\geq 0\text{ for all }u_1,u_2}{\underbrace{\Big\{ C_W\left( u_1,u_2\right) -C\left( u_1,u_2\right) \Big\}}}
dC_W\left( u_1,u_2\right)
\]</span>
<span class="math display">\[
+\int\int_{\left[ 0,1\right]^2}\underset{\geq 0\text{ for all }u_1,u_2}{\underbrace{\Big\{C_W\left( u_1,u_2\right) -C\left( u_1,u_2\right) \Big\}}}
dC_M\left( u_1,u_2\right) =0,
\]</span>
meaning that <span class="math inline">\(C=C_W\Leftrightarrow\boldsymbol{X}\)</span> is comonotone.</p>
</div>
</div>
</div>
</div>
<div id="archimedean-copulas" class="section level2 hasAnchor" number="9.5">
<h2><span class="header-section-number">9.5</span> Archimedean Copulas<a href="chap8.html#archimedean-copulas" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="definition-32" class="section level3 hasAnchor" number="9.5.1">
<h3><span class="header-section-number">9.5.1</span> Definition<a href="chap8.html#definition-32" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Most one-parameter copulas presented in the previous section belong to a larger family with a functional parameter: Archimedean copulas.</p>
<div class="definition">
<p><span id="def:DefArchi" class="definition"><strong>Definition 9.15  </strong></span>Let <span class="math inline">\(\varphi :\left[ 0,1\right] \rightarrow {\mathbb{R}}^+\)</span> be a convex and strictly decreasing function, such that <span class="math inline">\(\varphi \left( 1\right) =0\)</span>. Let us define <span class="math inline">\(\varphi ^{\left[ -1\right] }\)</span>, the pseudo-inverse of the function $$, as
<span class="math display">\[\begin{equation*}
\varphi ^{[-1]}\left( t\right) =\left\{
\begin{array}{ll}
\varphi ^{-1}\left( t\right), &amp; \text{if }0\leq t\leq \varphi \left( 0\right), \\
0, &amp; \text{if }\varphi \left( 0\right) \leq t\leq +\infty.
\end{array}%
\right.
\end{equation*}\]</span>%
This pseudo-inverse defines a continuous function, decreasing on <span class="math inline">\({\mathbb{R}}^{+}\)</span>, and strictly decreasing on $$. For simplicity, we will denote <span class="math inline">\(\varphi ^{-1}\)</span> this pseudo-inverse.</p>
<p>The function
<span class="math display">\[\begin{equation*}
C\left( u_1,u_2\right) =\varphi ^{-1}\left( \varphi \left(
u_1\right) +\varphi \left( u_2\right) \right) \text{ for }0\leq
u_1,u_2\leq 1,
\end{equation*}\]</span>%
is a copula, called an Archimedean copula. The function $$ is called the generator of the copula</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-49" class="remark"><em>Remark</em>. </span>It is worth noting that there is uniqueness of the generator, up to a multiplicative constant: thus $$ and $$ generate the same copula, for any <span class="math inline">\(\kappa &gt;0\)</span>, and conversely, if
<span class="math display">\[
\varphi_1 ^{-1}\left( \varphi_1 \left( u\right) +\varphi_1 \left( v\right)
\right) =\varphi_2 ^{-1}\left( \varphi_2  \left( u\right) +\varphi_2  \left(
v\right) \right)
\]</span>
then there exists <span class="math inline">\(\kappa &gt;0\)</span> such that $_2 =_1 $. The proof is provided in <span class="citation">(<a href="#ref-schweizer1983probabilistic" role="doc-biblioref">B. Schweizer and Sklar 1983</a>)</span> in the context of probability metric spaces and for Archimedean binary operations (Theorem 5.4.8), but the proof remains unchanged for Archimedean copulas.</p>
<p>From now on, we will refer to the generator of an Archimedean copula, even though it is defined up to a multiplicative constant.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-50" class="example"><strong>Example 9.11  </strong></span>The independence copula <span class="math inline">\(C_I\)</span> is an Archimedean copula with the generator $( t) =( 1/t) $. Similarly, the Gumbel copula is an Archimedean copula with the generator <span class="math inline">\(\varphi \left( t\right) =\left( -\log t\right) ^{\alpha }\)</span>.</p>
</div>
<p>Of course, not all copulas are Archimedean, as the following example demonstrates.</p>
<div class="example">
<p><span id="exm:unlabeled-div-51" class="example"><strong>Example 9.12  </strong></span>The copula of the upper Fréchet bound, denoted as <span class="math inline">\(C_W\)</span>, is not archimedean. Indeed, if there existed a generator <span class="math inline">\(\varphi\)</span> such that the archimedean representation of Definition <span class="math inline">\(\ref{DefArchi}\)</span> applied, we should have
<span class="math display">\[
C_\varphi(u,u)=u\text{ for all $0&lt;u&lt;1$,}
\]</span>
which would imply
<span class="math display">\[
2\varphi(u)=\varphi(u)\text{ for all $0&lt;u&lt;1$,}
\]</span>
which is clearly impossible.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-52" class="remark"><em>Remark</em>. </span>
Archimedean copulas can easily generate copulas with two parameters. For this purpose, consider a family of one-parameter generators, denoted as <span class="math inline">\(\varphi_\theta\)</span>, and consider
<span class="math display">\[
\varphi_{\alpha,\theta}\left(t\right) = \varphi_\theta\left(t^\alpha\right) \text{ and }
\varphi_{\theta,\beta}\left(t\right) = \left[\varphi_\theta\left(t\right)\right]^\beta.
\]</span>
If <span class="math inline">\(\beta \geq 1\)</span>, then <span class="math inline">\(\varphi_{\theta,\beta}\)</span> is a generator of an archimedean copula. If <span class="math inline">\(\alpha \in (0,1)\)</span>, then <span class="math inline">\(\varphi_{\alpha,\theta}\)</span> is a generator of an archimedean copula (in fact, more generally, if <span class="math inline">\(\varphi_{\theta}\)</span> is twice differentiable, and if <span class="math inline">\(t\varphi_{\theta}^{\prime}\left(t\right)\)</span> is an increasing function on <span class="math inline">\((0,1)\)</span>, then <span class="math inline">\(\varphi_{\alpha,\theta}\)</span> is a generator for any <span class="math inline">\(\alpha &gt; 0\)</span>). Moreover, if <span class="math inline">\(\varphi^{\prime}\left(1\right) \neq 0\)</span>, then, denoting <span class="math inline">\(C_{\alpha,\theta}\)</span> and <span class="math inline">\(C_{\theta,\beta}\)</span> as the archimedean copulas generated by <span class="math inline">\(\varphi_{\alpha,\theta}\)</span> and <span class="math inline">\(\varphi_{\theta,\beta}\)</span>, respectively, we have
<span class="math display">\[
\lim_{\alpha \rightarrow 0}C_{\alpha,\theta}\left(u_1,u_2\right) = C_I\left(u_1,u_2\right)
\]</span>
and
<span class="math display">\[
\lim_{\beta \rightarrow \infty}C_{\theta,\beta}\left(u_1,u_2\right) = C_W\left(u_1,u_2\right).
\]</span>
One can also generate three-parameter archimedean copulas by considering generators of the form
<span class="math display">\[
\varphi_{\alpha,\beta,\theta}\left(t\right) = \left(\varphi_\theta\left(t^\alpha\right)\right)^\beta.
\]</span></p>
</div>
</div>
<div id="frailty-models-and-archimedean-copulas" class="section level3 hasAnchor" number="9.5.2">
<h3><span class="header-section-number">9.5.2</span> Frailty Models and Archimedean Copulas<a href="chap8.html#frailty-models-and-archimedean-copulas" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Archimedean copulas are closely related to frailty models. Specifically, suppose that <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are conditionally independent given a third unobservable variable <span class="math inline">\(Z\)</span> (which represents, for example, a level of risk or the magnitude of a catastrophe). Suppose that
<span class="math display">\[
\Pr[X_1 \leq x_1 | Z=z] = \left\{B_1(x_1)\right\}^z
\]</span>
and
<span class="math display">\[
\Pr[X_2 \leq x_2 | Z=z] = \left\{B_2(x_2)\right\}^z
\]</span>
for distribution functions <span class="math inline">\(B_1\)</span> and <span class="math inline">\(B_2\)</span>. The joint distribution function of the pair <span class="math inline">\(\boldsymbol{X}\)</span> is then given by
<span class="math display">\[\begin{eqnarray*}
F_{\boldsymbol{X}}(\boldsymbol{x}) &amp;=&amp; \mathbb{E}\left[\left\{B_1(x_1)\right\}^z \left\{B_2(x_2)\right\}^z\right]\\
&amp;=&amp; L_Z\left(-\ln B_1(x_1)-\ln B_2(x_2)\right), \quad \boldsymbol{x}\in{\mathbb{R}}^2.
\end{eqnarray*}\]</span>
This distribution function satisfies the archimedean representation with <span class="math inline">\(\varphi^{-1}=L_Z\)</span> and marginal distributions
<span class="math display">\[
F_i(x_i) = L_Z(-\ln B_i(x_i)), \quad i=1,2.
\]</span></p>
<div class="example">
<p><span id="exm:unlabeled-div-53" class="example"><strong>Example 9.13  </strong></span>For example, the Clayton family is obtained by considering variables <span class="math inline">\(Z\)</span> with a Gamma distribution (i.e., <span class="math inline">\({L}_Z\left(t\right) =\left( 1+t\right) ^{-1/\alpha }\)</span> where <span class="math inline">\(\alpha&gt;0\)</span>).</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-54" class="example"><strong>Example 9.14  </strong></span>Hougaard proposed in 1986 to take the stable distribution of Laplace transform for <span class="math inline">\(Z\)</span>, i.e., <span class="math inline">\(L_Z(t) = \exp(-t^\alpha)\)</span> with parameter <span class="math inline">\(\alpha\)</span>. Then we obtain
<span class="math display">\[
\overline{F}_{\boldsymbol{X}}(\boldsymbol{x})=\exp\left\{-\left(-\ln\overline{B}_1(x_1)-\ln\overline{B}_2(x_2)\right)^\alpha\right\}.
\]</span>
In particular, by taking marginal distributions as Weibull distributions, i.e.,
<span class="math display">\[
\overline{B}_i(x)=-\exp(-\alpha_ix^{\beta_i}), \quad x\in{\mathbb{R}}^+,
\]</span>
we have
<span class="math display">\[
\overline{F}_{\boldsymbol{X}}(\boldsymbol{x})=\exp\left\{-\left(\alpha_1x_1^{\beta_1}+\alpha_2x_2^{\beta_2}\right)^\alpha\right\}.
\]</span>
This choice ensures that both marginal and conditional distributions are Weibull.</p>
</div>
</div>
<div id="survival-function" class="section level3 hasAnchor" number="9.5.3">
<h3><span class="header-section-number">9.5.3</span> Survival Function<a href="chap8.html#survival-function" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Archimedean copulas can also be obtained in a third way. Consider pairs $( X_1,X_2) $ such that their joint survival function satisfies
<span class="math display">\[
\overline{F}_{\boldsymbol{X}}(x_1,x_2) = \overline{G}(x_1+x_2)
\]</span>
where <span class="math inline">\(\overline{G}\)</span> is a convex survival function with <span class="math inline">\(\overline{G}(0)=1\)</span>. The marginal distributions of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> must then be identical, i.e.,
<span class="math display">\[
\overline{F}_1(x) = \overline{F}_2(x) = \overline{G}(x).
\]</span>
In this case, the survival copula of the pair $( X_1,X_2) $ is
<span class="math display">\[\begin{eqnarray*}
\overline{C}(u_1,u_2) &amp;=&amp; \overline{F}_{\boldsymbol{X}}\left(\overline{F}_1^{-1}(u_1),\overline{F}_2^{-1}(u_2)\right)\\
&amp;=&amp; \overline{G}\left(\overline{G}^{-1}(u_1)+\overline{G}^{-1}(u_2)\right),
\end{eqnarray*}\]</span>%
which defines an archimedean copula with <span class="math inline">\(\varphi=\overline{G}^{-1}\)</span>.</p>
</div>
<div id="regression-function" class="section level3 hasAnchor" number="9.5.4">
<h3><span class="header-section-number">9.5.4</span> Regression Function<a href="chap8.html#regression-function" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The regression function <span class="math inline">\(x_1\mapsto\mathbb{E}[X_2|X_1=x_1]\)</span> is one of the most appreciated tools to describe the dependence between <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>. In the archimedean case, the conditional tail function of <span class="math inline">\(X_2\)</span> is given by
<span class="math display">\[\begin{eqnarray*}
\Pr[X_2&gt;x_2|X_1=x_1] &amp;=&amp; 1-C_{2|1}\big(F_1(x_1)|F_2(x_2)\big)\\
&amp;=&amp; 1-\frac{\partial}{\partial u_1}C\big(F_1(x_1),F_2(x_2)\big)\\
&amp;=&amp; 1-\frac{\varphi^{(1)}(F_1(x_1))}{\varphi^{(1)}\big(C\big(F_1(x_1)|F_2(x_2)\big)\big)}.
\end{eqnarray*}\]</span>
We then obtain
<span class="math display">\[
\mathbb{E}[X_2|X_1=x_1]=\int_{x_2=0}^{+\infty}\left(1-
\frac{\varphi^{(1)}(F_1(x_1))}{\varphi^{(1)}\big(C_\varphi(F_1(x_1),F_2(x_2))\big)}\right)dx_2.
\]</span></p>
<p>::: {.example}[Frank Copula]
In this case, we have
<span class="math display">\[
\frac{\partial}{\partial u_1}C(\boldsymbol{u})=\frac{\exp\big(-\alpha(u_1+u_2)\big)-\exp(-\alpha u_1)}{\exp\big(-\alpha(u_1+u_2)\big)
-\exp(-\alpha u_1)-\exp(-\alpha u_2)+\exp(-\alpha)}.
\]</span>
From which we can derive
<span class="math display">\[\begin{eqnarray*}
\mathbb{E}[U_2|U_1=u_1]
%&amp;=&amp;\int_{u_2=0}^1\left\{1-C_\alpha^{(1,0)}(u_1,u_2)\right\}du_2\\
&amp;=&amp;\frac{\big(1-\exp\alpha\big)u_1\exp(\alpha u_1)+\exp(\alpha)\big(\exp(\alpha u_1)-1\big)}
{\big(\exp(\alpha u_1)-1\big)\big(\exp(\alpha)-\exp(\alpha u_1)\big)}.
\end{eqnarray*}\]</span>
:::</p>
</div>
<div id="bivariate-integral-transformation" class="section level3 hasAnchor" number="9.5.5">
<h3><span class="header-section-number">9.5.5</span> Bivariate Integral Transformation<a href="chap8.html#bivariate-integral-transformation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Given a pair <span class="math inline">\(\boldsymbol{U}\)</span> of distribution functions <span class="math inline">\(C\)</span>, let <span class="math inline">\(K\)</span> be the distribution function of the random variable <span class="math inline">\(C\left(U_1,U_2\right)\)</span>, i.e.,
<span class="math display">\[
K\left(z\right) = {\Pr}[C\left(U_1,U_2\right) \leq z].
\]</span>
The following property is derived from <span class="citation">(<a href="#ref-genest1993statistical" role="doc-biblioref">Genest and Rivest 1993</a>)</span>.</p>
<p>The value <span class="math inline">\(K\left(t\right)\)</span> has a geometric interpretation: the tangent to the curve <span class="math inline">\(y=\varphi\left(x\right)\)</span> passing through the point <span class="math inline">\(t\)</span> intersects the x-axis at the point <span class="math inline">\(K\left(t\right)\)</span>.</p>
<p>When the generator is a sufficiently regular function, it is possible to obtain the density of the copula in terms of this generator.</p>
<div class="proposition">
<p><span id="prp:CopArchiDens" class="proposition"><strong>Proposition 9.13  </strong></span>Let <span class="math inline">\(C\)</span> be an archimedean copula with generator <span class="math inline">\(\varphi\)</span> that is twice differentiable. Then the copula <span class="math inline">\(C\)</span> has the density,
<span class="math display">\[\begin{equation*}
c\left(u_1,u_2\right) = -\frac{\varphi^{\prime\prime}\left(C\left(u_1,u_2\right)\right)\varphi^{\prime}\left(u_1\right)\varphi^{\prime}\left(u_2\right)}{\left\{\varphi^{\prime}\left(C\left(u_1,u_2\right)\right)\right\}^3}, \text{ for }u_1,u_2\in\left[0,1\right].
\end{equation*}\]</span>
\end{Proposition}
::: {.proof}
Let’s start by taking the derivative of <span class="math inline">\(C\)</span> with respect to <span class="math inline">\(u_1\)</span>, which yields
<span class="math display">\[\begin{equation}
\label{ExpDens1}
\varphi^{(1)}\left(C\left(\boldsymbol{u}\right)\right)\frac{\partial}{\partial u_1}C\left(\boldsymbol{u}\right)=\varphi^{(1)}\left(u_1\right).
\end{equation}\]</span>
If we then take the derivative of this expression with respect to <span class="math inline">\(u_2\)</span>, we obtain
<span class="math display">\[
\varphi^{(2)}\left(C\left(\boldsymbol{u}\right)\right)\frac{\partial}{\partial u_2}C\left(\boldsymbol{u}\right)
\frac{\partial}{\partial u_1}C\left(\boldsymbol{u}\right)+\varphi^{(1)}\left(C\left(\boldsymbol{u}\right)\right)\frac{\partial^2}{\partial u_1^2}
C\left(\boldsymbol{u}\right)=0.
\]</span>
By plugging the expression obtained in <span class="math inline">\(\eqref{ExpDens1}\)</span> for <span class="math inline">\(\frac{\partial}{\partial u_1}C\left(\boldsymbol{u}\right)\)</span> and <span class="math inline">\(\frac{\partial}{\partial u_2}C\left(\boldsymbol{u}\right)\)</span>, we get
<span class="math display">\[
c\left(\boldsymbol{u}\right)=\frac{\partial^2}{\partial u_1\partial u_2}C\left(\boldsymbol{u}\right)=
-\frac{\varphi^{(2)}\left(C\left(\boldsymbol{u}\right)\right)\varphi^{(1)}\left(u_1\right)\varphi^{(1)}\left(u_2\right)}
{\left\{\varphi^{(1)}\left(C\left(\boldsymbol{u}\right)\right)\right\}^3}
\]</span>
which completes the proof.</p>
</div>
<p>We are now able to establish the expression of Kendall’s Tau associated with an archimedean copula.</p>
<div class="proof">
<p><span id="unlabeled-div-55" class="proof"><em>Proof</em>. </span>Since <span class="math inline">\(C(\boldsymbol{u})=0\)</span> for all <span class="math inline">\(\boldsymbol{u}\)</span> such that <span class="math inline">\(\varphi(u_1)+\varphi(u_2)=\varphi(0)\)</span>, Proposition <span class="math inline">\(\ref{CopArchiDens}\)</span> allows us to write
<span class="math display">\[
\tau_\varphi = 4\int\int_{\mathcal{D}}C(\boldsymbol{u})
\frac{\varphi^{(2)}(C(\boldsymbol{u}))\varphi^{(1)}(u_1)\varphi^{(1)}(u_2)}
{\{\varphi^{(1)}(C(\boldsymbol{u}))\}^3}du_1du_2-1
\]</span>
where
<span class="math display">\[
\mathcal{D}=\big\{\boldsymbol{u}\in[0,1]^2\big|\varphi(u_1)+\varphi(u_2)&lt;\varphi(0)\big\}.
\]</span>
Now, let’s perform the change of variables
<span class="math display">\[
\left\{
\begin{array}{l}
v_1=C(\boldsymbol{u})=\varphi^{-1}\big(\varphi(u_1)+\varphi(u_2)\big),\\
v_2=u_1,
\end{array}
\right.
\]</span>
which ensures <span class="math inline">\(\boldsymbol{v}\in[0,1]^2\)</span>. For a fixed value of <span class="math inline">\(v_1\)</span>, it is easy to see that <span class="math inline">\(v_1\leq v_2\leq 1\)</span>. The Jacobian of this transformation is given by
<span class="math display">\[
\left\|\frac{\partial\boldsymbol{v}}{\partial\boldsymbol{u}}\right\| =
\text{det}\left(
\begin{array}{cc}
\frac{\varphi &#39;(u_1)}{\varphi &#39;\big(C(\boldsymbol{u})\big)}&amp;\frac{\varphi &#39;(u_2)}{\varphi &#39;\big(C(\boldsymbol{u})\big)}\\
1&amp;0
\end{array}
\right)
=-\frac{\varphi &#39;(u_2)}{\varphi &#39;\big(C(\boldsymbol{u})\big)},
\]</span>
and we easily obtain the announced result.</p>
</div>
</div>
<div id="order-relations-for-archimedean-copulas" class="section level3 hasAnchor" number="9.5.6">
<h3><span class="header-section-number">9.5.6</span> Order Relations for Archimedean Copulas<a href="chap8.html#order-relations-for-archimedean-copulas" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We have seen that when marginal distribution functions are continuous, supermodular comparison relies exclusively on underlying copulas. In the case of archimedean copulas, we can reduce a supermodular comparison to conditions on the generators, as shown in the following results.</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-56" class="proposition"><strong>Proposition 9.14  </strong></span>Let <span class="math inline">\(C_{1}\)</span> and <span class="math inline">\(C_{2}\)</span> be two archimedean copulas with respective generators <span class="math inline">\(\varphi _{1}\)</span> and <span class="math inline">\(\varphi _{2}\)</span>. Then <span class="math inline">\(C_{1}\lsmC_{2}\)</span> (i.e., $C_{1}() C_{2}() $ on $$) if, and only if, the function <span class="math inline">\(\varphi _{1}\circ \varphi _{2}^{-1}\)</span> is subadditive, i.e.,
<span class="math display">\[
\varphi
_{1}\circ \varphi _{2}^{-1}\left( x+y\right) \leq \varphi _{1}\circ \varphi
_{2}^{-1}\left( x\right) +\varphi _{1}\circ \varphi _{2}^{-1}\left(
y\right) .
\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-57" class="proof"><em>Proof</em>. </span>Let <span class="math inline">\(f=\varphi_1\circ\varphi_2^{-1}\)</span>. The function <span class="math inline">\(f\)</span> thus defined is continuous, non-decreasing, and satisfies <span class="math inline">\(f(0)=0\)</span>. Clearly,
<span class="math display">\[
C_1(\boldsymbol{u})\leq C_2(\boldsymbol{u})\text{ for all }\boldsymbol{u}\in[0,1]^2
\]</span>
if, and only if,
<span class="math display">\[\begin{equation}
\label{eqNels1}
\varphi_1^{-1}\big(\varphi_1(u_1)+\varphi_1(u_2)\big)\leq
\varphi_2^{-1}\big(\varphi_2(u_1)+\varphi_2(u_2)\big).
\end{equation}\]</span>
Let <span class="math inline">\(x=\varphi_2(u_1)\)</span> and <span class="math inline">\(y=\varphi_2(u_2)\)</span>. In this case, <span class="math inline">\(\eqref{eqNels1}\)</span> is equivalent to
<span class="math display">\[\begin{equation}
\label{eqNels2}
\varphi_1^{-1}\big(f(x)+f(y)\big)\leq\varphi_2^{-1}(x+y)
\end{equation}\]</span>
for all <span class="math inline">\(x,y\in[0,\varphi_2(0)]\)</span>. Furthermore, if <span class="math inline">\(x&gt;\varphi_2(0)\)</span> and <span class="math inline">\(y&gt;\varphi_2(0)\)</span>, then each term in <span class="math inline">\(\eqref{eqNels2}\)</span> vanishes.</p>
<p>Now suppose that <span class="math inline">\(C_1(\boldsymbol{u})\leq C_2(\boldsymbol{u})\)</span> for all <span class="math inline">\(\boldsymbol{u}\in[0,1]^2\)</span>. Transform both sides of <span class="math inline">\(\eqref{eqNels2}\)</span> by the generator <span class="math inline">\(\varphi_1\)</span> and note that <span class="math inline">\(\varphi_1\circ\varphi_2^{-1}(u)\leq u\)</span> for all <span class="math inline">\(u\geq 0\)</span>. This allows us to state that <span class="math inline">\(f(x+y)\leq f(x)+f(y)\)</span> for all <span class="math inline">\(x,y\in{\mathbb{R}}^+\)</span>, so <span class="math inline">\(f\)</span> is indeed subadditive. Conversely, if <span class="math inline">\(f\)</span> is subadditive, then by applying <span class="math inline">\(\varphi_1^{-1}\)</span> to each term of the inequality <span class="math inline">\(f(x+y)\leq f(x)+f(y)\)</span> and noting that <span class="math inline">\(\varphi_1^{-1}\circ f=\varphi_2^{-1}\)</span>, we obtain <span class="math inline">\(\eqref{eqNels1}\)</span>, which concludes the proof.</p>
</div>
<p>However, verifying the subadditivity of <span class="math inline">\(f=\varphi_1\circ\varphi_2^{-1}\)</span> may be as difficult as verifying directly that <span class="math inline">\(C_1\)</span> and <span class="math inline">\(C_2\)</span> are such that <span class="math inline">\(C_1(\boldsymbol{u})\leq C_2(\boldsymbol{u})\)</span> for all <span class="math inline">\(\boldsymbol{u}\in[0,1]^2\)</span>. That’s why we present below several sufficient conditions for <span class="math inline">\(\varphi_1\circ\varphi_2^{-1}\)</span> to be subadditive.</p>
<div class="example">
<p><span id="exm:unlabeled-div-58" class="example"><strong>Example 9.15  </strong></span>As noted in Remark (<span class="math inline">\(\ref{archimedien alpha beta}\)</span>), if $$ is a generator, then $_{,1}( t) =(t^{}) $ and <span class="math inline">\(\varphi _{1,\beta}\left( t\right) =\left[ \varphi \left( t\right) \right] ^{\beta }\)</span> for $(0,1] $ and <span class="math inline">\(\beta \geq 1\)</span> are also generators. Noting <span class="math inline">\(C_{\beta _{i}}\)</span> as the archimedean copula with generator <span class="math inline">\(\varphi _{1,\beta _{i}}\)</span>, then if <span class="math inline">\(\beta _{1}\leq \beta _{1}\)</span>, <span class="math inline">\(C_{\beta _{1}}\preceq_{\text{sm}}C_{\beta _{2}}\)</span>. A similar relation exists for the <span class="math inline">\(C_{\alpha _{i}}\)</span> archimedean copulas with generator <span class="math inline">\(\varphi\)</span>.</p>
</div>
</div>
<div id="study-of-a-function-of-two-correlated-risks" class="section level3 hasAnchor" number="9.5.7">
<h3><span class="header-section-number">9.5.7</span> Study of a Function of Two Correlated Risks<a href="chap8.html#study-of-a-function-of-two-correlated-risks" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="problem-presentation" class="section level4 hasAnchor" number="9.5.7.1">
<h4><span class="header-section-number">9.5.7.1</span> Problem Presentation<a href="chap8.html#problem-presentation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Given two possibly correlated risks, say <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>, it is interesting to study the distribution of their sum, <span class="math inline">\(X_1+X_2\)</span>. Kolmogorov formulated this problem relatively early; however, it was not until <span class="citation">(<a href="#ref-makarov1981inequalities" role="doc-biblioref">Makarov 1981</a>)</span> that an answer to this question was obtained.</p>
<p>Due to the technicality of the developments, most of the results in this section are stated without proof.</p>
<p>We introduced the convolution product <span class="math inline">\(\star\)</span> in Section <span class="math inline">\(\ref{SecProdConv}\)</span> to obtain the cumulative distribution function of the sum of independent risks. But what happens if the variables are no longer independent? In particular, one may wonder if the “worst” case corresponds to comonotonicity and the “best” case to antimonotonicity (here, “worst” and “best” are with respect to the <span class="math inline">\(\preceq_{\text{VaR}}\)</span> relation). This result is generally false, as shown in the following example.</p>
<div class="example">
<p><span id="exm:unlabeled-div-59" class="example"><strong>Example 9.16  </strong></span>Consider the risks <span class="math inline">\(X_1\sim\mathcal{Exp}(1)\)</span> and <span class="math inline">\(X_2\sim\mathcal{Exp}(1)\)</span>, possibly correlated. The inequalities
<span class="math display">\[
\exp(-x)\leq\Pr[X_1+X_2&gt; x]\leq\exp(-(x-2\ln 2)_+)/2
\]</span>
hold for all <span class="math inline">\(x\in\mathbb{R}^+\)</span>. Figure <span class="math inline">\(\ref{Bounds}\)</span> shows the bounds on the tail function of <span class="math inline">\(X_1+X_2\)</span>, as well as the values corresponding to independence (the tail function of the <span class="math inline">\(\mathcal{Gam}(2,1)\)</span> distribution) and comonotonicity (the tail function of the <span class="math inline">\(\mathcal{Exp}(2)\)</span> distribution). The same figure shows the bounds on the Value-at-Risk (VaR).</p>
<p>It can be seen that the curves corresponding to independence and comonotonicity intersect only once, and the tail function or extreme VaR values can be considerably larger than those obtained based on comonotonicity.</p>
</div>
<p>More generally, it is interesting to study the bounds of the cumulative distribution function of <span class="math inline">\(\Psi(X_1,X_2)\)</span> for functions <span class="math inline">\(\Psi:\mathbb{R}^{2}\rightarrow\mathbb{R}\)</span> with certain properties. In the probabilistic literature, this problem is a problem of probabilistic arithmetic (a very good overview of which is given in <span class="citation">(<a href="#ref-williamson1989probabilistic" role="doc-biblioref">Robert Charles Williamson et al. 1989</a>)</span>; also see <span class="citation">(<a href="#ref-williamson1990probabilistic" role="doc-biblioref">Robert C. Williamson and Downs 1990</a>)</span>): two continuous random variables, <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>, with cumulative distribution functions belonging to <span class="math inline">\(\mathcal{F}(F_1,F_2)\)</span> are considered, and one seeks to explain the behavior of <span class="math inline">\(\Psi(X_1,X_2)\)</span>. The cumulative distribution function of <span class="math inline">\(Z=\Psi(X_1,X_2)\)</span> satisfies
<span class="math display">\[
F_Z(z) = \int\int_{\mathcal{D}_\Psi(z)} dC\left(F_1(x_1), F_2(x_2)\right)
\]</span>
where
<span class="math display">\[
\mathcal{D}_\Psi(z) = \left\{(x_1, x_2)\in\mathbb{R}^2 \,|\, \Psi(x_1, x_2) &lt; z\right\}
\]</span>
and <span class="math inline">\(C\)</span> is the copula associated with the pair <span class="math inline">\((X_1,X_2)\)</span>. In particular, the convolution product <span class="math inline">\(F_1\star F_2\)</span> of <span class="math inline">\(F_1\)</span> and <span class="math inline">\(F_2\)</span> corresponds to the cumulative distribution function of <span class="math inline">\(X_1+X_2\)</span> when <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent. It can also be written in the form
<span class="math display">\[\begin{equation*}
\left(F_1\star F_2\right)(z) = \int\int_{\mathcal{D}_+(z)} dC_I\left(F_1(x_1), F_2(x_2)\right),
\end{equation*}\]</span>
where
<span class="math display">\[
\mathcal{D}_+(z) = \left\{(x_1, x_2)\in\mathbb{R}^2 \,|\, x_1 + x_2 &lt; z\right\}.
\]</span>
This convolution can then be generalized to other forms of dependence (changing the copula <span class="math inline">\(C_I\)</span> to a more general copula <span class="math inline">\(C\)</span>) and to other types of operations than addition. This leads to the following definition.</p>
<div class="definition">
<p><span id="def:unlabeled-div-60" class="definition"><strong>Definition 9.16  </strong></span>Given a copula <span class="math inline">\(C\)</span> and a function <span class="math inline">\(\Psi\)</span>, the <span class="math inline">\(\sigma\)</span>-convolution for <span class="math inline">\(\Psi\)</span> is defined as
<span class="math display">\[
\sigma _{C,\Psi}\left(F_1,F_2\right)(z) = \int\int_{\mathcal{D}_\Psi(z)} dC\left(F_1(x_1), F_2(x_2)\right).
\]</span>
The cumulative distribution function of <span class="math inline">\(Z=\Psi(X_1,X_2)\)</span> is <span class="math inline">\(\sigma_{C,\Psi}\left(F_1,F_2\right)\)</span>.</p>
</div>
<p>By taking certain specific functions <span class="math inline">\(\Psi\)</span>, the following result is obtained.</p>
<p><span class="citation">(<a href="#ref-schweizer1983probabilistic" role="doc-biblioref">B. Schweizer and Sklar 1983</a>)</span> then introduced specific functions: the sup-convolution and inf-convolution.</p>
<div class="definition">
<p><span id="def:unlabeled-div-61" class="definition"><strong>Definition 9.17  </strong></span>Given a copula <span class="math inline">\(C\)</span> and a function <span class="math inline">\(\Psi\)</span>, the sup-convolution is defined as
<span class="math display">\[\begin{equation}
\tau_{C,\Psi}\left(F,G\right)(z) = \sup \left\{
C\left(F(x), G(y)\right) \,|\, \Psi(x,y) = z\right\} \label{borne sup Schweizer et Sklar (1981)}
\end{equation}\]</span>
and the inf-convolution as
<span class="math display">\[\begin{equation}
\rho_{C,\Psi}\left(F,G\right)(z) = \inf \left\{
C\left(F(x), G(y)\right) \,|\, \Psi(x,y) = z\right\} \label{borne inf Schweizer et Sklar (1981)}
\end{equation}\]</span></p>
</div>
<p><span class="citation">(<a href="#ref-frank1987best" role="doc-biblioref">M. J. Frank, Nelsen, and Schweizer 1987</a>)</span> tackled Makarov’s problem using copula theory.</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-62" class="proposition"><strong>Proposition 9.15  </strong></span>Let <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> be two random variables with respective cumulative distribution functions <span class="math inline">\(F_1\)</span> and <span class="math inline">\(F_2\)</span>, both continuous. Then, for any <span class="math inline">\(t\in\mathbb{R}\)</span>, the following inequalities hold:
<span class="math display">\[\begin{equation*}
\tau_{C_M}(F_{X},F_{Y})(t) \leq \Pr[X_1+X_2\leq t] \leq \rho_{C_M}(F_{X},F_{Y})(t)
\end{equation*}\]</span>
where
<span class="math display">\[\begin{equation*}
\tau_{C}(F_1,F_2)(t) = \sup \left\{C\left(F_1(u),F_2(v)\right) \,|\, u+v=t\right\}
\end{equation*}\]</span>
and
<span class="math display">\[\begin{equation*}
\rho_{C}(F_1,F_2)(t) = \inf \left\{\widetilde{C}\left(F_1(u),F_2(v)\right) \,|\, u+v=t\right\}.
\end{equation*}\]</span></p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-63" class="example"><strong>Example 9.17  </strong></span>
Let <span class="math inline">\({\cal SE}xp(\alpha , \theta )\)</span> be the translated negative exponential distribution with cumulative distribution function
<span class="math display">\[
F(x) = 1 - \exp \left( -(x-\theta)/\alpha\right)
\]</span>
for <span class="math inline">\(\alpha &gt; 0\)</span> and <span class="math inline">\(x \ge \theta\)</span>. If <span class="math inline">\(X_1\sim{\cal SE}xp(\alpha_1, \theta_1)\)</span> and <span class="math inline">\(X_2\sim{\cal SE}exp(\alpha_2, \theta_2)\)</span>, a direct calculation shows that <span class="math inline">\(\tau _{C_M}\)</span> corresponds to the cumulative distribution function of the <span class="math inline">\({\cal SE}xp \left( \alpha_1 + \alpha_2, {\tilde \theta} \right)\)</span> distribution, where
<span class="math display">\[
{\tilde \theta} = \theta_1 + \theta_2 +
(\alpha_1 + \alpha_2)\log (\alpha_1+\alpha_2) - \alpha_1 \log(\alpha_1)
- \alpha_2 \log (\alpha_2) .
\]</span>
Similarly, it can be shown that <span class="math inline">\(\rho _{C_M}\)</span> corresponds to the cumulative distribution function of the <span class="math inline">\({\cal SE}xp ( \max (\alpha_1 , \alpha_2) ,\theta_1 + \theta_2 )\)</span> distribution.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-64" class="example"><strong>Example 9.18  </strong></span>
Let <span class="math inline">\({\cal SP}ar (\alpha , \lambda , \theta)\)</span> be the translated Pareto distribution with cumulative distribution function
<span class="math display">\[
F(x) = 1 - \left\{ \frac{\lambda}{\lambda + (x- \theta) } \right\}^{\alpha},
\quad x \ge \theta
\]</span>
where <span class="math inline">\(\alpha, \lambda &gt; 0\)</span>. If <span class="math inline">\(X_1\sim{\cal SP} (\alpha , \lambda_1,\theta_1)\)</span> and <span class="math inline">\(X_2\sim{\cal SP} (\alpha , \lambda_2,\theta_2)\)</span>, it can be verified that <span class="math inline">\(\tau _{C_M}\)</span> corresponds to the cumulative distribution function of the <span class="math inline">\({\cal SP}ar \left( \alpha, {\tilde \lambda}, \theta_1 + \theta_2 + {\tilde \lambda} - \lambda_1 - \lambda_2 \right)\)</span> distribution, where
<span class="math display">\[
{\tilde \lambda} = \left( \lambda_1^{\beta} + \lambda_2^{\beta}\right)^{1/\beta}\text{ with }\beta = \alpha/(\alpha + 1).
\]</span>
Similarly, <span class="math inline">\(\rho_{C_M}\)</span> corresponds to the cumulative distribution function of the <span class="math inline">\({\cal SP}ar ( \alpha, \max (\lambda_1, \lambda_2 ),\theta_1 + \theta_2)\)</span> distribution.</p>
</div>
<p>It is possible to refine these results by assuming that additional information is available.</p>
<div class="proposition">
<p><span id="prp:Embrechtsetal" class="proposition"><strong>Proposition 9.16  </strong></span>Suppose that two copulas <span class="math inline">\(C_{1}\)</span> and <span class="math inline">\(C_{2}\)</span> bounding <span class="math inline">\(C\)</span> are known, i.e., such that
<span class="math display">\[
C_{1}\left( u_1,u_2\right) \leq C\left( u_1,u_2\right)
\]</span>
and
<span class="math display">\[
\widetilde{C}\left(u_1,u_2\right) \leq \widetilde{C}_{2}\left( u_1,u_2\right)
\]</span>
for all <span class="math inline">\(0\leq u_1,u_2\leq 1\)</span>. Then,
<span class="math display">\[\begin{equation*}
\tau_{C_{1}}\left( F_1,F_2\right) \left( t\right) \leq
\Pr[X_1+X_2\leq t] \leq \rho_{\widetilde{C}_{2}}\left(
F_1,F_2\right) \left( t\right)
\end{equation*}\]</span></p>
</div>
<p>In particular, if it is known that <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are positively quadrant-dependent, then Proposition <span class="math inline">\(\ref{Embrechtsetal}\)</span> applies with <span class="math inline">\(C_1=C_I\)</span>.</p>
<div class="example">
<p><span id="exm:unlabeled-div-72" class="example"><strong>Example 9.19  </strong></span>(continuation of Example <span class="math inline">\(\ref{ExDGM1}\)</span>)
Suppose that <span class="math inline">\(X_i\sim {\cal SE}xp(\alpha_i, \theta_i)\)</span>, and denote <span class="math inline">\(f_i\)</span> as the probability density function, <span class="math inline">\(i=1,2\)</span>. Suppose also that <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are positively quadrant-dependent. In this case, it is not possible to improve the upper bound on <span class="math inline">\(\Pr[X_1+X_2\leq t]\)</span>. However, the lower bound <span class="math inline">\(\tau_{C_I}\)</span> is implicitly defined for <span class="math inline">\(s \ge \theta_1 + \theta_2\)</span> by
<span class="math display">\[
\tau _{C_I}(s) =F_1(t_s)F_2(s-t_s)
\]</span>
where <span class="math inline">\(t_s \in [\theta_1, s-\theta_2]\)</span> is the unique solution of the equation
<span class="math display">\[\begin{equation}\label{truc}
\frac{f_1(t)}{F_1(t)} = \frac{f_2(s-t)}{F_2(s-t)}.
\end{equation}\]</span></p>
<p>In the particular case where <span class="math inline">\(\alpha_1 = \alpha_2\)</span>, we have
<span class="math display">\[
t_s = \frac{s-\theta_1-\theta_2}{2\alpha},
\]</span>
which allows us to explicitly obtain the lower bound, which is
<span class="math display">\[
\tau _{C_I}(s) = \left\{ 1 - \exp\left(-\frac{s-\theta_1-\theta_2}{2\alpha}\right) \right\}^2
\]</span>
for all <span class="math inline">\(s \ge \theta_1 + \theta_2\)</span>.</p>
<div class="example">
<p><span id="exm:unlabeled-div-65" class="example"><strong>Example 9.20  </strong></span>(Continuation of Example <span class="math inline">\(\ref{ExDGM2}\)</span>)
Suppose that <span class="math inline">\(X_i\sim{\cal SP}ar(\alpha , \lambda_i, \theta_i)\)</span>, and denote <span class="math inline">\(f_i\)</span> as the probability density functions, <span class="math inline">\(i=1,2\)</span>. If <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are positively quadrant-dependent, it is not possible to improve the upper bound on the cumulative distribution function of <span class="math inline">\(X_1+X_2\)</span>. However, the lower bound becomes
<span class="math display">\[
\tau _{C_I}(s) = F_1(t_s) F_2(s-t_s)\text{ for }s \ge \theta_1 + \theta_2;
\]</span>
with <span class="math inline">\(\theta_1 \le t_s \le s-\theta_2\)</span> being the solution of the equation (<span class="math inline">\(\ref{truc}\)</span>).</p>
</div>
<p>One can also be interested in bounds on the Value-at-Risk (VaR) of a sum <span class="math inline">\(X_1+X_2\)</span>. It can be shown that for any probability level <span class="math inline">\(\alpha\)</span>, the following inequalities hold:
<span class="math display">\[\begin{equation*}
\inf_{C_M(u,v)=\alpha}\big\{ {\text{VaR}}[X_1;u]+{\text{VaR}}[X_2;v]\big\} \leq {\text{VaR}}[X_1+X_2;\alpha]
\end{equation*}\]</span>%
and
<span class="math display">\[\begin{equation*}
{\text{VaR}}[X_1+X_2;\alpha] \leq
\sup_{\left( \widetilde{C}_M\right)(u,v)=\alpha}\big\{ {\text{VaR}}[X_1;u]+{\text{VaR}}[X_2;v]\big\}.
\end{equation*}\]</span></p>
<p>The principle of duality stated by <span class="citation">(<a href="#ref-frank1979duality" role="doc-biblioref">M. Frank and Schweizer 1979</a>)</span> says that if <span class="math inline">\(C_{0}\leq C,\)</span> <span class="math inline">\(\widetilde{C}\leq \widetilde{C}_{1}\)</span>, and if $:^{2}$ is a continuous increasing function (with $-a&lt;b+$), then, for <span class="math inline">\(0\leq \alpha&lt;1\)</span>,
<span class="math display">\[
{\text{VaR}}\big[\Psi(X_1,X_2);\alpha\big]\geq
\inf_{C_{0}\left( u,v\right)=\alpha} \Psi \Big(
{\text{VaR}}[X_1;u],{\text{VaR}}[X_2;v]\Big),
\]</span>
and
<span class="math display">\[
{\text{VaR}}\big[\Psi(X_1,X_2);\alpha\big]\leq
\sup_{\widetilde{C}_{1}\left( u,v\right)=\alpha} \Psi \Big(
{\text{VaR}}[X_1;u],{\text{VaR}}[X_2;v]\Big).
\]</span></p>
<div id="multivariate-discrete-distributions" class="section level2 hasAnchor" number="9.6">
<h2><span class="header-section-number">9.6</span> Multivariate Discrete Distributions<a href="chap8.html#multivariate-discrete-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="two-classes-of-correlated-risks-model" class="section level3 hasAnchor" number="9.6.1">
<h3><span class="header-section-number">9.6.1</span> Two Classes of Correlated Risks Model<a href="chap8.html#two-classes-of-correlated-risks-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="definition-33" class="section level4 hasAnchor" number="9.6.1.1">
<h4><span class="header-section-number">9.6.1.1</span> Definition<a href="chap8.html#definition-33" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Here we consider the case where the total loss amount <span class="math inline">\(S\)</span> can be expressed as the sum of risks within two classes of risks, <span class="math inline">\(S=S_{1}+S_{2}\)</span>, where
<span class="math display">\[\begin{equation}
\label{DefTotalLoss}
S_1=\sum_{j=1}^{N_1}X_{1,j}\text{ and }S_2=\sum_{j=1}^{N_2}X_{2,j},
\end{equation}\]</span>
where the random variables <span class="math inline">\(N_1\)</span> and <span class="math inline">\(N_2\)</span> represent the numbers of claims for classes 1 and 2. We then make the following assumptions:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(X_{1,1},X_{1,2},\ldots\)</span> are independent positive random variables with the same cumulative distribution function <span class="math inline">\(F_{1}\)</span>;</li>
<li><span class="math inline">\(X_{2,1},X_{2,2},\ldots\)</span> are independent positive random variables with the same cumulative distribution function <span class="math inline">\(F_{2}\)</span>;</li>
<li>the random variable <span class="math inline">\(N_1\)</span> is independent of the sequence <span class="math inline">\(X_{1,1},X_{1,2},\ldots\)</span>; likewise, <span class="math inline">\(N_2\)</span> is independent of the sequence <span class="math inline">\(X_{2,1},X_{2,2},\ldots\)</span>.</li>
<li>the sequences <span class="math inline">\(X_{1,1},X_{1,2,\ldots}\)</span> and <span class="math inline">\(X_{2,1},X_{2,{2}},\ldots\)</span> are assumed to be mutually independent.</li>
</ol>
<div class="example">
<p><span id="exm:unlabeled-div-66" class="example"><strong>Example 9.21  </strong></span>Consider a portfolio consisting of individual auto insurance policies and multi-risk home insurance policies. In the event of a major flood, for example, both classes of risks can be affected, which leads to the rejection of the independence assumption between classes.</p>
</div>
<p>In this simple model, the dependence between <span class="math inline">\(S_1\)</span> and <span class="math inline">\(S_2\)</span> is induced by the correlation between <span class="math inline">\(N_1\)</span> and <span class="math inline">\(N_2\)</span>, as shown in the following result.</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-67" class="proposition"><strong>Proposition 9.17  </strong></span>In the model <span class="math inline">\(\eqref{DefTotalLoss}\)</span> based on the assumptions (1)-(4) stated above,
<span class="math display">\[\begin{equation*}
\mathbb{C}[S_{1},S_{2}] =\mathbb{E}[X_{1,1}]\mathbb{E}[X_{2,1}]\mathbb{C}[N_{1},N_{2}]
\end{equation*}\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-68" class="proof"><em>Proof</em>. </span>Since
<span class="math display">\[
\mathbb{E}[S_{1}S_{2}|N_{1},N_{2}]=N_{1}\mathbb{E}[X_{1,1}] \times N_{2}\mathbb{E}[X_{2,1}]
\]</span>
we have
<span class="math display">\[
\mathbb{E}[S_{1}S_{2}] =\mathbb{E}[N_{1} N_{2}]\mathbb{E}[X_{1,1}]\mathbb{E}[X_{2,1}]
\]</span>
which completes the proof.</p>
</div>
<p>To study this type of model, it is necessary to have distributions for the pair <span class="math inline">\((N_1,N_2)\)</span>. This is precisely the subject of the following sections.</p>
</div>
</div>
<div id="multivariate-bernoulli-distribution" class="section level3 hasAnchor" number="9.6.2">
<h3><span class="header-section-number">9.6.2</span> Multivariate Bernoulli Distribution<a href="chap8.html#multivariate-bernoulli-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let $N_1er( _1) $ and $N_2er( _2) $. The distribution of the pair is described in the following table:
<span class="math display">\[\begin{equation*}
\vline
\begin{array}[t]{rccl}
\hline N_1\backslash N_2\text{ }\vline &amp; 0 &amp; 1 &amp; \vline\text{ } \\
\hline 0\text{ }\vline &amp; p_{00} &amp; p_{01} &amp; \vline\text{
}\overline{\pi }_1=1-\pi_1 \\
1\text{ }\vline &amp; p_{10} &amp; p_{11} &amp; \vline\text{ }\pi _1 \\
\hline \text{ }\vline &amp; \overline{\pi }_2=1-\pi _2&amp; \pi _2
&amp; \vline\text{ }1
\\ \hline
\end{array}%
\vline
\end{equation*}\]</span></p>
<p>The following result shows that the linear correlation coefficient between <span class="math inline">\(N_1\)</span> and <span class="math inline">\(N_2\)</span> cannot reach the bounds of -1 and +1.</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-69" class="proposition"><strong>Proposition 9.18  </strong></span>The correlation between <span class="math inline">\(N_1\)</span> and <span class="math inline">\(N_2\)</span> is bounded as follows,
<span class="math display">\[\begin{equation*}
\max \left\{ -\sqrt{\frac{\pi _1\pi _2}{\overline{\pi
}_1\overline{\pi
}_2}},-\sqrt{\frac{\overline{\pi }_1\overline{\pi }_2}{\pi _1\pi _2
}}\right\} \leq r\left( N_1,N_2\right) \leq \sqrt{\frac{\pi _{\min
}\left( 1-\pi _{\max }\right) }{\pi _{\max }\left( 1-\pi _{\min
}\right) }},
\end{equation*}\]</span>%
where
<span class="math display">\[\begin{equation*}
\left\{
\begin{array}{c}
\pi _{\min }=\min \left\{ \pi _1,\pi _2\right\} \\
\pi _{\max }=\max \left\{ \pi _1,\pi _2\right\}%
\end{array}%
\right.
\end{equation*}\]</span></p>
</div>
<p>The graphs in Figure <span class="math inline">\(\ref{FIG-BORNES-BERNOULLI}\)</span> show how the bounds change depending on <span class="math inline">\(\pi _1\)</span> and <span class="math inline">\(\pi_2\)</span>, and the evolution of the bounds depending on <span class="math inline">\(\pi _1\)</span>, assuming that <span class="math inline">\(\pi _2\)</span> takes on the values <span class="math inline">\(0.1\)</span>, <span class="math inline">\(0.4\)</span>, <span class="math inline">\(0.6\)</span>, and <span class="math inline">\(0.9\)</span>.</p>
</div>
<div id="common-shock-poisson-model-bivariate-poisson-distribution" class="section level3 hasAnchor" number="9.6.3">
<h3><span class="header-section-number">9.6.3</span> Common Shock Poisson Model: Bivariate Poisson Distribution<a href="chap8.html#common-shock-poisson-model-bivariate-poisson-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Given three independent Poisson variables, <span class="math inline">\(M_{1}\)</span>, <span class="math inline">\(M_{2}\)</span>, and <span class="math inline">\(L\)</span>, with parameters <span class="math inline">\(\lambda _{1}\)</span>, <span class="math inline">\(\lambda _{2}\)</span>, and $$, the bivariate Poisson distribution $oi( <em>{1},</em>{2},) $ is the distribution of the pair $( N_{1},N_{2}) $, where <span class="math inline">\(N_{1}=M_{1}+L\)</span> and <span class="math inline">\(N_{2}=M_{2}+L\)</span>.</p>
<div class="definition">
<p><span id="def:unlabeled-div-70" class="definition"><strong>Definition 9.18  </strong></span>The bivariate Poisson distribution $oi( <em>{1},
</em>{2},) $ is defined by
<span class="math display">\[\begin{eqnarray*}
&amp;&amp;{\Pr}[N_{1}=n_{1},N_{2}=n_{2}]\\
&amp;=&amp;\exp \left( -\lambda _{1}-\lambda _{2}-\mu \right)
\sum_{j=0}^{\min \left( n_{1},n_{2}\right) }\frac{\mu
^{j}}{j!}\frac{\lambda _{1}^{n_{1}-j}}{\left( n_{1}-j\right)
!}\frac{\lambda _{2}^{n_{2}-j}}{\left( n_{2}-j\right) !},
\end{eqnarray*}\]</span>%
for <span class="math inline">\(n_{1},n_{2}\in {\mathbb{N}}\)</span>.</p>
</div>
<p>For the marginal distributions <span class="math inline">\(N_{i}\)</span>, we can note that
<span class="math display">\[\begin{equation*}
\mathbb{E}[N_{i}] =\mathbb{V}[ N_{i}] =\lambda_{i}+\mu \text{ for }i=1,2.
\end{equation*}\]</span>
We also know that
<span class="math display">\[
\mathbb{E}[N_{1}|N_{2}=n_{2}] =n_{2}\frac{\mu}{\lambda _{2}+\mu }+\lambda _{1},
\]</span>
and
<span class="math display">\[
\mathbb{V}[N_{1}|N_{2}=n_{2}] =n_{2}\frac{\mu \lambda _{2}}{\left(
\lambda _{2}+\mu \right) ^{2}}+\lambda _{1}.
\]</span>
Now, let’s study the correlation coefficient between <span class="math inline">\(N_1\)</span> and <span class="math inline">\(N_2\)</span>.</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-71" class="proposition"><strong>Proposition 9.19  </strong></span>The correlation coefficient between <span class="math inline">\(N_{1}\)</span> and <span class="math inline">\(N_{2}\)</span> is given by
<span class="math display">\[\begin{equation*}
r\left( N_{1},N_{2}\right) =\frac{\mu }{\sqrt{\left( \lambda
_{1}+\mu \right) \left( \lambda _{2}+\mu \right) }}
\end{equation*}\]</span>%
which satisfies
<span class="math display">\[\begin{equation*}
0\leq r\left( N_{1},N_{2}\right) \leq \max \left\{
\sqrt{\frac{\lambda _{1}+\mu }{\lambda _{2}+\mu
}},\sqrt{\frac{\lambda _{2}+\mu }{\lambda _{1}+\mu }}\right\} .
\end{equation*}\]</span></p>
</div>
</div>
<div id="common-shock-bernoulli-model-the-marceau-model" class="section level3 hasAnchor" number="9.6.4">
<h3><span class="header-section-number">9.6.4</span> Common Shock Bernoulli Model: The Marceau Model<a href="chap8.html#common-shock-bernoulli-model-the-marceau-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider a portfolio consisting of <span class="math inline">\(n\)</span> policies where the claim costs follow the standard individual model, i.e.,
<span class="math display">\[\begin{eqnarray*}
X_{i}&amp;=&amp;\left\{
\begin{array}{cc}
Y_{i} &amp; \text{if\thinspace }I_{i}=1 \\
0 &amp; \text{if\thinspace }I_{i}=0%
\end{array}%
\right.
\\
&amp;=&amp;Y_{i}I_{i}\text{ where }I_{i}\thicksim \mathcal{B}er\left(q_{i}\right)
\end{eqnarray*}\]</span>%
for <span class="math inline">\(i=1,2,...,n\)</span>. Let the random variables <span class="math inline">\(Y_{1},...,Y_{n}\)</span> and <span class="math inline">\(I_{1},...,I_{n}\)</span> be independent, while the <span class="math inline">\(Y_{i}\)</span> are mutually independent. In contrast, assume that the <span class="math inline">\(I_{i}\)</span> are correlated, with dependence modeled as follows:
<span class="math display">\[\begin{eqnarray*}
I_{i}&amp;=&amp;\min \left\{ J_{i}+J_{0},1\right\}\\
&amp;&amp;\text{where }J_{i}\thicksim \mathcal{B}er\left( q_{i}^{\prime }\right)
\text{ and }J_{0}\thicksim \mathcal{B}er\left( q_{0}\right)
\\
&amp;&amp;
\text{are independent variables}.
\end{eqnarray*}\]</span>
The probability generating function of <span class="math inline">\(I_{i}\)</span> is then
<span class="math display">\[\begin{equation*}
\mathbb{E}[s^{I_{i}}] =p_{0}\mathbb{E}[s^{J_{i}}] +q_{0}s=p_{0}p_{i}^{\prime }+\left(
1-p_{0}p_{i}^{\prime }\right) s
\end{equation*}\]</span>%
meaning that
<span class="math display">\[
I_{i}\thicksim \mathcal{B}er\left( q_{i}\right)\text{ with }q_{i}=1-p_{0}p_{i}^{\prime }.
\]</span></p>
<p>The variance of the cumulative cost <span class="math inline">\(S=X_{1}+...+X_{n}\)</span> is then
<span class="math display">\[\begin{equation*}
\mathbb{V}[S] =\sum_{i=1}^{n}\mathbb{V}[X_{i}]+2\sum_{i=1}^{n-1}\sum_{j=i+1}^{n}\mathbb{C}[X_{i},X_{j}]
\end{equation*}\]</span>%
where%
<span class="math display">\[\begin{eqnarray*}
\mathbb{C}[X_{i},X_{j}] &amp;=&amp;\mathbb{E}[ X_{i}X_{j}] -\mathbb{E}[X_{i}] \mathbb{E}[ X_{j}]\\
&amp;=&amp;\mathbb{E}[Y_{i}I_{i}Y_{j}I_{j}] -\mathbb{E}[ Y_{i}I_{i}] \mathbb{E}[ Y_{j}I_{j}] \\
&amp;=&amp;\mathbb{E}[ Y_{i}] \mathbb{E}[ Y_{j}] \mathbb{E}[ I_{i}I_{j}] -\mathbb{E}[ Y_{i}]\mathbb{E}[Y_{j}] \mathbb{E}[ I_{i}] \mathbb{E}[ I_{j}] \\
&amp;=&amp;\mathbb{E}[ Y_{i}] \mathbb{E}[ Y_{j}]\mathbb{C}[ I_{i},I_{j}]
\end{eqnarray*}\]</span>%
with
<span class="math display">\[\begin{eqnarray*}
\mathbb{C}[ I_{i},I_{j}] &amp;=&amp;\mathbb{E}[ I_{i}I_{j}] -\mathbb{E}[ I_{i}] \mathbb{E}[ I_{j}] \\
&amp;=&amp;\mathbb{E}\big[ \mathbb{E}[ I_{i}I_{j}|J_{0}] \big] -q_{i}q_{j} \\
&amp;=&amp;q_{0}+p_{0}q_{i}^{\prime }q_{j}^{\prime }-q_{i}q_{j}\\
&amp;=&amp;\frac{q_{0}}{1-q_{0}}\left( 1-q_{i}\right) \left( 1-q_{j}\right),
\end{eqnarray*}\]</span>%
so that the correlation between <span class="math inline">\(I_{i}\)</span> and <span class="math inline">\(I_{j}\)</span> is positive and increases with <span class="math inline">\(q_{0}\)</span>. The variance of the sum <span class="math inline">\(S\)</span> is then
<span class="math display">\[\begin{equation*}
\mathbb{V}[ S] =\sum_{i=1}^{n}\mathbb{V}[ X_{i}] +2\frac{q_{0}}{1-q_{0}}\sum_{i=1}^{n-1}\sum_{j=1+1}^{n}p_{i}p_{j}\mathbb{E}[Y_{i}] \mathbb{E}[ Y_{j}].
\end{equation*}\]</span></p>
<p>Consider a portfolio consisting of <span class="math inline">\(n\)</span> policies where the claim costs follow the standard individual model, i.e.,
<span class="math display">\[\begin{eqnarray*}
X_{i}&amp;=&amp;\left\{
\begin{array}{cc}
Y_{i} &amp; \text{if\thinspace }I_{i}=1 \\
0 &amp; \text{if\thinspace }I_{i}=0%
\end{array}%
\right.
\\
&amp;=&amp;Y_{i}I_{i}\text{ where }I_{i}\thicksim \mathcal{B}er\left(q_{i}\right)
\end{eqnarray*}\]</span>%
for <span class="math inline">\(i=1,2,...,n\)</span>. Let the random variables <span class="math inline">\(Y_{1},...,Y_{n}\)</span> and <span class="math inline">\(I_{1},...,I_{n}\)</span> be independent, while the <span class="math inline">\(Y_{i}\)</span> are mutually independent. In contrast, assume that the <span class="math inline">\(I_{i}\)</span> are correlated, with dependence modeled as follows:
<span class="math display">\[\begin{eqnarray*}
I_{i}&amp;=&amp;\min \left\{ J_{i}+J_{0},1\right\}\\
&amp;&amp;\text{where }J_{i}\thicksim \mathcal{B}er\left( q_{i}^{\prime }\right)
\text{ and }J_{0}\thicksim \mathcal{B}er\left( q_{0}\right)
\\
&amp;&amp;
\text{are independent variables}.
\end{eqnarray*}\]</span>
The probability generating function of <span class="math inline">\(I_{i}\)</span> is then
<span class="math display">\[\begin{equation*}
\mathbb{E}[s^{I_{i}}] =p_{0}\mathbb{E}[s^{J_{i}}] +q_{0}s=p_{0}p_{i}^{\prime }+\left(
1-p_{0}p_{i}^{\prime }\right) s
\end{equation*}\]</span>%
meaning that
<span class="math display">\[
I_{i}\thicksim \mathcal{B}er\left( q_{i}\right)\text{ with }q_{i}=1-p_{0}p_{i}^{\prime }.
\]</span></p>
<p>The variance of the cumulative cost <span class="math inline">\(S=X_{1}+...+X_{n}\)</span> is then
<span class="math display">\[\begin{equation*}
\mathbb{V}[S] =\sum_{i=1}^{n}\mathbb{V}[X_{i}]+2\sum_{i=1}^{n-1}\sum_{j=i+1}^{n}\mathbb{C}[X_{i},X_{j}]
\end{equation*}\]</span>%
where%
<span class="math display">\[\begin{eqnarray*}
\mathbb{C}[X_{i},X_{j}] &amp;=&amp;\mathbb{E}[ X_{i}X_{j}] -\mathbb{E}[X_{i}] \mathbb{E}[ X_{j}]\\
&amp;=&amp;\mathbb{E}[Y_{i}I_{i}Y_{j}I_{j}] -\mathbb{E}[ Y_{i}I_{i}] \mathbb{E}[ Y_{j}I_{j}] \\
&amp;=&amp;\mathbb{E}[ Y_{i}] \mathbb{E}[ Y_{j}] \mathbb{E}[ I_{i}I_{j}] -\mathbb{E}[ Y_{i}]\mathbb{E}[Y_{j}] \mathbb{E}[ I_{i}] \mathbb{E}[ I_{j}] \\
&amp;=&amp;\mathbb{E}[ Y_{i}] \mathbb{E}[ Y_{j}]\mathbb{C}[ I_{i},I_{j}]
\end{eqnarray*}\]</span>%
with
<span class="math display">\[\begin{eqnarray*}
\mathbb{C}[ I_{i},I_{j}] &amp;=&amp;\mathbb{E}[ I_{i}I_{j}] -\mathbb{E}[ I_{i}] \mathbb{E}[ I_{j}] \\
&amp;=&amp;\mathbb{E}\big[ \mathbb{E}[ I_{i}I_{j}|J_{0}] \big] -q_{i}q_{j} \\
&amp;=&amp;q_{0}+p_{0}q_{i}^{\prime }q_{j}^{\prime }-q_{i}q_{j}\\
&amp;=&amp;\frac{q_{0}}{1-q_{0}}\left( 1-q_{i}\right) \left( 1-q_{j}\right),
\end{eqnarray*}\]</span>%
so that the correlation between <span class="math inline">\(I_{i}\)</span> and <span class="math inline">\(I_{j}\)</span> is positive and increases with <span class="math inline">\(q_{0}\)</span>. The variance of the sum <span class="math inline">\(S\)</span> is then
<span class="math display">\[\begin{equation*}
\mathbb{V}[ S] =\sum_{i=1}^{n}\mathbb{V}[ X_{i}] +2\frac{q_{0}}{1-q_{0}}\sum_{i=1}^{n-1}\sum_{j=1+1}^{n}p_{i}p_{j}\mathbb{E}[Y_{i}] \mathbb{E}[ Y_{j}].
\end{equation*}\]</span></p>
</div>
</div>
<div id="exercices" class="section level2 hasAnchor" number="9.7">
<h2><span class="header-section-number">9.7</span> Exercices<a href="chap8.html#exercices" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>::: {.exercise}[Loi exponentielle bivariée de Gumbel]
Considérons la fonction de répartition
<span class="math display">\[
F_{\boldsymbol{X}}\left(\boldsymbol{x}\right) =1-\exp \left( -x_1\right) -\exp \left(-x_2\right)
+\exp \big( -x_1-x_2-\theta x_1x_2\big)
\]</span>
où <span class="math inline">\(\theta \in \left[ 0,1\right]\)</span>. Montrez que</p>
<ol style="list-style-type: decimal">
<li>la densité est donnée par
<span class="math display">\[\begin{equation*}
f_{\boldsymbol{X}}\left( \boldsymbol{x}\right) =\exp \left( -x_1-x_2-\theta x_1x_2 \right) %
\Big( \left( 1+\theta x_1\right) \left( 1+\theta x_2\right) -\theta\Big) .
\end{equation*}\]</span>%</li>
<li>cette loi traduit une absence de mémoire au sens où
<span class="math display">\[
\mathbb{E}[X_1-x_1|X_1&gt;x_1\text{ et }X_2&gt;x_2] =\mathbb{E}[ X_1|X_2&gt;x_2].
\]</span></li>
</ol>
</div>
</div>
<p>::: {.exercise}[Loi exponentielle bivariée de Marshall &amp; Olkin]</p>
<p>Soient <span class="math inline">\(Z_{1}\sim\mathcal{E}xp(\lambda_1)\)</span>, <span class="math inline">\(Z_{2}\sim\mathcal{E}xp(\lambda_2)\)</span> et <span class="math inline">\(Z_{12}\sim\mathcal{E}xp(\lambda_{12})\)</span> trois variables aléatoires indépendantes, et posons
<span class="math display">\[
X_1=\min \left\{ Z_{1},Z_{12}\right\}\text{ et }X_2=\min \left\{Z_{2},Z_{12}\right\}.
\]</span></p>
<ol style="list-style-type: decimal">
<li>Montrez que
<span class="math display">\[\begin{eqnarray*}
\overline{F}_{\boldsymbol{X}}\left( \boldsymbol{x}\right) &amp;=&amp;\exp \big( -\lambda
_{1}x_1-\lambda _{2}x_2-\lambda _{12}\max \left\{ x_1,x_2\right\}
\big)\label{Marshall Olkin survie}
\end{eqnarray*}\]</span>%</li>
<li>Montrez que
<span class="math display">\[
\overline{F}_1\left( x_1\right) =\exp \big( -\left( \lambda
_{1}+\lambda _{12}\right) x_1\big)
\]</span>
et
<span class="math display">\[
\overline{F}_2\left(x_2\right) =\exp \big( -\left( \lambda _{2}+\lambda _{12}\right)x_2\big).
\]</span></li>
<li>Montrez que cette fonction de queue vérifie une autre propriété d’absence de mémoire,
<span class="math display">\[\begin{equation*}
\overline{F}_{\boldsymbol{X}}\left( x_1+h,x_2+h\right) =\overline{F}_{\boldsymbol{X}}\left(
x_1,x_2\right) \overline{F}_{\boldsymbol{X}}\left( h,h\right).
\end{equation*}\]</span>%</li>
<li>En définissant
<span class="math display">\[
\alpha =\frac{\lambda _{12}}{\lambda _{1}+\lambda _{12}}\text{ et }\beta =\frac{\lambda_{12}}
{ \lambda_{2}+\lambda _{12}},
\]</span>
montrez que la copule associée à la cette loi s’écrit
<span class="math display">\[\begin{eqnarray*}
C\left( u_1,u_2\right) &amp;=&amp;\min \left\{ u_1^{1-\alpha }u_2,u_1u_2^{1-\beta}\right\}\\
&amp;=&amp;\left\{
\begin{array}{cc}
u_1^{1-\alpha }u_2 &amp; \text{ si }u_1^{\alpha }\geq u_2^{\beta } \\
u_1u_2^{1-\beta } &amp; \text{ si }u_1^{\alpha }\leq u_2^{\beta }%
\end{array}%
\right.
\end{eqnarray*}\]</span>
\end{enumerate}
:::</li>
</ol>
<p>::: {.exercise}[Bivariate Gumbel Exponential Distribution]
Consider the cumulative distribution function
<span class="math display">\[
F_{\boldsymbol{X}}\left(\boldsymbol{x}\right) =1-\exp \left( -x_1\right) -\exp \left(-x_2\right)
+\exp \big( -x_1-x_2-\theta x_1x_2\big)
\]</span>
where <span class="math inline">\(\theta \in \left[ 0,1\right]\)</span>. Show that</p>
<ol style="list-style-type: decimal">
<li>the probability density function is given by
<span class="math display">\[\begin{equation*}
f_{\boldsymbol{X}}\left( \boldsymbol{x}\right) =\exp \left( -x_1-x_2-\theta x_1x_2 \right) %
\Big( \left( 1+\theta x_1\right) \left( 1+\theta x_2\right) -\theta\Big) .
\end{equation*}\]</span>%</li>
<li>this distribution exhibits a lack of memory in the sense that
<span class="math display">\[
\mathbb{E}[X_1-x_1|X_1&gt;x_1\text{ and }X_2&gt;x_2] =\mathbb{E}[ X_1|X_2&gt;x_2].
\]</span></li>
</ol>
<p>:::</p>
<p>::: {.exercise}[Marshall &amp; Olkin Bivariate Exponential Distribution]</p>
<p>Let <span class="math inline">\(Z_{1}\sim\mathcal{E}xp(\lambda_1)\)</span>, <span class="math inline">\(Z_{2}\sim\mathcal{E}xp(\lambda_2)\)</span>, and <span class="math inline">\(Z_{12}\sim\mathcal{E}xp(\lambda_{12})\)</span> be three independent random variables, and define
<span class="math display">\[
X_1=\min \left\{ Z_{1},Z_{12}\right\}\text{ and }X_2=\min \left\{Z_{2},Z_{12}\right\}.
\]</span></p>
<ol style="list-style-type: decimal">
<li>Show that
<span class="math display">\[\begin{eqnarray*}
\overline{F}_{\boldsymbol{X}}\left( \boldsymbol{x}\right) &amp;=&amp;\exp \big( -\lambda
_{1}x_1-\lambda _{2}x_2-\lambda _{12}\max \left\{ x_1,x_2\right\}
\big)\label{Marshall Olkin survie}
\end{eqnarray*}\]</span>%</li>
<li>Show that
<span class="math display">\[
\overline{F}_1\left( x_1\right) =\exp \big( -\left( \lambda
_{1}+\lambda _{12}\right) x_1\big)
\]</span>
and
<span class="math display">\[
\overline{F}_2\left(x_2\right) =\exp \big( -\left( \lambda _{2}+\lambda _{12}\right)x_2\big).
\]</span></li>
<li>Show that this tail function satisfies another memoryless property,
<span class="math display">\[\begin{equation*}
\overline{F}_{\boldsymbol{X}}\left( x_1+h,x_2+h\right) =\overline{F}_{\boldsymbol{X}}\left(
x_1,x_2\right) \overline{F}_{\boldsymbol{X}}\left( h,h\right).
\end{equation*}\]</span>%</li>
<li>By defining
<span class="math display">\[
\alpha =\frac{\lambda _{12}}{\lambda _{1}+\lambda _{12}}\text{ and }\beta =\frac{\lambda_{12}}
{ \lambda_{2}+\lambda _{12}},
\]</span>
show that the copula associated with this distribution can be written as
<span class="math display">\[\begin{eqnarray*}
C\left( u_1,u_2\right) &amp;=&amp;\min \left\{ u_1^{1-\alpha }u_2,u_1u_2^{1-\beta}\right\}\\
&amp;=&amp;\left\{
\begin{array}{cc}
u_1^{1-\alpha }u_2 &amp; \text{ if }u_1^{\alpha }\geq u_2^{\beta } \\
u_1u_2^{1-\beta } &amp; \text{ if }u_1^{\alpha }\leq u_2^{\beta }%
\end{array}%
\right.
\end{eqnarray*}\]</span>
\end{enumerate}
:::</li>
</ol>
<p>::: {.exercise}[Basu &amp; Block Bivariate Exponential Distribution]</p>
<p>Consider the following bivariate tail function:
<span class="math display">\[\begin{eqnarray*}
\overline{F}_{\boldsymbol{X}}\left( \boldsymbol{x}\right) &amp;=&amp;\frac{\lambda _{1}+\lambda
_{2}+\lambda _{12}}{\lambda _{1}+\lambda _{2}}\exp\big( -\lambda
_{1}x_1-\lambda _{2}x_2-\lambda _{12}\max \left\{ x_1,x_2\right\}\big)\\
&amp;&amp;-\frac{\lambda _{12}}{%
\lambda _{1}+\lambda _{2}}\exp \big( -\left( \lambda _{1}+\lambda
_{2}+\lambda _{12}\right) \max \left\{ x_1,x_2\right\} \big).
\end{eqnarray*}\]</span>%</p>
<ol style="list-style-type: decimal">
<li>Show that the tail function of <span class="math inline">\(X_1\)</span> is given by
<span class="math display">\[\begin{eqnarray*}
\overline{F}_1\left( x\right) &amp;=&amp;\frac{\lambda _{1}+\lambda
_{2}+\lambda _{12}}{\lambda _{1}+\lambda _{2}}\exp \big( -\left(
\lambda _{1}-\lambda_{12}\right) x\big)\\
&amp;&amp;-\frac{\lambda _{12}}{\lambda _{1}+\lambda _{2}}\exp %
\big( -\left( \lambda _{1}+\lambda _{2}+\lambda _{12}\right)x\big).
\end{eqnarray*}\]</span>%</li>
<li>Show that
<span class="math inline">\(\min \left( X_1,X_2\right) \sim \mathcal{E}xp\left( \lambda _{1}+\lambda _{2}+\lambda _{12}\right) .\)</span></li>
</ol>
<p>:::</p>
<p>::: {.exercise}[Cherian Distribution]</p>
<p>Consider three independent random variables, $Z_{i}xp( _{i}) $, and define
<span class="math display">\[
X_1=Z_{1}+Z_{3}\text{ and }X_2=Z_{2}+Z_{3}.
\]</span>
Show that the density of the couple <span class="math inline">\(\boldsymbol{X}\)</span> is given by
<span class="math display">\[\begin{equation*}
f_{\boldsymbol{X}}\left(\boldsymbol{x}\right) =\frac{\exp \left( -x_1-x_2 \right) }{
\Gamma \left( \theta _{1}\right) \Gamma \left( \theta _{2}\right)
\Gamma \left( \theta _{3}\right) }\int_{0}^{\min \left\{x_1,x_2\right\} }\left( x_1-t\right) ^{\theta _{1}-1}\left( x_2-t\right)
^{\theta _{2}-1}t^{\theta _{3}-1}dt
\end{equation*}\]</span>%
:::</p>
<p>::: {.exercise}[Bivariate Beta Distribution]</p>
<p>Let $( X,Y) $ be a couple with density given by
<span class="math display">\[\begin{equation*}
f_{\boldsymbol{X}}\left( \boldsymbol{x}\right) =\frac{\Gamma \left( \theta _{1}+\theta
_{2}+\theta _{3}\right) }{\Gamma \left( \theta _{1}\right) \Gamma
\left( \theta _{2}\right) \Gamma \left( \theta _{3}\right)
}x_1^{\theta _{1}-1}x_2^{\theta _{2}-1}(1-x_1-x_2) ^{\theta
_{3}-1},
\end{equation*}\]</span></p>
<ol style="list-style-type: decimal">
<li>Show that $X_1et( <em>{1},</em>{2}+<em>{3}) $ and $X_2et(</em>{2},<em>{1}+</em>{3}) $.</li>
<li>Show that conditionally on <span class="math inline">\(X_1=x_1\)</span>,
<span class="math display">\[
\frac{X_2}{1-x_1}\sim\mathcal{B}er\left( \theta _{2},\theta_{3}\right) .
\]</span></li>
<li>Consider three independent random variables, $Z_1xp( _1) $, $Z_2xp( _2) $ and $Z_3xp( _3) $, and define
<span class="math display">\[
X_1=\frac{Z_{1}}{Z_{1}+Z_{2}+Z_{3}}\text{ and }X_{2}=\frac{Z_2}{Z_{1}+Z_{2}+Z_{3}}.
\]</span>
Show that <span class="math inline">\(\boldsymbol{X}\)</span> has the density given above.
\end{enumerate}
:::</li>
</ol>
<p>::: {.exercise}[Bivariate Return Periods]</p>
<p>The return period of a risk <span class="math inline">\(X\)</span> with level <span class="math inline">\(x^{\ast }\)</span> (a “catastrophe” being defined as an event such that <span class="math inline">\(X&gt;x^{\ast }\)</span>) is defined by the random variable $T_{X}( x^{}) $ corresponding to the time between two catastrophes. Let <span class="math inline">\(Z_{i}\)</span> be the time between two events, and $N_{X}( x^{}) $ be the number of events between two catastrophes, so that
<span class="math display">\[
T_{X}\left( x^{\ast }\right) =Z_{1}+Z_{2}+...+Z_{N_{X}\left(
x^{\ast
}\right) }
\]</span>
and
<span class="math display">\[
\mathbb{E}[T_{X}\left( x^{\ast }\right)] =\mathbb{E}[ N_{X}\left( x^{\ast }\right)]\mathbb{E}[ Z_1] ,
\]</span></p>
<ol style="list-style-type: decimal">
<li>Show that $N_{X}( x^{}) $ follows a geometric distribution, i.e.,
<span class="math display">\[\begin{equation*}
\Pr[ N_{X}( x^{\ast })=n]
=F_{X}\left( x^{\ast }\right) ^{n-1}\left[ 1-F_{X}\left( x^{\ast }\right)
\right].
\end{equation*}\]</span>%</li>
<li>Deduce that
<span class="math display">\[\begin{equation*}
\mathbb{E}[ T_{X}\left( x^{\ast }\right)] =\frac{\mathbb{E}[ Z] }{1-F_{X}\left( x^{\ast }\right)}.
\end{equation*}\]</span>%</li>
<li>In the case where both risks <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are based on the same events (i.e., the same <span class="math inline">\(Z_{i}\)</span>), it is possible to consider the following two return periods: $T_{XY}^{-}( x<sup>{},y</sup>{}) $ and <span class="math inline">\(T_{XY}^{+}\left( x^{\ast },y^{\ast }\right)\)</span>, corresponding respectively to the return periods <span class="math inline">\(\{X&gt;x^{\ast }\)</span> or <span class="math inline">\(Y&gt;y^{\ast }\}\)</span>, and <span class="math inline">\(\{X&gt;x^{\ast }\)</span> and <span class="math inline">\(Y&gt;y^{\ast }\}\)</span>. Show that
<span class="math display">\[\begin{equation*}
\mathbb{E}[T_{XY}^{-}\left( x^{\ast },y^{\ast }\right)] =\frac{\mathbb{E}[ Z] }{1-F_{XY}\left( x^{\ast },y^{\ast}\right)},
\end{equation*}\]</span>%
and
<span class="math display">\[\begin{equation*}
\mathbb{E}[ T_{XY}^{-}\left( x^{\ast },y^{\ast }\right)] =%
\frac{\mathbb{E}[ Z] }{1-F_{X}\left( x^{\ast }\right)
-F_{Y}\left( y^{\ast }\right) +F_{XY}\left( x^{\ast },y^{\ast
}\right) }.
\end{equation*}\]</span></li>
<li>It is also possible to consider conditional return periods <span class="math inline">\(T_{X|Y}\left( x^{\ast }|y^{\ast }\right)\)</span>, for example, <span class="math inline">\(X&gt;x^{\ast }\)</span> given <span class="math inline">\(Y&gt;y^{\ast }\)</span>. Show that
<span class="math display">\[\begin{eqnarray*}
&amp;&amp;\mathbb{E}[ T_{X|Y}\left( x^{\ast }|y^{\ast }\right) ]\\
&amp;=&amp;\frac{\mathbb{E}[ Z] }{\big( 1-F_{Y}\left( y^{\ast }\right) \big)
\big( 1-F_{X}\left( x^{\ast }\right) -F_{Y}\left( y^{\ast
}\right) +F_{XY}\left( x^{\ast },y^{\ast }\right) \big) }.
\end{eqnarray*}\]</span></li>
</ol>
<p>:::</p>
<div class="exercise">
<p><span id="exr:unlabeled-div-73" class="exercise"><strong>Exercise 9.1  </strong></span>Suppose that <span class="math inline">\(\boldsymbol{X}{\preceq_{\text{sm}}}\boldsymbol{Y}\)</span>. Show that in this case:
<span class="math display">\[\begin{eqnarray*}
\mathbb{E}[X_2\big|X_1&gt;x_1]
&amp;\leq&amp;\mathbb{E}[Y_2\big|Y_1&gt;x_1].
\end{eqnarray*}\]</span></p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-74" class="exercise"><strong>Exercise 9.2  </strong></span>Let <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> be independent random variables. Prove that <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_1+X_2\)</span> are associated.</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-75" class="exercise"><strong>Exercise 9.3  </strong></span>
Suppose <span class="math inline">\(I_1\sim\mathcal{B}er(q_1)\)</span> and <span class="math inline">\(I_2\sim\mathcal{B}er(q_2)\)</span>. Show that the following conditions are equivalent:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\mathbb{C}[I_1,I_2]\geq 0\)</span>;</li>
<li><span class="math inline">\(I_1\)</span> and <span class="math inline">\(I_2\)</span> are positively quadrant-dependent;</li>
<li><span class="math inline">\(I_1\)</span> and <span class="math inline">\(I_2\)</span> are associated;</li>
<li><span class="math inline">\(I_1\)</span> and <span class="math inline">\(I_2\)</span> are conditionally increasing.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-76" class="exercise"><strong>Exercise 9.4  </strong></span>Consider the random variable:
<span class="math display">\[
Z=F_{\boldsymbol{X}}\left( X_1,X_2\right) -F_1\left(X_1\right) F_2\left( X_2\right).
\]</span>
Show that:
<span class="math display">\[
\mathbb{E}[Z] =\frac{ 3\tau -\rho}{12}.
\]</span></p>
</div>
<p>::: {.exercise}[Tail Decreasing]</p>
<p><span class="citation">(<a href="#ref-esary1972relationships" role="doc-biblioref">Esary and Proschan 1972</a>)</span> introduced the notions of left-tail decreasing (<span class="math inline">\(LTD\)</span>) and right-tail increasing (<span class="math inline">\(RTI\)</span>) as follows: <span class="math inline">\(Y\)</span> is left-tail decreasing with respect to <span class="math inline">\(X\)</span> (denoted $LTD( Y|X) $) if and only if, for all <span class="math inline">\(x,x^{\prime },y\)</span>:
<span class="math display">\[\begin{equation*}
x&lt;x^{\prime }\text{ \ }\Longrightarrow \text{\ \ }{\Pr}[Y\leq y|X\leq x^{\prime }] \leq {\Pr}[ Y\leq y|X\leq x]
\end{equation*}\]</span>
and similarly, <span class="math inline">\(Y\)</span> is right-tail increasing with respect to <span class="math inline">\(X\)</span> (denoted $RTI(Y|X) $) if and only if, for all <span class="math inline">\(x,x^{\prime },y\)</span>:
<span class="math display">\[\begin{equation*}
x&lt;x^{\prime }\text{ \ }\Longrightarrow \text{\ \ }{\Pr}[Y\leq y|X&gt;x^{\prime }] \leq {\Pr}[Y\leq y|X&gt;x].
\end{equation*}\]</span></p>
<p>Show that:</p>
<ol style="list-style-type: decimal">
<li>$LTD( Y|X) $ if, and only if, for all <span class="math inline">\(0\leq v\leq 1\)</span>,
<span class="math display">\[
u\longmapsto \frac{C\left( u,v\right)}{u}
\]</span>
is strictly decreasing in <span class="math inline">\(u\)</span>.</li>
<li>$RTI( Y|X) $ if, and only if, for all <span class="math inline">\(0\leq v\leq 1\)</span>,
<span class="math display">\[
u\longmapsto \frac{ 1-u-v+C\left( u,v\right)}{1-u}
\]</span>
is strictly increasing in <span class="math inline">\(u\)</span>, or equivalently,
<span class="math display">\[
u\longmapsto \frac{ v-C\left( u,v\right)}{ 1-u}
\]</span>
is strictly decreasing in <span class="math inline">\(u.\)</span></li>
<li>$LTD( Y|X) $ if, and only if, for all <span class="math inline">\(0\leq v\leq 1\)</span>,
<span class="math display">\[
\frac{\partial}{\partial u} C\left( u,v\right)\leq
\frac{C\left( u,v\right)}{u}
\]</span>
almost everywhere in <span class="math inline">\(u\)</span>.</li>
<li>$RTI( Y|X) $ if, and only if, for all <span class="math inline">\(0\leq v\leq 1\)</span>,
<span class="math display">\[
\frac{\partial}{\partial u} C\left( u,v\right)\geq
\frac{ v-C\left( u,v\right)}{ 1-u}
\]</span>
almost everywhere in <span class="math inline">\(u\)</span>.</li>
</ol>
<p>:::</p>
<p>::: {.exercise}[Notions of Negative Dependence]</p>
<p>The concept of positive quadrant dependence can be reversed to define a notion of negative dependence. Specifically, $( X_1,X_2) $ is said to be negatively quadrant dependent if, for all <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>:
<span class="math display">\[\begin{equation*}
{\Pr}[X_1\leq x_1,X_2\leq x_2] \leq {\Pr}[X_1\leq x_1] {\Pr}[X_2\leq x_2].
\end{equation*}\]</span>
Show:</p>
<ol style="list-style-type: decimal">
<li>Consider <span class="math inline">\(\boldsymbol{N}\sim\mathcal{M}ult(n,p_1,p_2)\)</span>. Show that <span class="math inline">\(N_1\)</span> and <span class="math inline">\(N_2\)</span> are negatively quadrant dependent.</li>
<li>Show that the Frank copulas express negative quadrant dependence when <span class="math inline">\(\alpha\leq 0\)</span>.</li>
</ol>
<p>:::</p>
<p>::: {.exercise}[Min-Max Copula]</p>
<p>Consider the pair <span class="math inline">\(\left( X_{1:n},X_{n:n}\right)\)</span> where
<span class="math display">\[
X_{1:n}=\min\{X_1,\ldots,X_n\}\text{ and }X_{n:n}=\max\{X_1,\ldots,X_n\},
\]</span>
with <span class="math inline">\(X_1,\ldots,X_n\)</span> being independent and identically distributed with cumulative distribution function <span class="math inline">\(F\)</span>. Show that:</p>
<ol style="list-style-type: decimal">
<li>The cumulative distribution functions of <span class="math inline">\(X_{1:n}\)</span> and <span class="math inline">\(X_{n:n}\)</span> are given by:
<span class="math display">\[\begin{equation*}
F_{1:n}\left( x\right) =1-(\overline{F}\left( x\right)) ^{n}\text{ and }%
F_{n:n}\left( x\right) =( F\left( x\right)) ^{n},\hspace{2mm}x\in{\mathbb{R}}.
\end{equation*}\]</span></li>
<li>The cumulative distribution function of the pair <span class="math inline">\(\left( X_{1:n},X_{n:n}\right)\)</span> is given by:</li>
<li>The cumulative distribution function of the pair <span class="math inline">\(\left( X_{1:n}, X_{n:n}\right)\)</span> is given by%
<span class="math display">\[\begin{equation*}
F_{1,n}\left( x_1, x_2\right) =\left\{
\begin{array}{ll}
\big(F\left( x_1\right)\big) ^{n} - \left( F\left( x_2\right) - F\left( x_1\right)\right) ^{n},
&amp; \text{ if }x_1 &lt; x_2, \\
\big(F\left( x_2\right)\big) ^{n}, &amp; \text{ if }x_1 \geq x_2.
\end{array}%
\right.
\end{equation*}\]</span>%</li>
<li>The copula of the pair <span class="math inline">\(\left(X_{1:n}, X_{n:n}\right)\)</span>, denoted as <span class="math inline">\(C_{n}\)</span>, is in the following form%
<span class="math display">\[\begin{equation*}
C_{n}\left( u_1, u_2\right) =\left\{
\begin{array}{l}
u_2 - \left( u_2^{1/n} + \left( 1-u_1\right) ^{1/n}-1\right) ^{n} ,\\
\hspace{15mm}\text{ if }%
1-\left( 1-u_1\right) ^{1/n} &lt; u_2^{1/n}, \\
u_2, \text{ if }1-\left( 1-u_1\right) ^{1/n} \geq u_2^{1/n}.
\end{array}%
\right.
\end{equation*}\]</span>%</li>
<li>If $n$, then
<span class="math inline">\(C_{n}\rightarrow C_I\)</span>.</li>
<li>The Kendall’s tau and Spearman’s rho associated with the min-max copula are given respectively by%
<span class="math display">\[\begin{equation*}
\tau  =\frac{1}{2n-1},
\end{equation*}\]</span>%
and
<span class="math display">\[\begin{equation*}
\rho  =3-12\left(
\begin{array}{c}
2n \\
n%
\end{array}%
\right) ^{-1}\sum_{k=0}^{n}\frac{\left( -1\right)
^{k}}{2n-k}\left(
\begin{array}{c}
2n \\
n+k%
\end{array}%
\right) +12\frac{\left( n!\right) ^{3}}{\left( 3n\right) !}\left(
-1\right) ^{n}.
\end{equation*}\]</span>%
\end{enumerate}
:::</li>
</ol>
<div class="exercise">
<p><span id="exr:unlabeled-div-77" class="exercise"><strong>Exercise 9.5  </strong></span>Consider the normal copula given in <span class="math inline">\(\eqref{CopNorm}\)</span>. Show that</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\tau=\frac{2}{\pi}\arcsin(\alpha)\)</span>.</li>
<li>When <span class="math inline">\(\alpha\leq\alpha &#39;\)</span>,
<span class="math display">\[
C_\alpha(\boldsymbol{u})\leq C_{\alpha &#39;}(\boldsymbol{u}),\text{ for all }\boldsymbol{u}\in[0,1]^2,
\]</span>
so that <span class="math inline">\(C_\alpha\)</span> increases with <span class="math inline">\(\alpha\)</span> in the sense of <span class="math inline">\(\preceq_{\text{sm}}\)</span>.</li>
<li><span class="math inline">\(C_1=C_W\)</span>, <span class="math inline">\(C_0=C_I\)</span>, and <span class="math inline">\(C_{-1}=C_M\)</span>.</li>
<li>Show that <span class="math inline">\(C_\alpha\)</span> expresses positive quadrant dependence whenever <span class="math inline">\(\alpha\geq 0\)</span>.</li>
<li><span class="math inline">\(\rho =\frac{6}{\pi} \arcsin \left( \alpha/2\right) .\)</span></li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-78" class="exercise"><strong>Exercise 9.6  </strong></span>Morgenstein introduced the family in 1956
<span class="math display">\[
C_\alpha(u_1,u_2)=u_1u_2\big(1+\alpha(1-u_1)(1-u_2)\big),\hspace{2mm}
\alpha\in[-1,1].
\]</span>
Show that</p>
<ol style="list-style-type: decimal">
<li>The associated density is
<span class="math display">\[
c_\alpha(\boldsymbol{u})=1+\alpha(1-2u_1)(1-2u_2).
\]</span></li>
<li>The Kendall’s tau associated is
<span class="math display">\[
\tau=\frac{2}{9}\alpha\in\left[-\frac{2}{9},\frac{2}{9}\right].
\]</span></li>
<li>The linear correlation coefficient for a pair of random variables with joint distribution function <span class="math inline">\(C_\alpha\)</span> satisfies <span class="math inline">\(r\in [-1/3,1/3]\)</span>.</li>
<li>For any pair <span class="math inline">\(\boldsymbol{X}\)</span> with <span class="math inline">\(C_\alpha\)</span> as its copula, <span class="math inline">\(r(X_1,X_2)\leq 1/3\)</span>.</li>
<li>If <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are exponentially distributed with rate parameter 1 and have <span class="math inline">\(C_\alpha\)</span> as their copula,
<span class="math inline">\(r(X_1,X_2)\in[-1/4,1/4]\)</span>.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-79" class="exercise"><strong>Exercise 9.7  </strong></span>The family of copulas
<span class="math display">\[
C(u,v,\theta )=uv/\left[ 1-\theta (1-u)(1-v)\right]
\]</span>
where <span class="math inline">\(-1\leq \theta\leq +1\)</span> is known as the Ali-Mikhail-Haq copula. Show that</p>
<ol style="list-style-type: decimal">
<li>The associated density is
<span class="math display">\[\begin{equation*}
c(u,v,\theta )=\frac{1+\theta \left[ \left( u+1\right) \left( v+1\right) -3%
\right] +\theta ^{2}\left[ \left( u-1\right) \left( v-1\right) \right] }{%
\left[ 1-\theta (1-u)(1-v)\right] ^{3}}
\end{equation*}\]</span></li>
<li>Kendall’s tau is given by%
<span class="math display">\[\begin{eqnarray*}
\tau &amp;=&amp;\frac{3\theta -2}{3\theta }-\frac{2\left( 1-\theta \right) ^{2}}{%
3\theta ^{2}}\log \left( 1-\theta \right) .
\end{eqnarray*}\]</span></li>
<li>Spearman’s rho is given by%
<span class="math display">\[\begin{eqnarray*}
\rho &amp;=&amp;-\frac{12\left( 1+\theta \right) }{\theta ^{2}}\int_{0}^{\theta }\frac{%
\log \left( 1-x\right) }{x}dx-\frac{3\left( 12+\theta \right) }{\theta }\\
&amp;&amp;-%
\frac{24\left( 1-\theta \right) }{\theta ^{2}}\log \left( 1-\theta
\right) .
\end{eqnarray*}\]</span></li>
<li>This copula is an Archimedean copula with generator 
<span class="math display">\[
\varphi \left( t\right) =\log \left( 1-\theta \left[
1-t\right] \right) -\log t.
\]</span></li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-80" class="exercise"><strong>Exercise 9.8  </strong></span><span class="citation">(<a href="#ref-joe1993parametric" role="doc-biblioref">Joe 1993</a>)</span> introduced the family
<span class="math display">\[
C(u,v,\theta )=1-\left(
[1-u]^{\theta }+[1-v]^{\theta }-[1-u]^{\theta }[1-v]^{\theta
}\right) ^{1/\theta }
\]</span>
where <span class="math inline">\(\theta \geq 1\)</span>. Show that</p>
<ol style="list-style-type: decimal">
<li>This copula has the density
<span class="math display">\[\begin{eqnarray*}
c(u,v,\theta )&amp;=&amp;\Big([1-u]^{\theta }+[1-v]^{\theta }-[1-u]^{\theta}[1-v]^{\theta }\Big)^{-2+1/\theta }\\
&amp;&amp;[1-u]^{\theta -1}[1-v]^{\theta-1}\\
&amp;&amp;\Big(\theta -1+[1-u]^{\theta }+[1-v]^{\theta }-[1-u]^{\theta
}[1-v]^{\theta }\Big).
\end{eqnarray*}\]</span></li>
<li>This copula is Archimedean with generator
<span class="math display">\[
\varphi \left( t\right) =-\log \left[ 1-\left(
1-t\right) ^{\theta }\right].
\]</span></li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-81" class="exercise"><strong>Exercise 9.9  </strong></span>The Farlie-Gumbel-Morgenstein family has the copula
<span class="math display">\[\begin{equation*}
C(u,v,\theta )=\frac{uv}{\left[ 1+(1-u^{\theta })(1-v^{\theta
})\right] ^{1/\theta }}
\end{equation*}\]</span>%
where <span class="math inline">\(0&lt;\theta \leq +1\)</span>. Show that</p>
<ol style="list-style-type: decimal">
<li>The associated density is
<span class="math display">\[\begin{equation*}
c(u,v,\theta )=\frac{4-2\left( u^{\theta }+v^{\theta }\right)
+\left( 1-\theta \right) u^{\theta }v^{\theta }}{\left(
2-u^{\theta }-y^{\theta }+u^{\theta }v^{\theta }\right)
^{2+1/\theta }}.
\end{equation*}\]</span></li>
<li>This copula is Archimedean with generator
<span class="math display">\[
\varphi \left( t\right) =\log
\left( 2t^{-\theta }-1\right).
\]</span></li>
</ol>
</div>
<p>::: {.exercise}[Mardia Copula]</p>
<p>Show that the copulas associated with the bivariate distribution functions defined in Remark <span class="math inline">\(\ref{FrechetNonVide}\)</span> are
<span class="math display">\[\begin{equation*}
C\left( u,v\right) =\theta C_M\left( u,v\right) +\left( 1-\theta
\right)
C_W\left( u,v\right) \text{ for all }\theta \in \left[ 0,1\right] \text{.%
}
\end{equation*}\]</span>%
and
<span class="math display">\[\begin{eqnarray*}
C\left( u,v\right) &amp;=&amp;\frac{\theta ^{2}\left( 1-\theta \right)
}{2}C_M\left(
u,v\right) +\left( 1-\theta ^{2}\right) C_I\left( u,v\right) \\
&amp;&amp;+\frac{%
\theta ^{2}\left( 1+\theta \right) }{2}C_W\left( u,v\right),
\end{eqnarray*}\]</span>
for <span class="math inline">\(\theta \in \left[ 0,1\right]\)</span>.
:::</p>
<div class="exercise">
<p><span id="exr:unlabeled-div-82" class="exercise"><strong>Exercise 9.10  </strong></span>Show that the Gumbel-Barnett copula, given by
<span class="math display">\[
C\left( u,v,\theta \right)
=uv\exp \left( -\theta \log u\log v\right),
\]</span>
is Archimedean with generator $
( t) =( 1-t) $.</p>
</div>
</div>
</div>
</div>
<div id="bibliographical-notes-6" class="section level2 hasAnchor" number="9.8">
<h2><span class="header-section-number">9.8</span> Bibliographical Notes<a href="chap8.html#bibliographical-notes-6" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>References on dependence modeling in statistics and probability are numerous, starting with <span class="citation">(<a href="#ref-lehmann1966some" role="doc-biblioref">Lehmann 1966</a>)</span> and <span class="citation">(<a href="#ref-esary1967association" role="doc-biblioref">Esary, Proschan, and Walkup 1967</a>)</span>.</p>
<p>A comprehensive reference on the study of dependence is undoubtedly <span class="citation">(<a href="#ref-joe1997multivariate" role="doc-biblioref">Joe 1997</a>)</span>. This highly detailed book covers all the concepts developed in this chapter, with a focus on temporal dependence through Markov chains, as well as longitudinal and categorical data. The properties of dependence measures originate from <span class="citation">(<a href="#ref-scarsini1984measures" role="doc-biblioref">Scarsini 1984</a>)</span>. Other bibliographical references on the study of multiple risks include <span class="citation">(<a href="#ref-muller2002comparison" role="doc-biblioref">Müller and Stoyan 2002</a>)</span> and <span class="citation">(<a href="#ref-shaked2007stochastic" role="doc-biblioref">Shaked and Shanthikumar 2007</a>)</span>. This essential reference book on risk comparison addresses both single random variables and random vectors. Several chapters of <span class="citation">(<a href="#ref-denuit2006actuarial" role="doc-biblioref">Denuit et al. 2006</a>)</span> are also dedicated to dependence concepts, dependence relationships, and copulas. The connections between dependence concepts and stochastic orders were developed, for example, in <span class="citation">(<a href="#ref-dhaene1996dependency" role="doc-biblioref">Dhaene and Goovaerts 1996</a>)</span>.</p>
<p>Comonotonicity has seen significant development in actuarial science in recent years. See the two review articles by <span class="citation">(<a href="#ref-dhaene2002concepttheory" role="doc-biblioref">Dhaene et al. 2002b</a>)</span> and <span class="citation">(<a href="#ref-dhaene2002conceptapplication" role="doc-biblioref">Dhaene et al. 2002a</a>)</span>. Antimonotonicity is discussed in <span class="citation">(<a href="#ref-dhaene1999safest" role="doc-biblioref">Dhaene and Denuit 1999</a>)</span>.</p>
<p>Readers interested in further studying copulas introduced by <span class="citation">(<a href="#ref-sklar1959fonctions" role="doc-biblioref">Sklar 1959</a>)</span> can refer to <span class="citation">(<a href="#ref-nelsen1998introduction" role="doc-biblioref">Nelsen 1998</a>)</span>. Among the pioneers, <span class="citation">(<a href="#ref-deheuvels1979proprietes" role="doc-biblioref">Deheuvels 1979</a>)</span> is also mentioned. Those interested in a historical approach to copulas (and in particular, the connection with <span class="math inline">\(t\)</span>-norms in metric spaces of probability) can consult <span class="citation">(<a href="#ref-schweizer1991thirty" role="doc-biblioref">Berthold Schweizer 1991</a>)</span>. Also noteworthy is the article by <span class="citation">(<a href="#ref-frees1998understanding" role="doc-biblioref">Frees and Valdez 1998</a>)</span>, which introduced the concept of copula in actuarial science, and the comprehensive study by <span class="citation">(<a href="#ref-wang1998aggregation" role="doc-biblioref">Wang 1998</a>)</span>.</p>
<p>Finally, on risk aggregation, note <span class="citation">(<a href="#ref-williamson1990probabilistic" role="doc-biblioref">Robert C. Williamson and Downs 1990</a>)</span>, which deals mathematically with the sum of non-independent random variables or, more generally, transformations of multiple random variables. Another quality reference is <span class="citation">(<a href="#ref-frank1991convolutions" role="doc-biblioref">M. Frank 1991</a>)</span>. Applications to actuarial science can be found in <span class="citation">(<a href="#ref-cossette2001stochastic" role="doc-biblioref">Helene Cossette et al. 2001</a>)</span> and <span class="citation">(<a href="#ref-cossette2002distributional" role="doc-biblioref">Hélène Cossette, Denuit, and Marceau 2002</a>)</span>.</p>
<p>For readers wishing to delve deeper into the mathematical problems associated with multiple risks, the reference work is <span class="citation">(<a href="#ref-schweizer1983probabilistic" role="doc-biblioref">B. Schweizer and Sklar 1983</a>)</span>.</p>

</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-clayton1978model" class="csl-entry">
Clayton, David G. 1978. <span>“A Model for Association in Bivariate Life Tables and Its Application in Epidemiological Studies of Familial Tendency in Chronic Disease Incidence.”</span> <em>Biometrika</em> 65 (1): 141–51.
</div>
<div id="ref-cossette2001stochastic" class="csl-entry">
Cossette, Helene, Michel Denuit, Jan Dhaene, and Etienne Marceau. 2001. <span>“Stochastic Approximations of Present Value Functions.”</span> <em>Bulletin of the Swiss Association of Actuaries</em> 1: 15–28.
</div>
<div id="ref-cossette2002distributional" class="csl-entry">
Cossette, Hélène, Michel Denuit, and Etienne Marceau. 2002. <span>“Distributional Bounds for Functions of Dependent Risks.”</span> <em>Schweiz. Aktuarver. Mitt</em> 1 (45-65): 69.
</div>
<div id="ref-deheuvels1979proprietes" class="csl-entry">
Deheuvels, Paul. 1979. <span>“Propri<span>é</span>t<span>é</span>s d’existence Et Propri<span>é</span>t<span>é</span>s Topologiques Des Fonctions de d<span>é</span>pendance Avec Applicationsa La Convergence Des Types Pour Des Lois Multivari<span>é</span>es.”</span> <em>Comptes Rendus de l’Academie Des Sciences de Paris S<span>é</span>rie AB</em> 288 (2): A145–48.
</div>
<div id="ref-denuit2006actuarial" class="csl-entry">
Denuit, Michel, Jan Dhaene, Marc Goovaerts, and Rob Kaas. 2006. <em>Actuarial Theory for Dependent Risks: Measures, Orders and Models</em>. John Wiley &amp; Sons.
</div>
<div id="ref-dhaene1999safest" class="csl-entry">
Dhaene, Jan, and Michel Denuit. 1999. <span>“The Safest Dependence Structure Among Risks.”</span> <em>Insurance: Mathematics and Economics</em> 25 (1): 11–21.
</div>
<div id="ref-dhaene2002conceptapplication" class="csl-entry">
Dhaene, Jan, Michel Denuit, Marc J Goovaerts, Rob Kaas, and David Vyncke. 2002a. <span>“The Concept of Comonotonicity in Actuarial Science and Finance: Applications.”</span> <em>Insurance: Mathematics and Economics</em> 31 (2): 133–61.
</div>
<div id="ref-dhaene2002concepttheory" class="csl-entry">
———. 2002b. <span>“The Concept of Comonotonicity in Actuarial Science and Finance: Theory.”</span> <em>Insurance: Mathematics and Economics</em> 31 (1): 3–33.
</div>
<div id="ref-dhaene1996dependency" class="csl-entry">
Dhaene, Jan, and Marc J Goovaerts. 1996. <span>“Dependency of Risks and Stop-Loss Order1.”</span> <em>ASTIN Bulletin: The Journal of the IAA</em> 26 (2): 201–12.
</div>
<div id="ref-esary1972relationships" class="csl-entry">
Esary, James D, and Frank Proschan. 1972. <span>“Relationships Among Some Concepts of Bivariate Dependence.”</span> <em>The Annals of Mathematical Statistics</em>, 651–55.
</div>
<div id="ref-esary1967association" class="csl-entry">
Esary, James D, Frank Proschan, and David W Walkup. 1967. <span>“Association of Random Variables, with Applications.”</span> <em>The Annals of Mathematical Statistics</em> 38 (5): 1466–74.
</div>
<div id="ref-frank1979simultaneous" class="csl-entry">
Frank, Maurice J. 1979. <span>“On the Simultaneous Associativity of <span class="math inline">\(F (x, y)\)</span> and <span class="math inline">\(x+ y- F (x, y)\)</span>.”</span> <em>Aequationes Mathematicae</em> 19: 194–226.
</div>
<div id="ref-frank1987best" class="csl-entry">
Frank, Maurice J, Roger B Nelsen, and Berthold Schweizer. 1987. <span>“Best-Possible Bounds for the Distribution of a Sum—a Problem of Kolmogorov.”</span> <em>Probability Theory and Related Fields</em> 74 (2): 199–211.
</div>
<div id="ref-frank1991convolutions" class="csl-entry">
Frank, MJ. 1991. <span>“Convolutions for Dependent Random Variables.”</span> In <em>Advances in Probability Distributions with Given Marginals: Beyond the Copulas</em>, 75–93. Springer.
</div>
<div id="ref-frank1979duality" class="csl-entry">
Frank, MJ, and B Schweizer. 1979. <span>“On the Duality of Generalized Infimal and Supremal Convolutions.”</span> <em>Rendiconti Di Matematica</em> 12 (1): 1–23.
</div>
<div id="ref-frees1998understanding" class="csl-entry">
Frees, Edward W, and Emiliano A Valdez. 1998. <span>“Understanding Relationships Using Copulas.”</span> <em>North American Actuarial Journal</em> 2 (1): 1–25.
</div>
<div id="ref-genest1993statistical" class="csl-entry">
Genest, Christian, and Louis-Paul Rivest. 1993. <span>“Statistical Inference Procedures for Bivariate Archimedean Copulas.”</span> <em>Journal of the American Statistical Association</em> 88 (423): 1034–43.
</div>
<div id="ref-gumbel1960multivariate" class="csl-entry">
Gumbel, Emil Julius. 1960. <span>“Multivariate Extremal Distributions.”</span> <em>Annals of Mathematical Statistics</em> 31 (4): 1216–16.
</div>
<div id="ref-hutchinson1991engineering" class="csl-entry">
Hutchinson, TP, and Chin Diew Lai. 1991. <em>The Engineering Statistician’s Guide to Continuous Bivariate Distributions</em>. Rumsby Scientific Publisher.
</div>
<div id="ref-joe1993parametric" class="csl-entry">
Joe, Harry. 1993. <span>“Parametric Families of Multivariate Distributions with Given Margins.”</span> <em>Journal of Multivariate Analysis</em> 46 (2): 262–82.
</div>
<div id="ref-joe1997multivariate" class="csl-entry">
———. 1997. <em>Multivariate Models and Multivariate Dependence Concepts</em>. CRC press.
</div>
<div id="ref-kimeldorf1975uniform" class="csl-entry">
Kimeldorf, George, and Allan Sampson. 1975. <span>“Uniform Representations of Bivariate Distributions.”</span> <em>Communications in Statistics–Theory and Methods</em> 4 (7): 617–27.
</div>
<div id="ref-kimeldorf1978monotone" class="csl-entry">
Kimeldorf, George, and Allan R Sampson. 1978. <span>“Monotone Dependence.”</span> <em>The Annals of Statistics</em>, 895–903.
</div>
<div id="ref-kruskal1958ordinal" class="csl-entry">
Kruskal, William H. 1958. <span>“Ordinal Measures of Association.”</span> <em>Journal of the American Statistical Association</em> 53 (284): 814–61.
</div>
<div id="ref-lehmann1966some" class="csl-entry">
Lehmann, Erich Leo. 1966. <span>“Some Concepts of Dependence.”</span> <em>The Annals of Mathematical Statistics</em> 37 (5): 1137–53.
</div>
<div id="ref-makarov1981inequalities" class="csl-entry">
Makarov, GD. 1981. <span>“Inequalities for a Distribution Function of a Sum of Two Random Variables When Marginals Are Fixed.”</span> <em>Teoriya Veroyatnostei i Ee Primeneniya</em> 26 (4): 815–17.
</div>
<div id="ref-muller2002comparison" class="csl-entry">
Müller, Alfred, and Dietrich Stoyan. 2002. <em>Comparison Methods for Stochastic Models and Risks</em>. Wiley.
</div>
<div id="ref-nelsen1998introduction" class="csl-entry">
Nelsen, Roger B. 1998. <em>An Introduction to Copulas</em>. Springer.
</div>
<div id="ref-scarsini1984measures" class="csl-entry">
Scarsini, Marco. 1984. <span>“On Measures of Concordance.”</span> <em>Stochastica</em> 8 (3): 201–18.
</div>
<div id="ref-schweizer1991thirty" class="csl-entry">
Schweizer, Berthold. 1991. <span>“Thirty Years of Copulas.”</span> In <em>Advances in Probability Distributions with Given Marginals: Beyond the Copulas</em>, 13–50. Springer.
</div>
<div id="ref-schweizer1983probabilistic" class="csl-entry">
Schweizer, B., and A. Sklar. 1983. <em>Probabilistic Metric Spaces</em>. North-Holland.
</div>
<div id="ref-shaked2007stochastic" class="csl-entry">
Shaked, Moshe, and J George Shanthikumar. 2007. <em>Stochastic Orders</em>. Springer.
</div>
<div id="ref-sklar1959fonctions" class="csl-entry">
Sklar, M. 1959. <span>“Fonctions de r<span>é</span>partition <span class="nocase">à</span> <span class="math inline">\(n\)</span> Dimensions Et Leurs Marges.”</span> <em>Annales de l’Institut de Statistique de l’Université de Paris</em> 8 (3): 229–31.
</div>
<div id="ref-wang1998aggregation" class="csl-entry">
Wang, Shaun. 1998. <span>“Aggregation of Correlated Risk Portfolios: Models and Algorithms.”</span> In <em>Proceedings of the Casualty Actuarial Society</em>, 85:848–939. 163. Citeseer.
</div>
<div id="ref-williamson1990probabilistic" class="csl-entry">
Williamson, Robert C, and Tom Downs. 1990. <span>“Probabilistic Arithmetic. I. Numerical Methods for Calculating Convolutions and Dependency Bounds.”</span> <em>International Journal of Approximate Reasoning</em> 4 (2): 89–158.
</div>
<div id="ref-williamson1989probabilistic" class="csl-entry">
Williamson, Robert Charles et al. 1989. <span>“Probabilistic Arithmetic.”</span> PhD thesis, University of Queensland Brisbane.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="chap7.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="chap9.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/09-Multiple.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
