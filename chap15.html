<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 16 Monte Carlo | Non Life Insurance Mathematics</title>
  <meta name="description" content="Chapter 16 Monte Carlo | Non Life Insurance Mathematics" />
  <meta name="generator" content="bookdown 0.35 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 16 Monte Carlo | Non Life Insurance Mathematics" />
  <meta property="og:type" content="book" />
  
  
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 16 Monte Carlo | Non Life Insurance Mathematics" />
  
  
  

<meta name="author" content="Arthur Charpentier and Michel Denuit" />


<meta name="date" content="2023-09-05" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="chap14.html"/>
<link rel="next" href="chap16.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Non Life Insurance Mathematics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#general-context"><i class="fa fa-check"></i><b>1.1</b> General Context</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#the-risk-and-its-contractual-coverage"><i class="fa fa-check"></i><b>1.2</b> The risk and its contractual coverage</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#actuarial-risk-modeling"><i class="fa fa-check"></i><b>1.3</b> Actuarial risk modeling</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#pure-premium"><i class="fa fa-check"></i><b>1.4</b> Pure premium</a></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#from-pure-to-net-premium"><i class="fa fa-check"></i><b>1.5</b> From pure to net Premium</a></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#measurement-and-comparison-of-risks"><i class="fa fa-check"></i><b>1.6</b> Measurement and comparison of risks</a></li>
<li class="chapter" data-level="1.7" data-path="intro.html"><a href="intro.html#collective-model"><i class="fa fa-check"></i><b>1.7</b> Collective model</a></li>
<li class="chapter" data-level="1.8" data-path="intro.html"><a href="intro.html#solvency"><i class="fa fa-check"></i><b>1.8</b> Solvency</a></li>
<li class="chapter" data-level="1.9" data-path="intro.html"><a href="intro.html#multiple-risks"><i class="fa fa-check"></i><b>1.9</b> Multiple risks</a></li>
<li class="chapter" data-level="1.10" data-path="intro.html"><a href="intro.html#prior-ratemaking"><i class="fa fa-check"></i><b>1.10</b> Prior ratemaking</a></li>
<li class="chapter" data-level="1.11" data-path="intro.html"><a href="intro.html#credibility"><i class="fa fa-check"></i><b>1.11</b> Credibility</a></li>
<li class="chapter" data-level="1.12" data-path="intro.html"><a href="intro.html#bonus-malus"><i class="fa fa-check"></i><b>1.12</b> Bonus-Malus</a></li>
<li class="chapter" data-level="1.13" data-path="intro.html"><a href="intro.html#economics-of-insurance"><i class="fa fa-check"></i><b>1.13</b> Economics of insurance</a></li>
<li class="chapter" data-level="1.14" data-path="intro.html"><a href="intro.html#claims-reserving"><i class="fa fa-check"></i><b>1.14</b> Claims reserving</a></li>
<li class="chapter" data-level="1.15" data-path="intro.html"><a href="intro.html#large-risks-and-extreme-value"><i class="fa fa-check"></i><b>1.15</b> Large risks and extreme value</a></li>
<li class="chapter" data-level="1.16" data-path="intro.html"><a href="intro.html#monte-carlo"><i class="fa fa-check"></i><b>1.16</b> Monte Carlo</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chap1.html"><a href="chap1.html"><i class="fa fa-check"></i><b>2</b> The risk and its contractual coverage</a>
<ul>
<li class="chapter" data-level="2.1" data-path="chap1.html"><a href="chap1.html#risk"><i class="fa fa-check"></i><b>2.1</b> Risk</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="chap1.html"><a href="chap1.html#did-you-say-risk"><i class="fa fa-check"></i><b>2.1.1</b> Did you say risk?</a></li>
<li class="chapter" data-level="2.1.2" data-path="chap1.html"><a href="chap1.html#the-reason-for-insurance-risquophobia"><i class="fa fa-check"></i><b>2.1.2</b> The reason for insurance: risquophobia</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="chap1.html"><a href="chap1.html#risk-management-methods"><i class="fa fa-check"></i><b>2.2</b> Risk management methods</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="chap1.html"><a href="chap1.html#caution-and-self-insurance"><i class="fa fa-check"></i><b>2.2.1</b> Caution and self-insurance</a></li>
<li class="chapter" data-level="2.2.2" data-path="chap1.html"><a href="chap1.html#risk-management-vehicles"><i class="fa fa-check"></i><b>2.2.2</b> Risk management vehicles</a></li>
<li class="chapter" data-level="2.2.3" data-path="chap1.html"><a href="chap1.html#risks-assumed-by-insurers"><i class="fa fa-check"></i><b>2.2.3</b> Risks assumed by insurers</a></li>
<li class="chapter" data-level="2.2.4" data-path="chap1.html"><a href="chap1.html#risk-management-by-the-insurer"><i class="fa fa-check"></i><b>2.2.4</b> Risk management by the insurer</a></li>
<li class="chapter" data-level="2.2.5" data-path="chap1.html"><a href="chap1.html#transfer-of-risks"><i class="fa fa-check"></i><b>2.2.5</b> Transfer of risks</a></li>
<li class="chapter" data-level="2.2.6" data-path="chap1.html"><a href="chap1.html#insurance-and-financial-risks"><i class="fa fa-check"></i><b>2.2.6</b> Insurance and financial risks</a></li>
<li class="chapter" data-level="2.2.7" data-path="chap1.html"><a href="chap1.html#common-point-risk"><i class="fa fa-check"></i><b>2.2.7</b> Common point: risk</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="chap1.html"><a href="chap1.html#alea-iacta-est"><i class="fa fa-check"></i><b>2.3</b> <em>Alea iacta est…</em></a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="chap1.html"><a href="chap1.html#insolvency-of-the-insurer"><i class="fa fa-check"></i><b>2.3.1</b> Insolvency of the insurer</a></li>
<li class="chapter" data-level="2.3.2" data-path="chap1.html"><a href="chap1.html#settlements"><i class="fa fa-check"></i><b>2.3.2</b> Settlements</a></li>
<li class="chapter" data-level="2.3.3" data-path="chap1.html"><a href="chap1.html#necessity-of-reserving"><i class="fa fa-check"></i><b>2.3.3</b> Necessity of reserving</a></li>
<li class="chapter" data-level="2.3.4" data-path="chap1.html"><a href="chap1.html#inversion-of-production-cycle"><i class="fa fa-check"></i><b>2.3.4</b> Inversion of production cycle</a></li>
<li class="chapter" data-level="2.3.5" data-path="chap1.html"><a href="chap1.html#liabilities-a-reflection-of-the-insurers-business"><i class="fa fa-check"></i><b>2.3.5</b> Liabilities: a reflection of the insurer’s business</a></li>
<li class="chapter" data-level="2.3.6" data-path="chap1.html"><a href="chap1.html#liabilities-of-an-insurance-company"><i class="fa fa-check"></i><b>2.3.6</b> Liabilities of an insurance company</a></li>
<li class="chapter" data-level="2.3.7" data-path="chap1.html"><a href="chap1.html#the-assets-of-an-insurance-company"><i class="fa fa-check"></i><b>2.3.7</b> The assets of an insurance company</a></li>
<li class="chapter" data-level="2.3.8" data-path="chap1.html"><a href="chap1.html#premiums-written-premiums-earned"><i class="fa fa-check"></i><b>2.3.8</b> Premiums written, premiums earned</a></li>
<li class="chapter" data-level="2.3.9" data-path="chap1.html"><a href="chap1.html#insurers-institutional-investors"><i class="fa fa-check"></i><b>2.3.9</b> Insurers, institutional investors</a></li>
<li class="chapter" data-level="2.3.10" data-path="chap1.html"><a href="chap1.html#asset-and-liability-management"><i class="fa fa-check"></i><b>2.3.10</b> Asset and liability management</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="chap1.html"><a href="chap1.html#the-insurance-contract"><i class="fa fa-check"></i><b>2.4</b> The insurance contract</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="chap1.html"><a href="chap1.html#the-origins-the-marine-insurance-contract"><i class="fa fa-check"></i><b>2.4.1</b> The origins: the marine insurance contract</a></li>
<li class="chapter" data-level="2.4.2" data-path="chap1.html"><a href="chap1.html#the-birth-of-terrestrial-insurance-the-great-fire-of-london"><i class="fa fa-check"></i><b>2.4.2</b> The birth of terrestrial insurance: the great fire of London</a></li>
<li class="chapter" data-level="2.4.3" data-path="chap1.html"><a href="chap1.html#from-informal-solidarity-to-insurance"><i class="fa fa-check"></i><b>2.4.3</b> From informal solidarity to insurance</a></li>
<li class="chapter" data-level="2.4.4" data-path="chap1.html"><a href="chap1.html#contract-and-policy"><i class="fa fa-check"></i><b>2.4.4</b> Contract and policy</a></li>
<li class="chapter" data-level="2.4.5" data-path="chap1.html"><a href="chap1.html#insured-policyholder-and-beneficiary"><i class="fa fa-check"></i><b>2.4.5</b> Insured, policyholder and beneficiary</a></li>
<li class="chapter" data-level="2.4.6" data-path="chap1.html"><a href="chap1.html#what-about-technology"><i class="fa fa-check"></i><b>2.4.6</b> What about technology?</a></li>
<li class="chapter" data-level="2.4.7" data-path="chap1.html"><a href="chap1.html#party-representations"><i class="fa fa-check"></i><b>2.4.7</b> Party representations</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="chap1.html"><a href="chap1.html#bibliographical-notes"><i class="fa fa-check"></i><b>2.5</b> Bibliographical notes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chap2.html"><a href="chap2.html"><i class="fa fa-check"></i><b>3</b> Actuarial risk modeling</a>
<ul>
<li class="chapter" data-level="3.1" data-path="chap2.html"><a href="chap2.html#introduction"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="chap2.html"><a href="chap2.html#probabilistic-description-of-risk"><i class="fa fa-check"></i><b>3.2</b> Probabilistic description of risk</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="chap2.html"><a href="chap2.html#events"><i class="fa fa-check"></i><b>3.2.1</b> Events</a></li>
<li class="chapter" data-level="3.2.2" data-path="chap2.html"><a href="chap2.html#elementary-events"><i class="fa fa-check"></i><b>3.2.2</b> Elementary events</a></li>
<li class="chapter" data-level="3.2.3" data-path="chap2.html"><a href="chap2.html#set-formalism"><i class="fa fa-check"></i><b>3.2.3</b> Set formalism</a></li>
<li class="chapter" data-level="3.2.4" data-path="chap2.html"><a href="chap2.html#properties-satisfied-by-the-set-of-events"><i class="fa fa-check"></i><b>3.2.4</b> Properties satisfied by the set of events</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="chap2.html"><a href="chap2.html#probability-calculation-and-lack-of-arbitrage-opportunity"><i class="fa fa-check"></i><b>3.3</b> Probability calculation and lack of arbitrage opportunity</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="chap2.html"><a href="chap2.html#the-notion-of-probability"><i class="fa fa-check"></i><b>3.3.1</b> The notion of probability</a></li>
<li class="chapter" data-level="3.3.2" data-path="chap2.html"><a href="chap2.html#risk-and-uncertainty"><i class="fa fa-check"></i><b>3.3.2</b> Risk and uncertainty</a></li>
<li class="chapter" data-level="3.3.3" data-path="chap2.html"><a href="chap2.html#probability-and-insurance-premium"><i class="fa fa-check"></i><b>3.3.3</b> Probability and insurance premium</a></li>
<li class="chapter" data-level="3.3.4" data-path="chap2.html"><a href="chap2.html#absence-of-arbitrage-opportunity"><i class="fa fa-check"></i><b>3.3.4</b> Absence of arbitrage opportunity</a></li>
<li class="chapter" data-level="3.3.5" data-path="chap2.html"><a href="chap2.html#property-of-additivity-for-incompatible-events"><i class="fa fa-check"></i><b>3.3.5</b> Property of additivity for incompatible events</a></li>
<li class="chapter" data-level="3.3.6" data-path="chap2.html"><a href="chap2.html#premiums-grow-with-risk"><i class="fa fa-check"></i><b>3.3.6</b> Premiums grow with risk</a></li>
<li class="chapter" data-level="3.3.7" data-path="chap2.html"><a href="chap2.html#fairness-property"><i class="fa fa-check"></i><b>3.3.7</b> Fairness property</a></li>
<li class="chapter" data-level="3.3.8" data-path="chap2.html"><a href="chap2.html#subadditivity-property"><i class="fa fa-check"></i><b>3.3.8</b> Subadditivity property</a></li>
<li class="chapter" data-level="3.3.9" data-path="chap2.html"><a href="chap2.html#poincaré-equality"><i class="fa fa-check"></i><b>3.3.9</b> Poincaré equality</a></li>
<li class="chapter" data-level="3.3.10" data-path="chap2.html"><a href="chap2.html#conditional-probability"><i class="fa fa-check"></i><b>3.3.10</b> Conditional probability</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="chap2.html"><a href="chap2.html#independent-events"><i class="fa fa-check"></i><b>3.4</b> Independent events</a></li>
<li class="chapter" data-level="3.5" data-path="chap2.html"><a href="chap2.html#multiplication-rule-bayes"><i class="fa fa-check"></i><b>3.5</b> Multiplication rule (Bayes)</a></li>
<li class="chapter" data-level="3.6" data-path="chap2.html"><a href="chap2.html#conditionally-independent-events"><i class="fa fa-check"></i><b>3.6</b> Conditionally independent events</a></li>
<li class="chapter" data-level="3.7" data-path="chap2.html"><a href="chap2.html#total-probability-theorem"><i class="fa fa-check"></i><b>3.7</b> Total probability theorem</a></li>
<li class="chapter" data-level="3.8" data-path="chap2.html"><a href="chap2.html#bayes-theorem"><i class="fa fa-check"></i><b>3.8</b> Bayes’ theorem</a></li>
<li class="chapter" data-level="3.9" data-path="chap2.html"><a href="chap2.html#random-variables"><i class="fa fa-check"></i><b>3.9</b> Random variables</a>
<ul>
<li class="chapter" data-level="3.9.1" data-path="chap2.html"><a href="chap2.html#definition"><i class="fa fa-check"></i><b>3.9.1</b> Definition</a></li>
<li class="chapter" data-level="3.9.2" data-path="chap2.html"><a href="chap2.html#distribution-function"><i class="fa fa-check"></i><b>3.9.2</b> Distribution function</a></li>
<li class="chapter" data-level="3.9.3" data-path="chap2.html"><a href="chap2.html#support-of-a-random-variable"><i class="fa fa-check"></i><b>3.9.3</b> Support of a random variable</a></li>
<li class="chapter" data-level="3.9.4" data-path="chap2.html"><a href="chap2.html#tail-or-survival-function"><i class="fa fa-check"></i><b>3.9.4</b> Tail (or Survival) function</a></li>
<li class="chapter" data-level="3.9.5" data-path="chap2.html"><a href="chap2.html#equality-in-distribution"><i class="fa fa-check"></i><b>3.9.5</b> Equality in distribution</a></li>
<li class="chapter" data-level="3.9.6" data-path="chap2.html"><a href="chap2.html#quantiles-and-generalized-inverses"><i class="fa fa-check"></i><b>3.9.6</b> Quantiles and generalized inverses</a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="chap2.html"><a href="chap2.html#discrete-random-variables-and-counts"><i class="fa fa-check"></i><b>3.10</b> Discrete random variables and counts</a>
<ul>
<li class="chapter" data-level="3.10.1" data-path="chap2.html"><a href="chap2.html#notion"><i class="fa fa-check"></i><b>3.10.1</b> Notion</a></li>
<li class="chapter" data-level="3.10.2" data-path="chap2.html"><a href="chap2.html#uniform-discrete-variable"><i class="fa fa-check"></i><b>3.10.2</b> Uniform discrete variable</a></li>
<li class="chapter" data-level="3.10.3" data-path="chap2.html"><a href="chap2.html#bernoulli-variables"><i class="fa fa-check"></i><b>3.10.3</b> Bernoulli variables</a></li>
<li class="chapter" data-level="3.10.4" data-path="chap2.html"><a href="chap2.html#binomial-variable"><i class="fa fa-check"></i><b>3.10.4</b> Binomial variable</a></li>
<li class="chapter" data-level="3.10.5" data-path="chap2.html"><a href="chap2.html#geometric-variable"><i class="fa fa-check"></i><b>3.10.5</b> Geometric variable</a></li>
<li class="chapter" data-level="3.10.6" data-path="chap2.html"><a href="chap2.html#negative-binomial-variable"><i class="fa fa-check"></i><b>3.10.6</b> Negative binomial variable</a></li>
<li class="chapter" data-level="3.10.7" data-path="chap2.html"><a href="chap2.html#poissons-distribution"><i class="fa fa-check"></i><b>3.10.7</b> Poisson’s distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.11" data-path="chap2.html"><a href="chap2.html#continuous-random-variables"><i class="fa fa-check"></i><b>3.11</b> Continuous random variables</a>
<ul>
<li class="chapter" data-level="3.11.1" data-path="chap2.html"><a href="chap2.html#notion-1"><i class="fa fa-check"></i><b>3.11.1</b> Notion</a></li>
<li class="chapter" data-level="3.11.2" data-path="chap2.html"><a href="chap2.html#continuous-uniform-distribution"><i class="fa fa-check"></i><b>3.11.2</b> Continuous uniform distribution</a></li>
<li class="chapter" data-level="3.11.3" data-path="chap2.html"><a href="chap2.html#beta-distribution"><i class="fa fa-check"></i><b>3.11.3</b> Beta Distribution</a></li>
<li class="chapter" data-level="3.11.4" data-path="chap2.html"><a href="chap2.html#normal-or-gaussian-distribution"><i class="fa fa-check"></i><b>3.11.4</b> Normal (or Gaussian) distribution</a></li>
<li class="chapter" data-level="3.11.5" data-path="chap2.html"><a href="chap2.html#log-normal-variable"><i class="fa fa-check"></i><b>3.11.5</b> Log-normal variable</a></li>
<li class="chapter" data-level="3.11.6" data-path="chap2.html"><a href="chap2.html#negative-exponential-distribution"><i class="fa fa-check"></i><b>3.11.6</b> (Negative) exponential distribution</a></li>
<li class="chapter" data-level="3.11.7" data-path="chap2.html"><a href="chap2.html#gamma-distribution"><i class="fa fa-check"></i><b>3.11.7</b> Gamma distribution</a></li>
<li class="chapter" data-level="3.11.8" data-path="chap2.html"><a href="chap2.html#pareto-distribution"><i class="fa fa-check"></i><b>3.11.8</b> Pareto distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.12" data-path="chap2.html"><a href="chap2.html#random-vector"><i class="fa fa-check"></i><b>3.12</b> Random vector</a>
<ul>
<li class="chapter" data-level="3.12.1" data-path="chap2.html"><a href="chap2.html#definition-1"><i class="fa fa-check"></i><b>3.12.1</b> Definition</a></li>
<li class="chapter" data-level="3.12.2" data-path="chap2.html"><a href="chap2.html#distribution-function-1"><i class="fa fa-check"></i><b>3.12.2</b> Distribution function</a></li>
<li class="chapter" data-level="3.12.3" data-path="chap2.html"><a href="chap2.html#support-of-vector-xvec"><i class="fa fa-check"></i><b>3.12.3</b> Support of vector <span class="math inline">\(\Xvec\)</span></a></li>
<li class="chapter" data-level="3.12.4" data-path="chap2.html"><a href="chap2.html#independence"><i class="fa fa-check"></i><b>3.12.4</b> Independence</a></li>
<li class="chapter" data-level="3.12.5" data-path="chap2.html"><a href="chap2.html#gaussian-vector"><i class="fa fa-check"></i><b>3.12.5</b> Gaussian vector</a></li>
<li class="chapter" data-level="3.12.6" data-path="chap2.html"><a href="chap2.html#ellipticpart"><i class="fa fa-check"></i><b>3.12.6</b> Elliptical vectors</a></li>
<li class="chapter" data-level="3.12.7" data-path="chap2.html"><a href="chap2.html#definition-2"><i class="fa fa-check"></i><b>3.12.7</b> Definition</a></li>
<li class="chapter" data-level="3.12.8" data-path="chap2.html"><a href="chap2.html#multinomial-vector"><i class="fa fa-check"></i><b>3.12.8</b> Multinomial vector</a></li>
</ul></li>
<li class="chapter" data-level="3.13" data-path="chap2.html"><a href="chap2.html#conditional-variables"><i class="fa fa-check"></i><b>3.13</b> Conditional Variables</a>
<ul>
<li class="chapter" data-level="3.13.1" data-path="chap2.html"><a href="chap2.html#the-case-of-counting-variables"><i class="fa fa-check"></i><b>3.13.1</b> The case of counting variables</a></li>
<li class="chapter" data-level="3.13.2" data-path="chap2.html"><a href="chap2.html#the-case-of-continuous-variables"><i class="fa fa-check"></i><b>3.13.2</b> The case of continuous variables</a></li>
<li class="chapter" data-level="3.13.3" data-path="chap2.html"><a href="chap2.html#the-mixed-case-one-counting-variable-and-another-continuous"><i class="fa fa-check"></i><b>3.13.3</b> The mixed case: one counting variable and another continuous</a></li>
<li class="chapter" data-level="3.13.4" data-path="chap2.html"><a href="chap2.html#conditional-independence"><i class="fa fa-check"></i><b>3.13.4</b> Conditional independence</a></li>
</ul></li>
<li class="chapter" data-level="3.14" data-path="chap2.html"><a href="chap2.html#compound-distributions"><i class="fa fa-check"></i><b>3.14</b> Compound distributions</a>
<ul>
<li class="chapter" data-level="3.14.1" data-path="chap2.html"><a href="chap2.html#definition-4"><i class="fa fa-check"></i><b>3.14.1</b> Definition</a></li>
<li class="chapter" data-level="3.14.2" data-path="chap2.html"><a href="chap2.html#SecProdConv"><i class="fa fa-check"></i><b>3.14.2</b> Convolution product</a></li>
<li class="chapter" data-level="3.14.3" data-path="chap2.html"><a href="chap2.html#distribution-function-associated-with-a-compound-distribution"><i class="fa fa-check"></i><b>3.14.3</b> Distribution function associated with a compound distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.15" data-path="chap2.html"><a href="chap2.html#RiskTransformations"><i class="fa fa-check"></i><b>3.15</b> Transformations of risks and conventional damage clauses</a>
<ul>
<li class="chapter" data-level="3.15.1" data-path="chap2.html"><a href="chap2.html#concept"><i class="fa fa-check"></i><b>3.15.1</b> Concept</a></li>
<li class="chapter" data-level="3.15.2" data-path="chap2.html"><a href="chap2.html#DecOblig"><i class="fa fa-check"></i><b>3.15.2</b> The compulsory overdraft</a></li>
<li class="chapter" data-level="3.15.3" data-path="chap2.html"><a href="chap2.html#the-deductible"><i class="fa fa-check"></i><b>3.15.3</b> The deductible</a></li>
<li class="chapter" data-level="3.15.4" data-path="chap2.html"><a href="chap2.html#upper-limit-of-indemnity"><i class="fa fa-check"></i><b>3.15.4</b> (Upper) limit of indemnity</a></li>
<li class="chapter" data-level="3.15.5" data-path="chap2.html"><a href="chap2.html#technical-consequence-censored-data"><i class="fa fa-check"></i><b>3.15.5</b> Technical consequence: censored data</a></li>
<li class="chapter" data-level="3.15.6" data-path="chap2.html"><a href="chap2.html#poissons-distribution-and-damage-clauses"><i class="fa fa-check"></i><b>3.15.6</b> Poisson’s distribution and damage clauses</a></li>
<li class="chapter" data-level="3.15.7" data-path="chap2.html"><a href="chap2.html#perverse-effects-of-contractual-damage-clauses"><i class="fa fa-check"></i><b>3.15.7</b> Perverse effects of contractual damage clauses</a></li>
</ul></li>
<li class="chapter" data-level="3.16" data-path="chap2.html"><a href="chap2.html#exercises"><i class="fa fa-check"></i><b>3.16</b> Exercises</a></li>
<li class="chapter" data-level="3.17" data-path="chap2.html"><a href="chap2.html#bibliographical-notes-1"><i class="fa fa-check"></i><b>3.17</b> Bibliographical notes</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="chap3.html"><a href="chap3.html"><i class="fa fa-check"></i><b>4</b> Pure Premium</a>
<ul>
<li class="chapter" data-level="4.1" data-path="chap3.html"><a href="chap3.html#introduction-1"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="chap3.html"><a href="chap3.html#pure-premium-and-mathematical-expectation"><i class="fa fa-check"></i><b>4.2</b> Pure Premium and Mathematical Expectation</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="chap3.html"><a href="chap3.html#mathematical-expectation"><i class="fa fa-check"></i><b>4.2.1</b> Mathematical Expectation</a></li>
<li class="chapter" data-level="4.2.2" data-path="chap3.html"><a href="chap3.html#probabilities-and-expectations-of-indicators"><i class="fa fa-check"></i><b>4.2.2</b> Probabilities and Expectations of Indicators</a></li>
<li class="chapter" data-level="4.2.3" data-path="chap3.html"><a href="chap3.html#determination-of-the-pure-premium"><i class="fa fa-check"></i><b>4.2.3</b> Determination of the Pure Premium</a></li>
<li class="chapter" data-level="4.2.4" data-path="chap3.html"><a href="chap3.html#mean-squared-error-is-it-a-must"><i class="fa fa-check"></i><b>4.2.4</b> Mean Squared Error, Is It a Must?</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="chap3.html"><a href="chap3.html#variance"><i class="fa fa-check"></i><b>4.3</b> Variance</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="chap3.html"><a href="chap3.html#definition-5"><i class="fa fa-check"></i><b>4.3.1</b> Definition</a></li>
<li class="chapter" data-level="4.3.2" data-path="chap3.html"><a href="chap3.html#actuarial-interpretation"><i class="fa fa-check"></i><b>4.3.2</b> Actuarial Interpretation</a></li>
<li class="chapter" data-level="4.3.3" data-path="chap3.html"><a href="chap3.html#some-examples"><i class="fa fa-check"></i><b>4.3.3</b> Some Examples</a></li>
<li class="chapter" data-level="4.3.4" data-path="chap3.html"><a href="chap3.html#properties"><i class="fa fa-check"></i><b>4.3.4</b> Properties</a></li>
<li class="chapter" data-level="4.3.5" data-path="chap3.html"><a href="chap3.html#variance-of-common-distributions"><i class="fa fa-check"></i><b>4.3.5</b> Variance of Common Distributions</a></li>
<li class="chapter" data-level="4.3.6" data-path="chap3.html"><a href="chap3.html#variance-of-composite-distributions"><i class="fa fa-check"></i><b>4.3.6</b> Variance of Composite Distributions</a></li>
<li class="chapter" data-level="4.3.7" data-path="chap3.html"><a href="chap3.html#coefficient-of-variation-and-risk-pooling"><i class="fa fa-check"></i><b>4.3.7</b> Coefficient of Variation and Risk Pooling</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="chap3.html"><a href="chap3.html#insurance-and-bienaymé-chebyshev-inequality"><i class="fa fa-check"></i><b>4.4</b> Insurance and Bienaymé-Chebyshev Inequality</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="chap3.html"><a href="chap3.html#markovs-inequality"><i class="fa fa-check"></i><b>4.4.1</b> Markov’s Inequality</a></li>
<li class="chapter" data-level="4.4.2" data-path="chap3.html"><a href="chap3.html#bienaymé-chebyshev-inequality"><i class="fa fa-check"></i><b>4.4.2</b> Bienaymé-Chebyshev Inequality</a></li>
<li class="chapter" data-level="4.4.3" data-path="chap3.html"><a href="chap3.html#actuarial-interpretation-of-the-bienaymé-chebyshev-inequality"><i class="fa fa-check"></i><b>4.4.3</b> Actuarial Interpretation of the Bienaymé-Chebyshev Inequality</a></li>
<li class="chapter" data-level="4.4.4" data-path="chap3.html"><a href="chap3.html#conservative-nature-of-the-bienaymé-chebyshev-inequality"><i class="fa fa-check"></i><b>4.4.4</b> Conservative Nature of the Bienaymé-Chebyshev Inequality</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="chap3.html"><a href="chap3.html#insurance-and-law-of-large-numbers"><i class="fa fa-check"></i><b>4.5</b> Insurance and Law of Large Numbers</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="chap3.html"><a href="chap3.html#convergence-in-probability"><i class="fa fa-check"></i><b>4.5.1</b> Convergence in Probability</a></li>
<li class="chapter" data-level="4.5.2" data-path="chap3.html"><a href="chap3.html#convergence-of-average-claim-amount-per-policy-to-the-pure-premium"><i class="fa fa-check"></i><b>4.5.2</b> Convergence of Average Claim Amount per Policy to the Pure Premium</a></li>
<li class="chapter" data-level="4.5.3" data-path="chap3.html"><a href="chap3.html#case-of-flat-indemnity"><i class="fa fa-check"></i><b>4.5.3</b> Case of Flat Indemnity</a></li>
<li class="chapter" data-level="4.5.4" data-path="chap3.html"><a href="chap3.html#case-of-indemnity-compensation"><i class="fa fa-check"></i><b>4.5.4</b> Case of Indemnity Compensation</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="chap3.html"><a href="chap3.html#characteristic-functions"><i class="fa fa-check"></i><b>4.6</b> Characteristic Functions</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="chap3.html"><a href="chap3.html#probability-generating-function"><i class="fa fa-check"></i><b>4.6.1</b> Probability Generating Function</a></li>
<li class="chapter" data-level="4.6.2" data-path="chap3.html"><a href="chap3.html#laplace-transform"><i class="fa fa-check"></i><b>4.6.2</b> Laplace Transform</a></li>
<li class="chapter" data-level="4.6.3" data-path="chap3.html"><a href="chap3.html#moment-generating-function"><i class="fa fa-check"></i><b>4.6.3</b> Moment Generating Function</a></li>
<li class="chapter" data-level="4.6.4" data-path="chap3.html"><a href="chap3.html#hazard-rate"><i class="fa fa-check"></i><b>4.6.4</b> Hazard Rate</a></li>
<li class="chapter" data-level="4.6.5" data-path="chap3.html"><a href="chap3.html#stop-loss-premiums"><i class="fa fa-check"></i><b>4.6.5</b> Stop-Loss Premiums</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="chap3.html"><a href="chap3.html#Hetero"><i class="fa fa-check"></i><b>4.7</b> Heterogeneity and Mixtures</a></li>
<li class="chapter" data-level="4.8" data-path="chap3.html"><a href="chap3.html#context"><i class="fa fa-check"></i><b>4.8</b> Context</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="chap3.html"><a href="chap3.html#a-simple-example"><i class="fa fa-check"></i><b>4.8.1</b> A Simple Example…</a></li>
<li class="chapter" data-level="4.8.2" data-path="chap3.html"><a href="chap3.html#poisson-mixtures"><i class="fa fa-check"></i><b>4.8.2</b> Poisson Mixtures</a></li>
<li class="chapter" data-level="4.8.3" data-path="chap3.html"><a href="chap3.html#shakeds-theorem"><i class="fa fa-check"></i><b>4.8.3</b> Shaked’s Theorem</a></li>
<li class="chapter" data-level="4.8.4" data-path="chap3.html"><a href="chap3.html#composite-mixed-poisson-distributions"><i class="fa fa-check"></i><b>4.8.4</b> Composite Mixed Poisson Distributions</a></li>
<li class="chapter" data-level="4.8.5" data-path="chap3.html"><a href="chap3.html#exponential-mixtures"><i class="fa fa-check"></i><b>4.8.5</b> Exponential Mixtures</a></li>
<li class="chapter" data-level="4.8.6" data-path="chap3.html"><a href="chap3.html#identifiability-of-exponential-mixtures"><i class="fa fa-check"></i><b>4.8.6</b> Identifiability of Exponential Mixtures</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="chap3.html"><a href="chap3.html#pure-premium-in-segmented-universe"><i class="fa fa-check"></i><b>4.9</b> Pure Premium in Segmented Universe</a>
<ul>
<li class="chapter" data-level="4.9.1" data-path="chap3.html"><a href="chap3.html#segmentation-techniques"><i class="fa fa-check"></i><b>4.9.1</b> Segmentation Techniques</a></li>
<li class="chapter" data-level="4.9.2" data-path="chap3.html"><a href="chap3.html#conditional-expectation"><i class="fa fa-check"></i><b>4.9.2</b> Conditional Expectation</a></li>
<li class="chapter" data-level="4.9.3" data-path="chap3.html"><a href="chap3.html#customization-of-premiums"><i class="fa fa-check"></i><b>4.9.3</b> Customization of Premiums</a></li>
<li class="chapter" data-level="4.9.4" data-path="chap3.html"><a href="chap3.html#segmentation-pooling-and-solidarity"><i class="fa fa-check"></i><b>4.9.4</b> Segmentation, Pooling, and Solidarity</a></li>
<li class="chapter" data-level="4.9.5" data-path="chap3.html"><a href="chap3.html#DeWit"><i class="fa fa-check"></i><b>4.9.5</b> Formalization of the Segmentation Concept</a></li>
<li class="chapter" data-level="4.9.6" data-path="chap3.html"><a href="chap3.html#drawbacks-resulting-from-extensive-segmentation"><i class="fa fa-check"></i><b>4.9.6</b> Drawbacks Resulting from Extensive Segmentation</a></li>
<li class="chapter" data-level="4.9.7" data-path="chap3.html"><a href="chap3.html#segmentation-and-information-asymmetry"><i class="fa fa-check"></i><b>4.9.7</b> Segmentation and Information Asymmetry</a></li>
</ul></li>
<li class="chapter" data-level="4.10" data-path="chap3.html"><a href="chap3.html#exercises-1"><i class="fa fa-check"></i><b>4.10</b> Exercises</a></li>
<li class="chapter" data-level="4.11" data-path="chap3.html"><a href="chap3.html#bibliographical-notes-2"><i class="fa fa-check"></i><b>4.11</b> Bibliographical notes</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="chap4.html"><a href="chap4.html"><i class="fa fa-check"></i><b>5</b> From Pure to Net Premium</a>
<ul>
<li class="chapter" data-level="5.1" data-path="chap4.html"><a href="chap4.html#introduction-2"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="chap4.html"><a href="chap4.html#insurance-and-the-central-limit-theorem"><i class="fa fa-check"></i><b>5.2</b> Insurance and the Central Limit Theorem</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="chap4.html"><a href="chap4.html#central-limit-theorem"><i class="fa fa-check"></i><b>5.2.1</b> Central Limit Theorem</a></li>
<li class="chapter" data-level="5.2.2" data-path="chap4.html"><a href="chap4.html#quality-of-the-approximation-based-on-the-central-limit-theorem"><i class="fa fa-check"></i><b>5.2.2</b> Quality of the Approximation Based on the Central Limit Theorem</a></li>
<li class="chapter" data-level="5.2.3" data-path="chap4.html"><a href="chap4.html#central-limit-theorem-and-law-of-large-numbers"><i class="fa fa-check"></i><b>5.2.3</b> Central Limit Theorem and Law of Large Numbers</a></li>
<li class="chapter" data-level="5.2.4" data-path="chap4.html"><a href="chap4.html#central-limit-theorem-for-the-compound-poisson-distribution"><i class="fa fa-check"></i><b>5.2.4</b> Central Limit Theorem for the Compound Poisson Distribution</a></li>
<li class="chapter" data-level="5.2.5" data-path="chap4.html"><a href="chap4.html#tail-function-approximation-in-the-case-of-fixed-forfaiture-policy"><i class="fa fa-check"></i><b>5.2.5</b> Tail Function Approximation in the Case of Fixed-Forfaiture Policy</a></li>
<li class="chapter" data-level="5.2.6" data-path="chap4.html"><a href="chap4.html#tail-function-approximation-in-the-case-of-indemnity-policy"><i class="fa fa-check"></i><b>5.2.6</b> Tail Function Approximation in the Case of Indemnity Policy</a></li>
<li class="chapter" data-level="5.2.7" data-path="chap4.html"><a href="chap4.html#pure-premium-as-the-minimum-price-for-risk"><i class="fa fa-check"></i><b>5.2.7</b> Pure Premium as the Minimum Price for Risk</a></li>
<li class="chapter" data-level="5.2.8" data-path="chap4.html"><a href="chap4.html#sensitivity-of-results-to-possible-dependence"><i class="fa fa-check"></i><b>5.2.8</b> Sensitivity of Results to Possible Dependence</a></li>
<li class="chapter" data-level="5.2.9" data-path="chap4.html"><a href="chap4.html#stable-distributions"><i class="fa fa-check"></i><b>5.2.9</b> Stable Distributions</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="chap4.html"><a href="chap4.html#probability-of-ruin-over-a-period"><i class="fa fa-check"></i><b>5.3</b> Probability of Ruin over a Period</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="chap4.html"><a href="chap4.html#definition-14"><i class="fa fa-check"></i><b>5.3.1</b> Definition</a></li>
<li class="chapter" data-level="5.3.2" data-path="chap4.html"><a href="chap4.html#approximation-based-on-the-central-limit-theorem"><i class="fa fa-check"></i><b>5.3.2</b> Approximation based on the central limit theorem</a></li>
<li class="chapter" data-level="5.3.3" data-path="chap4.html"><a href="chap4.html#the-case-of-the-flat-rate-deductible"><i class="fa fa-check"></i><b>5.3.3</b> The case of the flat-rate deductible</a></li>
<li class="chapter" data-level="5.3.4" data-path="chap4.html"><a href="chap4.html#the-case-of-indemnity-based-claims"><i class="fa fa-check"></i><b>5.3.4</b> The case of indemnity-based claims</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="chap4.html"><a href="chap4.html#security-loading"><i class="fa fa-check"></i><b>5.4</b> Security Loading</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="chap4.html"><a href="chap4.html#concept-1"><i class="fa fa-check"></i><b>5.4.1</b> Concept</a></li>
<li class="chapter" data-level="5.4.2" data-path="chap4.html"><a href="chap4.html#determining-the-security-loading-based-on-the-central-limit-theorem"><i class="fa fa-check"></i><b>5.4.2</b> Determining the security loading Based on the Central Limit Theorem</a></li>
<li class="chapter" data-level="5.4.3" data-path="chap4.html"><a href="chap4.html#the-absolute-necessity-of-security-load"><i class="fa fa-check"></i><b>5.4.3</b> The Absolute Necessity of Security Load</a></li>
<li class="chapter" data-level="5.4.4" data-path="chap4.html"><a href="chap4.html#PrincCalculPrim"><i class="fa fa-check"></i><b>5.4.4</b> Premium Calculation Principle</a></li>
<li class="chapter" data-level="5.4.5" data-path="chap4.html"><a href="chap4.html#comments"><i class="fa fa-check"></i><b>5.4.5</b> Comments</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="chap4.html"><a href="chap4.html#security-coefficient"><i class="fa fa-check"></i><b>5.5</b> Security Coefficient</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="chap4.html"><a href="chap4.html#companys-technical-result"><i class="fa fa-check"></i><b>5.5.1</b> Company’s Technical Result</a></li>
<li class="chapter" data-level="5.5.2" data-path="chap4.html"><a href="chap4.html#determining-the-security-coefficient"><i class="fa fa-check"></i><b>5.5.2</b> Determining the Security Coefficient</a></li>
<li class="chapter" data-level="5.5.3" data-path="chap4.html"><a href="chap4.html#determining-the-safety-loading-based-on-the-bienaymé-chebyshev-inequality"><i class="fa fa-check"></i><b>5.5.3</b> Determining the Safety Loading Based on the Bienaymé-Chebyshev Inequality</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="chap4.html"><a href="chap4.html#the-normal-power-np-approximation"><i class="fa fa-check"></i><b>5.6</b> The Normal-Power (NP) Approximation</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="chap4.html"><a href="chap4.html#skewness-coefficient"><i class="fa fa-check"></i><b>5.6.1</b> Skewness Coefficient</a></li>
<li class="chapter" data-level="5.6.2" data-path="chap4.html"><a href="chap4.html#edgeworth-expansion"><i class="fa fa-check"></i><b>5.6.2</b> Edgeworth Expansion</a></li>
<li class="chapter" data-level="5.6.3" data-path="chap4.html"><a href="chap4.html#esscher-approximation"><i class="fa fa-check"></i><b>5.6.3</b> Esscher Approximation</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="chap4.html"><a href="chap4.html#using-the-np-approximation-to-determine-the-safety-loading-rate-rho-in-the-expected-value-principle"><i class="fa fa-check"></i><b>5.7</b> Using the NP Approximation to Determine the Safety Loading Rate <span class="math inline">\(\rho\)</span> in the Expected Value Principle</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="chap4.html"><a href="chap4.html#concluding-remarks-on-esscher-and-np-approximations"><i class="fa fa-check"></i><b>5.7.1</b> Concluding Remarks on Esscher and NP Approximations</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="chap4.html"><a href="chap4.html#the-premium"><i class="fa fa-check"></i><b>5.8</b> The Premium</a></li>
<li class="chapter" data-level="5.9" data-path="chap4.html"><a href="chap4.html#calculation-of-the-commercial-premium"><i class="fa fa-check"></i><b>5.9</b> Calculation of the Commercial Premium</a></li>
<li class="chapter" data-level="5.10" data-path="chap4.html"><a href="chap4.html#exercises-2"><i class="fa fa-check"></i><b>5.10</b> Exercises</a></li>
<li class="chapter" data-level="5.11" data-path="chap4.html"><a href="chap4.html#bibliographical-notes-3"><i class="fa fa-check"></i><b>5.11</b> Bibliographical Notes</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="chap5.html"><a href="chap5.html"><i class="fa fa-check"></i><b>6</b> Measurement and Comparison of Risks</a>
<ul>
<li class="chapter" data-level="6.1" data-path="chap5.html"><a href="chap5.html#introduction-3"><i class="fa fa-check"></i><b>6.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="chap5.html"><a href="chap5.html#measuring-risk-an-essential-task-for-actuaries"><i class="fa fa-check"></i><b>6.1.1</b> Measuring Risk: An Essential Task for Actuaries</a></li>
<li class="chapter" data-level="6.1.2" data-path="chap5.html"><a href="chap5.html#comparing-risks-another-specialty-of-actuaries"><i class="fa fa-check"></i><b>6.1.2</b> Comparing Risks: Another Specialty of Actuaries</a></li>
<li class="chapter" data-level="6.1.3" data-path="chap5.html"><a href="chap5.html#measuring-and-then-comparing-risks-two-related-tasks"><i class="fa fa-check"></i><b>6.1.3</b> Measuring and then Comparing Risks, Two Related Tasks</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="chap5.html"><a href="chap5.html#risk-measures"><i class="fa fa-check"></i><b>6.2</b> Risk Measures</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="chap5.html"><a href="chap5.html#definition-17"><i class="fa fa-check"></i><b>6.2.1</b> Definition</a></li>
<li class="chapter" data-level="6.2.2" data-path="chap5.html"><a href="chap5.html#coherence"><i class="fa fa-check"></i><b>6.2.2</b> Coherence</a></li>
<li class="chapter" data-level="6.2.3" data-path="chap5.html"><a href="chap5.html#value-at-risk"><i class="fa fa-check"></i><b>6.2.3</b> Value-at-Risk</a></li>
<li class="chapter" data-level="6.2.4" data-path="chap5.html"><a href="chap5.html#tail-var-and-related-measures"><i class="fa fa-check"></i><b>6.2.4</b> Tail-VaR and Related Measures</a></li>
<li class="chapter" data-level="6.2.5" data-path="chap5.html"><a href="chap5.html#esscher-risk-measure"><i class="fa fa-check"></i><b>6.2.5</b> Esscher Risk Measure</a></li>
<li class="chapter" data-level="6.2.6" data-path="chap5.html"><a href="chap5.html#wang-risk-measures"><i class="fa fa-check"></i><b>6.2.6</b> Wang Risk Measures</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="chap5.html"><a href="chap5.html#main-properties-of-wangs-risk-measures"><i class="fa fa-check"></i><b>6.3</b> Main Properties of Wang’s Risk Measures</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="chap5.html"><a href="chap5.html#concavity-of-the-distortion-function"><i class="fa fa-check"></i><b>6.3.1</b> Concavity of the Distortion Function</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="chap5.html"><a href="chap5.html#uniform-comparison-of-var"><i class="fa fa-check"></i><b>6.4</b> Uniform Comparison of VaR</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="chap5.html"><a href="chap5.html#definition-22"><i class="fa fa-check"></i><b>6.4.1</b> Definition</a></li>
<li class="chapter" data-level="6.4.2" data-path="chap5.html"><a href="chap5.html#equivalent-conditions"><i class="fa fa-check"></i><b>6.4.2</b> Equivalent Conditions</a></li>
<li class="chapter" data-level="6.4.3" data-path="chap5.html"><a href="chap5.html#properties-3"><i class="fa fa-check"></i><b>6.4.3</b> Properties</a></li>
<li class="chapter" data-level="6.4.4" data-path="chap5.html"><a href="chap5.html#hazard-rate-and-ph-risk-measure"><i class="fa fa-check"></i><b>6.4.4</b> Hazard Rate and PH Risk Measure</a></li>
<li class="chapter" data-level="6.4.5" data-path="chap5.html"><a href="chap5.html#likelihood-ratio-and-esscher-principle"><i class="fa fa-check"></i><b>6.4.5</b> Likelihood Ratio and Esscher Principle</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="chap5.html"><a href="chap5.html#uniform-comparison-of-tvars"><i class="fa fa-check"></i><b>6.5</b> Uniform Comparison of TVaRs</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="chap5.html"><a href="chap5.html#definition-24"><i class="fa fa-check"></i><b>6.5.1</b> Definition</a></li>
<li class="chapter" data-level="6.5.2" data-path="chap5.html"><a href="chap5.html#equivalent-conditions-1"><i class="fa fa-check"></i><b>6.5.2</b> Equivalent Conditions</a></li>
<li class="chapter" data-level="6.5.3" data-path="chap5.html"><a href="chap5.html#sufficient-condition"><i class="fa fa-check"></i><b>6.5.3</b> Sufficient Condition</a></li>
<li class="chapter" data-level="6.5.4" data-path="chap5.html"><a href="chap5.html#properties-4"><i class="fa fa-check"></i><b>6.5.4</b> Properties</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="chap5.html"><a href="chap5.html#optimal-form-of-risk-transfer"><i class="fa fa-check"></i><b>6.6</b> Optimal Form of Risk Transfer</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="chap5.html"><a href="chap5.html#the-problem"><i class="fa fa-check"></i><b>6.6.1</b> The Problem</a></li>
<li class="chapter" data-level="6.6.2" data-path="chap5.html"><a href="chap5.html#admissible-indemnity-functions"><i class="fa fa-check"></i><b>6.6.2</b> Admissible Indemnity Functions</a></li>
<li class="chapter" data-level="6.6.3" data-path="chap5.html"><a href="chap5.html#ordering-of-contracts"><i class="fa fa-check"></i><b>6.6.3</b> Ordering of Contracts</a></li>
<li class="chapter" data-level="6.6.4" data-path="chap5.html"><a href="chap5.html#optimality-of-the-stop-loss-contract"><i class="fa fa-check"></i><b>6.6.4</b> Optimality of the Stop-Loss Contract</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="chap5.html"><a href="chap5.html#incomplete-information"><i class="fa fa-check"></i><b>6.7</b> Incomplete Information</a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="chap5.html"><a href="chap5.html#context-2"><i class="fa fa-check"></i><b>6.7.1</b> Context</a></li>
<li class="chapter" data-level="6.7.2" data-path="chap5.html"><a href="chap5.html#known-mean-and-support"><i class="fa fa-check"></i><b>6.7.2</b> Known Mean and Support</a></li>
<li class="chapter" data-level="6.7.3" data-path="chap5.html"><a href="chap5.html#application-to-calculating-a-grouped-data-stop-loss-premium"><i class="fa fa-check"></i><b>6.7.3</b> Application to Calculating a Grouped Data Stop-Loss Premium</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="chap5.html"><a href="chap5.html#exercises-3"><i class="fa fa-check"></i><b>6.8</b> Exercises</a></li>
<li class="chapter" data-level="6.9" data-path="chap5.html"><a href="chap5.html#bibliographical-notes-4"><i class="fa fa-check"></i><b>6.9</b> Bibliographical notes</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="chap6.html"><a href="chap6.html"><i class="fa fa-check"></i><b>7</b> Collective Model</a>
<ul>
<li class="chapter" data-level="7.1" data-path="chap6.html"><a href="chap6.html#introduction-4"><i class="fa fa-check"></i><b>7.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="chap6.html"><a href="chap6.html#different-levels-of-analysis"><i class="fa fa-check"></i><b>7.1.1</b> Different Levels of Analysis</a></li>
<li class="chapter" data-level="7.1.2" data-path="chap6.html"><a href="chap6.html#the-individual-model"><i class="fa fa-check"></i><b>7.1.2</b> The Individual Model</a></li>
<li class="chapter" data-level="7.1.3" data-path="chap6.html"><a href="chap6.html#total-claims-in-the-individual-model"><i class="fa fa-check"></i><b>7.1.3</b> Total Claims in the Individual Model</a></li>
<li class="chapter" data-level="7.1.4" data-path="chap6.html"><a href="chap6.html#difficulty-of-calculations-in-the-individual-model"><i class="fa fa-check"></i><b>7.1.4</b> Difficulty of Calculations in the Individual Model</a></li>
<li class="chapter" data-level="7.1.5" data-path="chap6.html"><a href="chap6.html#the-collective-model"><i class="fa fa-check"></i><b>7.1.5</b> The Collective Model</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="chap6.html"><a href="chap6.html#approximation-of-the-individual-model"><i class="fa fa-check"></i><b>7.2</b> Approximation of the Individual Model</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="chap6.html"><a href="chap6.html#formalization-of-the-individual-model"><i class="fa fa-check"></i><b>7.2.1</b> Formalization of the Individual Model</a></li>
<li class="chapter" data-level="7.2.2" data-path="chap6.html"><a href="chap6.html#representation-of-the-total-claims-amount-in-the-individual-model"><i class="fa fa-check"></i><b>7.2.2</b> Representation of the Total Claims Amount in the Individual Model</a></li>
<li class="chapter" data-level="7.2.3" data-path="chap6.html"><a href="chap6.html#justification-of-approximating-the-individual-model-with-the"><i class="fa fa-check"></i><b>7.2.3</b> Justification of Approximating the Individual Model with the</a></li>
<li class="chapter" data-level="7.2.4" data-path="chap6.html"><a href="chap6.html#transition-from-the-individual-model-to-the-collective-model"><i class="fa fa-check"></i><b>7.2.4</b> Transition from the Individual Model to the Collective Model</a></li>
<li class="chapter" data-level="7.2.5" data-path="chap6.html"><a href="chap6.html#choice-of-parameters-for-the-collective-model"><i class="fa fa-check"></i><b>7.2.5</b> Choice of Parameters for the Collective Model</a></li>
<li class="chapter" data-level="7.2.6" data-path="chap6.html"><a href="chap6.html#bounds-on-the-approximation-error-cumulative-distribution-function"><i class="fa fa-check"></i><b>7.2.6</b> Bounds on the Approximation Error: Cumulative Distribution Function</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="chap6.html"><a href="chap6.html#ApproxPoisson"><i class="fa fa-check"></i><b>7.3</b> Analysis of Proposition</a></li>
<li class="chapter" data-level="7.4" data-path="chap6.html"><a href="chap6.html#numerical-analysis"><i class="fa fa-check"></i><b>7.4</b> Numerical Analysis</a></li>
<li class="chapter" data-level="7.5" data-path="chap6.html"><a href="chap6.html#approximating-the-individual-model-by-a-collective-model"><i class="fa fa-check"></i><b>7.5</b> Approximating the Individual Model by a Collective Model</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="chap6.html"><a href="chap6.html#bounds-on-approximation-error-stop-loss-premiums"><i class="fa fa-check"></i><b>7.5.1</b> Bounds on Approximation Error: Stop-Loss Premiums</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="chap6.html"><a href="chap6.html#discretization-of-claim-amounts"><i class="fa fa-check"></i><b>7.6</b> Discretization of Claim Amounts</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="chap6.html"><a href="chap6.html#necessity-of-discretization"><i class="fa fa-check"></i><b>7.6.1</b> Necessity of Discretization</a></li>
<li class="chapter" data-level="7.6.2" data-path="chap6.html"><a href="chap6.html#discretization-in-accordance-with-var"><i class="fa fa-check"></i><b>7.6.2</b> Discretization in Accordance with VaR</a></li>
<li class="chapter" data-level="7.6.3" data-path="chap6.html"><a href="chap6.html#discretization-in-accordance-with-tvar"><i class="fa fa-check"></i><b>7.6.3</b> Discretization in Accordance with TVaR</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="chap6.html"><a href="chap6.html#panjers-algorithm"><i class="fa fa-check"></i><b>7.7</b> Panjer’s Algorithm</a>
<ul>
<li class="chapter" data-level="7.7.1" data-path="chap6.html"><a href="chap6.html#the-challenge-of-a-direct-approach"><i class="fa fa-check"></i><b>7.7.1</b> The Challenge of a Direct Approach</a></li>
<li class="chapter" data-level="7.7.2" data-path="chap6.html"><a href="chap6.html#panjer-family"><i class="fa fa-check"></i><b>7.7.2</b> Panjer Family</a></li>
<li class="chapter" data-level="7.7.3" data-path="chap6.html"><a href="chap6.html#panjers-algorithm-for-positive-claim-costs"><i class="fa fa-check"></i><b>7.7.3</b> Panjer’s Algorithm for Positive Claim Costs</a></li>
<li class="chapter" data-level="7.7.4" data-path="chap6.html"><a href="chap6.html#panjers-algorithm-for-non-negative-claim-costs"><i class="fa fa-check"></i><b>7.7.4</b> Panjer’s Algorithm for Non-Negative Claim Costs</a></li>
<li class="chapter" data-level="7.7.5" data-path="chap6.html"><a href="chap6.html#evaluating-ruin-probabilities-over-a-period"><i class="fa fa-check"></i><b>7.7.5</b> Evaluating Ruin Probabilities over a Period</a></li>
<li class="chapter" data-level="7.7.6" data-path="chap6.html"><a href="chap6.html#evaluation-of-var-and-solvency-margin"><i class="fa fa-check"></i><b>7.7.6</b> Evaluation of VaR and Solvency Margin</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="chap6.html"><a href="chap6.html#evaluation-of-stop-loss-premiums"><i class="fa fa-check"></i><b>7.8</b> Evaluation of Stop-Loss Premiums</a>
<ul>
<li class="chapter" data-level="7.8.1" data-path="chap6.html"><a href="chap6.html#iterative-scheme-for-calculating-stop-loss-premiums"><i class="fa fa-check"></i><b>7.8.1</b> Iterative Scheme for Calculating Stop-Loss Premiums</a></li>
<li class="chapter" data-level="7.8.2" data-path="chap6.html"><a href="chap6.html#error-due-to-discretization"><i class="fa fa-check"></i><b>7.8.2</b> Error Due to Discretization</a></li>
</ul></li>
<li class="chapter" data-level="7.9" data-path="chap6.html"><a href="chap6.html#exercises-4"><i class="fa fa-check"></i><b>7.9</b> Exercises</a></li>
<li class="chapter" data-level="7.10" data-path="chap6.html"><a href="chap6.html#bibliographical-notes-5"><i class="fa fa-check"></i><b>7.10</b> Bibliographical Notes</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="chap7.html"><a href="chap7.html"><i class="fa fa-check"></i><b>8</b> Solvency</a></li>
<li class="chapter" data-level="9" data-path="chap8.html"><a href="chap8.html"><i class="fa fa-check"></i><b>9</b> Multiple Risks</a>
<ul>
<li class="chapter" data-level="9.1" data-path="chap8.html"><a href="chap8.html#introduction-5"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="chap8.html"><a href="chap8.html#comonotonicity-and-antimonotonicity"><i class="fa fa-check"></i><b>9.2</b> Comonotonicity and Antimonotonicity</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="chap8.html"><a href="chap8.html#fréchet-classes"><i class="fa fa-check"></i><b>9.2.1</b> Fréchet Classes</a></li>
<li class="chapter" data-level="9.2.2" data-path="chap8.html"><a href="chap8.html#fréchet-bounds"><i class="fa fa-check"></i><b>9.2.2</b> Fréchet Bounds</a></li>
<li class="chapter" data-level="9.2.3" data-path="chap8.html"><a href="chap8.html#perfect-dependence-comonotonicity-and-antimonotonicity"><i class="fa fa-check"></i><b>9.2.3</b> Perfect Dependence: Comonotonicity and Antimonotonicity</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="chap8.html"><a href="chap8.html#measures-of-dependence"><i class="fa fa-check"></i><b>9.3</b> Measures of Dependence</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="chap8.html"><a href="chap8.html#concept-2"><i class="fa fa-check"></i><b>9.3.1</b> Concept</a></li>
<li class="chapter" data-level="9.3.2" data-path="chap8.html"><a href="chap8.html#linear-correlation-or-pearson-correlation"><i class="fa fa-check"></i><b>9.3.2</b> Linear Correlation or Pearson Correlation</a></li>
<li class="chapter" data-level="9.3.3" data-path="chap8.html"><a href="chap8.html#values-of-the-linear-correlation-coefficient"><i class="fa fa-check"></i><b>9.3.3</b> Values of the Linear Correlation Coefficient</a></li>
<li class="chapter" data-level="9.3.4" data-path="chap8.html"><a href="chap8.html#kendalls-rank-correlation-coefficient"><i class="fa fa-check"></i><b>9.3.4</b> Kendall’s Rank Correlation Coefficient</a></li>
<li class="chapter" data-level="9.3.5" data-path="chap8.html"><a href="chap8.html#spearmans-rank-correlation-coefficient"><i class="fa fa-check"></i><b>9.3.5</b> Spearman’s Rank Correlation Coefficient</a></li>
<li class="chapter" data-level="9.3.6" data-path="chap8.html"><a href="chap8.html#relationship-between-kendalls-tau-and-spearmans-rho"><i class="fa fa-check"></i><b>9.3.6</b> Relationship Between Kendall’s Tau and Spearman’s Rho</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="chap8.html"><a href="chap8.html#comparison-of-dependence"><i class="fa fa-check"></i><b>9.4</b> Comparison of Dependence</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="chap8.html"><a href="chap8.html#order-of-dependence"><i class="fa fa-check"></i><b>9.4.1</b> Order of Dependence</a></li>
<li class="chapter" data-level="9.4.2" data-path="chap8.html"><a href="chap8.html#supermodular-comparison"><i class="fa fa-check"></i><b>9.4.2</b> Supermodular Comparison</a></li>
<li class="chapter" data-level="9.4.3" data-path="chap8.html"><a href="chap8.html#functional-stability-of-supermodular-comparisons"><i class="fa fa-check"></i><b>9.4.3</b> Functional Stability of Supermodular Comparisons</a></li>
<li class="chapter" data-level="9.4.4" data-path="chap8.html"><a href="chap8.html#supermodular-comparison-and-fréchet-space"><i class="fa fa-check"></i><b>9.4.4</b> Supermodular Comparison and Fréchet Space</a></li>
<li class="chapter" data-level="9.4.5" data-path="chap8.html"><a href="chap8.html#supermodular-comparison-and-joint-distributiontail-functions"><i class="fa fa-check"></i><b>9.4.5</b> Supermodular Comparison and Joint Distribution/Tail Functions</a></li>
<li class="chapter" data-level="9.4.6" data-path="chap8.html"><a href="chap8.html#extreme-structures-of-supermodular-dependence"><i class="fa fa-check"></i><b>9.4.6</b> Extreme Structures of Supermodular Dependence</a></li>
<li class="chapter" data-level="9.4.7" data-path="chap8.html"><a href="chap8.html#supermodular-comparison-and-correlation-coefficients"><i class="fa fa-check"></i><b>9.4.7</b> Supermodular Comparison and Correlation Coefficients</a></li>
<li class="chapter" data-level="9.4.8" data-path="chap8.html"><a href="chap8.html#lcx-order-and-supermodular-comparison"><i class="fa fa-check"></i><b>9.4.8</b> <span class="math inline">\(\lcx\)</span> Order and Supermodular Comparison</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="chap8.html"><a href="chap8.html#notions-of-positive-dependence"><i class="fa fa-check"></i><b>9.5</b> Notions of Positive Dependence</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="chap8.html"><a href="chap8.html#concept-3"><i class="fa fa-check"></i><b>9.5.1</b> Concept</a></li>
<li class="chapter" data-level="9.5.2" data-path="chap8.html"><a href="chap8.html#positive-quadrant-dependence"><i class="fa fa-check"></i><b>9.5.2</b> Positive Quadrant Dependence</a></li>
<li class="chapter" data-level="9.5.3" data-path="chap8.html"><a href="chap8.html#association"><i class="fa fa-check"></i><b>9.5.3</b> Association</a></li>
<li class="chapter" data-level="9.5.4" data-path="chap8.html"><a href="chap8.html#conditional-growth"><i class="fa fa-check"></i><b>9.5.4</b> Conditional Growth</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="chap8.html"><a href="chap8.html#introduction-to-copula-theory"><i class="fa fa-check"></i><b>9.6</b> Introduction to Copula Theory</a>
<ul>
<li class="chapter" data-level="9.6.1" data-path="chap8.html"><a href="chap8.html#principle-2"><i class="fa fa-check"></i><b>9.6.1</b> Principle</a></li>
<li class="chapter" data-level="9.6.2" data-path="chap8.html"><a href="chap8.html#definition-32"><i class="fa fa-check"></i><b>9.6.2</b> Definition</a></li>
<li class="chapter" data-level="9.6.3" data-path="chap8.html"><a href="chap8.html#sklars-theorem"><i class="fa fa-check"></i><b>9.6.3</b> Sklar’s Theorem</a></li>
<li class="chapter" data-level="9.6.4" data-path="chap8.html"><a href="chap8.html#properties-of-copulas"><i class="fa fa-check"></i><b>9.6.4</b> Properties of Copulas</a></li>
<li class="chapter" data-level="9.6.5" data-path="chap8.html"><a href="chap8.html#dependencemeasures"><i class="fa fa-check"></i><b>9.6.5</b> Dependence Measures and Copulas</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="chap8.html"><a href="chap8.html#archimedean-copulas"><i class="fa fa-check"></i><b>9.7</b> Archimedean Copulas</a>
<ul>
<li class="chapter" data-level="9.7.1" data-path="chap8.html"><a href="chap8.html#definition-33"><i class="fa fa-check"></i><b>9.7.1</b> Definition</a></li>
<li class="chapter" data-level="9.7.2" data-path="chap8.html"><a href="chap8.html#frailty-models-and-archimedean-copulas"><i class="fa fa-check"></i><b>9.7.2</b> Frailty Models and Archimedean Copulas</a></li>
<li class="chapter" data-level="9.7.3" data-path="chap8.html"><a href="chap8.html#survival-function"><i class="fa fa-check"></i><b>9.7.3</b> Survival Function</a></li>
<li class="chapter" data-level="9.7.4" data-path="chap8.html"><a href="chap8.html#regression-function"><i class="fa fa-check"></i><b>9.7.4</b> Regression Function</a></li>
<li class="chapter" data-level="9.7.5" data-path="chap8.html"><a href="chap8.html#bivariate-integral-transformation"><i class="fa fa-check"></i><b>9.7.5</b> Bivariate Integral Transformation</a></li>
<li class="chapter" data-level="9.7.6" data-path="chap8.html"><a href="chap8.html#order-relations-for-archimedean-copulas"><i class="fa fa-check"></i><b>9.7.6</b> Order Relations for Archimedean Copulas</a></li>
<li class="chapter" data-level="9.7.7" data-path="chap8.html"><a href="chap8.html#study-of-a-function-of-two-correlated-risks"><i class="fa fa-check"></i><b>9.7.7</b> Study of a Function of Two Correlated Risks</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="chap8.html"><a href="chap8.html#multivariate-discrete-distributions"><i class="fa fa-check"></i><b>9.8</b> Multivariate Discrete Distributions</a>
<ul>
<li class="chapter" data-level="9.8.1" data-path="chap8.html"><a href="chap8.html#two-classes-of-correlated-risks-model"><i class="fa fa-check"></i><b>9.8.1</b> Two Classes of Correlated Risks Model</a></li>
<li class="chapter" data-level="9.8.2" data-path="chap8.html"><a href="chap8.html#multivariate-bernoulli-distribution"><i class="fa fa-check"></i><b>9.8.2</b> Multivariate Bernoulli Distribution</a></li>
<li class="chapter" data-level="9.8.3" data-path="chap8.html"><a href="chap8.html#common-shock-poisson-model-bivariate-poisson-distribution"><i class="fa fa-check"></i><b>9.8.3</b> Common Shock Poisson Model: Bivariate Poisson Distribution</a></li>
<li class="chapter" data-level="9.8.4" data-path="chap8.html"><a href="chap8.html#common-shock-bernoulli-model-the-marceau-model"><i class="fa fa-check"></i><b>9.8.4</b> Common Shock Bernoulli Model: The Marceau Model</a></li>
</ul></li>
<li class="chapter" data-level="9.9" data-path="chap8.html"><a href="chap8.html#exercices"><i class="fa fa-check"></i><b>9.9</b> Exercices</a></li>
<li class="chapter" data-level="9.10" data-path="chap8.html"><a href="chap8.html#bibliographical-notes-6"><i class="fa fa-check"></i><b>9.10</b> Bibliographical Notes</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="chap9.html"><a href="chap9.html"><i class="fa fa-check"></i><b>10</b> Prior Ratemaking</a>
<ul>
<li class="chapter" data-level="10.1" data-path="chap9.html"><a href="chap9.html#introduction-6"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="chap9.html"><a href="chap9.html#Sec965New"><i class="fa fa-check"></i><b>10.2</b> Rating Variables</a></li>
<li class="chapter" data-level="10.3" data-path="chap9.html"><a href="chap9.html#basic-principles-of-statistics"><i class="fa fa-check"></i><b>10.3</b> Basic Principles of Statistics</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="chap9.html"><a href="chap9.html#empirical-cumulative-distribution-function"><i class="fa fa-check"></i><b>10.3.1</b> Empirical Cumulative Distribution Function</a></li>
<li class="chapter" data-level="10.3.2" data-path="chap9.html"><a href="chap9.html#the-parametric-approach"><i class="fa fa-check"></i><b>10.3.2</b> The Parametric Approach</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="chap9.html"><a href="chap9.html#fishers-information"><i class="fa fa-check"></i><b>10.4</b> Fisher’s Information</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="chap9.html"><a href="chap9.html#conditions-on-the-statistical-model"><i class="fa fa-check"></i><b>10.4.1</b> Conditions on the Statistical Model</a></li>
<li class="chapter" data-level="10.4.2" data-path="chap9.html"><a href="chap9.html#definition-35"><i class="fa fa-check"></i><b>10.4.2</b> Definition</a></li>
<li class="chapter" data-level="10.4.3" data-path="chap9.html"><a href="chap9.html#parameter-estimation-using-maximum-likelihood-method"><i class="fa fa-check"></i><b>10.4.3</b> Parameter Estimation Using Maximum Likelihood Method</a></li>
<li class="chapter" data-level="10.4.4" data-path="chap9.html"><a href="chap9.html#likelihood-ratio-test"><i class="fa fa-check"></i><b>10.4.4</b> Likelihood Ratio Test</a></li>
<li class="chapter" data-level="10.4.5" data-path="chap9.html"><a href="chap9.html#other-estimation-methods"><i class="fa fa-check"></i><b>10.4.5</b> Other Estimation Methods</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="chap9.html"><a href="chap9.html#SecPCA"><i class="fa fa-check"></i><b>10.5</b> Data Analysis</a>
<ul>
<li class="chapter" data-level="10.5.1" data-path="chap9.html"><a href="chap9.html#principle-3"><i class="fa fa-check"></i><b>10.5.1</b> Principle</a></li>
<li class="chapter" data-level="10.5.2" data-path="chap9.html"><a href="chap9.html#principal-component-analysis-pca"><i class="fa fa-check"></i><b>10.5.2</b> Principal Component Analysis (PCA)</a></li>
<li class="chapter" data-level="10.5.3" data-path="chap9.html"><a href="chap9.html#multiple-correspondence-analysis-mca"><i class="fa fa-check"></i><b>10.5.3</b> Multiple Correspondence Analysis (MCA)</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="chap9.html"><a href="chap9.html#Sec923MC"><i class="fa fa-check"></i><b>10.6</b> Scoring Methods</a>
<ul>
<li class="chapter" data-level="10.6.1" data-path="chap9.html"><a href="chap9.html#classification-methods"><i class="fa fa-check"></i><b>10.6.1</b> Classification Methods</a></li>
<li class="chapter" data-level="10.6.2" data-path="chap9.html"><a href="chap9.html#definition-of-a-score"><i class="fa fa-check"></i><b>10.6.2</b> Definition of a Score</a></li>
<li class="chapter" data-level="10.6.3" data-path="chap9.html"><a href="chap9.html#principle-of-scoring"><i class="fa fa-check"></i><b>10.6.3</b> Principle of Scoring</a></li>
<li class="chapter" data-level="10.6.4" data-path="chap9.html"><a href="chap9.html#optimal-classification-and-threshold-selection"><i class="fa fa-check"></i><b>10.6.4</b> Optimal Classification and Threshold Selection</a></li>
<li class="chapter" data-level="10.6.5" data-path="chap9.html"><a href="chap9.html#practical-construction-of-a-score"><i class="fa fa-check"></i><b>10.6.5</b> Practical Construction of a Score</a></li>
<li class="chapter" data-level="10.6.6" data-path="chap9.html"><a href="chap9.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>10.6.6</b> (Linear) Discriminant Analysis</a></li>
<li class="chapter" data-level="10.6.7" data-path="chap9.html"><a href="chap9.html#discriminant-analysis"><i class="fa fa-check"></i><b>10.6.7</b> Discriminant Analysis</a></li>
<li class="chapter" data-level="10.6.8" data-path="chap9.html"><a href="chap9.html#the-disqual-method"><i class="fa fa-check"></i><b>10.6.8</b> The DISQUAL Method</a></li>
<li class="chapter" data-level="10.6.9" data-path="chap9.html"><a href="chap9.html#the-probit-model"><i class="fa fa-check"></i><b>10.6.9</b> The Probit Model</a></li>
<li class="chapter" data-level="10.6.10" data-path="chap9.html"><a href="chap9.html#the-logit-model"><i class="fa fa-check"></i><b>10.6.10</b> The Logit Model</a></li>
<li class="chapter" data-level="10.6.11" data-path="chap9.html"><a href="chap9.html#duality-of-approaches"><i class="fa fa-check"></i><b>10.6.11</b> Duality of Approaches</a></li>
<li class="chapter" data-level="10.6.12" data-path="chap9.html"><a href="chap9.html#performance-and-selection-curves"><i class="fa fa-check"></i><b>10.6.12</b> Performance and Selection Curves</a></li>
<li class="chapter" data-level="10.6.13" data-path="chap9.html"><a href="chap9.html#desirable-properties-of-a-score"><i class="fa fa-check"></i><b>10.6.13</b> Desirable Properties of a Score</a></li>
<li class="chapter" data-level="10.6.14" data-path="chap9.html"><a href="chap9.html#score-comparison"><i class="fa fa-check"></i><b>10.6.14</b> Score Comparison</a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="chap9.html"><a href="chap9.html#Sec93MLMC"><i class="fa fa-check"></i><b>10.7</b> Linear Model</a>
<ul>
<li class="chapter" data-level="10.7.1" data-path="chap9.html"><a href="chap9.html#definition-36"><i class="fa fa-check"></i><b>10.7.1</b> Definition</a></li>
<li class="chapter" data-level="10.7.2" data-path="chap9.html"><a href="chap9.html#matrix-formalism"><i class="fa fa-check"></i><b>10.7.2</b> Matrix Formalism</a></li>
<li class="chapter" data-level="10.7.3" data-path="chap9.html"><a href="chap9.html#parameter-estimation"><i class="fa fa-check"></i><b>10.7.3</b> Parameter Estimation</a></li>
<li class="chapter" data-level="10.7.4" data-path="chap9.html"><a href="chap9.html#prediction-matrix"><i class="fa fa-check"></i><b>10.7.4</b> Prediction Matrix</a></li>
<li class="chapter" data-level="10.7.5" data-path="chap9.html"><a href="chap9.html#estimation-of-means-and-variance"><i class="fa fa-check"></i><b>10.7.5</b> Estimation of Means and Variance</a></li>
<li class="chapter" data-level="10.7.6" data-path="chap9.html"><a href="chap9.html#measurement-of-fit-quality-the-coefficient-of-determination"><i class="fa fa-check"></i><b>10.7.6</b> Measurement of Fit Quality: The Coefficient of Determination</a></li>
<li class="chapter" data-level="10.7.7" data-path="chap9.html"><a href="chap9.html#standardized-residuals"><i class="fa fa-check"></i><b>10.7.7</b> Standardized Residuals</a></li>
<li class="chapter" data-level="10.7.8" data-path="chap9.html"><a href="chap9.html#inferential-results-for-parameters"><i class="fa fa-check"></i><b>10.7.8</b> Inferential Results for Parameters</a></li>
<li class="chapter" data-level="10.7.9" data-path="chap9.html"><a href="chap9.html#testing-a-simple-hypothesis"><i class="fa fa-check"></i><b>10.7.9</b> Testing a Simple Hypothesis</a></li>
<li class="chapter" data-level="10.7.10" data-path="chap9.html"><a href="chap9.html#comparison-of-nested-models"><i class="fa fa-check"></i><b>10.7.10</b> Comparison of Nested Models</a></li>
<li class="chapter" data-level="10.7.11" data-path="chap9.html"><a href="chap9.html#confidence-regions"><i class="fa fa-check"></i><b>10.7.11</b> Confidence Regions</a></li>
<li class="chapter" data-level="10.7.12" data-path="chap9.html"><a href="chap9.html#confidence-intervals"><i class="fa fa-check"></i><b>10.7.12</b> Confidence Intervals</a></li>
<li class="chapter" data-level="10.7.13" data-path="chap9.html"><a href="chap9.html#measures-of-influence"><i class="fa fa-check"></i><b>10.7.13</b> Measures of Influence</a></li>
<li class="chapter" data-level="10.7.14" data-path="chap9.html"><a href="chap9.html#weighted-least-squares"><i class="fa fa-check"></i><b>10.7.14</b> Weighted Least Squares</a></li>
</ul></li>
<li class="chapter" data-level="10.8" data-path="chap9.html"><a href="chap9.html#additive-models"><i class="fa fa-check"></i><b>10.8</b> Additive Models</a>
<ul>
<li class="chapter" data-level="10.8.1" data-path="chap9.html"><a href="chap9.html#principle-5"><i class="fa fa-check"></i><b>10.8.1</b> Principle</a></li>
<li class="chapter" data-level="10.8.2" data-path="chap9.html"><a href="chap9.html#single-regressor-case"><i class="fa fa-check"></i><b>10.8.2</b> Single Regressor Case</a></li>
<li class="chapter" data-level="10.8.3" data-path="chap9.html"><a href="chap9.html#estimation-with-multiple-regressors-backfitting"><i class="fa fa-check"></i><b>10.8.3</b> Estimation with Multiple Regressors: Backfitting</a></li>
<li class="chapter" data-level="10.8.4" data-path="chap9.html"><a href="chap9.html#comparison-of-different-approaches"><i class="fa fa-check"></i><b>10.8.4</b> Comparison of Different Approaches</a></li>
</ul></li>
<li class="chapter" data-level="10.9" data-path="chap9.html"><a href="chap9.html#Sec96GLM"><i class="fa fa-check"></i><b>10.9</b> Generalized Linear Models (GLM)</a>
<ul>
<li class="chapter" data-level="10.9.1" data-path="chap9.html"><a href="chap9.html#a-brief-history-of-actuarial-applications-of-regression-models"><i class="fa fa-check"></i><b>10.9.1</b> A Brief History of Actuarial Applications of Regression Models</a></li>
<li class="chapter" data-level="10.9.2" data-path="chap9.html"><a href="chap9.html#definition-38"><i class="fa fa-check"></i><b>10.9.2</b> Definition</a></li>
<li class="chapter" data-level="10.9.3" data-path="chap9.html"><a href="chap9.html#mean-and-variance"><i class="fa fa-check"></i><b>10.9.3</b> Mean and Variance</a></li>
<li class="chapter" data-level="10.9.4" data-path="chap9.html"><a href="chap9.html#regression-model"><i class="fa fa-check"></i><b>10.9.4</b> Regression Model</a></li>
<li class="chapter" data-level="10.9.5" data-path="chap9.html"><a href="chap9.html#canonical-link-function"><i class="fa fa-check"></i><b>10.9.5</b> Canonical Link Function</a></li>
<li class="chapter" data-level="10.9.6" data-path="chap9.html"><a href="chap9.html#likelihood-equations"><i class="fa fa-check"></i><b>10.9.6</b> Likelihood Equations</a></li>
<li class="chapter" data-level="10.9.7" data-path="chap9.html"><a href="chap9.html#solving-likelihood-equations"><i class="fa fa-check"></i><b>10.9.7</b> Solving Likelihood Equations</a></li>
<li class="chapter" data-level="10.9.8" data-path="chap9.html"><a href="chap9.html#fisher-information"><i class="fa fa-check"></i><b>10.9.8</b> Fisher Information</a></li>
<li class="chapter" data-level="10.9.9" data-path="chap9.html"><a href="chap9.html#confidence-interval-for-parameters"><i class="fa fa-check"></i><b>10.9.9</b> Confidence Interval for Parameters</a></li>
<li class="chapter" data-level="10.9.10" data-path="chap9.html"><a href="chap9.html#model-comparison"><i class="fa fa-check"></i><b>10.9.10</b> Model Comparison</a></li>
<li class="chapter" data-level="10.9.11" data-path="chap9.html"><a href="chap9.html#hypothesis-tests-on-parameters"><i class="fa fa-check"></i><b>10.9.11</b> Hypothesis Tests on Parameters</a></li>
<li class="chapter" data-level="10.9.12" data-path="chap9.html"><a href="chap9.html#estimation-of-the-dispersion-parameter"><i class="fa fa-check"></i><b>10.9.12</b> Estimation of the Dispersion Parameter</a></li>
<li class="chapter" data-level="10.9.13" data-path="chap9.html"><a href="chap9.html#residual-analysis"><i class="fa fa-check"></i><b>10.9.13</b> Residual Analysis</a></li>
<li class="chapter" data-level="10.9.14" data-path="chap9.html"><a href="chap9.html#the-practice-of-generalized-linear-models"><i class="fa fa-check"></i><b>10.9.14</b> The Practice of Generalized Linear Models</a></li>
</ul></li>
<li class="chapter" data-level="10.10" data-path="chap9.html"><a href="chap9.html#generalized-additive-models-gams"><i class="fa fa-check"></i><b>10.10</b> Generalized Additive Models (GAMs)</a>
<ul>
<li class="chapter" data-level="10.10.1" data-path="chap9.html"><a href="chap9.html#principle-6"><i class="fa fa-check"></i><b>10.10.1</b> Principle</a></li>
<li class="chapter" data-level="10.10.2" data-path="chap9.html"><a href="chap9.html#in-practice"><i class="fa fa-check"></i><b>10.10.2</b> In Practice…</a></li>
</ul></li>
<li class="chapter" data-level="10.11" data-path="chap9.html"><a href="chap9.html#practical-case-of-auto-insurance-pricing"><i class="fa fa-check"></i><b>10.11</b> Practical Case of Auto Insurance Pricing</a>
<ul>
<li class="chapter" data-level="10.11.1" data-path="chap9.html"><a href="chap9.html#portfolio-description"><i class="fa fa-check"></i><b>10.11.1</b> Portfolio Description</a></li>
<li class="chapter" data-level="10.11.2" data-path="chap9.html"><a href="chap9.html#variables-describing-claims"><i class="fa fa-check"></i><b>10.11.2</b> Variables Describing Claims</a></li>
<li class="chapter" data-level="10.11.3" data-path="chap9.html"><a href="chap9.html#measuring-exposure-to-risk-the-variable"><i class="fa fa-check"></i><b>10.11.3</b> Measuring Exposure to Risk: the Variable</a></li>
<li class="chapter" data-level="10.11.4" data-path="chap9.html"><a href="chap9.html#characteristics-of-the-policyholder"><i class="fa fa-check"></i><b>10.11.4</b> Characteristics of the Policyholder</a></li>
<li class="chapter" data-level="10.11.5" data-path="chap9.html"><a href="chap9.html#vehicle-characteristics"><i class="fa fa-check"></i><b>10.11.5</b> Vehicle Characteristics</a></li>
<li class="chapter" data-level="10.11.6" data-path="chap9.html"><a href="chap9.html#interaction-between-rating-variables-1"><i class="fa fa-check"></i><b>10.11.6</b> Interaction Between Rating Variables</a></li>
<li class="chapter" data-level="10.11.7" data-path="chap9.html"><a href="chap9.html#initial-screening-of-rating-variables"><i class="fa fa-check"></i><b>10.11.7</b> Initial Screening of Rating Variables</a></li>
<li class="chapter" data-level="10.11.8" data-path="chap9.html"><a href="chap9.html#analysis-of-claim-frequencies"><i class="fa fa-check"></i><b>10.11.8</b> Analysis of Claim Frequencies</a></li>
<li class="chapter" data-level="10.11.9" data-path="chap9.html"><a href="chap9.html#analysis-of-claim-costs"><i class="fa fa-check"></i><b>10.11.9</b> Analysis of Claim Costs</a></li>
</ul></li>
<li class="chapter" data-level="10.12" data-path="chap9.html"><a href="chap9.html#panel-data-rating"><i class="fa fa-check"></i><b>10.12</b> Panel Data Rating</a>
<ul>
<li class="chapter" data-level="10.12.1" data-path="chap9.html"><a href="chap9.html#rating-based-on-panel-data"><i class="fa fa-check"></i><b>10.12.1</b> Rating Based on Panel Data</a></li>
<li class="chapter" data-level="10.12.2" data-path="chap9.html"><a href="chap9.html#notation"><i class="fa fa-check"></i><b>10.12.2</b> Notation</a></li>
<li class="chapter" data-level="10.12.3" data-path="chap9.html"><a href="chap9.html#presentation-of-the-dataset"><i class="fa fa-check"></i><b>10.12.3</b> Presentation of the Dataset</a></li>
<li class="chapter" data-level="10.12.4" data-path="chap9.html"><a href="chap9.html#poisson-regression-assuming-temporal-independence"><i class="fa fa-check"></i><b>10.12.4</b> Poisson Regression Assuming Temporal Independence</a></li>
<li class="chapter" data-level="10.12.5" data-path="chap9.html"><a href="chap9.html#accounting-for-temporal-dependence"><i class="fa fa-check"></i><b>10.12.5</b> Accounting for Temporal Dependence</a></li>
<li class="chapter" data-level="10.12.6" data-path="chap9.html"><a href="chap9.html#modeling-dependence-using-the-working-correlation-matrix"><i class="fa fa-check"></i><b>10.12.6</b> Modeling Dependence Using the “Working Correlation Matrix”</a></li>
<li class="chapter" data-level="10.12.7" data-path="chap9.html"><a href="chap9.html#obtaining-estimates"><i class="fa fa-check"></i><b>10.12.7</b> Obtaining Estimates</a></li>
<li class="chapter" data-level="10.12.8" data-path="chap9.html"><a href="chap9.html#numerical-illustration-1"><i class="fa fa-check"></i><b>10.12.8</b> Numerical Illustration</a></li>
</ul></li>
<li class="chapter" data-level="10.13" data-path="chap9.html"><a href="chap9.html#technical-justifications-for-segmentation"><i class="fa fa-check"></i><b>10.13</b> Technical Justifications for Segmentation</a>
<ul>
<li class="chapter" data-level="10.13.1" data-path="chap9.html"><a href="chap9.html#technical-rate-and-commercial-rate"><i class="fa fa-check"></i><b>10.13.1</b> Technical Rate and Commercial Rate</a></li>
<li class="chapter" data-level="10.13.2" data-path="chap9.html"><a href="chap9.html#segmentation-of-technical-and-commercial-rates"><i class="fa fa-check"></i><b>10.13.2</b> Segmentation of Technical and Commercial Rates</a></li>
<li class="chapter" data-level="10.13.3" data-path="chap9.html"><a href="chap9.html#adverse-selection-and-segmentation"><i class="fa fa-check"></i><b>10.13.3</b> Adverse Selection and Segmentation</a></li>
<li class="chapter" data-level="10.13.4" data-path="chap9.html"><a href="chap9.html#inequity-of-prior-pricing"><i class="fa fa-check"></i><b>10.13.4</b> Inequity of Prior Pricing</a></li>
</ul></li>
<li class="chapter" data-level="10.14" data-path="chap9.html"><a href="chap9.html#bibliographical-notes-7"><i class="fa fa-check"></i><b>10.14</b> Bibliographical Notes</a></li>
<li class="chapter" data-level="10.15" data-path="chap9.html"><a href="chap9.html#exercises-5"><i class="fa fa-check"></i><b>10.15</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="chap10.html"><a href="chap10.html"><i class="fa fa-check"></i><b>11</b> Credibility</a>
<ul>
<li class="chapter" data-level="11.1" data-path="chap10.html"><a href="chap10.html#introduction-7"><i class="fa fa-check"></i><b>11.1</b> Introduction</a></li>
<li class="chapter" data-level="11.2" data-path="chap10.html"><a href="chap10.html#bayesian-credibility"><i class="fa fa-check"></i><b>11.2</b> Bayesian Credibility</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="chap10.html"><a href="chap10.html#introductory-example"><i class="fa fa-check"></i><b>11.2.1</b> Introductory Example</a></li>
<li class="chapter" data-level="11.2.2" data-path="chap10.html"><a href="chap10.html#bayesian-posterior-pricing-model"><i class="fa fa-check"></i><b>11.2.2</b> Bayesian Posterior Pricing Model</a></li>
<li class="chapter" data-level="11.2.3" data-path="chap10.html"><a href="chap10.html#frequentist-bayesian-credibility-without-a-priori-pricing"><i class="fa fa-check"></i><b>11.2.3</b> Frequentist Bayesian Credibility without A Priori Pricing</a></li>
<li class="chapter" data-level="11.2.4" data-path="chap10.html"><a href="chap10.html#bayesian-frequentist-credibility-with-prior-rate-making"><i class="fa fa-check"></i><b>11.2.4</b> Bayesian-Frequentist Credibility with Prior Rate Making</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="chap10.html"><a href="chap10.html#linear-credibility"><i class="fa fa-check"></i><b>11.3</b> Linear Credibility</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="chap10.html"><a href="chap10.html#bühlmann-model"><i class="fa fa-check"></i><b>11.3.1</b> Bühlmann Model</a></li>
<li class="chapter" data-level="11.3.2" data-path="chap10.html"><a href="chap10.html#bühlmann-straub-model"><i class="fa fa-check"></i><b>11.3.2</b> Bühlmann-Straub Model</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="chap10.html"><a href="chap10.html#SecCredTot"><i class="fa fa-check"></i><b>11.4</b> Total Credibility</a></li>
<li class="chapter" data-level="11.5" data-path="chap10.html"><a href="chap10.html#multivariate-credibility"><i class="fa fa-check"></i><b>11.5</b> Multivariate Credibility</a></li>
<li class="chapter" data-level="11.6" data-path="chap10.html"><a href="chap10.html#sec:matcorp"><i class="fa fa-check"></i><b>11.6</b> Modeling</a>
<ul>
<li class="chapter" data-level="11.6.1" data-path="chap10.html"><a href="chap10.html#linear-credibility-premium"><i class="fa fa-check"></i><b>11.6.1</b> Linear Credibility Premium</a></li>
<li class="chapter" data-level="11.6.2" data-path="chap10.html"><a href="chap10.html#an-approach-on-disaggregated-data"><i class="fa fa-check"></i><b>11.6.2</b> An Approach on Disaggregated Data</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="chap10.html"><a href="chap10.html#hierarchical-credibility"><i class="fa fa-check"></i><b>11.7</b> Hierarchical Credibility</a>
<ul>
<li class="chapter" data-level="11.7.1" data-path="chap10.html"><a href="chap10.html#motivation-fleet-insurance"><i class="fa fa-check"></i><b>11.7.1</b> Motivation: Fleet Insurance</a></li>
<li class="chapter" data-level="11.7.2" data-path="chap10.html"><a href="chap10.html#linear-credibility-1"><i class="fa fa-check"></i><b>11.7.2</b> Linear Credibility</a></li>
<li class="chapter" data-level="11.7.3" data-path="chap10.html"><a href="chap10.html#the-case-of-new-vehicles"><i class="fa fa-check"></i><b>11.7.3</b> The Case of New Vehicles</a></li>
<li class="chapter" data-level="11.7.4" data-path="chap10.html"><a href="chap10.html#the-case-of-existing-vehicles"><i class="fa fa-check"></i><b>11.7.4</b> The Case of Existing Vehicles</a></li>
<li class="chapter" data-level="11.7.5" data-path="chap10.html"><a href="chap10.html#open-fleets"><i class="fa fa-check"></i><b>11.7.5</b> Open Fleets</a></li>
</ul></li>
<li class="chapter" data-level="11.8" data-path="chap10.html"><a href="chap10.html#bibliographical-notes-8"><i class="fa fa-check"></i><b>11.8</b> Bibliographical Notes</a></li>
<li class="chapter" data-level="11.9" data-path="chap10.html"><a href="chap10.html#exercises-6"><i class="fa fa-check"></i><b>11.9</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="chap11.html"><a href="chap11.html"><i class="fa fa-check"></i><b>12</b> Bonus-Malus</a>
<ul>
<li class="chapter" data-level="12.1" data-path="chap11.html"><a href="chap11.html#introduction-8"><i class="fa fa-check"></i><b>12.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="chap11.html"><a href="chap11.html#residual-heterogeneity-1"><i class="fa fa-check"></i><b>12.1.1</b> Residual Heterogeneity</a></li>
<li class="chapter" data-level="12.1.2" data-path="chap11.html"><a href="chap11.html#nature-of-dependency"><i class="fa fa-check"></i><b>12.1.2</b> Nature of Dependency</a></li>
<li class="chapter" data-level="12.1.3" data-path="chap11.html"><a href="chap11.html#objectives-of-bonus-malus-systems"><i class="fa fa-check"></i><b>12.1.3</b> Objectives of Bonus-Malus Systems</a></li>
<li class="chapter" data-level="12.1.4" data-path="chap11.html"><a href="chap11.html#nature-of-dependency-1"><i class="fa fa-check"></i><b>12.1.4</b> Nature of Dependency</a></li>
<li class="chapter" data-level="12.1.5" data-path="chap11.html"><a href="chap11.html#objectives-of-bonus-malus-systems-1"><i class="fa fa-check"></i><b>12.1.5</b> Objectives of Bonus-Malus Systems</a></li>
<li class="chapter" data-level="12.1.6" data-path="chap11.html"><a href="chap11.html#nature-of-dependency-2"><i class="fa fa-check"></i><b>12.1.6</b> Nature of Dependency</a></li>
<li class="chapter" data-level="12.1.7" data-path="chap11.html"><a href="chap11.html#bjectives-of-bonus-malus-systems"><i class="fa fa-check"></i><b>12.1.7</b> bjectives of Bonus-Malus Systems</a></li>
<li class="chapter" data-level="12.1.8" data-path="chap11.html"><a href="chap11.html#nature-of-dependency-3"><i class="fa fa-check"></i><b>12.1.8</b> Nature of Dependency</a></li>
<li class="chapter" data-level="12.1.9" data-path="chap11.html"><a href="chap11.html#objectives-of-bonus-malus-systems-2"><i class="fa fa-check"></i><b>12.1.9</b> Objectives of Bonus-Malus Systems</a></li>
<li class="chapter" data-level="12.1.10" data-path="chap11.html"><a href="chap11.html#bonus-thirst"><i class="fa fa-check"></i><b>12.1.10</b> Bonus Thirst</a></li>
<li class="chapter" data-level="12.1.11" data-path="chap11.html"><a href="chap11.html#class-based-systems-and-french-style-systems"><i class="fa fa-check"></i><b>12.1.11</b> Class-Based Systems and “<em>French-Style</em>” Systems</a></li>
<li class="chapter" data-level="12.1.12" data-path="chap11.html"><a href="chap11.html#a-brief-history-of-the-bonus-malus-system-in-france"><i class="fa fa-check"></i><b>12.1.12</b> A Brief History of the Bonus-Malus System in France</a></li>
<li class="chapter" data-level="12.1.13" data-path="chap11.html"><a href="chap11.html#a-brief-history-of-the-bonus-malus-system-in-belgium"><i class="fa fa-check"></i><b>12.1.13</b> A Brief History of the Bonus-Malus System in Belgium</a></li>
<li class="chapter" data-level="12.1.14" data-path="chap11.html"><a href="chap11.html#chapter-outline"><i class="fa fa-check"></i><b>12.1.14</b> Chapter Outline</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="chap11.html"><a href="chap11.html#scales-in-unsegmented-universes"><i class="fa fa-check"></i><b>12.2</b> Scales in Unsegmented Universes}</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="chap11.html"><a href="chap11.html#SecExIntro"><i class="fa fa-check"></i><b>12.2.1</b> Introductory Example: the Good/Bad Driver Model</a></li>
<li class="chapter" data-level="12.2.2" data-path="chap11.html"><a href="chap11.html#scales-and-markov-chains"><i class="fa fa-check"></i><b>12.2.2</b> Scales and Markov Chains</a></li>
<li class="chapter" data-level="12.2.3" data-path="chap11.html"><a href="chap11.html#norbergs-method"><i class="fa fa-check"></i><b>12.2.3</b> Norberg’s Method</a></li>
<li class="chapter" data-level="12.2.4" data-path="chap11.html"><a href="chap11.html#gilde-and-sundts-method"><i class="fa fa-check"></i><b>12.2.4</b> Gilde and Sundt’s Method</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="chap11.html"><a href="chap11.html#segmented-universe-scales"><i class="fa fa-check"></i><b>12.3</b> Segmented Universe Scales</a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="chap11.html"><a href="chap11.html#introductory-example-1"><i class="fa fa-check"></i><b>12.3.1</b> Introductory Example</a></li>
<li class="chapter" data-level="12.3.2" data-path="chap11.html"><a href="chap11.html#modeling-claims-frequency-in-segmented-universe"><i class="fa fa-check"></i><b>12.3.2</b> Modeling Claims Frequency in Segmented Universe</a></li>
<li class="chapter" data-level="12.3.3" data-path="chap11.html"><a href="chap11.html#severity-of-posterior-adjustments-depending-on-the-degree-of-a-priori-differentiation"><i class="fa fa-check"></i><b>12.3.3</b> Severity of Posterior Adjustments Depending on the Degree of A priori Differentiation</a></li>
<li class="chapter" data-level="12.3.4" data-path="chap11.html"><a href="chap11.html#norberg-method-in-a-segmented-universe"><i class="fa fa-check"></i><b>12.3.4</b> Norberg Method in a Segmented Universe</a></li>
<li class="chapter" data-level="12.3.5" data-path="chap11.html"><a href="chap11.html#interaction-between-posterior-adjustments-induced-by-the-scale-and-prior-pricing"><i class="fa fa-check"></i><b>12.3.5</b> Interaction between Posterior Adjustments Induced by the Scale and Prior Pricing</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="chap11.html"><a href="chap11.html#numerical-illustrations"><i class="fa fa-check"></i><b>12.4</b> Numerical Illustrations</a>
<ul>
<li class="chapter" data-level="12.4.1" data-path="chap11.html"><a href="chap11.html#prior-pricing"><i class="fa fa-check"></i><b>12.4.1</b> Prior Pricing</a></li>
<li class="chapter" data-level="12.4.2" data-path="chap11.html"><a href="chap11.html#scale--1top"><i class="fa fa-check"></i><b>12.4.2</b> Scale “-1/top”</a></li>
<li class="chapter" data-level="12.4.3" data-path="chap11.html"><a href="chap11.html#scale"><i class="fa fa-check"></i><b>12.4.3</b> “-1/+2” Scale</a></li>
<li class="chapter" data-level="12.4.4" data-path="chap11.html"><a href="chap11.html#scale-1"><i class="fa fa-check"></i><b>12.4.4</b> “-1/+4” Scale</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="chap11.html"><a href="chap11.html#performance-of-bonus-malus-scales"><i class="fa fa-check"></i><b>12.5</b> Performance of Bonus-Malus Scales</a>
<ul>
<li class="chapter" data-level="12.5.1" data-path="chap11.html"><a href="chap11.html#the-relative-stationary-average-level-rsal"><i class="fa fa-check"></i><b>12.5.1</b> The Relative Stationary Average Level (RSAL)</a></li>
<li class="chapter" data-level="12.5.2" data-path="chap11.html"><a href="chap11.html#the-relative-stationary-average-premium-rsap"><i class="fa fa-check"></i><b>12.5.2</b> The Relative Stationary Average Premium (RSAP)</a></li>
<li class="chapter" data-level="12.5.3" data-path="chap11.html"><a href="chap11.html#the-coefficient-of-variation-of-premiums"><i class="fa fa-check"></i><b>12.5.3</b> The Coefficient of Variation of Premiums</a></li>
<li class="chapter" data-level="12.5.4" data-path="chap11.html"><a href="chap11.html#loimarantas-efficiency"><i class="fa fa-check"></i><b>12.5.4</b> Loimaranta’s Efficiency</a></li>
<li class="chapter" data-level="12.5.5" data-path="chap11.html"><a href="chap11.html#lemaires-efficiency"><i class="fa fa-check"></i><b>12.5.5</b> Lemaire’s Efficiency</a></li>
<li class="chapter" data-level="12.5.6" data-path="chap11.html"><a href="chap11.html#lemaires-efficiency-1"><i class="fa fa-check"></i><b>12.5.6</b> Lemaire’s Efficiency</a></li>
<li class="chapter" data-level="12.5.7" data-path="chap11.html"><a href="chap11.html#optimal-average-retention"><i class="fa fa-check"></i><b>12.5.7</b> Optimal Average Retention</a></li>
</ul></li>
<li class="chapter" data-level="12.6" data-path="chap11.html"><a href="chap11.html#bibliographical-notes-9"><i class="fa fa-check"></i><b>12.6</b> Bibliographical Notes</a></li>
<li class="chapter" data-level="12.7" data-path="chap11.html"><a href="chap11.html#exercises-7"><i class="fa fa-check"></i><b>12.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="chap12.html"><a href="chap12.html"><i class="fa fa-check"></i><b>13</b> Economics of Insurance</a>
<ul>
<li class="chapter" data-level="13.1" data-path="chap12.html"><a href="chap12.html#introduction-9"><i class="fa fa-check"></i><b>13.1</b> Introduction</a></li>
<li class="chapter" data-level="13.2" data-path="chap12.html"><a href="chap12.html#decision-under-uncertainty"><i class="fa fa-check"></i><b>13.2</b> Decision under Uncertainty</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="chap12.html"><a href="chap12.html#the-von-neumann-and-morgenstern-expected-utility-model"><i class="fa fa-check"></i><b>13.2.1</b> The von Neumann and Morgenstern Expected Utility Model</a></li>
<li class="chapter" data-level="13.2.2" data-path="chap12.html"><a href="chap12.html#on-the-notion-of-subjectivity-in-probabilities"><i class="fa fa-check"></i><b>13.2.2</b> On the Notion of Subjectivity in Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="chap12.html"><a href="chap12.html#the-role-of-time-and-prudence"><i class="fa fa-check"></i><b>13.3</b> The Role of Time and Prudence</a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="chap12.html"><a href="chap12.html#limits-of-the-expected-utility-model-in-risk-allais-paradox-and-the-certainty-effect"><i class="fa fa-check"></i><b>13.3.1</b> Limits of the Expected Utility Model in Risk: Allais’ Paradox and the Certainty Effect</a></li>
<li class="chapter" data-level="13.3.2" data-path="chap12.html"><a href="chap12.html#the-certainty-effect-and-non-linear-probability-processing"><i class="fa fa-check"></i><b>13.3.2</b> The Certainty Effect and Non-Linear Probability Processing</a></li>
<li class="chapter" data-level="13.3.3" data-path="chap12.html"><a href="chap12.html#limits-of-the-expected-utility-model-in-non-probabilistic-uncertainty-the-ellsberg-paradox-and-the-concept-of-ambiguity"><i class="fa fa-check"></i><b>13.3.3</b> Limits of the Expected Utility Model in Non-Probabilistic Uncertainty: The Ellsberg Paradox and the Concept of Ambiguity</a></li>
<li class="chapter" data-level="13.3.4" data-path="chap12.html"><a href="chap12.html#section-choquet"><i class="fa fa-check"></i><b>13.3.4</b> Extension of the Notion of Expectation: Choquet Integral</a></li>
<li class="chapter" data-level="13.3.5" data-path="chap12.html"><a href="chap12.html#generalization-of-utility-expectation-models-rank-dependent-models"><i class="fa fa-check"></i><b>13.3.5</b> Generalization of Utility Expectation Models: Rank-Dependent Models</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="chap12.html"><a href="chap12.html#risk-measure-and-risk-aversion"><i class="fa fa-check"></i><b>13.4</b> Risk Measure and Risk Aversion</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="chap12.html"><a href="chap12.html#risk-aversion-and-risk-price"><i class="fa fa-check"></i><b>13.4.1</b> Risk Aversion and Risk Price</a></li>
<li class="chapter" data-level="13.4.2" data-path="chap12.html"><a href="chap12.html#measures-of-prudence-and-aversion"><i class="fa fa-check"></i><b>13.4.2</b> Measures of Prudence and Aversion</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="chap12.html"><a href="chap12.html#supply-demand-section"><i class="fa fa-check"></i><b>13.5</b> Insurance Supply and Demand</a>
<ul>
<li class="chapter" data-level="13.5.1" data-path="chap12.html"><a href="chap12.html#normalization-of-behaviors"><i class="fa fa-check"></i><b>13.5.1</b> Normalization of Behaviors</a></li>
<li class="chapter" data-level="13.5.2" data-path="chap12.html"><a href="chap12.html#insurance-demand"><i class="fa fa-check"></i><b>13.5.2</b> Insurance Demand</a></li>
<li class="chapter" data-level="13.5.3" data-path="chap12.html"><a href="chap12.html#the-mossin-model"><i class="fa fa-check"></i><b>13.5.3</b> The Mossin Model</a></li>
<li class="chapter" data-level="13.5.4" data-path="chap12.html"><a href="chap12.html#the-general-model-of-insurance-demand"><i class="fa fa-check"></i><b>13.5.4</b> The General Model of Insurance Demand</a></li>
<li class="chapter" data-level="13.5.5" data-path="chap12.html"><a href="chap12.html#special-case-of-proportional-insurance"><i class="fa fa-check"></i><b>13.5.5</b> Special Case of Proportional Insurance</a></li>
<li class="chapter" data-level="13.5.6" data-path="chap12.html"><a href="chap12.html#special-case-of-insurance-with-mandatory-deductible"><i class="fa fa-check"></i><b>13.5.6</b> Special Case of Insurance with Mandatory Deductible</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="chap12.html"><a href="chap12.html#information-asymmetry-and-adverse-selection"><i class="fa fa-check"></i><b>13.6</b> Information Asymmetry and Adverse Selection</a>
<ul>
<li class="chapter" data-level="13.6.1" data-path="chap12.html"><a href="chap12.html#incomplete-information-1"><i class="fa fa-check"></i><b>13.6.1</b> Incomplete Information</a></li>
<li class="chapter" data-level="13.6.2" data-path="chap12.html"><a href="chap12.html#adverse-selection-moral-hazard-and-signals"><i class="fa fa-check"></i><b>13.6.2</b> Adverse Selection, Moral Hazard, and Signals</a></li>
<li class="chapter" data-level="13.6.3" data-path="chap12.html"><a href="chap12.html#the-rothschild-stiglitz-equilibrium-model"><i class="fa fa-check"></i><b>13.6.3</b> The Rothschild &amp; Stiglitz Equilibrium Model</a></li>
<li class="chapter" data-level="13.6.4" data-path="chap12.html"><a href="chap12.html#study-of-adverse-selection-mechanisms"><i class="fa fa-check"></i><b>13.6.4</b> Study of Adverse Selection Mechanisms</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="chap12.html"><a href="chap12.html#coverage-of-multiple-risks"><i class="fa fa-check"></i><b>13.7</b> Coverage of Multiple Risks</a>
<ul>
<li class="chapter" data-level="13.7.1" data-path="chap12.html"><a href="chap12.html#context-3"><i class="fa fa-check"></i><b>13.7.1</b> Context</a></li>
<li class="chapter" data-level="13.7.2" data-path="chap12.html"><a href="chap12.html#insurable-risk-and-non-insurable-risk"><i class="fa fa-check"></i><b>13.7.2</b> Insurable Risk and Non-Insurable Risk</a></li>
<li class="chapter" data-level="13.7.3" data-path="chap12.html"><a href="chap12.html#presence-of-multiple-insurable-risks"><i class="fa fa-check"></i><b>13.7.3</b> Presence of Multiple Insurable Risks</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="chap13.html"><a href="chap13.html"><i class="fa fa-check"></i><b>14</b> Claims Reserving</a>
<ul>
<li class="chapter" data-level="14.1" data-path="chap13.html"><a href="chap13.html#introduction-10"><i class="fa fa-check"></i><b>14.1</b> Introduction</a></li>
<li class="chapter" data-level="14.2" data-path="chap13.html"><a href="chap13.html#notation-and-motivation"><i class="fa fa-check"></i><b>14.2</b> Notation and Motivation</a>
<ul>
<li class="chapter" data-level="14.2.1" data-path="chap13.html"><a href="chap13.html#the-dynamics-of-claim-life"><i class="fa fa-check"></i><b>14.2.1</b> The Dynamics of Claim Life</a></li>
<li class="chapter" data-level="14.2.2" data-path="chap13.html"><a href="chap13.html#the-dynamics-of-claims"><i class="fa fa-check"></i><b>14.2.2</b> The Dynamics of Claims</a></li>
<li class="chapter" data-level="14.2.3" data-path="chap13.html"><a href="chap13.html#time-lags-before-reporting"><i class="fa fa-check"></i><b>14.2.3</b> Time Lags Before Reporting</a></li>
<li class="chapter" data-level="14.2.4" data-path="chap13.html"><a href="chap13.html#run-off-triangles"><i class="fa fa-check"></i><b>14.2.4</b> Run-off Triangles</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="chap13.html"><a href="chap13.html#section-chain-ladder"><i class="fa fa-check"></i><b>14.3</b> Deterministic Methods</a>
<ul>
<li class="chapter" data-level="14.3.1" data-path="chap13.html"><a href="chap13.html#the-chain-ladder-method"><i class="fa fa-check"></i><b>14.3.1</b> The Chain Ladder Method</a></li>
<li class="chapter" data-level="14.3.2" data-path="chap13.html"><a href="chap13.html#link-ratios"><i class="fa fa-check"></i><b>14.3.2</b> Link Ratios</a></li>
<li class="chapter" data-level="14.3.3" data-path="chap13.html"><a href="chap13.html#boni-mali"><i class="fa fa-check"></i><b>14.3.3</b> Bonuses and Penalties, or Updating Estimates</a></li>
<li class="chapter" data-level="14.3.4" data-path="chap13.html"><a href="chap13.html#critiques-of-the-chain-ladder-method"><i class="fa fa-check"></i><b>14.3.4</b> Critiques of the Chain Ladder Method</a></li>
<li class="chapter" data-level="14.3.5" data-path="chap13.html"><a href="chap13.html#variations-on-the-chain-ladder-method"><i class="fa fa-check"></i><b>14.3.5</b> Variations on the Chain-Ladder Method</a></li>
<li class="chapter" data-level="14.3.6" data-path="chap13.html"><a href="chap13.html#projected-case-estimate-method"><i class="fa fa-check"></i><b>14.3.6</b> Projected Case Estimate Method</a></li>
<li class="chapter" data-level="14.3.7" data-path="chap13.html"><a href="chap13.html#de-vylders-least-squares-method"><i class="fa fa-check"></i><b>14.3.7</b> De Vylder’s Least Squares Method</a></li>
<li class="chapter" data-level="14.3.8" data-path="chap13.html"><a href="chap13.html#taylors-separation-method"><i class="fa fa-check"></i><b>14.3.8</b> Taylor’s Separation Method</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="chap13.html"><a href="chap13.html#stochastic-methods"><i class="fa fa-check"></i><b>14.4</b> Stochastic Methods</a>
<ul>
<li class="chapter" data-level="14.4.1" data-path="chap13.html"><a href="chap13.html#Section-Mack"><i class="fa fa-check"></i><b>14.4.1</b> The Mack Model</a></li>
<li class="chapter" data-level="14.4.2" data-path="chap13.html"><a href="chap13.html#the-christophides-log-linear-model"><i class="fa fa-check"></i><b>14.4.2</b> The Christophides Log-Linear Model</a></li>
<li class="chapter" data-level="14.4.3" data-path="chap13.html"><a href="chap13.html#the-renshaw-and-verrall-poisson-model"><i class="fa fa-check"></i><b>14.4.3</b> The Renshaw and Verrall Poisson Model</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="chap13.html"><a href="chap13.html#glm-and-reserving"><i class="fa fa-check"></i><b>14.5</b> GLM and Reserving</a>
<ul>
<li class="chapter" data-level="14.5.1" data-path="chap13.html"><a href="chap13.html#principle-10"><i class="fa fa-check"></i><b>14.5.1</b> Principle</a></li>
<li class="chapter" data-level="14.5.2" data-path="chap13.html"><a href="chap13.html#tweedie-models"><i class="fa fa-check"></i><b>14.5.2</b> Tweedie Models</a></li>
<li class="chapter" data-level="14.5.3" data-path="chap13.html"><a href="chap13.html#model-factors"><i class="fa fa-check"></i><b>14.5.3</b> Which Factorial Model to Choose?</a></li>
</ul></li>
<li class="chapter" data-level="14.6" data-path="chap13.html"><a href="chap13.html#choosing-the-reserving-method"><i class="fa fa-check"></i><b>14.6</b> Choosing the Reserving Method</a></li>
<li class="chapter" data-level="14.7" data-path="chap13.html"><a href="chap13.html#practical-case-studies"><i class="fa fa-check"></i><b>14.7</b> Practical Case Studies</a>
<ul>
<li class="chapter" data-level="14.7.1" data-path="chap13.html"><a href="chap13.html#automobile-insurance"><i class="fa fa-check"></i><b>14.7.1</b> Automobile Insurance</a></li>
<li class="chapter" data-level="14.7.2" data-path="chap13.html"><a href="chap13.html#medical-malpractice"><i class="fa fa-check"></i><b>14.7.2</b> Medical Malpractice</a></li>
</ul></li>
<li class="chapter" data-level="14.8" data-path="chap13.html"><a href="chap13.html#bibliographical-notes-10"><i class="fa fa-check"></i><b>14.8</b> Bibliographical Notes</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="chap14.html"><a href="chap14.html"><i class="fa fa-check"></i><b>15</b> Large Risks</a>
<ul>
<li class="chapter" data-level="15.1" data-path="chap14.html"><a href="chap14.html#introduction-11"><i class="fa fa-check"></i><b>15.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="15.1.1" data-path="chap14.html"><a href="chap14.html#the-concept-of-catastrophe"><i class="fa fa-check"></i><b>15.1.1</b> The Concept of Catastrophe</a></li>
<li class="chapter" data-level="15.1.2" data-path="chap14.html"><a href="chap14.html#why-manage-extreme-events-ex-ante"><i class="fa fa-check"></i><b>15.1.2</b> Why Manage Extreme Events <em>Ex Ante</em>?</a></li>
<li class="chapter" data-level="15.1.3" data-path="chap14.html"><a href="chap14.html#what-types-of-catastrophes"><i class="fa fa-check"></i><b>15.1.3</b> What Types of Catastrophes?</a></li>
<li class="chapter" data-level="15.1.4" data-path="chap14.html"><a href="chap14.html#data-presentation"><i class="fa fa-check"></i><b>15.1.4</b> Data Presentation</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="chap14.html"><a href="chap14.html#limit-law-of-maxima"><i class="fa fa-check"></i><b>15.2</b> Limit Law of Maxima</a>
<ul>
<li class="chapter" data-level="15.2.1" data-path="chap14.html"><a href="chap14.html#behavior-of-the-maximum-in-large-samples"><i class="fa fa-check"></i><b>15.2.1</b> Behavior of the Maximum in Large Samples</a></li>
<li class="chapter" data-level="15.2.2" data-path="chap14.html"><a href="chap14.html#large-deviations"><i class="fa fa-check"></i><b>15.2.2</b> Large Deviations</a></li>
<li class="chapter" data-level="15.2.3" data-path="chap14.html"><a href="chap14.html#estimation-of-the-maximum-distribution"><i class="fa fa-check"></i><b>15.2.3</b> Estimation of the Maximum Distribution</a></li>
<li class="chapter" data-level="15.2.4" data-path="chap14.html"><a href="chap14.html#the-n-th-largest-value"><i class="fa fa-check"></i><b>15.2.4</b> The <span class="math inline">\(n\)</span>-th Largest Value</a></li>
<li class="chapter" data-level="15.2.5" data-path="chap14.html"><a href="chap14.html#random-frequency-of-claims"><i class="fa fa-check"></i><b>15.2.5</b> Random Frequency of Claims</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="chap14.html"><a href="chap14.html#tail-thickness-of-distributions"><i class="fa fa-check"></i><b>15.3</b> Tail Thickness of Distributions</a>
<ul>
<li class="chapter" data-level="15.3.1" data-path="chap14.html"><a href="chap14.html#the-concept-of-regular-variation"><i class="fa fa-check"></i><b>15.3.1</b> The Concept of Regular Variation</a></li>
<li class="chapter" data-level="15.3.2" data-path="chap14.html"><a href="chap14.html#regular-variation-and-the-max-domain-of-attraction-of-the-fréchet-distribution"><i class="fa fa-check"></i><b>15.3.2</b> Regular Variation and the Max-Domain of Attraction of the Fréchet Distribution</a></li>
<li class="chapter" data-level="15.3.3" data-path="chap14.html"><a href="chap14.html#sum-maximum-and-subexponential-distribution"><i class="fa fa-check"></i><b>15.3.3</b> Sum, Maximum, and Subexponential Distribution</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="chap14.html"><a href="chap14.html#study-of-the-excess-distribution"><i class="fa fa-check"></i><b>15.4</b> Study of the Excess Distribution</a>
<ul>
<li class="chapter" data-level="15.4.1" data-path="chap14.html"><a href="chap14.html#generalized-pareto-distribution"><i class="fa fa-check"></i><b>15.4.1</b> Generalized Pareto Distribution</a></li>
<li class="chapter" data-level="15.4.2" data-path="chap14.html"><a href="chap14.html#pickands-balkema-de-haan-theorem"><i class="fa fa-check"></i><b>15.4.2</b> Pickands-Balkema-de Haan Theorem</a></li>
<li class="chapter" data-level="15.4.3" data-path="chap14.html"><a href="chap14.html#number-of-exceedances"><i class="fa fa-check"></i><b>15.4.3</b> Number of Exceedances</a></li>
<li class="chapter" data-level="15.4.4" data-path="chap14.html"><a href="chap14.html#sample-size-with-a-poisson-distribution"><i class="fa fa-check"></i><b>15.4.4</b> Sample Size with a Poisson Distribution</a></li>
<li class="chapter" data-level="15.4.5" data-path="chap14.html"><a href="chap14.html#sec-comportement-ex"><i class="fa fa-check"></i><b>15.4.5</b> Examples of Tail Behavior</a></li>
<li class="chapter" data-level="15.4.6" data-path="chap14.html"><a href="chap14.html#heavy-tailed-distributions-the-max-domain-of-attraction-of-the-fréchet-distribution"><i class="fa fa-check"></i><b>15.4.6</b> Heavy-Tailed Distributions: the Max-Domain of Attraction of the Fréchet Distribution</a></li>
<li class="chapter" data-level="15.4.7" data-path="chap14.html"><a href="chap14.html#thin-tailed-distributions-the-max-domain-of-attraction-of-the-gumbel-distribution"><i class="fa fa-check"></i><b>15.4.7</b> Thin-Tailed Distributions: the Max-Domain of Attraction of the Gumbel Distribution</a></li>
</ul></li>
<li class="chapter" data-level="15.5" data-path="chap14.html"><a href="chap14.html#estimation-of-extreme-quantiles"><i class="fa fa-check"></i><b>15.5</b> Estimation of Extreme Quantiles</a>
<ul>
<li class="chapter" data-level="15.5.1" data-path="chap14.html"><a href="chap14.html#estimation-of-the-tail-index"><i class="fa fa-check"></i><b>15.5.1</b> Estimation of the Tail Index</a></li>
<li class="chapter" data-level="15.5.2" data-path="chap14.html"><a href="chap14.html#time-and-return-period"><i class="fa fa-check"></i><b>15.5.2</b> Time and Return Period</a></li>
<li class="chapter" data-level="15.5.3" data-path="chap14.html"><a href="chap14.html#gpd-approximation-for-var"><i class="fa fa-check"></i><b>15.5.3</b> GPD Approximation for VaR</a></li>
<li class="chapter" data-level="15.5.4" data-path="chap14.html"><a href="chap14.html#hill-estimator-for-var"><i class="fa fa-check"></i><b>15.5.4</b> Hill Estimator for VaR</a></li>
<li class="chapter" data-level="15.5.5" data-path="chap14.html"><a href="chap14.html#cumulative-risk-extremes-and-compound-distributions"><i class="fa fa-check"></i><b>15.5.5</b> Cumulative Risk, Extremes, and Compound Distributions</a></li>
</ul></li>
<li class="chapter" data-level="15.6" data-path="chap14.html"><a href="chap14.html#multivariate-extreme-value-theory"><i class="fa fa-check"></i><b>15.6</b> Multivariate Extreme Value Theory</a>
<ul>
<li class="chapter" data-level="15.6.1" data-path="chap14.html"><a href="chap14.html#componentwise-maxima"><i class="fa fa-check"></i><b>15.6.1</b> Componentwise Maxima</a></li>
<li class="chapter" data-level="15.6.2" data-path="chap14.html"><a href="chap14.html#expression-of-limit-distributions"><i class="fa fa-check"></i><b>15.6.2</b> Expression of Limit Distributions</a></li>
<li class="chapter" data-level="15.6.3" data-path="chap14.html"><a href="chap14.html#representation-of-the-dependence-function-a"><i class="fa fa-check"></i><b>15.6.3</b> Representation of the Dependence Function <span class="math inline">\(A\)</span></a></li>
<li class="chapter" data-level="15.6.4" data-path="chap14.html"><a href="chap14.html#estimation-of-the-dependence-function"><i class="fa fa-check"></i><b>15.6.4</b> Estimation of the Dependence Function</a></li>
<li class="chapter" data-level="15.6.5" data-path="chap14.html"><a href="chap14.html#copulas-of-multivariate-extreme-value-distributions"><i class="fa fa-check"></i><b>15.6.5</b> Copulas of Multivariate Extreme Value Distributions</a></li>
<li class="chapter" data-level="15.6.6" data-path="chap14.html"><a href="chap14.html#correlation-coefficient"><i class="fa fa-check"></i><b>15.6.6</b> Correlation Coefficient</a></li>
<li class="chapter" data-level="15.6.7" data-path="chap14.html"><a href="chap14.html#comparison-of-dependence-1"><i class="fa fa-check"></i><b>15.6.7</b> Comparison of Dependence</a></li>
<li class="chapter" data-level="15.6.8" data-path="chap14.html"><a href="chap14.html#tail-dependence-coefficient"><i class="fa fa-check"></i><b>15.6.8</b> Tail Dependence Coefficient</a></li>
<li class="chapter" data-level="15.6.9" data-path="chap14.html"><a href="chap14.html#application-in-reinsurance-cost-vs.-expenses"><i class="fa fa-check"></i><b>15.6.9</b> Application in Reinsurance, Cost vs. Expenses</a></li>
</ul></li>
<li class="chapter" data-level="15.7" data-path="chap14.html"><a href="chap14.html#standard-reinsurance-treaties"><i class="fa fa-check"></i><b>15.7</b> Standard Reinsurance Treaties</a>
<ul>
<li class="chapter" data-level="15.7.1" data-path="chap14.html"><a href="chap14.html#reinsurance"><i class="fa fa-check"></i><b>15.7.1</b> Reinsurance</a></li>
<li class="chapter" data-level="15.7.2" data-path="chap14.html"><a href="chap14.html#proportional-reinsurance"><i class="fa fa-check"></i><b>15.7.2</b> Proportional Reinsurance</a></li>
<li class="chapter" data-level="15.7.3" data-path="chap14.html"><a href="chap14.html#Reass-NP"><i class="fa fa-check"></i><b>15.7.3</b> Non-Proportional Reinsurance</a></li>
<li class="chapter" data-level="15.7.4" data-path="chap14.html"><a href="chap14.html#pricing-of-non-proportional-treaties"><i class="fa fa-check"></i><b>15.7.4</b> Pricing of Non-Proportional Treaties</a></li>
<li class="chapter" data-level="15.7.5" data-path="chap14.html"><a href="chap14.html#quantile-quantile-q-q-plots"><i class="fa fa-check"></i><b>15.7.5</b> Quantile-Quantile (Q-Q) Plots</a></li>
<li class="chapter" data-level="15.7.6" data-path="chap14.html"><a href="chap14.html#mean-excess-function"><i class="fa fa-check"></i><b>15.7.6</b> Mean Excess Function</a></li>
<li class="chapter" data-level="15.7.7" data-path="chap14.html"><a href="chap14.html#the-lorenz-curve"><i class="fa fa-check"></i><b>15.7.7</b> The Lorenz Curve</a></li>
<li class="chapter" data-level="15.7.8" data-path="chap14.html"><a href="chap14.html#approximation-of-pure-premium"><i class="fa fa-check"></i><b>15.7.8</b> Approximation of Pure Premium</a></li>
<li class="chapter" data-level="15.7.9" data-path="chap14.html"><a href="chap14.html#approximation-of-a-wang-premium"><i class="fa fa-check"></i><b>15.7.9</b> Approximation of a Wang Premium</a></li>
<li class="chapter" data-level="15.7.10" data-path="chap14.html"><a href="chap14.html#estimation-of-tvar"><i class="fa fa-check"></i><b>15.7.10</b> Estimation of TVaR</a></li>
<li class="chapter" data-level="15.7.11" data-path="chap14.html"><a href="chap14.html#index-based-coverage-and-securitization"><i class="fa fa-check"></i><b>15.7.11</b> Index-Based Coverage and Securitization</a></li>
</ul></li>
<li class="chapter" data-level="15.8" data-path="chap14.html"><a href="chap14.html#insolvency-and-large-risks"><i class="fa fa-check"></i><b>15.8</b> Insolvency and Large Risks</a>
<ul>
<li class="chapter" data-level="15.8.1" data-path="chap14.html"><a href="chap14.html#probability-of-ruin-in-the-presence-of-large-losses-von-bahrs-approximation"><i class="fa fa-check"></i><b>15.8.1</b> Probability of Ruin in the Presence of Large Losses: von Bahr’s Approximation</a></li>
<li class="chapter" data-level="15.8.2" data-path="chap14.html"><a href="chap14.html#using-the-pollaczeck-khinchine-beekman-formula"><i class="fa fa-check"></i><b>15.8.2</b> Using the Pollaczeck-Khinchine-Beekman Formula</a></li>
</ul></li>
<li class="chapter" data-level="15.9" data-path="chap14.html"><a href="chap14.html#bibliographical-notes-11"><i class="fa fa-check"></i><b>15.9</b> Bibliographical Notes</a></li>
<li class="chapter" data-level="15.10" data-path="chap14.html"><a href="chap14.html#exercises-8"><i class="fa fa-check"></i><b>15.10</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="chap15.html"><a href="chap15.html"><i class="fa fa-check"></i><b>16</b> Monte Carlo</a>
<ul>
<li class="chapter" data-level="16.1" data-path="chap15.html"><a href="chap15.html#introduction-12"><i class="fa fa-check"></i><b>16.1</b> Introduction</a></li>
<li class="chapter" data-level="16.2" data-path="chap15.html"><a href="chap15.html#general-principles"><i class="fa fa-check"></i><b>16.2</b> General Principles</a>
<ul>
<li class="chapter" data-level="16.2.1" data-path="chap15.html"><a href="chap15.html#pseudo-random-numbers"><i class="fa fa-check"></i><b>16.2.1</b> Pseudo-Random Numbers</a></li>
<li class="chapter" data-level="16.2.2" data-path="chap15.html"><a href="chap15.html#Section-inversion"><i class="fa fa-check"></i><b>16.2.2</b> The Inversion Method</a></li>
<li class="chapter" data-level="16.2.3" data-path="chap15.html"><a href="chap15.html#Section-rejet"><i class="fa fa-check"></i><b>16.2.3</b> Rejection Method</a></li>
<li class="chapter" data-level="16.2.4" data-path="chap15.html"><a href="chap15.html#using-mixture-distributions"><i class="fa fa-check"></i><b>16.2.4</b> Using Mixture Distributions</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="chap15.html"><a href="chap15.html#bootstrap-resampling"><i class="fa fa-check"></i><b>16.3</b> Bootstrap Resampling</a>
<ul>
<li class="chapter" data-level="16.3.1" data-path="chap15.html"><a href="chap15.html#principles"><i class="fa fa-check"></i><b>16.3.1</b> Principles</a></li>
<li class="chapter" data-level="16.3.2" data-path="chap15.html"><a href="chap15.html#bootstrap-confidence-intervals"><i class="fa fa-check"></i><b>16.3.2</b> Bootstrap Confidence Intervals</a></li>
<li class="chapter" data-level="16.3.3" data-path="chap15.html"><a href="chap15.html#bootstrap-and-quantiles"><i class="fa fa-check"></i><b>16.3.3</b> Bootstrap and Quantiles</a></li>
<li class="chapter" data-level="16.3.4" data-path="chap15.html"><a href="chap15.html#bootstrap-and-correlated-samples"><i class="fa fa-check"></i><b>16.3.4</b> Bootstrap and Correlated Samples</a></li>
<li class="chapter" data-level="16.3.5" data-path="chap15.html"><a href="chap15.html#application-to-loss-reserving"><i class="fa fa-check"></i><b>16.3.5</b> Application to Loss Reserving</a></li>
<li class="chapter" data-level="16.3.6" data-path="chap15.html"><a href="chap15.html#bootstrap-and-correlated-samples-1"><i class="fa fa-check"></i><b>16.3.6</b> Bootstrap and Correlated Samples</a></li>
<li class="chapter" data-level="16.3.7" data-path="chap15.html"><a href="chap15.html#application-to-loss-reserving-1"><i class="fa fa-check"></i><b>16.3.7</b> Application to Loss Reserving</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="chap15.html"><a href="chap15.html#simulation-of-univariate-common-probability-distributions"><i class="fa fa-check"></i><b>16.4</b> Simulation of Univariate Common Probability Distributions</a>
<ul>
<li class="chapter" data-level="16.4.1" data-path="chap15.html"><a href="chap15.html#uniform-distribution"><i class="fa fa-check"></i><b>16.4.1</b> Uniform Distribution</a></li>
<li class="chapter" data-level="16.4.2" data-path="chap15.html"><a href="chap15.html#normal-gaussian-distribution"><i class="fa fa-check"></i><b>16.4.2</b> Normal (Gaussian) Distribution</a></li>
<li class="chapter" data-level="16.4.3" data-path="chap15.html"><a href="chap15.html#lognormal-distribution"><i class="fa fa-check"></i><b>16.4.3</b> Lognormal Distribution</a></li>
<li class="chapter" data-level="16.4.4" data-path="chap15.html"><a href="chap15.html#gamma-distribution-1"><i class="fa fa-check"></i><b>16.4.4</b> Gamma Distribution</a></li>
<li class="chapter" data-level="16.4.5" data-path="chap15.html"><a href="chap15.html#beta-distribution-1"><i class="fa fa-check"></i><b>16.4.5</b> Beta Distribution</a></li>
<li class="chapter" data-level="16.4.6" data-path="chap15.html"><a href="chap15.html#poisson-distribution"><i class="fa fa-check"></i><b>16.4.6</b> Poisson Distribution</a></li>
<li class="chapter" data-level="16.4.7" data-path="chap15.html"><a href="chap15.html#poisson-distribution-1"><i class="fa fa-check"></i><b>16.4.7</b> Poisson Distribution</a></li>
<li class="chapter" data-level="16.4.8" data-path="chap15.html"><a href="chap15.html#geometric-distribution"><i class="fa fa-check"></i><b>16.4.8</b> Geometric Distribution</a></li>
<li class="chapter" data-level="16.4.9" data-path="chap15.html"><a href="chap15.html#binomial-distribution"><i class="fa fa-check"></i><b>16.4.9</b> Binomial Distribution</a></li>
<li class="chapter" data-level="16.4.10" data-path="chap15.html"><a href="chap15.html#negative-binomial-distribution"><i class="fa fa-check"></i><b>16.4.10</b> Negative Binomial Distribution</a></li>
<li class="chapter" data-level="16.4.11" data-path="chap15.html"><a href="chap15.html#negative-binomial-distribution-1"><i class="fa fa-check"></i><b>16.4.11</b> Negative Binomial Distribution</a></li>
<li class="chapter" data-level="16.4.12" data-path="chap15.html"><a href="chap15.html#elliptical-distributions"><i class="fa fa-check"></i><b>16.4.12</b> Elliptical Distributions</a></li>
<li class="chapter" data-level="16.4.13" data-path="chap15.html"><a href="chap15.html#using-copulas"><i class="fa fa-check"></i><b>16.4.13</b> Using Copulas</a></li>
</ul></li>
<li class="chapter" data-level="16.5" data-path="chap15.html"><a href="chap15.html#simulation-of-stochastic-processes"><i class="fa fa-check"></i><b>16.5</b> Simulation of Stochastic Processes</a>
<ul>
<li class="chapter" data-level="16.5.1" data-path="chap15.html"><a href="chap15.html#simulation-of-markov-chains"><i class="fa fa-check"></i><b>16.5.1</b> Simulation of Markov Chains</a></li>
<li class="chapter" data-level="16.5.2" data-path="chap15.html"><a href="chap15.html#simulation-of-a-poisson-process"><i class="fa fa-check"></i><b>16.5.2</b> Simulation of a Poisson Process</a></li>
<li class="chapter" data-level="16.5.3" data-path="chap15.html"><a href="chap15.html#calculating-ruin-probability-through-simulation"><i class="fa fa-check"></i><b>16.5.3</b> Calculating Ruin Probability through Simulation</a></li>
</ul></li>
<li class="chapter" data-level="16.6" data-path="chap15.html"><a href="chap15.html#monte-carlo-via-markov-chains"><i class="fa fa-check"></i><b>16.6</b> Monte Carlo via Markov Chains</a>
<ul>
<li class="chapter" data-level="16.6.1" data-path="chap15.html"><a href="chap15.html#principle-11"><i class="fa fa-check"></i><b>16.6.1</b> Principle</a></li>
<li class="chapter" data-level="16.6.2" data-path="chap15.html"><a href="chap15.html#Ergodicity"><i class="fa fa-check"></i><b>16.6.2</b> Some Notions of Ergodic Theory</a></li>
<li class="chapter" data-level="16.6.3" data-path="chap15.html"><a href="chap15.html#simulation-of-an-invariant-measure-hastings-metropolis-algorithm"><i class="fa fa-check"></i><b>16.6.3</b> Simulation of an Invariant Measure: Hastings-Metropolis Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="16.7" data-path="chap15.html"><a href="chap15.html#variance-reduction"><i class="fa fa-check"></i><b>16.7</b> Variance Reduction</a>
<ul>
<li class="chapter" data-level="16.7.1" data-path="chap15.html"><a href="chap15.html#use-of-antithetic-variables"><i class="fa fa-check"></i><b>16.7.1</b> Use of Antithetic Variables</a></li>
<li class="chapter" data-level="16.7.2" data-path="chap15.html"><a href="chap15.html#use-of-control-variates"><i class="fa fa-check"></i><b>16.7.2</b> Use of Control Variates</a></li>
<li class="chapter" data-level="16.7.3" data-path="chap15.html"><a href="chap15.html#use-of-conditioning"><i class="fa fa-check"></i><b>16.7.3</b> Use of Conditioning</a></li>
<li class="chapter" data-level="16.7.4" data-path="chap15.html"><a href="chap15.html#stratified-sampling"><i class="fa fa-check"></i><b>16.7.4</b> Stratified Sampling</a></li>
<li class="chapter" data-level="16.7.5" data-path="chap15.html"><a href="chap15.html#importance-sampling"><i class="fa fa-check"></i><b>16.7.5</b> Importance Sampling</a></li>
</ul></li>
<li class="chapter" data-level="16.8" data-path="chap15.html"><a href="chap15.html#convergence-control-and-stopping-criteria"><i class="fa fa-check"></i><b>16.8</b> Convergence Control and Stopping Criteria</a>
<ul>
<li class="chapter" data-level="16.8.1" data-path="chap15.html"><a href="chap15.html#two-step-estimation"><i class="fa fa-check"></i><b>16.8.1</b> Two-Step Estimation</a></li>
<li class="chapter" data-level="16.8.2" data-path="chap15.html"><a href="chap15.html#sequential-approach"><i class="fa fa-check"></i><b>16.8.2</b> Sequential Approach</a></li>
</ul></li>
<li class="chapter" data-level="16.9" data-path="chap15.html"><a href="chap15.html#bibliographical-notes-12"><i class="fa fa-check"></i><b>16.9</b> Bibliographical Notes</a></li>
<li class="chapter" data-level="16.10" data-path="chap15.html"><a href="chap15.html#exercises-9"><i class="fa fa-check"></i><b>16.10</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="chap16.html"><a href="chap16.html"><i class="fa fa-check"></i><b>17</b> Conclusion</a></li>
<li class="chapter" data-level="" data-path="postface.html"><a href="postface.html"><i class="fa fa-check"></i>Postface</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Non Life Insurance Mathematics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chap15" class="section level1 hasAnchor" number="16">
<h1><span class="header-section-number">Chapter 16</span> Monte Carlo<a href="chap15.html#chap15" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="introduction-12" class="section level2 hasAnchor" number="16.1">
<h2><span class="header-section-number">16.1</span> Introduction<a href="chap15.html#introduction-12" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The use of simulations can be useful and precise when obtaining explicit expressions is impossible or when numerical calculations are too time-consuming, as in the following example.</p>
<div class="example">
<p><span id="exm:unlabeled-div-1" class="example"><strong>Example 16.1  </strong></span>In the case of the traveling salesman problem, the goal is to find the optimal route for visiting <span class="math inline">\(n\)</span> cities in order to minimize the traveled distance. This involves calculating the length of <span class="math inline">\((n-1)!\)</span> trajectories. With just 12 cities to visit, there are already 40 million trajectories to compute. However, simulation methods, based on iterative minimization (similar to genetic algorithms, where, starting from a route, two cities are randomly chosen and their positions in the current path are swapped), can approximate the optimum. While it may not guarantee the best solution (which is infeasible for <span class="math inline">\(n&gt;20\)</span>), it provides a very good solution with a high probability.</p>
</div>
<p>As we have seen in detail in Chapter 3, the central tool in non-life insurance is the pure premium, which is essentially a mathematical expectation (under the conditions of the law of large numbers). However, calculating the expectation of a random variable <span class="math inline">\(X\)</span> with distribution function <span class="math inline">\(F\)</span> often involves computing <span class="math inline">\(\int xdF(x)\)</span>. This calculation can be complex. A numerical method involves simulating <span class="math inline">\(X\)</span> a “large” number of times, independently, and approximating the expectation as the empirical mean of the obtained realizations <span class="math inline">\(x_1, \ldots, x_n\)</span>, i.e.,
<span class="math display">\[\begin{equation*}
\frac{1}{n}(x_{1}+\ldots+x_{n}) \approx \mathbb{E}[X],
\end{equation*}\]</span>
by virtue of the law of large numbers (Chapter 3, Property 3.5.2). It’s worth noting that we can also often assess the order of magnitude of the error. Indeed, if <span class="math inline">\(\mathbb{E}[X^{2}]&lt;\infty\)</span>, then by the central limit theorem (Chapter 4, Theorem 4.2.1), when denoting the error as <span class="math inline">\(\varepsilon _{n}=(X_{1}+\ldots+X_{n})/n-\mathbb{E}[X]=\overline{X}-\mathbb{E}[X]\)</span>, we have
<span class="math display">\[\begin{equation*}
\sqrt{n}\frac{\varepsilon _{n}}{\sqrt{\mathbb{E}[X^{2}]-\left(\mathbb{E}[X]\right)^{2}}}\rightarrow_{\text{distribution}}\mathcal{N}or\left( 0,1\right) .
\end{equation*}\]</span>
In practice, this yields a 95% confidence interval for the expectation, approximated by
<span class="math display">\[\begin{equation*}
\left[ \overline{x}-1.96\frac{s _{n}}{\sqrt{n}},\overline{x}+1.96\frac{s _{n}}{\sqrt{n}}\right],
\end{equation*}\]</span>
where <span class="math inline">\(s _{n}^{2}\)</span> unbiasedly estimates the variance of <span class="math inline">\(X\)</span>, i.e.,
<span class="math display">\[\begin{equation*}
s _{n}^{2}=\frac{1}{n-1}\sum_{i=1}^{n}\left(
x_{i}-\overline{x}\right) ^{2}.
\end{equation*}\]</span>
This allows us to determine the number <span class="math inline">\(n\)</span> of simulations required to achieve the desired level of precision.</p>
<p>In general, Monte Carlo methods have been widely used to approximate integrals of the form <span class="math inline">\(\int_{0}^{1}g\left( x\right) dx\)</span>. Notably, if <span class="math inline">\(U\sim\mathcal{U}ni(0,1)\)</span>, then $<em>{0}^{1}g( x) dx=$ can be approximated by $( g( u</em>{1})+…+g( u_{n}) ) $, where the <span class="math inline">\(u_{i}\)</span> are random samples from $ni( 0,1) $, demonstrating the close relationship between evaluating integrals through simulation and mathematical expectations.
If the convergence rate is slower compared to standard numerical methods (on the order of <span class="math inline">\(\sigma /\sqrt{n}\)</span>), it’s important to note that here, no regularity properties are assumed for <span class="math inline">\(g\)</span> (unlike standard numerical methods). The convergence in $O( 1/) $ is a drawback, but it can be shown that for multiple integrals, the speed remains the same (unlike standard numerical methods). Specifically, if <span class="math inline">\(U_1,U_2,\ldots,U_d\)</span> are independent random variables with <span class="math inline">\(\mathcal{U}ni(0,1)\)</span> distribution, then
<span class="math display">\[\begin{eqnarray*}
\int_0^1\ldots\int_0^1g(x_{1},...,x_{d})dx_{1}...dx_{d}
&amp;=&amp;\mathbb{E}[ g\left( U_{1},...,U_{d}\right) ]  \\
&amp;\approx &amp;\frac{1}{n}\sum_{i=1}^{n}g\left(
u_{i,1},u_{i,2},...,u_{i,d }\right)
\end{eqnarray*}\]</span>%
still converges in <span class="math inline">\(O(1/\sqrt{n})\)</span>.</p>
<div class="example">
<p><span id="exm:unlabeled-div-2" class="example"><strong>Example 16.2  </strong></span>This method can be used to approximate the value of $$. Indeed, it is easy to see that
<span class="math display">\[\begin{equation*}
\pi =4\frac{\text{Area of the disk with diameter 2}}{\text{Area of
the square with side length 2}}.
\end{equation*}\]</span>%
Also, if $( X,Y) $ is uniformly distributed over the square $$, then we have
<span class="math display">\[\begin{equation*}
\pi =\Pr\left[ X^{2}+Y^{2}\leq 1\right].
\end{equation*}\]</span>
This leads to the result presented in Figure
<span class="math inline">\(\ref{FIG-Pi}\)</span>, where we calculate the ratio of the number of realizations falling within the disk of radius 1 to the total number of simulations.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-3" class="remark"><em>Remark</em>. </span>Note that Georges Louis Leclerc Buffon proposed a different method to calculate $$ in 1777, giving rise to the first historical example of using Monte Carlo methods. Consider a series of evenly spaced lines (strips of a smooth floor, a tiled floor, or a checkerboard) and a needle or a thin rod that is balanced such that its length matches the spacing between the lines. Toss the needle randomly onto the floor, recording <span class="math inline">\(X=1\)</span>
if the needle crosses a line and <span class="math inline">\(X=0\)</span> otherwise. If this experiment is repeated <span class="math inline">\(n\)</span> times, with <span class="math inline">\(p\)</span> being the number
of successes (<span class="math inline">\(X=1\)</span>), then the proportion <span class="math inline">\(n/p\)</span> should provide a good approximation of $$. For a 95% confidence interval, an accuracy of <span class="math inline">\(10^{-3}\)</span> is achieved with 900,000 tosses.</p>
</div>
<p>FIG</p>
<p>The above illustrates the importance of generating random numbers from a <span class="math inline">\(\mathcal{U}ni(0,1)\)</span> distribution.
We will denote as a function that produces such random samples.
Regarding the concept of randomness and random number generators on $$, it is essential to highlight the two most critical properties that a function must satisfy:</p>
<p>$( i) $ for any <span class="math inline">\(0\leq a\leq b\leq 1\)</span>,%
<span class="math display">\[
\Pr\Big[ \mathtt{Random}\in \left] a,b\right] \Big]=b-a\text{.}
\]</span></p>
<p>$( ii) $ successive calls to this function must generate independent observations; that is, for any <span class="math inline">\(0\leq a\leq b\leq 1\)</span>, <span class="math inline">\(0\leq c\leq d\leq 1\)</span>%
<span class="math display">\[
\Pr\Big[ \mathtt{Random}_{1}\in \left] a,b\right] ,\mathtt{Random}%
_{2}\in \left] c,d\right] \Big] =\left( b-a\right) \left( d-c\right) \text{%
.}
\]</span>%
More generally, we refer to <span class="math inline">\(k\)</span>-uniformity if, for any <span class="math inline">\(0\leq a_{i}\leq b_{i}\leq 1\)</span>, <span class="math inline">\(i=1,...,k\)</span>,%
<span class="math display">\[
\Pr\Big[ \mathtt{Random}_{1}\in \left] a_{1},b_{1}\right] ,...,%
\mathtt{Random}_{k}\in \left] a_{k},b_{k}\right] \Big]
=\prod_{i=1}^{k}\left( b_{i}-a_{i}\right) \text{.}
\]</span></p>
<p>In fact, the most intuitive and crucial notion of randomness has no connection with the concepts of uniformity and independence: randomness is what is unpredictable.</p>
</div>
<div id="general-principles" class="section level2 hasAnchor" number="16.2">
<h2><span class="header-section-number">16.2</span> General Principles<a href="chap15.html#general-principles" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The standard library of any computer programming language contains a “random generator,” hereafter referred to as <span class="math inline">\(\mathtt{Random}\)</span>, which provides a sequence of numbers <span class="math inline">\(x_{1},...,x_{n}\)</span> in <span class="math inline">\([0,1]\)</span>, indistinguishable from realizations <span class="math inline">\(X_{1}(\omega),...,X_{n}(\omega)\)</span> of <span class="math inline">\(n\)</span> independent random variables with the same <span class="math inline">\(\mathcal{U}ni(0,1)\)</span> distribution. The randomness is revealed through successive calls to this function.</p>
<div id="pseudo-random-numbers" class="section level3 hasAnchor" number="16.2.1">
<h3><span class="header-section-number">16.2.1</span> Pseudo-Random Numbers<a href="chap15.html#pseudo-random-numbers" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There are many methods to simulate uniform distributions, known as random number generators or pseudo-random number generators. The general idea is to construct a random number generator by drawing uniformly from ${ 0,1,….,M-1} $, where <span class="math inline">\(M\)</span> is a “large” number (on the order of <span class="math inline">\(10^{8}\)</span>), and then dividing by <span class="math inline">\(M\)</span> to obtain “real” numbers in $$.</p>
<p>To generate a sequence of pseudo-random numbers in ${ 0,1,….,M-1} $, starting with a seed <span class="math inline">\(u_{0}\)</span>, we construct a sequence $u_{n+1}=g( u_{n}) $ where <span class="math inline">\(g\)</span> is a function defined on ${
0,1,….,M-1} $ with values in ${
0,1,….,M-1} $. The most commonly used method is the “congruential generators,” where the function <span class="math inline">\(g\)</span> is of the form
<span class="math display">\[\begin{equation*}
g\left( u\right) =\left( Au+B\right) \text{ modulo }M.
\end{equation*}\]</span></p>
<p>The modulus <span class="math inline">\(M\)</span> is often a power of 2 (<span class="math inline">\(2^{16}\)</span> or <span class="math inline">\(2^{32}\)</span>) because division by <span class="math inline">\(M\)</span> is numerically easier. Another common choice is <span class="math inline">\(2^{32}-1\)</span>, which has the advantage of being a prime number. The period is the largest number of integers in <span class="math inline">\({0,...,M-1}\)</span> that can be generated. Arithmetic conditions on <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>, and <span class="math inline">\(M\)</span> are imposed to achieve the largest possible period.</p>
<div class="remark">
<p><span id="unlabeled-div-4" class="remark"><em>Remark</em>. </span>The choice of constants <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>, and <span class="math inline">\(M\)</span> is a fundamental issue in simulation. Figure <span class="math inline">\(\ref{RANDU}\)</span> shows a three-dimensional plot of points $( U_{i+1},U_{i},U_{i-1}) $ - 3000 simulations - using the <span class="math inline">\(RANDU\)</span> algorithm from IBM (<span class="math inline">\(A=65539\)</span>, <span class="math inline">\(B=0\)</span>, and <span class="math inline">\(M=2^{31}\)</span>) and one of those proposed by <span class="citation">(<a href="#ref-l1994uniform" role="doc-biblioref">L’Ecuyer 1994</a>)</span> (<span class="math inline">\(A=41358\)</span>, <span class="math inline">\(B=0\)</span>, and <span class="math inline">\(M=2^{31}-1\)</span>). Note that <span class="math inline">\(65539=2^{16}+3\)</span>, and it can be shown that for the <span class="math inline">\(RANDU\)</span> algorithm,
<span class="math display">\[
U_{n+1}-6U_{n}+9U_{n-1}=0\text{ mod }1,
\]</span>
which results in planes in the point cloud. This generator can pose serious problems due to the dependence that exists between the draws. In contrast, <span class="citation">(<a href="#ref-l1994uniform" role="doc-biblioref">L’Ecuyer 1994</a>)</span>’s algorithm does not reveal any structure in the generated triplets $( U_{i+1},U_{i},U_{i-1}) $.</p>
</div>
<p>FIG</p>
<p>In general, simulating random variables (of any kind) relies on simulating <span class="math inline">\(\mathcal{U}ni(0,1)\)</span> variables. This will be the case, in particular, for the inversion method based on Proposition 2.5.2 from Volume 1. More generally, we can note the following result, which extends Proposition 2.5.2 to arbitrary dimensions.</p>
<div class="lemma">
<p><span id="lem:lemme-MC-unif" class="lemma"><strong>Lemma 16.1  </strong></span>For any random vector <span class="math inline">\(\boldsymbol{X}\,\)</span> in <span class="math inline">\({\mathbb{R}}^{d}\)</span>, there exists <span class="math inline">\(\Psi :% \R^{n}\rightarrow \R^{d}\)</span> almost surely continuous such that $=<em>{}( U</em>{1},…,U_{n}) $ where <span class="math inline">\(U_{1},...,U_{n}\)</span> are independent and identically distributed as <span class="math inline">\(\mathcal{U}ni(0,1)\)</span>.</p>
</div>
<p>It is possible for <span class="math inline">\(n\)</span> to be infinite here. Furthermore, the central assumption is that the <span class="math inline">\(U_{i}\)</span> are independent (they can follow a distribution other than the uniform distribution).</p>
<div class="example">
<p><span id="exm:ExoSimulExpo" class="example"><strong>Example 16.3  </strong></span>If $( u) =-( u) /$, then
$( U)xp() $ when <span class="math inline">\(U\sim\mathcal{U}ni(0,1)\)</span>.
This is a direct application of Proposition 2.5.2.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-5" class="example"><strong>Example 16.4  </strong></span>If
<span class="math display">\[
\Psi \left( u_{1},u_{2}\right) =\sqrt{\left( -2\log u_{1}\right)
}\cos \left( 2\pi u_{2}\right),
\]</span>
then <span class="math inline">\(\Psi \left( U_{1},U_{2}\right) \sim\mathcal{N}or(0,1)\)</span> when <span class="math inline">\(U_1\)</span> and <span class="math inline">\(U_2\)</span>
are independent and identically distributed as <span class="math inline">\(\mathcal{U}ni(0,1)\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-6" class="example"><strong>Example 16.5  </strong></span>If $( u_{1},…,u_{n})
=_{i=1}^{n}[ u_{i}p] $, then <span class="math inline">\(\Psi \left( U_{1},...,U_{n}\right) \sim\mathcal{B}in(n,p)\)</span> when <span class="math inline">\(U_1,\ldots,U_n\)</span>
are independent and identically distributed as <span class="math inline">\(\mathcal{U}ni(0,1)\)</span></p>
</div>
</div>
<div id="Section-inversion" class="section level3 hasAnchor" number="16.2.2">
<h3><span class="header-section-number">16.2.2</span> The Inversion Method<a href="chap15.html#Section-inversion" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="the-general-case" class="section level4 hasAnchor" number="16.2.2.1">
<h4><span class="header-section-number">16.2.2.1</span> The General Case<a href="chap15.html#the-general-case" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>This method is the simplest of simulation methods and is based on the following property (Proposition 2.5.2 from Volume 1): let <span class="math inline">\(F\)</span> be a distribution function defined on <span class="math inline">\({\mathbb{R}}\)</span>, and let $U( 0,1) $, then $X=F^{-1}( U) $ has the distribution function <span class="math inline">\(F\)</span>.</p>
<div class="example">
<p><span id="exm:unlabeled-div-7" class="example"><strong>Example 16.6  </strong></span></p>
If <span class="math inline">\(F\)</span> is the distribution function corresponding to the <span class="math inline">\(\mathcal{E}xp(\lambda)\)</span> distribution, then $F^{-1}(x)=-( 1-u)
/$, leading to the simulation algorithm,
<p>Note that it is equivalent to consider $(
) $ and $( 1-)
$, which follow the same distribution.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="chap15.html#cb1-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100000</span></span>
<span id="cb1-2"><a href="chap15.html#cb1-2" aria-hidden="true" tabindex="-1"></a>u <span class="ot">&lt;-</span> <span class="fu">runif</span>(n)</span>
<span id="cb1-3"><a href="chap15.html#cb1-3" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="sc">-</span><span class="fu">log</span>(<span class="dv">1</span><span class="sc">-</span>u)</span></code></pre></div>
</div>
<div id="the-discrete-case" class="section level5 hasAnchor" number="16.2.2.1.1">
<h5><span class="header-section-number">16.2.2.1.1</span> The Discrete Case<a href="chap15.html#the-discrete-case" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>This method can also be used for discrete distributions (including those taking values in <span class="math inline">\({\mathbb{N}}\)</span>), especially if there exists a recurrence relation for calculating <span class="math inline">\(\Pr% [ X=k+1]\)</span> from <span class="math inline">\(\Pr\left[ X=k\right] .\)</span><br />
If we denote <span class="math inline">\(q_{k}=\Pr[X\leq k]\)</span>, then, with the convention <span class="math inline">\(q_{-1}=0\)</span>,%
<span class="math display">\[\begin{equation*}
X=\min \left\{ k\in\mathbb{N}|q_{k-1}\leq U&lt;q_{k}, \text{ where }U\sim
\mathcal{U}ni \left( 0,1\right) \right\} .
\end{equation*}\]</span>%
Suppose there exists a recurrence relation between
<span class="math inline">\(\Pr[X=k+1]\)</span> and <span class="math inline">\(\Pr[X=k]\)</span>, of the form
<span class="math display">\[\begin{equation*}
\frac{\Pr[X=k+1]}{\Pr[ X=k] }=f\left(k+1\right) ,
\end{equation*}\]</span>%
then the algorithm can be written as</p>
<hr />
<p><strong>Algorithm</strong>: My algo</p>
<hr />
<ol style="list-style-type: decimal">
<li><span class="math inline">\(U\longleftarrow \mathtt{Random}\)</span> and <span class="math inline">\(T\longleftarrow \mathtt{Random}\)</span>,</li>
<li><span class="math inline">\(a\longleftarrow 1-U\)</span>, $bT(a)^{2}-$, <span class="math inline">\(c\longleftarrow\left[\theta +1\right] -2\theta aT\)</span> and $dc^{2}-4b(T-1) $,</li>
<li>$V-/$.</li>
</ol>
<hr />
<p>il}  <span class="math inline">\(U&gt;q\)</span>
\end{quote}
A very useful special case for actuaries is that of the Panjer family of distributions, as presented in Section 6.4.</p>
</div>
</div>
</div>
<div id="Section-rejet" class="section level3 hasAnchor" number="16.2.3">
<h3><span class="header-section-number">16.2.3</span> Rejection Method<a href="chap15.html#Section-rejet" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This method can be used if one wants to simulate a distribution, knowing that it is the conditional distribution of a distribution that can be simulated.</p>
<div class="example">
<p><span id="exm:Exo127V0" class="example"><strong>Example 16.7  </strong></span>Trivially, if <span class="math inline">\(0&lt;\alpha &lt;1\)</span> and we want to simulate <span class="math inline">\(X\sim\mathcal{U}ni(0,\alpha)\)</span>, we can note that <span class="math inline">\(X\)</span> has the same distribution as $[U|U] $
where $Uni( 0,1) $. It is sufficient to implement the following algorithm:</p>
<hr />
<p><strong>Algorithm</strong>: Rejection method</p>
<hr />
<ul>
<li><strong>Repeat</strong> <span class="math inline">\(X\longleftarrow \mathtt{Random}\)</span>,</li>
<li><strong>Until</strong> <span class="math inline">\(X\leq \alpha\)</span></li>
</ul>
<hr />
</div>
<div class="example">
<p><span id="exm:Exo128V0" class="example"><strong>Example 16.8  </strong></span>It is also possible to simulate a pair $(X,Y) $ uniformly distributed over the unit disk. For this, we simulate a pair $(X,Y) $, uniformly distributed over the unit square $$, until it falls within the unit disk:</p>
<hr />
<p><strong>Algorithm</strong>: Rejection method</p>
<hr />
<ul>
<li><span class="math inline">\(X\longleftarrow 2\times \texttt{Random}-1\)</span></li>
<li><span class="math inline">\(Y\longleftarrow 2\times \texttt{Random}-1\)</span></li>
<li><strong>Until</strong> <span class="math inline">\(X^{2}+Y^{2}\leq 1\)</span></li>
</ul>
<hr />
</div>
<p>However, Example <span class="math inline">\(\ref{Exo127V0}\)</span> is not the optimal method for
simulating a realization of the uniform distribution on $$, especially
if $$ is small. Indeed, the number of iterations
required follows a geometric distribution with parameter
$$, whose expectation is $1/$. In Example <span class="math inline">\(\ref{Exo128V0}\)</span>,
the average number of iterations is%
<span class="math display">\[\begin{equation*}
n=\frac{\text{Area of the unit disk} }{\text{Area of the unit square}}=\frac{\pi }{4}\approx 1.27.
\end{equation*}\]</span></p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="chap15.html#cb2-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100000</span></span>
<span id="cb2-2"><a href="chap15.html#cb2-2" aria-hidden="true" tabindex="-1"></a>u <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="sc">-</span><span class="dv">1</span>, <span class="sc">+</span><span class="dv">1</span>)</span>
<span id="cb2-3"><a href="chap15.html#cb2-3" aria-hidden="true" tabindex="-1"></a>v <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="sc">-</span><span class="dv">1</span>, <span class="sc">+</span><span class="dv">1</span>)</span>
<span id="cb2-4"><a href="chap15.html#cb2-4" aria-hidden="true" tabindex="-1"></a>s <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(u<span class="sc">^</span><span class="dv">2</span><span class="sc">+</span>v<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb2-5"><a href="chap15.html#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(s<span class="sc">&lt;=</span><span class="dv">1</span>)<span class="sc">*</span><span class="dv">4</span></span></code></pre></div>
<p>More generally, the following result is used, which allows simulating a random variable with any density <span class="math inline">\(f\)</span> from an easy-to-simulate density <span class="math inline">\(g\)</span>.</p>
<div class="property">
<p>Let <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> be two densities such that there exists a constant <span class="math inline">\(c\)</span> such that $cg( x) f( x) $ for all <span class="math inline">\(x\)</span>. Let <span class="math inline">\(X\)</span> be a random variable with density <span class="math inline">\(g\)</span>, and let <span class="math inline">\(U\sim\mathcal{U}ni(0,1)\)</span>, independent of <span class="math inline">\(X\)</span>. Then,
the conditional distribution of <span class="math inline">\(X\)</span> given <span class="math inline">\(U &lt;h\left( X\right)\)</span> has density <span class="math inline">\(f\)</span>, where <span class="math inline">\(h(x)=f(x)/cg(x)\)</span>.
Also, if <span class="math inline">\(U_1,U_2,\ldots\)</span> are independent and identically distributed with <span class="math inline">\(\mathcal{U}ni(0,1)\)</span>, and <span class="math inline">\(X_1,X_2,\ldots\)</span> are independent and identically distributed with density <span class="math inline">\(g\)</span>, then by defining
<span class="math display">\[
\tau=\inf\{i\geq 1| U_i\leq h(X_i)\},
\]</span>
<span class="math inline">\(X_\tau\)</span> will have density <span class="math inline">\(f\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-8" class="proof"><em>Proof</em>. </span>Let <span class="math inline">\(U\sim\mathcal{U}ni(0,1)\)</span>, and define
<span class="math inline">\(h(x)=f(x)/cg(x)\)</span>. Then, if <span class="math inline">\(X\)</span> has density <span class="math inline">\(g\)</span>,
<span class="math display">\[
\Pr[U&gt;h(X)]=\int_{-\infty}^{+\infty} \int_{h(y)}^1
g(y)dudy=1-\frac{1}{c}.
\]</span>
Furthermore, for <span class="math inline">\(t\in{\mathbb{R}}\)</span>,
<span class="math display">\[\begin{eqnarray*}
\Pr[X_\tau\leq t]&amp;=&amp; \sum_{k=1}^{+\infty}\Pr[\tau=k,X_\tau\leq t]\\
&amp;=&amp;\sum_{k=1}^{+\infty}\Pr[U_i&gt;h(X_i)\text{ for }i=1,\ldots,k-1]\Pr[U_k&lt;h(X_k),X_k\leq t]\\
&amp;=&amp;\sum_{k=1}^{\infty}\left(1-\frac{1}{c}\right)^{k-1}\int_{-\infty}^t \int_{0}^{h(z)} g(z)dzdu\\
&amp;=&amp;\sum_{k=1}^{+\infty}\left(1-\frac{1}{c}\right)^{k-1}\int_{-\infty}^t g(z)h(z)dz=\int_{-\infty}^t
f(z)dz, \end{eqnarray*}\]</span>
which means that <span class="math inline">\(X_\tau\)</span> has density
<span class="math inline">\(f\)</span>.</p>
</div>
<p>Note that the probability densities involved in the result can be either discrete or continuous. Counting distributions can also be simulated using this method.
The rejection algorithm is then written as follows:</p>
<hr />
<p><strong>Algorithm</strong>: Rejection method</p>
<hr />
<ul>
<li><strong>Repeat</strong> <span class="math inline">\(X\longleftarrow \mathtt{simulate\ according\ to\ density\ }g\)</span></li>
<li><span class="math inline">\(U\longleftarrow \mathtt{Random}\)</span></li>
<li><strong>Until</strong> <span class="math inline">\(c\times U\leq f\left( X\right) /g\left( X\right)\)</span></li>
</ul>
<hr />
<div class="example">
<p><span id="exm:unlabeled-div-9" class="example"><strong>Example 16.9  </strong></span></p>
Consider the random variable <span class="math inline">\(Y\)</span> with density
<span class="math display">\[\begin{equation}
\label{DensFig15.3}
g\left( x\right) =\left\{
\begin{array}{l}
3\left( 1-x^{2}\right) /4,\text{ on }\left[
-1,1\right].\\
0,\text{ otherwise.}
\end{array}
\right.
\end{equation}\]</span>
It is then possible to use either of the two following methods to simulate such a distribution:
</div>
<div class="example">
<p><span id="exm:unlabeled-div-10" class="example"><strong>Example 16.10  </strong></span>As we have seen in Example <span class="math inline">\(\ref{ExoSimulExpo}\)</span>, it is straightforward to
simulate an exponential distribution using the inversion of the
cumulative distribution function. Noting that for all <span class="math inline">\(x\geq 0\)</span>,
<span class="math display">\[\begin{equation*}
\frac{1}{\sqrt{2\pi }}\exp \left( -\frac{x^{2}}{2}\right) \leq
\frac{\exp \left( 1/2\right) }{\sqrt{2\pi }}\exp \left( -x\right)
,
\end{equation*}\]</span>%
we can simulate a normal distribution using the rejection method from the simulation of the <span class="math inline">\(\mathcal{E}xp(1)\)</span> distribution. If <span class="math inline">\(U\sim\mathcal{U}ni(0,1)\)</span>
and <span class="math inline">\(X\sim\mathcal{E}xp(1)\)</span>, the rejection inequality is then written as%
<span class="math display">\[\begin{equation*}
\exp \left( 1/2\right) \exp \left( -X\right) U\leq \exp \left(
-X^{2}/2\right) .
\end{equation*}\]</span>%
This inequality can also be simplified by taking the logarithm: <span class="math inline">\(\left( 1-X\right) ^{2}\leq -2\log U.\)</span>%
 Therefore, to simulate a variable with <span class="math inline">\(\mathcal{N}or(0,1)\)</span> distribution, we simulate two independent random variables from <span class="math inline">\(\mathcal{E}xp(1)\)</span>, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, until <span class="math inline">\(\left( 1-X\right) ^{2}\leq 2Y\)</span> since <span class="math inline">\(-\ln U\sim\mathcal{E}xp(1)\)</span>.
We randomly select the
sign <span class="math inline">\(S\)</span> according to the <span class="math inline">\(\mathcal{B}er(1/2)\)</span> distribution, and the product <span class="math inline">\(S\times X\)</span> then follows the <span class="math inline">\(\mathcal{N}or(0,1)\)</span> distribution.
Figure <span class="math inline">\(\ref{FIG-Rejet-2}\)</span> illustrates this procedure.
We will see later that there are
faster methods for simulating normal distributions.</p>
</div>
<p>FIG</p>
</div>
<div id="using-mixture-distributions" class="section level3 hasAnchor" number="16.2.4">
<h3><span class="header-section-number">16.2.4</span> Using Mixture Distributions<a href="chap15.html#using-mixture-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the discrete case, suppose that the probability distribution of <span class="math inline">\(X\)</span> can be written as%
<span class="math display">\[\begin{equation*}
\Pr\left[ X=n\right] =\alpha p_{n}+\left( 1-\alpha \right) q_{n}
\end{equation*}\]</span>%
where <span class="math inline">\(p_{n}=\Pr[Y=n]\)</span> and $q_{n}=$, <span class="math inline">\(n=0,1,\ldots\)</span> with <span class="math inline">\(0&lt;\alpha&lt;1\)</span>
Then, a simple way to simulate <span class="math inline">\(X\)</span>
is to simulate <span class="math inline">\(Y\)</span> and <span class="math inline">\(Z\)</span>, and then set <span class="math inline">\(X=Y\)</span> with probability $$ or <span class="math inline">\(X=Z\)</span> with probability $1-<span class="math inline">\(. The algorithm is then written as follows: \begin{quote} \texttt{If }\)</span>$ $%
X$ $(p_{n}) $</p>
<p>$X$
$( q_{n}) $
\end{quote}
In the continuous case, this method can also be used if the density of <span class="math inline">\(X\)</span> can be written as a mixture.</p>
</div>
</div>
<div id="bootstrap-resampling" class="section level2 hasAnchor" number="16.3">
<h2><span class="header-section-number">16.3</span> Bootstrap Resampling<a href="chap15.html#bootstrap-resampling" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="principles" class="section level3 hasAnchor" number="16.3.1">
<h3><span class="header-section-number">16.3.1</span> Principles<a href="chap15.html#principles" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The bootstrap method is very practical (the term “to pull oneself up by one’s bootstrap” means “to get oneself out of a difficult situation” (from “<em>The Adventures of Baron Munchausen</em>”)). because it allows us to overcome sometimes restrictive assumptions about a family of probability distributions. In particular, this method is widely used to obtain better estimators from small samples (e.g., in actuarial reserving methods).</p>
<p>Let <span class="math inline">\(X\)</span> be a random variable with cumulative distribution function <span class="math inline">\(F\)</span>, and assume that we have a sample <span class="math inline">\(X_{1},...,X_{n}\)</span> following this distribution. We are then interested in studying the behavior of the function $T( X_{1},…,X_{n},F) $, which depends on the observations and the distribution function <span class="math inline">\(F\)</span>.</p>
The bootstrap algorithm is as follows:
1. Starting from $%
={ X_{1},…,X_{n}} $, we calculate the empirical cumulative distribution function <span class="math inline">\(F_{n}\)</span>, defined as%
<span class="math display">\[\begin{equation*}
F_{n}\left( x\right)
=\frac{1}{n}\sum_{k=1}^{n}\mathbb{I}[X_{k}\leq x].
\end{equation*}\]</span>
2. Conditionally on <span class="math inline">\(F_{n}\)</span>, we perform <span class="math inline">\(N\)</span> equiprobable resampling with replacement in <span class="math inline">\(\mathcal{E}\)</span>: $%
^{}={ X_{1}<sup>{},…,X_{N}</sup>{}} $ is
then the
new sample (<span class="citation">(<a href="#ref-efron1994introduction" role="doc-biblioref">Efron and Tibshirani 1994</a>)</span> suggested
taking <span class="math inline">\(N=n\)</span>, however, better results can be obtained by sub-sampling and performing sampling without replacement (one can then reduce it to <span class="math inline">\(U\)</span>-statistics)).
<p>We define the bootstrapped statistic $T^{}=T( X_{1}<sup>{},…,X_{N}</sup>{
},F_{n}) $.</p>
<p>We can then repeat the last step to
obtain approximations using the Monte Carlo method. We repeat the second step $%
m $ times, generating <span class="math inline">\(m\)</span> samples <span class="math inline">\(% \mathcal{E}_{1}^{\ast },...,\mathcal{E}_{m}^{\ast }\)</span>, and observe <span class="math inline">\(m\)</span> values <span class="math inline">\(T_{1}^{\ast },...,T_{m}^{\ast }\)</span> of
<span class="math inline">\(T\)</span>.</p>
<p>::: {.example}
Let’s assume, for example, that $Xer(p) $, and let <span class="math inline">\(\widehat{p}_{n}\)</span> be the frequency of <span class="math inline">\(1\)</span> in the sample
$={ X_{1},…,X_{n}} $, i.e., <span class="math inline">\(\widehat{p}_n=\frac{1}{n}\sum_{i=1}^nX_i\)</span>. Here, we consider
<span class="math inline">\(T\left( \mathcal{E},F\right) =\widehat{p}_{n}-p\)</span>. The bootstrapped sample <span class="math inline">\(\mathcal{E}^{\ast }\)</span>
is then the sequence of <span class="math inline">\(N\)</span> equiprobable resamples with replacement from <span class="math inline">\(\mathcal{E}\)</span>.
Conditionally on <span class="math inline">\(\mathcal{E}\)</span>, the distribution of <span class="math inline">\(% X_{i}^{\ast }\)</span> is therefore $er(_{n}) $%
. The bootstrapped statistic <span class="math inline">\(T^{\ast }\)</span> is%
<span class="math display">\[
T^{\ast }=T\left( \mathcal{E}^{\ast },F_{n}\right) =\frac{1}{N}%
\sum_{i=1}^{N}X_{i}^{\ast }-\widehat{p}_{n},
\]</span>
for which we know the first two moments:
<span class="math display">\[
\mathbb{E}\left[ T^{\ast }\right] =0\text{ and }\mathbb{V}\left[ T^{\ast }\right] =%
\frac{\widehat{p}_{n}\left( 1-\widehat{p}_{n}\right) }{N}\text{.}
\]</span></p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="chap15.html#cb3-1" aria-hidden="true" tabindex="-1"></a>xb <span class="ot">&lt;-</span> <span class="fu">sample</span>(x, <span class="at">size =</span> <span class="fu">length</span>(x), <span class="at">replace =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
</div>
<div id="bootstrap-confidence-intervals" class="section level3 hasAnchor" number="16.3.2">
<h3><span class="header-section-number">16.3.2</span> Bootstrap Confidence Intervals<a href="chap15.html#bootstrap-confidence-intervals" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Bootstrap can be useful when looking for a
confidence interval for a parameter <span class="math inline">\(\theta\)</span>. In parametric statistics, <span class="math inline">\(\theta\)</span> is generally estimated using maximum likelihood, which provides an estimator <span class="math inline">\(T\)</span> and ensures the asymptotic normality of $T-$. Then, for a sufficiently large sample, we can deduce a confidence interval of the form%
<span class="math display">\[\begin{equation*}
\left[ T-z_{1-\alpha }\sqrt{I^{-1}\left( T\right)
},T+z_{1-\alpha }\sqrt{I^{-1}\left( T\right) }\right] .
\end{equation*}\]</span>%
However, this result is only valid asymptotically and can be particularly poor in finite samples (especially if the sample is small) without normality assumptions. Without normality assumptions, it is necessary to estimate the quantiles <span class="math inline">\(T_{\alpha }\)</span> and <span class="math inline">\(T_{1-\alpha }\)</span> such that
<span class="math inline">\(\Pr\left( \theta \in \left[ T_{\alpha },T_{1-\alpha }\right] \right) =\alpha .\)</span></p>
<p>Let <span class="math inline">\(G\)</span> be the cumulative distribution function of $T-$, and
<span class="math inline">\(G^{\ast }\)</span> be the cumulative distribution function of <span class="math inline">\(T^{\ast }-T\)</span>, such that $T_{
}=G^{-1}(
) $ is estimated by $G^{}( ) $. In general, it suffices to use Monte Carlo methods, and
we approximate <span class="math inline">\(G^{\ast }\)</span> by%
<span class="math display">\[\begin{equation*}
\widehat{G}^{\ast }\left( x\right) =\frac{1}{m}\sum_{i=1}^{m}\mathbb{I}%
\left[ T_{i}^{\ast }-T\leq u\right] .
\end{equation*}\]</span></p>
<p>Now, as we saw in Chapter <span class="math inline">\(\ref{ChapExtremes}\)</span>, if
<span class="math inline">\(X_{1},...,X_{n}\)</span> are independent random variables with the same distribution function <span class="math inline">\(F_X\)</span>, an
estimator of $F_X^{-1}( ) $ is <span class="math inline">\(X_{[\left( n+1\right) \alpha] :n}\)</span>. Therefore, <span class="math inline">\(T_{\alpha }\)</span> can be
approximated by <span class="math inline">\(\widehat{T}_{\alpha }=T_{\left( m+1\right) \alpha :m}^{\ast }\)</span> (by choosing $$ and <span class="math inline">\(m\)</span> such that $( m+1) $ and $( m+1) (
1-) $ are integers; otherwise, interpolations can be used). Under these conditions, it is possible to
show that
<span class="math display">\[
\Pr\Big[ \theta \in \left[ \widehat{T}_{\alpha },%
\widehat{T}_{1-\alpha }\right] \Big] =\alpha +O\left(
n^{-1/2}\right).
\]</span>
One possible method to improve the
convergence rate is to
use studentized bootstrap (yielding an $O( n^{-1}) $ rate).</p>
<p>The use of bootstrap for heavy-tailed claims can pose several practical issues. Very large values tend to appear too frequently. As a result, bootstrap cannot be used to estimate an average or a probability of exceeding a value far from the mean.</p>
<div class="example">
<p><span id="exm:unlabeled-div-11" class="example"><strong>Example 16.11  </strong></span>Figure <span class="math inline">\(\ref{Fig-ex-Pareto}\)</span> shows the estimation of the mean of <span class="math inline">\(n=100\)</span> claims following
$ar( 1,0) $ - on the left - and
$ar(
2,0) $ - on the right. In the first case, the distribution does not have a finite mean, and in the second, the variance is not finite. The dashed curve represents the density of the normal distribution associated with the empirical mean and variance.</p>
</div>
<p>FIG</p>
</div>
<div id="bootstrap-and-quantiles" class="section level3 hasAnchor" number="16.3.3">
<h3><span class="header-section-number">16.3.3</span> Bootstrap and Quantiles<a href="chap15.html#bootstrap-and-quantiles" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As explained in detail in <span class="citation">(<a href="#ref-davison1997bootstrap" role="doc-biblioref">Davison and Hinkley 1997</a>)</span>, what legitimizes bootstrap is primarily the existence of an Edgeworth expansion for the statistic being calculated. Some statistics, especially empirical quantiles, can be irregular and unstable. While asymptotically these resampling methods are valid, the results at finite distances can be relatively poor.</p>
</div>
<div id="bootstrap-and-correlated-samples" class="section level3 hasAnchor" number="16.3.4">
<h3><span class="header-section-number">16.3.4</span> Bootstrap and Correlated Samples<a href="chap15.html#bootstrap-and-correlated-samples" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Among the other cases where bootstrap can yield “false” results, one can note the situation where the data is not independent. As we will see later, this can pose problems when using the bootstrap in loss reserving triangles, where there are intuitively strong chances that the residuals are not independent.</p>
</div>
<div id="application-to-loss-reserving" class="section level3 hasAnchor" number="16.3.5">
<h3><span class="header-section-number">16.3.5</span> Application to Loss Reserving<a href="chap15.html#application-to-loss-reserving" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider the example of the log-Poisson regression (yielding the same estimators as the Chain Ladder method). For the data in Example <span class="math inline">\(\ref{ExPro1323}\)</span>, the graphical outputs of the regression are presented in Figure <span class="math inline">\(\ref{Fig-CL-log-Poisson}\)</span>, including the deviance residuals and Pearson residuals. The Pearson residuals <span class="math inline">\(r_{i,j}^P\)</span> are given in Table <span class="math inline">\(\ref{tab-resid-CL-1}\)</span>, and the deviance residuals <span class="math inline">\(r_{i,j}^D\)</span> are given in Table <span class="math inline">\(\ref{tab-resid-CL-2}\)</span>.</p>
<p>FIG</p>
<p>Most stochastic models, as we have noted, have been implemented with the aim of replicating the Chain-Ladder method, to provide the same amount of reserves but allowing for error margin assessment. Using Monte Carlo methods, it is sufficient to independently simulate the increment errors, generate new triangles, and then use the Chain-Ladder method to obtain reserve amounts for this simulated triangle. If Pearson residuals are used, as recommended by <span class="citation">(<a href="#ref-england1999analytic" role="doc-biblioref">England and Verrall 1999</a>)</span>, the bootstrapped increment is given by</p>
<p><span class="math display">\[
Y_{i,j}^* = r_{i,j}^{P*} \cdot \sqrt{\mu_{i,j}} + \mu_{i,j}
\]</span></p>
<p>where <span class="math inline">\(r_{i,j}^{P*}\)</span> is the bootstrapped Pearson residual.</p>
</div>
<div id="bootstrap-and-correlated-samples-1" class="section level3 hasAnchor" number="16.3.6">
<h3><span class="header-section-number">16.3.6</span> Bootstrap and Correlated Samples<a href="chap15.html#bootstrap-and-correlated-samples-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Among other cases where the bootstrap can yield “false” results, we can note the case where the data is not independent.</p>
<p>As we will see later, this can especially pose problems when using the bootstrap in loss reserving triangles, where there are intuitively strong chances that the residuals are not independent.</p>
</div>
<div id="application-to-loss-reserving-1" class="section level3 hasAnchor" number="16.3.7">
<h3><span class="header-section-number">16.3.7</span> Application to Loss Reserving<a href="chap15.html#application-to-loss-reserving-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s consider the example of the log-Poisson regression (providing the same estimators as the Chain Ladder method). For the data in Example <span class="math inline">\(\ref{ExPro1323}\)</span>, the graphical outputs of the regression are presented in Figure <span class="math inline">\(\ref{Fig-CL-log-Poisson}\)</span>, including the deviance residuals and Pearson residuals. The Pearson residuals <span class="math inline">\(r_{i,j}^P\)</span> are given in Table <span class="math inline">\(\ref{tab-resid-CL-1}\)</span>, and the deviance residuals <span class="math inline">\(r_{i,j}^D\)</span> are given in Table <span class="math inline">\(\ref{tab-resid-CL-2}\)</span>.</p>
<p>FIG</p>
<p>Most stochastic models, as we have noted, have been implemented with the aim of replicating the Chain-Ladder method, providing the exact same amount of reserves while allowing for error margin assessment. Using Monte Carlo methods, it is sufficient to independently simulate the increment errors, generate new triangles, and then use the Chain-Ladder method to obtain reserve amounts for this simulated triangle. If Pearson residuals are used, as recommended by <span class="citation">(<a href="#ref-england1999analytic" role="doc-biblioref">England and Verrall 1999</a>)</span>, the bootstrapped increment is given by</p>
<p><span class="math display">\[
Y_{i,j}^* = r_{i,j}^{P*} \cdot \sqrt{\mu_{i,j}}+\mu_{i,j}
\]</span></p>
<p>where <span class="math inline">\(r_{i,j}^{P*}\)</span> is the value of the bootstrapped Pearson residual.</p>
<p>In the case of the log-Poisson regression, the Pearson residuals are given in Table <span class="math inline">\(\ref{tab-resid-CL-1}\)</span>. A bootstrap simulation of the error yields the bootstrap residuals triangle in Table <span class="math inline">\(\ref{tab-resid-CL-boot-1}\)</span>. If we return to the increments, defining
<span class="math inline">\(Y_{i,j}^* = r_{i,j}^{P*}\sqrt{\mu_{i,j}}+\mu_{i,j}\)</span>, we obtain the values in Table <span class="math inline">\(\ref{tab-resid-CL-boot-2}\)</span>.</p>
<p>Switching to the cumulative triangle, which can finally be completed using the standard Chain-Ladder method, we obtain the values in Table <span class="math inline">\(\ref{tab-resid-CL-3}\)</span>.</p>
<p>For this simulation, the required reserve amount amounts to <span class="math inline">\(425,160\)</span>. By repeating a large number of times (here <span class="math inline">\(50,000\)</span>), we obtain the values in Table <span class="math inline">\(\ref{Tab-dist-boot-CL}\)</span>.</p>
<div class="remark">
<p><span id="unlabeled-div-12" class="remark"><em>Remark</em>. </span>However, using the bootstrap on the triangle relies on very strong assumptions: the residuals must be independent and identically distributed. From a practical</p>
</div>
</div>
</div>
<div id="simulation-of-univariate-common-probability-distributions" class="section level2 hasAnchor" number="16.4">
<h2><span class="header-section-number">16.4</span> Simulation of Univariate Common Probability Distributions<a href="chap15.html#simulation-of-univariate-common-probability-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="uniform-distribution" class="section level3 hasAnchor" number="16.4.1">
<h3><span class="header-section-number">16.4.1</span> Uniform Distribution<a href="chap15.html#uniform-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We assume here that we have software capable of generating <span class="math inline">\(\mathcal{U}ni(0,1)\)</span> random numbers, and we will refer to this function as .</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="chap15.html#cb4-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100000</span></span>
<span id="cb4-2"><a href="chap15.html#cb4-2" aria-hidden="true" tabindex="-1"></a>u <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="at">min=</span><span class="dv">0</span>, <span class="at">max=</span><span class="dv">1</span>)</span></code></pre></div>
</div>
<div id="normal-gaussian-distribution" class="section level3 hasAnchor" number="16.4.2">
<h3><span class="header-section-number">16.4.2</span> Normal (Gaussian) Distribution<a href="chap15.html#normal-gaussian-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>By applying the usual results on the normal distribution (see Section 2.5.4 of Volume 1), simulating a realization from the <span class="math inline">\(\mathcal{N}or(\mu, \sigma^2)\)</span> distribution is straightforward. It involves generating a sample from the <span class="math inline">\(\mathcal{N}or(0,1)\)</span> distribution and then properly transforming it (multiplication by <span class="math inline">\(\sigma\)</span> and addition of <span class="math inline">\(\mu\)</span>).</p>
<p>The following result provides an efficient method for simulating from the <span class="math inline">\(\mathcal{N}or(0,1)\)</span> distribution.</p>
<div class="property">
<p>Let $( X,Y) $ be a pair of random variables, uniformly distributed within the unit disk $={
( x,y)<sup>2|x</sup>{2}+y^{2}} $. Denote <span class="math inline">\(R\)</span> and
$$ as the polar coordinates associated with $( X,Y) $, i.e., $X=R$ and
$Y=R$. Let <span class="math inline">\(T=\sqrt{-4\log R}\)</span>, then $U=T$ and $V=T$ are independent and follow the <span class="math inline">\(\mathcal{N}or\left( 0,1\right)\)</span> distribution.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-13" class="proof"><em>Proof</em>. </span>Consider the change of variables that transforms Cartesian coordinates into polar coordinates,
<span class="math display">\[\begin{equation*}
\phi :\left\{
\begin{array}{rcl}
\mathcal{S}\backslash \left\{ \left] 0,1\right[ \times \left\{
0\right\}
\right\} &amp; \rightarrow &amp; \left] 0,1\right[ \times \left] 0,2\pi \right[ \\
\left( x,y\right) &amp; \mapsto &amp; \left( r,\theta \right)%
\end{array}%
\right. \text{ }
\end{equation*}\]</span>%
and <span class="math display">\[
\phi ^{-1}:\left\{
\begin{array}{rcl}
\left] 0,1\right[ \times \left] 0,2\pi \right[ &amp; \rightarrow &amp; \mathcal{S}%
\backslash \left\{ \left] 0,1\right[ \times \left\{ 0\right\} \right\} \\
\left( r,\theta \right) &amp; \mapsto &amp; \left( x,y\right) =\left(
r\cos \theta
,r\sin \theta \right) ,%
\end{array}%
\right.
\]</span>
with the Jacobian <span class="math inline">\(J_{\phi^{-1}}=r\)</span>. The density of the pair <span class="math inline">\(\left( R,\Theta \right)\)</span> is then given by
<span class="math display">\[\begin{equation*}
g\left( r,\theta \right) =\frac{r}{\pi }\mathbb{I}\left[ (r\cos \theta ,r\sin
\theta )\in\mathcal{S}\right] ,
\end{equation*}\]</span>%
where the marginal distributions are respectively
<span class="math display">\[\begin{equation*}
g\left( r\right) =2r\mathbb{I}\left[ 0&lt;r&lt;1\right] \text{ and }
g\left( \theta \right) =\frac{1}{2\pi }\mathbb{I}\left[
0&lt;\theta&lt;2\pi \right] .
\end{equation*}\]</span>%
Hence, <span class="math inline">\(R\)</span> and <span class="math inline">\(\Theta\)</span> are independent, with <span class="math inline">\(R\)</span> following a triangular distribution on <span class="math inline">\(\left] 0,1\right[\)</span> and <span class="math inline">\(\Theta\)</span> being uniformly distributed on <span class="math inline">\(\left] 0,2\pi \right[\)</span>. The distribution of <span class="math inline">\(T\)</span> is then given by
<span class="math display">\[\begin{eqnarray*}
\Pr[T\leq t] &amp;=&amp;\Pr\left[ \sqrt{-4\log R}\leq t\right]\\
&amp;=&amp;\Pr\left[ R&gt;\exp \left( -\frac{t^{2}}{4}\right) \right]\\
&amp;=&amp;1-G\left( \exp \left( -\frac{r^{2}}{4}\right) \right) ,
\end{eqnarray*}\]</span>%
where <span class="math inline">\(G\)</span> is the cumulative distribution function of <span class="math inline">\(R\)</span>. Thus, the density of <span class="math inline">\(T\)</span> is
<span class="math display">\[\begin{equation*}
t\mapsto t\exp \left( -\frac{t^{2}}{2}\right) \mathbb{I}\left[
t&gt;0\right] .
\end{equation*}\]</span>%
To determine the density of the pair <span class="math inline">\(\left( U,V\right)\)</span>, obtained by the change of variables
<span class="math display">\[\begin{equation*}
\psi :\left\{
\begin{array}{rcl}
\left] 0,+\infty \right[ \times \left] 0,2\pi \right[ &amp;
\rightarrow &amp; {\mathbb{R}}^{2}\backslash \left( \left] 0,+\infty \right[
\times \left\{
0\right\} \right) \\
\left( t,\theta \right) &amp; \mapsto &amp; \left( u,v\right) =\left(
t\cos \theta
,t\sin \theta \right)%
\end{array}%
\right. \text{ }
\end{equation*}\]</span>%
Noting that <span class="math inline">\(\psi\)</span> restricted to <span class="math inline">\(\left] 0,1\right[ \times \left] 0,2\pi \right[\)</span> coincides with <span class="math inline">\(\phi^{-1}\)</span>, we conclude that the Jacobian of <span class="math inline">\(\psi^{-1}\)</span> is
<span class="math display">\[\begin{equation*}
J_{\psi^{-1}}=\frac{1}{J_{\phi^{-1}}\left( t\left( u,v\right)
,\theta \left( u,v\right) \right)
}=\frac{1}{t}=\frac{1}{\sqrt{u^{2}+v^{2}}},
\end{equation*}\]</span>%
and finally, the density of the pair <span class="math inline">\(\left( U,V\right)\)</span> is
<span class="math display">\[\begin{eqnarray*}
f\left( u,v\right) &amp;=&amp;\frac{1}{\sqrt{u^{2}+v^{2}}}\frac{1}{2\pi }\sqrt{%
u^{2}+v^{2}}\exp \left( -\frac{u^{2}+v^{2}}{2}\right) \\
&amp;=&amp;\frac{1}{\sqrt{2\pi }}\exp \left( -\frac{u^{2}}{2}\right) \frac{1}{\sqrt{%
2\pi }}\exp \left( -\frac{v^{2}}{2}\right) ,
\end{eqnarray*}\]</span>%
which means that the variables <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> are independent and follow the standard normal distribution.</p>
</div>
<p>By noting that
<span class="math display">\[\begin{equation*}
Z = \sqrt{-\frac{2\log R^{2}}{R^{2}}} = \frac{T}{R} \text{ and } \left\{
\begin{array}{l}
U = T\cos \Theta = Z\cdot X \\
V = T\sin \Theta = Z\cdot Y,
\end{array}
\right.
\end{equation*}\]</span>
this provides the polar algorithm, allowing the simulation of a centered, reduced, and independent Gaussian vector.</p>
<hr />
<p><strong>Algorithm</strong>: Polar algorithm</p>
<hr />
<ul>
<li><strong>Repeat</strong>
<ul>
<li><span class="math inline">\(U \leftarrow 2 \times \texttt{Random}-1\)</span></li>
<li><span class="math inline">\(V \leftarrow 2 \times \texttt{Random}-1\)</span></li>
<li><span class="math inline">\(S \leftarrow U^{2} + V^{2}\)</span></li>
</ul></li>
<li><strong>Until</strong>
<ul>
<li><span class="math inline">\(S &lt; 1\)</span></li>
<li><span class="math inline">\(Z \Leftarrow \sqrt{-2\left( \log S\right) /S}\)</span></li>
<li><span class="math inline">\(X \Leftarrow Z \times U\)</span></li>
<li><span class="math inline">\(Y \Leftarrow Z \times V\)</span></li>
</ul></li>
</ul>
<hr />
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="chap15.html#cb5-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100000</span></span>
<span id="cb5-2"><a href="chap15.html#cb5-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">runif</span>(n)<span class="sc">-</span><span class="dv">1</span></span>
<span id="cb5-3"><a href="chap15.html#cb5-3" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">runif</span>(n)<span class="sc">-</span><span class="dv">1</span></span>
<span id="cb5-4"><a href="chap15.html#cb5-4" aria-hidden="true" tabindex="-1"></a>s <span class="ot">&lt;-</span> x<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> y<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb5-5"><a href="chap15.html#cb5-5" aria-hidden="true" tabindex="-1"></a>idx <span class="ot">&lt;-</span> (s<span class="sc">&lt;</span><span class="dv">1</span>)</span>
<span id="cb5-6"><a href="chap15.html#cb5-6" aria-hidden="true" tabindex="-1"></a>z <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="sc">-</span><span class="dv">2</span><span class="sc">*</span><span class="fu">log</span>(s[idx])<span class="sc">/</span>s[idx])</span>
<span id="cb5-7"><a href="chap15.html#cb5-7" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> z<span class="sc">*</span>x[idx]</span>
<span id="cb5-8"><a href="chap15.html#cb5-8" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> z<span class="sc">*</span>y[idx]</span></code></pre></div>
<p>The first step allows simulating the uniform distribution within the unit disk. However, the most commonly used method for simulating a Gaussian distribution is different and relies on a slightly different programming of the previous result. Indeed, it is possible to simulate not the pair $( X,Y) $ but the pair $( T,) $ of independent variables. This leads to the Box-Müller algorithm:</p>
<hr />
<p><strong>Algorithm</strong>: Box-Müller Algorithm</p>
<hr />
<ul>
<li><span class="math inline">\(T \leftarrow \sqrt{-2\log \texttt{Random}}\)</span></li>
<li><span class="math inline">\(\Theta \leftarrow 2\pi \times \texttt{Random}\)</span></li>
<li>$U T $</li>
<li>$V T $</li>
</ul>
<hr />
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="chap15.html#cb6-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100000</span></span>
<span id="cb6-2"><a href="chap15.html#cb6-2" aria-hidden="true" tabindex="-1"></a>t <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="sc">-</span><span class="dv">2</span><span class="sc">*</span><span class="fu">log</span>(<span class="fu">runif</span>(n)))</span>
<span id="cb6-3"><a href="chap15.html#cb6-3" aria-hidden="true" tabindex="-1"></a>theta <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">runif</span>(n)<span class="sc">*</span>pi</span>
<span id="cb6-4"><a href="chap15.html#cb6-4" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> t<span class="sc">*</span><span class="fu">cos</span>(theta)</span>
<span id="cb6-5"><a href="chap15.html#cb6-5" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> t<span class="sc">*</span><span class="fu">sin</span>(theta)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:simGauss"></span>
<img src="bookdown-demo_files/figure-html/simGauss-1.png" alt="Histogram of simulated normal sample" width="768" />
<p class="caption">
Figure 16.1: Histogram of simulated normal sample
</p>
</div>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="chap15.html#cb7-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="at">mu=</span><span class="dv">0</span>, <span class="at">sigma=</span><span class="dv">1</span>)</span></code></pre></div>
</div>
<div id="lognormal-distribution" class="section level3 hasAnchor" number="16.4.3">
<h3><span class="header-section-number">16.4.3</span> Lognormal Distribution<a href="chap15.html#lognormal-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Simulating a log-normal distribution <span class="math inline">\(\mathcal{LN}or(\mu,\sigma^2)\)</span> is done based on the simulation of a normal distribution <span class="math inline">\(\mathcal{N}or(\mu,\sigma^2)\)</span>: <span class="math inline">\(Y = \exp(X)\)</span>, where <span class="math inline">\(X \sim \mathcal{N}or(\mu,\sigma^2)\)</span>.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="chap15.html#cb8-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rlnorm</span>(n)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:simLNorm"></span>
<img src="bookdown-demo_files/figure-html/simLNorm-1.png" alt="Histogram of simulated lognormal sample" width="768" />
<p class="caption">
Figure 16.2: Histogram of simulated lognormal sample
</p>
</div>
</div>
<div id="gamma-distribution-1" class="section level3 hasAnchor" number="16.4.4">
<h3><span class="header-section-number">16.4.4</span> Gamma Distribution<a href="chap15.html#gamma-distribution-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Using the results from Section 2.5.7 of Volume 1 regarding the Gamma distribution, it is possible to simulate it without too much difficulty using rejection methods. However, it should be noted that if <span class="math inline">\(X \sim \mathcal{G}am(a_X,b)\)</span> and <span class="math inline">\(Y \sim \mathcal{G}am(a_Y,b)\)</span> are independent, then <span class="math inline">\(X+Y \sim \mathcal{G}am(a_X+a_Y,b)\)</span>. The <span class="math inline">\(\mathcal{G}am(n,b)\)</span> distribution for <span class="math inline">\(n\in\mathbb{N}\)</span> can be simulated as the sum of <span class="math inline">\(n\)</span> independent <span class="math inline">\(\mathcal{E}xp(b)\)</span> variables. More precisely,</p>
<hr />
<p><strong>Algorithm</strong>: Exponential random variable</p>
<hr />
<ul>
<li><span class="math inline">\(X \leftarrow -b\log\left(\texttt{Random}\right)\)</span></li>
</ul>
<hr />
<hr />
<p><strong>Algorithm</strong>: Gamma Distribution Simulation <span class="math inline">\(\mathcal{G}am(a,1)\)</span> where <span class="math inline">\(a\in \mathbb{N}\)</span></p>
<hr />
<ul>
<li><span class="math inline">\(i \leftarrow 1\)</span> and <span class="math inline">\(Z \leftarrow 0\)</span></li>
<li><strong>Repeat</strong>
<ul>
<li><span class="math inline">\(Z \leftarrow Z - \log \left( \texttt{Random}\right)\)</span></li>
<li><span class="math inline">\(i \leftarrow i + 1\)</span></li>
</ul></li>
<li><strong>Until</strong> <span class="math inline">\(i &gt; \alpha\)</span></li>
</ul>
<hr />
<p>For a more general approach (with <span class="math inline">\(a\)</span> not necessarily an integer), without loss of generality, we assume that <span class="math inline">\(b=1\)</span>. Indeed, if <span class="math inline">\(X\)</span> follows a <span class="math inline">\(\mathcal{G}am(a_X, b)\)</span> distribution, then <span class="math inline">\(Y = bX\)</span> follows a <span class="math inline">\(\mathcal{G}am(a_Y, b)\)</span> distribution. The challenge lies in simulating <span class="math inline">\(\mathcal{G}am(a, 1)\)</span> distributions. Two algorithms can be used depending on the relationship between <span class="math inline">\(a\)</span> and <span class="math inline">\(1\)</span> (see <span class="citation">(<a href="#ref-devroye1992nonuniform" role="doc-biblioref">Devroye 1992</a>)</span> or <span class="citation">(<a href="#ref-fishman2013monte" role="doc-biblioref">Fishman 2013</a>)</span>):</p>
<ol style="list-style-type: decimal">
<li>If <span class="math inline">\(0 &lt; a &lt; 1\)</span>, and denoting <span class="math inline">\(e\)</span> as the exponential of <span class="math inline">\(1\)</span>, we can use a rejection method based on
<span class="math display">\[
g(x) = \frac{ae}{a+e}\left(x^{a-1}\mathbb{I}[0 &lt; x &lt; 1]+\exp(-x)\mathbb{I}[1 \leq x]\right).
\]</span>
A variable with such a density can be easily simulated using the inversion algorithm, as the quantile function is given by
<span class="math display">\[\begin{eqnarray*}
G^{-1}(u) &amp;=&amp; \left(\frac{a+e}{e}u\right)^{1/a}\mathbb{I}[0 &lt; x &lt; e/(a+e)]\\
&amp;&amp; - \log\left(\frac{a+e}{ae}(1-u)\right)\mathbb{I}[e/(a+e) \leq x].
\end{eqnarray*}\]</span></li>
</ol>
<p>To simulate realizations of the <span class="math inline">\(\mathcal{G}am(a, 1)\)</span> distribution, the Ahrens-Dieter algorithm is used, as shown below:</p>
<hr />
<p><strong>Algorithm</strong>: Ahrens-Dieter Algorithm</p>
<hr />
<ul>
<li><span class="math inline">\(\texttt{test}\leftarrow 0\)</span> and <span class="math inline">\(A\leftarrow\left[(a-1) - 1/6(a-1)\right] / (a-1)\)</span></li>
<li><strong>Repeat</strong> <span class="math inline">\(X \leftarrow (a+e)/e \times \texttt{Random}\)</span>
<ul>
<li><strong>If</strong> <span class="math inline">\(X \leq 1\)</span> <strong>Then</strong> <span class="math inline">\(Z \leftarrow X^{1/a}\)</span>,
<ul>
<li><strong>If</strong> <span class="math inline">\(Z \leq -\log \left(\texttt{Random}\right)\)</span> <strong>Then</strong> <span class="math inline">\(\texttt{test}\leftarrow 1\)</span></li>
</ul></li>
<li><strong>If</strong> <span class="math inline">\(X &gt; 1\)</span> <strong>Then</strong> <span class="math inline">\(Z \leftarrow -\log \left(\left((a+e)/e-X\right)/a\right)\)</span>
<ul>
<li><strong>If</strong> <span class="math inline">\(Z \geq \texttt{Random}^{1/(a-1)}\)</span> <strong>Then</strong> <span class="math inline">\(test\leftarrow 1\)</span></li>
</ul></li>
</ul></li>
<li><strong>Until</strong> <span class="math inline">\(\texttt{test}\leftarrow 1\)</span></li>
</ul>
<hr />
<ol start="2" style="list-style-type: decimal">
<li>If <span class="math inline">\(a &gt; 1\)</span> and is not an integer, the Cheng-Feast algorithm can be used:</li>
</ol>
<hr />
<p><strong>Algorithm</strong>: Cheng-Feast Algorithm</p>
<hr />
<ul>
<li><span class="math inline">\(\texttt{test}\leftarrow 0\)</span> and <span class="math inline">\(A\leftarrow\left[(a-1) - 1/6(a-1)\right] / (a-1)\)</span></li>
<li><strong>Repeat</strong>
<ul>
<li><span class="math inline">\(U \leftarrow \texttt{Random}\)</span></li>
<li><span class="math inline">\(V \leftarrow \texttt{Random}\)</span></li>
<li><span class="math inline">\(X \leftarrow A \times U/V\)</span></li>
<li><strong>If</strong> <span class="math inline">\(2U/a-(2/a+2)+X+1/X \leq 0\)</span> <strong>Then</strong> <span class="math inline">\(\texttt{test}\leftarrow 1\)</span></li>
</ul></li>
<li><strong>Until</strong> <span class="math inline">\(\texttt{test}\leftarrow 1\)</span>
<ul>
<li><span class="math inline">\(Z \leftarrow aX\)</span></li>
</ul></li>
</ul>
<hr />
</div>
<div id="beta-distribution-1" class="section level3 hasAnchor" number="16.4.5">
<h3><span class="header-section-number">16.4.5</span> Beta Distribution<a href="chap15.html#beta-distribution-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To simulate a Beta distribution with parameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> (see Section 2.5.3 for properties of the Beta distribution), the following result, known as Jöhnk’s Theorem, is used:</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-14" class="proposition"><strong>Proposition 16.1  </strong></span>Let <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> be two independent variables following <span class="math inline">\(\mathcal{U}ni(0,1)\)</span>. For positive <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, let <span class="math inline">\(S=U^{1/a}/\left(U^{1/a}+V^{1/b}\right)\)</span>. Then, <span class="math inline">\(S\)</span>, conditioned on <span class="math inline">\(T=U^{1/a}+V^{1/b}\leq 1\)</span>, follows a Beta distribution with parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-15" class="proof"><em>Proof</em>. </span>Note that <span class="math inline">\(X=U^{1/a}\)</span> has a cumulative distribution function <span class="math inline">\(x^a\)</span> on <span class="math inline">\([0,1]\)</span>, and its probability density function is <span class="math inline">\(x\mapsto ax^{a-1}\)</span>. Similarly, for <span class="math inline">\(Y=V^{1/b}\)</span>, the probability density function is <span class="math inline">\(y\mapsto by^{b-1}\)</span>. Thus, the joint density of the pair <span class="math inline">\((X,Y)\)</span> is given by
<span class="math display">\[
f(x,y) = abx^{a-1}y^{b-1} \text{ for } (x,y) \in [0,1] \times [0,1].
\]</span>
By considering the change of variables <span class="math inline">\((s,t) = (x+y,x/(x+y))\)</span>, which inverses to <span class="math inline">\((x,y) = (st,s(1-t))\)</span>, we can easily derive the density of the pair <span class="math inline">\((S,T)\)</span>, which takes the form
<span class="math display">\[
(s,t) = abt^{a-1}(1-t)^{b-1}s^{a+b-1}
\]</span>
for <span class="math inline">\((s,t) \in \{(x,y)|y&gt;0,0&lt;x&lt;\min\{1/y,1/(1-y)\}\}\)</span>. Thus, the density of <span class="math inline">\(T\)</span> is given by
<span class="math display">\[
t\mapsto \frac{1}{\kappa}\frac{ab}{a+b}t^{a-1}(1-t)^{b-1} \text{ for } t \in [0,1],
\]</span>
where <span class="math inline">\(\kappa\)</span> is a normalization constant. Consequently, <span class="math inline">\(T\)</span> follows a Beta distribution with parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>.</p>
</div>
<p>From this, we can derive the following algorithm:</p>
<hr />
<p><strong>Algorithm</strong>: Jöhnk Generator</p>
<hr />
<ul>
<li><strong>Repeat</strong>
<ul>
<li><span class="math inline">\(U\Leftarrow \texttt{Random}^{1/a}\)</span></li>
<li><span class="math inline">\(V\Leftarrow \texttt{Random}^{1/b}\)</span></li>
</ul></li>
<li><strong>Until</strong> <span class="math inline">\(U+V \leq 1\)</span></li>
<li><span class="math inline">\(S\leftarrow U/\left( U+V\right)\)</span>
******</li>
</ul>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="chap15.html#cb9-1" aria-hidden="true" tabindex="-1"></a>u <span class="ot">&lt;-</span> <span class="fu">runif</span>(n)<span class="sc">^</span>(<span class="dv">1</span><span class="sc">/</span>a)</span>
<span id="cb9-2"><a href="chap15.html#cb9-2" aria-hidden="true" tabindex="-1"></a>v <span class="ot">&lt;-</span> <span class="fu">runif</span>(n)<span class="sc">^</span>(<span class="dv">1</span><span class="sc">/</span>b)</span>
<span id="cb9-3"><a href="chap15.html#cb9-3" aria-hidden="true" tabindex="-1"></a>idx <span class="ot">&lt;-</span> ((u<span class="sc">+</span>v)<span class="sc">&lt;=</span><span class="dv">1</span>)</span>
<span id="cb9-4"><a href="chap15.html#cb9-4" aria-hidden="true" tabindex="-1"></a>s <span class="ot">&lt;-</span> (u<span class="sc">/</span>(u<span class="sc">+</span>v))[idx]</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:simbeta"></span>
<img src="bookdown-demo_files/figure-html/simbeta-1.png" alt="Histogram of simulated beta sample" width="768" />
<p class="caption">
Figure 16.3: Histogram of simulated beta sample
</p>
</div>
</div>
<div id="poisson-distribution" class="section level3 hasAnchor" number="16.4.6">
<h3><span class="header-section-number">16.4.6</span> Poisson Distribution<a href="chap15.html#poisson-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For several probability distributions we will discuss subsequently, probabilities generally involve calculations of factorials. If we want to simulate the number of claims in a relatively large portfolio following a Poisson distribution, the probability of having 20 claims would involve 20!, which, as a reminder, is of the order of <span class="math inline">\(10^{18}\)</span>. For some calculations, it may be necessary, to avoid zero probabilities, to approximate factorials.</p>
<div class="example">
<p><span id="exm:unlabeled-div-16" class="example"><strong>Example 16.12  </strong></span>Several relationships can be used to approximate <span class="math inline">\(n!\)</span> when <span class="math inline">\(n\)</span> is large, among which the most commonly used is Stirling’s formula:
<span class="math display">\[
\log n! \thickapprox \left( n+\frac{1}{2}\right) \log \left(n+1\right) - \left(n+1\right) + \frac{1}{2}\log \left(2\pi\right).
\]</span>
More precisely, using Binet’s series, we have the following inequalities:
<span class="math display">\[
\left( n+\frac{1}{2}\right) \log \left( n+1\right) - \left(n+1\right) + \frac{1}{2}\log \left( 2\pi \right) \leq \log n!
\]</span>
and
<span class="math display">\[
\log n! \leq \left(
n+\frac{1}{2}\right) \log \left( n+1\right) - \left(n+1\right)
+\frac{1}{2}\log \left( 2\pi \right) +\frac{1}{12\left(n+1\right) }.
\]</span></p>
</div>
</div>
<div id="poisson-distribution-1" class="section level3 hasAnchor" number="16.4.7">
<h3><span class="header-section-number">16.4.7</span> Poisson Distribution<a href="chap15.html#poisson-distribution-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In Chapter 7 of Volume 1, we saw that a Poisson process consists of durations separating two successive events that are independent and exponentially distributed with a parameter <span class="math inline">\(\lambda\)</span>. In other words, if <span class="math inline">\(\{ T_1, T_2, \ldots \}\)</span> is a sequence of independent <span class="math inline">\(\mathcal{Exp}(\lambda)\)</span> random variables, the distribution of
<span class="math display">\[\begin{eqnarray*}
N_{t} &amp;=&amp; \sum_{n\geq 1} n \mathbb{I}\big[ T_{1}+T_{2}+\ldots+T_{n} \leq t &lt; T_{1}+T_{2}+\ldots+T_{n}+T_{n+1}\big]\\
&amp;\sim&amp; \mathcal{P}oi(\lambda t).
\end{eqnarray*}\]</span>
To simulate a Poisson distribution with parameter <span class="math inline">\(\lambda\)</span>, we use the fact that <span class="math inline">\(-\log U/\lambda\)</span> follows an exponential distribution with parameter <span class="math inline">\(\lambda\)</span>. Therefore, if <span class="math inline">\(\{ U_1, U_2, \ldots \}\)</span> is a sequence of independent random variables following a <span class="math inline">\(\mathcal{U}ni(0,1)\)</span> distribution,
<span class="math display">\[\begin{equation*}
X = \sum_{n\geq 1} n \mathbb{I}\big[ U_{1}U_{2}\ldots U_{n}U_{n+1} \leq e^{-\lambda} &lt; U_{1}U_{2}\ldots U_{n}\big] \sim \mathcal{P}oi(\lambda).
\end{equation*}\]</span>
Hence, the following algorithm:</p>
<hr />
<p><strong>Algorithm</strong>: xxxxxx</p>
<hr />
<ul>
<li><span class="math inline">\(X\longleftarrow -1\)</span>, <span class="math inline">\(P\longleftarrow 1\)</span></li>
<li><strong>Repeat</strong>
<ul>
<li><span class="math inline">\(X\longleftarrow X+1\)</span>, <span class="math inline">\(Q=P\)</span></li>
<li><span class="math inline">\(P\longleftarrow P\times \mathtt{Random}\)</span></li>
</ul></li>
<li><strong>Until</strong> <span class="math inline">\(P\leq \exp \left( -\lambda \right)&lt;Q\)</span></li>
</ul>
<hr />
<div class="remark">
<p><span id="unlabeled-div-17" class="remark"><em>Remark</em>. </span>For relatively large values of <span class="math inline">\(\lambda\)</span>, it may be interesting to approximate the Poisson distribution with the normal distribution.</p>
</div>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="chap15.html#cb10-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rpois</span>(n, <span class="at">lambda =</span> <span class="dv">1</span>)</span></code></pre></div>
</div>
<div id="geometric-distribution" class="section level3 hasAnchor" number="16.4.8">
<h3><span class="header-section-number">16.4.8</span> Geometric Distribution<a href="chap15.html#geometric-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To simulate a geometric distribution with parameter <span class="math inline">\(p\)</span>, we will use the following result:</p>
<div class="lemma">
<p><span id="lem:unlabeled-div-18" class="lemma"><strong>Lemma 16.2  </strong></span>If <span class="math inline">\(X\)</span> follows an exponential distribution with parameter <span class="math inline">\(-1/\log p\)</span>, then <span class="math inline">\([X]\)</span> follows the geometric distribution with parameter <span class="math inline">\(p\)</span>, where <span class="math inline">\([\cdot]\)</span> denotes the floor function.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-19" class="proof"><em>Proof</em>. </span>Let <span class="math inline">\(Y=[X]\)</span> be the floor of <span class="math inline">\(X\)</span>, where <span class="math inline">\(X\sim\mathcal{Exp}\left(-1/\log p\right)\)</span>. Then, for <span class="math inline">\(k\in \mathbb{N}\)</span>, we can write
<span class="math display">\[\begin{eqnarray*}
\Pr[Y=k] &amp;=&amp; \Pr\left[ k\leq X&lt;k+1\right]\\
&amp;=&amp; F_X\left(k\right) - F_X\left(k+1\right)\\
&amp;=&amp; \exp \left( \left( k+1\right) \log p\right) - \exp \left( k\log p\right)\\
&amp;=&amp; \left( 1-p\right)p^{k},
\end{eqnarray*}\]</span>
which gives the desired result.</p>
</div>
<p>From this result, we derive the following algorithm:</p>
<hr />
<p><strong>Algorithm</strong>: xxxxxx</p>
<hr />
<ul>
<li>$X() $</li>
</ul>
<hr />
</div>
<div id="binomial-distribution" class="section level3 hasAnchor" number="16.4.9">
<h3><span class="header-section-number">16.4.9</span> Binomial Distribution<a href="chap15.html#binomial-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>By definition, the Binomial distribution counts the number of successes of an event with a probability <span class="math inline">\(p\)</span> of occurring in <span class="math inline">\(n\)</span> trials. The <span class="math inline">\(\mathcal{B}in(n, p)\)</span> distribution can be viewed as the sum of <span class="math inline">\(n\)</span> random variables with a <span class="math inline">\(\mathcal{B}in(1, p)\)</span> distribution. The simplest algorithm is as follows:</p>
<hr />
<p><strong>Algorithm</strong>: xxxxxx</p>
<hr />
<ul>
<li><span class="math inline">\(X\longleftarrow 0\)</span> <span class="math inline">\(\mathtt{et}\)</span> <span class="math inline">\(i\Longleftarrow 0\)</span></li>
<li><strong>Repeat</strong>
<ul>
<li><span class="math inline">\(X\longleftarrow X+\boldsymbol{1}(\mathtt{Random}\leq p)\)</span></li>
<li><span class="math inline">\(i\longleftarrow i+1\)</span></li>
</ul></li>
<li><strong>Until</strong> <span class="math inline">\(i=n\)</span></li>
</ul>
<hr />
<p>However, note that this algorithm can be relatively slow if <span class="math inline">\(n\)</span> is large (the time to obtain one realization is then proportional to <span class="math inline">\(n\)</span>). Two other algorithms can be used, thanks to the following result,</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-20" class="proposition"><strong>Proposition 16.2  (Waiting Times) </strong></span></p>
<ol style="list-style-type: decimal">
<li>Let <span class="math inline">\(G_{1},G_{2},...\)</span> be independent <span class="math inline">\(\mathcal{G}eo(p)\)</span> variables, and let <span class="math inline">\(X\)</span> be the smallest integer such that <span class="math inline">\(G_{1}+...+G_{X+1}&gt;n\)</span>. Then <span class="math inline">\(X\sim\mathcal{B}in(n,p)\)</span>.</li>
<li>Let <span class="math inline">\(E_{1},E_{2},...\)</span> be independent <span class="math inline">\(\mathcal{E}xp(1)\)</span> variables, and let <span class="math inline">\(X\)</span> be the smallest integer such that
<span class="math display">\[
\frac{E_{1}}{n}+\frac{E_{2}}{n-1}+\frac{E_{3}}{n-2}+...+\frac{E_{X+1}}{n-X}
&gt;-\log \left( 1-p\right).
\]</span>
Then <span class="math inline">\(X\sim\mathcal{B}in(n,p)\)</span>.</li>
</ol>
</div>
<div class="proof">
<p><span id="unlabeled-div-21" class="proof"><em>Proof</em>. </span>The first result can be verified directly or by noting that $G_{1} eo(
p) $ corresponds to the number of draws needed before obtaining the first success, <span class="math inline">\(G_{2}\)</span> corresponds to the additional draws needed before obtaining the second success, and so on.</p>
<p>To prove the second result, let <span class="math inline">\(X_{1},...,X_{n}\)</span> be independent random variables, all with the same $xp( ) $ distribution. Define <span class="math inline">\(X_{1:n} \leq X_{2:n } \leq \ldots \leq X_{n:n }\)</span> as the order statistics, and let <span class="math inline">\(Z_{1}=X_{1:n}\)</span>, $Z_{2}=X_{2:n }-X_{1:n} $, …, <span class="math inline">\(Z_{n}=X_{n:n }-X_{n-1:n }\)</span>. We will show that the variables <span class="math inline">\(Z_{k}\)</span> are independent and have an <span class="math inline">\(\mathcal{E}xp\left( (n-k+1) \alpha \right)\)</span> distribution.</p>
<p>The joint density of $(X_{1:n},…,X_{n:n}) $ is given by:
<span class="math display">\[\begin{equation*}
f\left( y_{1},...,y_{n}\right) =n!\alpha ^{n}\exp \left( -\alpha
\left( y_{1}+...y_{n}\right) \right)\mathbb{I}[0&lt;y_{1}&lt;...&lt;y_{n}].
\end{equation*}\]</span></p>
<p>To obtain the distribution of the vector ${}=( Z_{1},…,Z_{n})^t $, we consider an application <span class="math inline">\(h:{\mathbb{R}}^{n}\rightarrow \mathbb{R}\)</span> that is a positive Borel function, and write:
<span class="math display">\[\begin{eqnarray*}
\mathbb{E}[ h\left( \boldsymbol{Z}\right) ]
&amp;=&amp;\int_{{\mathbb{R}}^{n}}h\left( \boldsymbol{z}\right) dF_{\boldsymbol{Z}}\left( \boldsymbol{z}\right)\\
&amp;=&amp;\int_{{\mathbb{R}}^{n}}h\left(
y_{1},y_{2}-y_{1},...,y_{n}-y_{n-1}\right) f\left(
y_{1},...,y_{n}\right)
dy_{1}...dy_{n} \\
&amp;=&amp;\int_{{\mathbb{R}}^{n}}h\left(
y_{1},y_{2}-y_{1},...,y_{n}-y_{n-1}\right) \\
&amp;&amp;n!\alpha ^{n}\exp
\left( -\alpha \left[ y_{1}+...y_{n}\right] \right)
\mathbb{I}[0&lt;y_{1}&lt;...&lt;y_{n}]dy_{1}...dy_{n}.
\end{eqnarray*}\]</span>%</p>
<p>We then introduce the change of variables:
<span class="math display">\[\begin{equation*}
\left\{
\begin{array}{rcl}
y_{1} &amp; = &amp; z_{1} \\
y_{2}-y_{1} &amp; = &amp; z_{2} \\
&amp;  &amp;  \\
y_{n-1}-y_{n-2} &amp; = &amp; z_{n-1} \\
y_{n}-y_{n-1} &amp; = &amp; z_{n}%
\end{array}%
\right. \text{ or }\left\{
\begin{array}{rcl}
y_{1} &amp; = &amp; z_{1} \\
y_{2} &amp; = &amp; z_{1}+z_{2} \\
&amp;  &amp;  \\
y_{n-1} &amp; = &amp; z_{1}+z_{2}+...+z_{n-1} \\
y_{n} &amp; = &amp; z_{1}+z_{2}+...+z_{n-1}+z_{n}%
\end{array}%
\right.
\end{equation*}\]</span>%
with a Jacobian of 1. The expectation of $h( ) $ can then be written as:
<span class="math display">\[\begin{equation*}
\mathbb{E}[ h\left( \boldsymbol{Z}\right) ] =n!\alpha ^{n}\int_{{\mathbb{R}}^{n}}h\left( \boldsymbol{z}\right) \exp \left(
-\alpha \sum_{i=1}^n(n-i+1)z_i \right) dz_{1}...dz_{n}
\end{equation*}\]</span>%</p>
<p>This gives us the density of the vector <span class="math inline">\(\boldsymbol{Z}\)</span>:
<span class="math display">\[\begin{eqnarray*}
f_{ \boldsymbol{Z}}(\boldsymbol{z}) &amp;=&amp;n!\alpha ^{n}\exp
\left( -\alpha \sum_{i=1}^n(n-i+1)z_i
\right) \\
&amp;&amp;\mathbb{I}[z_{1}&gt;0,z_{2}&gt;0,...,z_{n}&gt;0] \\
&amp;=&amp;\prod_{k=1}^{n}f_{k}\left( z_{k}\right)
\end{eqnarray*}\]</span>
where
<span class="math display">\[
f_{k}\left( z_{k}\right) =\alpha \left( n-k+1\right) \exp \left(
-\left( n-k+1\right) \alpha z_{k}\right) \cdot\mathbb{I}[z_{k}&gt;0].
\]</span></p>
<p>We can observe that in this form, the <span class="math inline">\(f_{k}\)</span> functions are densities, and they correspond to the densities of <span class="math inline">\(Z_{k}\)</span>. Therefore, the variables <span class="math inline">\(Z_{k}\)</span> are independent and have an $xp(( n-k+1)) $ distribution, as claimed.</p>
<p>This allows us to conclude that
$
( E_{1:n},E_{2:n},…,E_{n:n})
$
<span class="math display">\[
=_{\text{loi}}\left( \frac{E_{1}}{n},%
\frac{E_{1}}{n}+\frac{E_{2}}{n-1},...,\frac{E_{1}}{n}+\frac{E_{2}}{n-1}+...+%
\frac{E_{n-1}}{2}+E_{n}\right) .
\]</span></p>
<p>Furthermore, the number of <span class="math inline">\(E_{i}\)</span> among the <span class="math inline">\(n\)</span> that are less than
$-( 1-p) $ follows a binomial distribution with parameters <span class="math inline">\(n\)</span> and%
<span class="math display">\[
\Pr\left[ E_{1}\leq -\log \left( 1-p\right) \right] =1-\exp \left(
\log \left( 1-p\right) \right) =p.
\]</span>%</p>
<p>This concludes the proof.</p>
</div>
<p>The simulation algorithms are as follows,</p>
<hr />
<p><strong>Algorithm</strong>: xxxxxx</p>
<hr />
<ul>
<li><span class="math inline">\(X\longleftarrow -1\)</span>, <span class="math inline">\(S\longleftarrow 0\)</span></li>
<li><strong>Repeat</strong>
<ul>
<li><span class="math inline">\(S\longleftarrow S+Int(\log(\mathtt{Random})/(1-p))\)</span></li>
<li><span class="math inline">\(X\longleftarrow X+1\)</span></li>
</ul></li>
<li><strong>Until</strong> <span class="math inline">\(S&gt;n\)</span></li>
</ul>
<hr />
<p>or, in the second case,</p>
<hr />
<p><strong>Algorithm</strong>: xxxxxx</p>
<hr />
<ul>
<li><span class="math inline">\(X\longleftarrow 0\)</span>, <span class="math inline">\(S\longleftarrow 0\)</span></li>
<li><strong>Repeat</strong>
<ul>
<li><span class="math inline">\(S\longleftarrow S-\log(\mathtt{Random})/(n-X)\)</span></li>
<li><span class="math inline">\(X\longleftarrow X+1\)</span></li>
</ul></li>
<li><strong>Until</strong> <span class="math inline">\(S&gt;-\log(1-p)\)</span></li>
</ul>
<hr />
<p>(for the latter algorithm, it is <span class="math inline">\(X-1\)</span> that will follow the
binomial distribution). These two algorithms can be relatively
efficient because the time is proportional to <span class="math inline">\(np+1\)</span>.</p>
</div>
<div id="negative-binomial-distribution" class="section level3 hasAnchor" number="16.4.10">
<h3><span class="header-section-number">16.4.10</span> Negative Binomial Distribution<a href="chap15.html#negative-binomial-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The simulation algorithms for the negative binomial distribution are as follows:</p>
<hr />
<p><strong>Algorithm</strong>: xxxxxx</p>
<hr />
<ul>
<li><span class="math inline">\(X\longleftarrow -1\)</span> and <span class="math inline">\(S\longleftarrow 0\)</span></li>
<li><strong>Repeat</strong>
<ul>
<li><span class="math inline">\(S\longleftarrow S+\text{Int}(\log(\text{Random})/(1-p))\)</span></li>
<li><span class="math inline">\(X\longleftarrow X+1\)</span></li>
</ul></li>
<li><strong>Until</strong> <span class="math inline">\(S&gt;n\)</span></li>
</ul>
<hr />
<p>Or, in the second case:</p>
<hr />
<p><strong>Algorithm</strong>: xxxxxx</p>
<hr />
<ul>
<li><span class="math inline">\(X\longleftarrow 0\)</span>, <span class="math inline">\(S\longleftarrow 0\)</span></li>
<li><strong>Repeat</strong>
<ul>
<li><span class="math inline">\(S\leftarrow S-\log(\text{Random})/(n-X)\)</span></li>
<li><span class="math inline">\(X\longleftarrow X+1\)</span></li>
</ul></li>
<li><strong>Until</strong> <span class="math inline">\(S&gt;-\log(1-p)\)</span></li>
</ul>
<hr />
<p>(In the latter algorithm, it’s <span class="math inline">\(X-1\)</span> that follows the binomial distribution.) These two algorithms can be relatively efficient because the time complexity is proportional to <span class="math inline">\(np+1\)</span>.</p>
</div>
<div id="negative-binomial-distribution-1" class="section level3 hasAnchor" number="16.4.11">
<h3><span class="header-section-number">16.4.11</span> Negative Binomial Distribution<a href="chap15.html#negative-binomial-distribution-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To simulate a negative binomial distribution, the following algorithm can be used:</p>
<hr />
<p><strong>Algorithm</strong>: xxxxxx</p>
<hr />
<ul>
<li>$Y<span class="math inline">\(\texttt{Gamma Distribution }\)</span>am(n,1)$</li>
<li>$X$ <span class="math inline">\(\mathcal{P}oi(Y/(1-p))\)</span></li>
</ul>
<hr />
<div class="example">
<p><span id="exm:unlabeled-div-22" class="example"><strong>Example 16.13  </strong></span></p>
In dimension <span class="math inline">\(2\)</span>, one can simulate a Gaussian vector as follows:
<span class="math display">\[\begin{equation*}
\left(
\begin{array}{c}
X \\
Y%
\end{array}%
\right) \sim \mathcal{N}or\left( \left(
\begin{array}{c}
\mu _{X} \\
\mu _{Y}%
\end{array}%
\right) ,\left(
\begin{array}{cc}
\sigma _{X}^{2} &amp; r\sigma _{X}\sigma _{Y} \\
r\sigma _{X}\sigma _{2} &amp; \sigma _{Y}^{2}%
\end{array}%
\right) \right) ,
\end{equation*}\]</span>
noting that the conditional distribution of <span class="math inline">\(X\)</span> given <span class="math inline">\(Y=y\)</span> is Gaussian, i.e.,
<span class="math display">\[\begin{equation*}
[ X|Y=y] \sim \mathcal{N}or\left( \mu _{X}+r\frac{\sigma _{X}}{%
\sigma _{Y}}\left( y-\mu _{Y}\right) ,\sigma _{X}^{2}-r^{2}\sigma
_{Y}^{2}\right) .
\end{equation*}\]</span>
The algorithm is as follows:
</div>
</div>
<div id="elliptical-distributions" class="section level3 hasAnchor" number="16.4.12">
<h3><span class="header-section-number">16.4.12</span> Elliptical Distributions<a href="chap15.html#elliptical-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Elliptical distributions (discussed in Section 2.6.5 of Volume 1) can be
easily simulated from their canonical representation.
Recall that a random vector <span class="math inline">\(X\)</span> follows an elliptical distribution
<span class="math inline">\(\mathcal{Ell}(g,\boldsymbol{\mu},\boldsymbol{\Sigma})\)</span> if, and only if,
<span class="math display">\[
{\boldsymbol{X}}\stackrel{\text{law}}{=} {\boldsymbol{\mu}}+R{\boldsymbol{A}}^t{\boldsymbol{U}},
\]</span>
where <span class="math inline">\({\boldsymbol{A}}^t{\boldsymbol{A}}=\mathbf{\Sigma}\)</span>, and <span class="math inline">\(R\)</span> is an independent
random variable of <span class="math inline">\(\boldsymbol{U}\)</span>, with <span class="math inline">\(\boldsymbol{U}\)</span> uniformly distributed on the unit sphere in <span class="math inline">\(\mathbb{R}^n\)</span> (see <span class="citation">(<a href="#ref-fang1990symmetric" role="doc-biblioref">Fang and Kotz 1990</a>)</span>). The law of <span class="math inline">\(R\)</span> depends on the
generator. Therefore, to simulate an elliptical vector, one simply
needs to simulate <span class="math inline">\(R\)</span>, and then simulate (independently) a uniform
distribution on the unit sphere.</p>
<div class="example">
<p><span id="exm:unlabeled-div-23" class="example"><strong>Example 16.14  </strong></span>The Gaussian vector is a particular elliptical vector, where
<span class="math inline">\(R^2\)</span> follows a chi-squared distribution. Therefore, we can use an algorithm similar to the one for simulating two independent Gaussian
variables.
To simulate a vector <span class="math inline">\(Z\)</span> uniformly distributed on the
sphere of radius <span class="math inline">\(r\)</span> in <span class="math inline">\(\mathbb{R}^d\)</span>,
<span class="math display">\[\begin{equation*}
\mathcal{S}=\left\{ \mathbf{z}\in\mathbb{R}^d|z_{1}^{2}+...+z_{d}^{2}=r^{2}\right\},
\end{equation*}\]</span>
with density given by
<span class="math display">\[\begin{equation}
f\left( \mathbf{z}\right) =\frac{\Gamma \left( d/2\right) }{\left( 2\pi
\right) ^{d/2}r^{d-1}},\qquad \mathbf{z}\in \mathcal{S},
\label{density-sphere}
\end{equation}\]</span>
we note that if <span class="math inline">\(\mathbf{X}\sim\mathcal{Nor}(\boldsymbol{0},\mathbf{I})\)</span>,
and if we let <span class="math inline">\(W=\sqrt{X_{1}^{2}+...+X_{d}^{2}}\)</span>, then the vector <span class="math inline">\(\mathbf{Z}\)</span>, where
<span class="math inline">\(Z_{i}=X_{i}/W\)</span> for <span class="math inline">\(i=1,...,d\)</span>, follows the density in Equation (<span class="math inline">\(\ref{density-sphere}\)</span>). Hence, the algorithm is as follows:</p>
<hr />
<p><strong>Algorithm</strong>: Simulating Uniform Distribution on the Sphere</p>
<hr />
<ul>
<li><span class="math inline">\(i\Longleftarrow 0\)</span> and <span class="math inline">\(W\Longleftarrow 0\)</span></li>
<li><strong>Repeat</strong>
<ul>
<li><span class="math inline">\(i\longleftarrow i+1\)</span></li>
<li><span class="math inline">\(X_{i}\Longleftarrow\cos\left( 2\pi \times \mathtt{Random}\right)\sqrt{-2\log\mathtt{Random}}\)</span></li>
<li><span class="math inline">\(W\Longleftarrow W+X_{i}\)</span></li>
</ul></li>
<li><strong>Until</strong> <span class="math inline">\(i=n\)</span></li>
</ul>
<hr />
</div>
</div>
<div id="using-copulas" class="section level3 hasAnchor" number="16.4.13">
<h3><span class="header-section-number">16.4.13</span> Using Copulas<a href="chap15.html#using-copulas" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As mentioned earlier, one method to simulate pairs of random variables is to use conditional distributions. The idea, generally attributed to <span class="citation">(<a href="#ref-rosenblatt1952remarks" role="doc-biblioref">Rosenblatt 1952</a>)</span>, is to use the following algorithm:</p>
<ol style="list-style-type: decimal">
<li>Simulate <span class="math inline">\(Y\)</span> following <span class="math inline">\(F_{Y}\)</span>.</li>
<li>Then, simulate <span class="math inline">\(X|Y\)</span> following <span class="math inline">\(F_{X|Y}\)</span>.</li>
</ol>
<p>However, it is also possible to transform the margins into pairs that can be easily simulated through transformations. The idea is to find transformations <span class="math inline">\(\phi\)</span> and <span class="math inline">\(\psi\)</span> such that the pair <span class="math inline">\(\left( \phi(X), \psi(Y)\right)\)</span> is simulatable. In particular, it is possible to generate Gaussian vectors and all families of distributions derived from them.</p>
<p>To simulate random vectors <span class="math inline">\(\left( X, Y\right)\)</span> with marginal distribution functions <span class="math inline">\(F_{X}\)</span> and <span class="math inline">\(F_{Y}\)</span>, it is possible to simulate the associated vector <span class="math inline">\(\left( U, V\right)\)</span> with copula <span class="math inline">\(C\)</span>. Then, by inverting the marginal distribution functions, <span class="math inline">\(\left( F_{X}^{-1}\left( U\right), F_{Y}^{-1}\left( V\right)\right)\)</span> will have marginal distribution functions <span class="math inline">\(F_{X}\)</span> and <span class="math inline">\(F_{Y}\)</span> and copula <span class="math inline">\(C\)</span> (the theory of copulas is discussed in detail in Section 8.6 of Volume 1).</p>
<div class="example">
<p><span id="exm:unlabeled-div-24" class="example"><strong>Example 16.15  </strong></span>To simulate <span class="math inline">\((U,V)\)</span> from the Clayton copula with parameter <span class="math inline">\(\alpha\)</span>
(defined in equation (8.22)), we use Example 8.6.8, which provides the following algorithm:</p>
<hr />
<p><strong>Algorithm</strong>: xxxx</p>
<hr />
<ul>
<li><span class="math inline">\(U\longleftarrow\mathtt{Random}\)</span> and <span class="math inline">\(T\Longleftarrow\mathtt{Random}\)</span></li>
<li><span class="math inline">\(V\longleftarrow [(T^{-\theta /(1+\theta )}-1)U^{-\theta}+1]^{-1/\theta }\)</span></li>
</ul>
<hr />
</div>
<p>This method, presented here in dimension <span class="math inline">\(2\)</span>, can
nonetheless be generalized to higher dimensions.</p>
</div>
</div>
<div id="simulation-of-stochastic-processes" class="section level2 hasAnchor" number="16.5">
<h2><span class="header-section-number">16.5</span> Simulation of Stochastic Processes<a href="chap15.html#simulation-of-stochastic-processes" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In many cases, it is not sufficient to simulate random variables, and the simulation of a process is required (for example, in Chapter 7 of Volume 1, for calculating ruin probabilities). A process corresponds to a sequence of random variables. Simulating a process is, therefore, equivalent to simulating a sequence of random variables.</p>
<div id="simulation-of-markov-chains" class="section level3 hasAnchor" number="16.5.1">
<h3><span class="header-section-number">16.5.1</span> Simulation of Markov Chains<a href="chap15.html#simulation-of-markov-chains" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Markov chains were used in Chapter 11 to model the trajectory of an insured person in a bonus-malus scale. Heuristically, a Markov chain is a sequence of random variables for which the best prediction that can be made at time <span class="math inline">\(n\)</span> for future times, given all previous values up to time <span class="math inline">\(n\)</span>, is identical to the prediction made if only the value at time <span class="math inline">\(n\)</span> is known, i.e.,
<span class="math display">\[\begin{equation*}
\left[ X_{n+1}|X_{n}=x_{n},X_{n-1}=x_{n-1},...\right]
\stackrel{\text{law}}{=}\left[ X_{n+1}|X_{n}=x_{n}\right].
\end{equation*}\]</span>
Equivalently, a Markov chain <span class="math inline">\(\{X_0,X_1,X_2,\ldots\}\)</span> is a sequence of random variables taking values in a space <span class="math inline">\(E\)</span> such that there exists a sequence <span class="math inline">\(\{U_0,U_1,U_2,\ldots\}\)</span> of independent random variables and a measurable function <span class="math inline">\(h\)</span> such that
<span class="math display">\[\begin{equation*}
X_{n+1}=h_n \left(X_{n},U_{n}\right), \text{ for all } n\in \mathbb{N}.
\end{equation*}\]</span>
In the case where this transformation does not depend on <span class="math inline">\(n\)</span>, we say that the chain is homogeneous. Simulating a trajectory of the Markov chain <span class="math inline">\(\{X_0,X_1,X_2,\ldots\}\)</span> involves, starting from a known <span class="math inline">\(X_0\)</span>, generating <span class="math inline">\(U_0,U_1,\ldots\)</span> to compute <span class="math inline">\(X_1=h(X_0,U_0)\)</span>, <span class="math inline">\(X_2=h(X_1,U_1)\)</span>, and so on.</p>
<div class="example">
<p><span id="exm:unlabeled-div-25" class="example"><strong>Example 16.16  </strong></span>Consider the case of a random walk. Let <span class="math inline">\(\{U_0,U_1,U_2,\ldots\}\)</span> be a sequence of independent random variables, and define the sequence <span class="math inline">\(\{X_0,X_1,X_2,\ldots\}\)</span> of variables by
<span class="math display">\[\begin{equation*}
X_{n+1}=X_{n}+U_{n},\hspace{3mm} n\in \mathbb{N}.
\end{equation*}\]</span>
A special case is when <span class="math inline">\(X_{0}\)</span> is an integer, and <span class="math inline">\(\Pr\left[ U_{n}=-1\right] =\Pr\left[ U_{n}=+1\right] =1/2\)</span>, corresponding to the coin toss game. Figure <span class="math inline">\(\ref{FIG-brownien2}\)</span> shows a trajectory of such a process in 2D. Similarly, Figure <span class="math inline">\(\ref{MA-Sphere}\)</span> illustrates the evolution of a random walk on the unit sphere in <span class="math inline">\(\mathbb{R}^{3}\)</span>.</p>
</div>
</div>
<div id="simulation-of-a-poisson-process" class="section level3 hasAnchor" number="16.5.2">
<h3><span class="header-section-number">16.5.2</span> Simulation of a Poisson Process<a href="chap15.html#simulation-of-a-poisson-process" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Continuous-time processes are difficult to simulate because from a numerical point of view, continuous time does not exist. However, simulating a Poisson process is a special case. In general, consider a Poisson process with intensity function $( t) $ (Section 7.3.1 of Volume 1 presented the main results concerning these processes). The idea is to simulate <span class="math inline">\(T_1\)</span>, the time of the first event, and then iteratively calculate the distribution of the time between the <span class="math inline">\((i+1)\)</span>-th and <span class="math inline">\(i\)</span>-th events, given that the <span class="math inline">\(i\)</span>-th event occurred at time <span class="math inline">\(T_i\)</span>. For this purpose, recall that for <span class="math inline">\(s&lt;t\)</span>, <span class="math inline">\(N_t-N_s\)</span> follows a Poisson distribution with parameter <span class="math inline">\(\Lambda(t)-\Lambda(s)\)</span>, where
<span class="math display">\[
\Lambda(t)=\int_{[0,t]}\lambda(s)ds.
\]</span></p>
<div class="example">
<p><span id="exm:unlabeled-div-26" class="example"><strong>Example 16.17  </strong></span>Consider a Poisson process with intensity function $( t) =1/( t+a) $ for <span class="math inline">\(t\geq 0\)</span>, where <span class="math inline">\(a\)</span> is a positive constant. Then,
<span class="math display">\[\begin{equation*}
\int_{0}^{x}\lambda \left( s+y\right) dy=\log \left( \frac{x+s+a}{s+a}
\right).
\end{equation*}\]</span>
The cumulative distribution function of the time between two successive events, given that an event occurred at time <span class="math inline">\(s\)</span>, is
<span class="math display">\[
\Pr\left[ \text{Time to Next Event}\leq x|\text{Event at }%
s\right]
\]</span>
<span class="math display">\[
=\frac{x}{x+s+a}
\]</span>
which can be easily inverted. The simulation algorithm for such a process is as follows:</p>
<hr />
<p><strong>Algorithm</strong>: xxxx</p>
<hr />
<ul>
<li>$U<span class="math inline">\(\texttt{Random and }\)</span>i=1$</li>
<li><span class="math inline">\(T_{1}\Longleftarrow a\times U/(1-U)\)</span></li>
<li><strong>Repeat</strong> <span class="math inline">\(i\longleftarrow i+1\)</span>
<ul>
<li><span class="math inline">\(U\longleftarrow \texttt{Random}\)</span></li>
<li><span class="math inline">\(T_{i}\longleftarrow \left(T_{i-1}+a\right) \times U/(1-U)\)</span></li>
</ul></li>
</ul>
<hr />
</div>
</div>
<div id="calculating-ruin-probability-through-simulation" class="section level3 hasAnchor" number="16.5.3">
<h3><span class="header-section-number">16.5.3</span> Calculating Ruin Probability through Simulation<a href="chap15.html#calculating-ruin-probability-through-simulation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="the-problem-1" class="section level4 hasAnchor" number="16.5.3.1">
<h4><span class="header-section-number">16.5.3.1</span> The Problem<a href="chap15.html#the-problem-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In Chapter 7, we presented classical results related to the calculation of ruin probability. Let’s revisit this problem and focus on the process corresponding to the company’s result:
<span class="math display">\[
R_t=\kappa+ct-S_t=\kappa+ct-\sum_{i=1}^{N_t}X_i,
\]</span>
where <span class="math inline">\(c\)</span> is the premium rate, and <span class="math inline">\(\kappa\)</span> is an initial capital. We assume that <span class="math inline">\(S_t\sim\mathcal{CP}oi(\lambda t,F)\)</span>. We will assume that <span class="math inline">\(F\)</span> has a finite mean <span class="math inline">\(\mu\)</span>. Figure <span class="math inline">\(\ref{FIG-simul-ruine}\)</span> presents simulations of <span class="math inline">\(6\)</span> trajectories of such a process over <span class="math inline">\(25\)</span> years.</p>
<p>FIG</p>
<p>The ruin time of the insurance company is defined as the stopping time:
<span class="math display">\[
T=\inf\{t\geq0| R_t&lt;0\}=\inf\{t\geq0| S_t&gt;\kappa+ct\}.
\]</span>
We define the probability of ultimate ruin as:
<span class="math display">\[
\psi(\kappa)=\Pr[T&lt;\infty|R_0=\kappa],
\]</span>
and the finite-time ruin probability (<span class="math inline">\(\tau\)</span>) as:
<span class="math display">\[
\psi(\kappa,\tau)=\Pr[T&lt;\tau|R_0=\kappa].
\]</span>
Apart from a few simple cases where these probabilities can be calculated without much difficulty, their evaluation is generally challenging. We show here how to estimate them through simulation.</p>
<div id="standard-monte-carlo-method-for-finite-time-ruin-probability" class="section level5 hasAnchor" number="16.5.3.1.1">
<h5><span class="header-section-number">16.5.3.1.1</span> Standard Monte Carlo Method for Finite-Time Ruin Probability<a href="chap15.html#standard-monte-carlo-method-for-finite-time-ruin-probability" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>For such a process, note that ruin can only occur at an event occurrence time (see Section 7.3.4 for a connection with de Finetti’s discrete model). In particular, we have seen that
<span class="math display">\[
\psi(\kappa)=\Pr\left[\sum_{j=0}^i\Delta_j&gt;\kappa \text{ for some }i\Big|R_0=\kappa\right],
\]</span>
where the variables <span class="math inline">\(\Delta_1,\Delta_2,...\)</span>, defined by
<span class="math display">\[
\Delta_j=X_j-c(T_j-T_{j-1}),
\]</span>
are independent and identically distributed. It is sufficient to simulate the claim amounts <span class="math inline">\(X_j\)</span> and the event occurrence times <span class="math inline">\(T_j\)</span>. Since we cannot simulate an infinite number of variables, we will focus on the finite-time ruin probability <span class="math inline">\(\tau\)</span> by simulating event occurrence times <span class="math inline">\(T_j\)</span> until <span class="math inline">\(T_j&lt;\tau\)</span>. We then simulate a large number of variables <span class="math inline">\(Z=\mathbb{I}[T&gt;\tau]\)</span> using the following algorithm:</p>
<hr />
<p><strong>Algorithm</strong>: xxxx</p>
<hr />
<ul>
<li><span class="math inline">\(T\Longleftarrow 0\)</span>,</li>
<li><span class="math inline">\(S\Longleftarrow 0\)</span>,</li>
<li><span class="math inline">\(Z\Longleftarrow 0\)</span></li>
<li><strong>Repeat</strong> <span class="math inline">\(i\longleftarrow i+1\)</span>
<ul>
<li><span class="math inline">\(U\Longleftarrow T-\log\left(\mathtt{Random}\right) /\lambda\)</span></li>
<li><span class="math inline">\(X\longleftarrow\texttt{cost distribution}\)</span></li>
<li><span class="math inline">\(D\longleftarrow X-c(T_j-T_{j-1})\)</span>,</li>
<li><span class="math inline">\(S\longleftarrow S+D\)</span></li>
<li><span class="math inline">\(T\longleftarrow U\)</span></li>
<li><strong>If</strong> <span class="math inline">\(S&gt;\kappa\)</span> <strong>Then</strong> <span class="math inline">\(Z\longleftarrow1\)</span></li>
</ul></li>
<li><strong>Until</strong> <span class="math inline">\(T\geq\tau\)</span> or <span class="math inline">\(Z=1\)</span></li>
</ul>
<hr />
</div>
<div id="using-the-pollaczeck-khinchine-beekman-formula-for-infinite-time-ruin-probability" class="section level5 hasAnchor" number="16.5.3.1.2">
<h5><span class="header-section-number">16.5.3.1.2</span> Using the Pollaczeck-Khinchine-Beekman Formula for Infinite-Time Ruin Probability<a href="chap15.html#using-the-pollaczeck-khinchine-beekman-formula-for-infinite-time-ruin-probability" class="anchor-section" aria-label="Anchor link to header"></a></h5>
In Remark 7.3.15, we saw that the infinite-time ruin probability can be expressed as
<span class="math display">\[
\psi(\kappa)=\Pr[M&gt;\kappa], \text{ where }M=\max_{t\geq0}\{S_t-ct\}=L_1+...+L_K,
\]</span>
where <span class="math inline">\(K\sim\mathcal{Geo}(q)\)</span>, with <span class="math inline">\(q=1-\psi(0)=1-\lambda\mu/c\)</span>, and the variables <span class="math inline">\(L_i\)</span> are defined iteratively. It can be shown that these variables are independent, and the distribution of <span class="math inline">\(L_i\)</span> is <span class="math inline">\(\tilde{F}\)</span>, defined as in Theorem 7.3.13 by
<span class="math display">\[
\tilde{F}(x)=\int_{y=0}^{x}\frac{\overline{F}(y)}{\mu}dy, \hspace{3mm}x&gt;0.
\]</span>
It is then possible to simulate a large number of variables <span class="math inline">\(Z=\mathbb{I}[T&gt;\tau]\)</span> using the following algorithm:
<p>While this method offers a relatively elegant algorithm for approaching infinite horizon scenarios, it is worth noting that the algorithm can be relatively inefficient for large values of <span class="math inline">\(\kappa\)</span>.</p>
</div>
</div>
</div>
</div>
<div id="monte-carlo-via-markov-chains" class="section level2 hasAnchor" number="16.6">
<h2><span class="header-section-number">16.6</span> Monte Carlo via Markov Chains<a href="chap15.html#monte-carlo-via-markov-chains" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="principle-11" class="section level3 hasAnchor" number="16.6.1">
<h3><span class="header-section-number">16.6.1</span> Principle<a href="chap15.html#principle-11" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Similar to Chapter 11, we consider a finite state space <span class="math inline">\(E\)</span>, which we will assume to be <span class="math inline">\(\left\{ 1,2,...,N\right\}\)</span>. The process <span class="math inline">\(\{X_1,X_2,\ldots\}\)</span> is a Markov chain if the equality
<span class="math display">\[
\Pr\left[ X_{n+1}=j|X_{n}=i,X_{n-1}=x_{n-1},...,X_{1}=x_{1}\right]
\]</span>
<span class="math display">\[
=\Pr\left[ X_{n+1}=j|X_{n}=i\right],
\]</span>
holds for all <span class="math inline">\(i,j\)</span>. The Markov chain is characterized by its transition matrices <span class="math inline">\(\boldsymbol{Q}_n\)</span>, of dimension <span class="math inline">\(N\times N\)</span>, such that
<span class="math display">\[\begin{equation*}
Q_{n}\left( i,j\right) =\Pr\left[ X_{n+1}=j|X_{n}=i\right]
\end{equation*}\]</span>
and its initial distribution <span class="math inline">\(\mu \left( i\right) =\Pr\left[ X_{0}=i\right]\)</span>. Here, we only consider the case of homogeneous Markov chains, i.e., the transition matrix is time-invariant, i.e., $Q_{n}( i,j) =Q( i,j) $ for all <span class="math inline">\(n\)</span>.</p>
<p>To simulate a Markov chain, we set the initial value to <span class="math inline">\(X_{0}=i\)</span>, then use the following algorithm:</p>
<hr />
<p><strong>Algorithm</strong>: xxxx</p>
<hr />
<ul>
<li><span class="math inline">\(n\Longleftarrow 0\)</span></li>
<li><span class="math inline">\(X_{0}\Longleftarrow i\)</span> (initial state)
<ul>
<li><strong>Repeat</strong></li>
<li><span class="math inline">\(n\longleftarrow n+1\)</span></li>
<li><span class="math inline">\(p\longleftarrow 0,\)</span> <span class="math inline">\(j=1,1\)</span></li>
<li><span class="math inline">\(U\longleftarrow \mathtt{Random}\)</span>
<ul>
<li><strong>Repeat</strong></li>
<li>$pp+Q(i,j) $</li>
<li><span class="math inline">\(j\longleftarrow j+1\)</span></li>
</ul></li>
<li><strong>Until</strong> <span class="math inline">\(U&gt;p\)</span></li>
<li><span class="math inline">\(X_{n}\longleftarrow j-1\)</span></li>
</ul></li>
</ul>
<hr />
<p>The distribution of <span class="math inline">\(X_{n}\)</span> is then given by
<span class="math display">\[\begin{equation*}
\Pr\left[ X_{n}=j\right] =\sum_{i=1}^{N}\mu _{i}Q^{n}\left(
i,j\right)
\end{equation*}\]</span>
where <span class="math inline">\(\mu_i=\Pr[X_0=i]\)</span>, and <span class="math inline">\(\boldsymbol{Q}^{n}\)</span> denotes the <span class="math inline">\(n\)</span>th matrix power of <span class="math inline">\(\boldsymbol{Q}\)</span>, which satisfies the Chapman-Kolmogorov equation:
<span class="math display">\[\begin{equation*}
Q^{n+1}\left( i,j\right) =\sum_{k=1}^{N}Q\left( i,k\right)
Q^{n}\left( k,j\right).
\end{equation*}\]</span></p>
<p>A probability distribution <span class="math inline">\(\boldsymbol{\mu}\)</span> (represented by the vector <span class="math inline">\((\mu_1,\ldots,\mu_N)^t\)</span> of probability masses assigned to <span class="math inline">\(1,\ldots,N\)</span>) satisfying <span class="math inline">\(\boldsymbol{\mu}=\boldsymbol{\mu}\boldsymbol{Q}\)</span> is called an invariant distribution associated with the Markov chain. Therefore, to simulate a random variable with distribution <span class="math inline">\(\boldsymbol{\mu}\)</span>, one can look for a family of Markov chains with an invariant measure <span class="math inline">\(\boldsymbol{\mu}\)</span> (because under certain conditions, the distribution of <span class="math inline">\(X_{n}\)</span> “converges” to <span class="math inline">\(\boldsymbol{\mu}\)</span>). Among the conditions allowing this convergence, we will retain the following two notions:</p>
<ol style="list-style-type: decimal">
<li>A Markov chain is called <strong>irreducible</strong> if one can go from any point in <span class="math inline">\(E\)</span> to any other point in <span class="math inline">\(E\)</span> with a non-zero probability, i.e., there exists <span class="math inline">\(n\)</span> such that
<span class="math display">\[
\Pr[X_{0}=i,X_{n}=j] =Q^{n}\left( i,j\right) &gt;0
\]</span>
for all <span class="math inline">\(i,j\)</span>.</li>
<li>A Markov chain is called <strong>aperiodic</strong> if there exists a state <span class="math inline">\(j\)</span> and an integer <span class="math inline">\(n\)</span> such that
<span class="math inline">\(Q^{n}\left( j,j\right) &gt;0\)</span> and <span class="math inline">\(Q^{n+1}\left( j,j\right) &gt;0\)</span>.
\end{enumerate}</li>
</ol>
<p>In the case where the state space <span class="math inline">\(E\)</span> is finite, and if the Markov chain is irreducible and aperiodic, with an invariant measure <span class="math inline">\(\boldsymbol{\mu}\)</span>, then there exist <span class="math inline">\(M\)</span> and <span class="math inline">\(c\)</span> such that
<span class="math display">\[\begin{equation*}
\left| Q^{n}\left( i,j\right) -\mu_j \right| \leq
M\exp \left( -cn\right) .
\end{equation*}\]</span></p>
<p>In this case, there is an exponential convergence rate towards the limit distribution.</p>
</div>
<div id="Ergodicity" class="section level3 hasAnchor" number="16.6.2">
<h3><span class="header-section-number">16.6.2</span> Some Notions of Ergodic Theory<a href="chap15.html#Ergodicity" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The central idea behind using Markov chains for Monte Carlo methods is to start from an initial value <span class="math inline">\(X_{0}\)</span> and then generate a Markov chain whose limiting distribution is <span class="math inline">\(f\)</span>, the density we want to simulate. After a certain number <span class="math inline">\(T\)</span> of iterations, we can assume that we have reached a steady state, and we can then consider that <span class="math inline">\(X_{T}\)</span> has density <span class="math inline">\(f\)</span>. A <em>non-independent</em> sample is then obtained by considering <span class="math inline">\(X_{T+1},...,X_{T+n}\)</span>. This lack of independence is generally not a problem for the evaluation of integrals. All that is required is to simulate a sequence of stationary and ergodic variables.</p>
<div class="definition">
<p><span id="def:unlabeled-div-27" class="definition"><strong>Definition 16.1  </strong></span>A sequence of random variables <span class="math inline">\(X_1,X_2,...\)</span> is called stationary if <span class="math inline">\((X_1,...,X_k)=_{\text{law}}(X_{1+h},...,X_{k+h})\)</span> for all <span class="math inline">\(k,h\geq 0\)</span>. Such a sequence is called ergodic if, and only if, for any Borel set <span class="math inline">\(B\)</span> and any <span class="math inline">\(k\in\mathbb{N}\)</span>,
<span class="math display">\[
\Pr\left[\frac{1}{n}\sum_{t=1}^n
\mathbb{I}_B(X_t,X_{t+1},...,X_{t+k})\rightarrow
\Pr[(X_t,X_{t+1},...,X_{t+k})\in B]\right]=1.
\]</span></p>
</div>
<div id="recurrence" class="section level5 hasAnchor" number="16.6.2.0.1">
<h5><span class="header-section-number">16.6.2.0.1</span> Recurrence<a href="chap15.html#recurrence" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>In practice, if <span class="math inline">\(\{X_1,X_2,\ldots\}\)</span> is a Markov chain, several results can be obtained. Among the central concepts, recall the notion of recurrence. For a finite state space, if <span class="math inline">\(N(x)\)</span> is the number of visits of the chain to state <span class="math inline">\(x\)</span>, then state <span class="math inline">\(x\)</span> is said to be recurrent if
<span class="math display">\[
\mathbb{E}[N(x)]=\mathbb{E}\left[\sum_{t=1}^\infty
\mathbb{I}[X_t=x]\right]=\infty.
\]</span>
If the expectation is finite, it is referred to as a transient state.</p>
<p>In the continuous case, we introduce the notion of Harris-recurrent set (or Harris-recurrent in the sense of Harris). By denoting <span class="math inline">\(N(A)\)</span> as the number of visits to the set <span class="math inline">\(A\)</span>, the set <span class="math inline">\(A\)</span> is said to be Harris-recurrent if
<span class="math display">\[
\Pr[N(A)=\infty]=\Pr\left[\sum_{t=1}^\infty \mathbb{I}[X_t\inA]\right]
=1.
\]</span>
The Markov chain will be called Harris-recurrent if every set <span class="math inline">\(A\)</span> is Harris-recurrent.</p>
<p>From this notion, we can introduce a law of large numbers for non-independent series. A rigorous proof of this result can be found in <span class="citation">(<a href="#ref-robert1996methodes" role="doc-biblioref">Robert 1996</a>)</span>.</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-28" class="proposition"><strong>Proposition 16.3  </strong></span>If a Markov chain <span class="math inline">\((X_t)\)</span> is Harris-recurrent, then for any <span class="math inline">\(h\)</span> (such that the expectation exists),
<span class="math display">\[
\lim_{n\rightarrow\infty}\frac{1}{n}\sum_{i=1}^nh(X_i)=\int
h(x)dF(x),
\]</span>
where <span class="math inline">\(F\)</span> denotes the cumulative distribution function of the invariant distribution associated with the chain.</p>
</div>
</div>
</div>
<div id="simulation-of-an-invariant-measure-hastings-metropolis-algorithm" class="section level3 hasAnchor" number="16.6.3">
<h3><span class="header-section-number">16.6.3</span> Simulation of an Invariant Measure: Hastings-Metropolis Algorithm<a href="chap15.html#simulation-of-an-invariant-measure-hastings-metropolis-algorithm" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Among the Monte Carlo methods by Markov Chain (MCMC), we note the Hastings-Metropolis algorithm, which allows us to simulate an invariant distribution $$. Here, we assume that for every state <span class="math inline">\(j\)</span>, <span class="math inline">\(\mu_j &gt;0\)</span>.</p>
<p>The Hastings-Metropolis algorithm uses an instrumental distribution $q( y|x) $ to simulate the target density <span class="math inline">\(f\)</span> (referred to as the target density). The iterative algorithm is as follows: starting from <span class="math inline">\(X_{t}\)</span>, we simulate <span class="math inline">\(Y_{t}\)</span> following the distribution $q( |X_{t}) $, and then we set%
<span class="math display">\[
X_{t+1}=\left\{
\begin{array}{ll}
Y_{t}, &amp; \text{with probability }\pi \left( X_{t},Y_{t}\right) ,\\
X_{t}, &amp; \text{with probability }1-\pi \left( X_{t},Y_{t}\right) ,%
\end{array}%
\right.
\]</span>%
where
<span class="math display">\[
\pi \left( x,y\right) =\min \left\{ \frac{f\left( y\right)
}{f\left( x\right) }\frac{q\left( x|y\right) }{q\left( y|x\right)
},1\right\} .
\]</span>%
It is then possible to show that the Markov chain is reversible, i.e., it satisfies <span class="math inline">\([X_{t+1}|X_{t+2}]=_{\text{law}}[X_{t+1}|X_t]\)</span>, indicating that the direction of time has no influence, if <span class="math inline">\(f\)</span> satisfies the condition known as “detailed balance,” i.e., there exists $K( ,) $ such that for all <span class="math inline">\(x,y\)</span>,%
<span class="math display">\[
f\left( y\right) K\left( y,x\right) =f\left( x\right) K\left(
x,y\right) .
\]</span>
Under this assumption, for any function <span class="math inline">\(h\)</span> such that
$[| h(X) |] &lt;$, then%
<span class="math display">\[
\lim_{t\rightarrow \infty }\frac{1}{t}\sum_{i=1}^{t}h\left(
X_{i}\right) =\int h\left( x\right) f\left( x\right)dx.
\]</span></p>
<p>The simplest case is when <span class="math inline">\(q\)</span> is independent of <span class="math inline">\(x\)</span>. Starting from <span class="math inline">\(X_{t}\)</span>, we simulate <span class="math inline">\(Y_{t}\)</span> following the distribution $q( ) $, and then we set%
<span class="math display">\[
X_{t+1}=\left\{
\begin{array}{ll}
Y_{t}, &amp; \text{with probability }\min \left\{ f\left(
Y_{t}\right) q\left( X_{t}\right) /f\left( X_{t}\right) q\left(
Y_{t}\right) ,1\right\},
\\
X_{t} ,&amp; \text{otherwise.}%
\end{array}%
\right.
\]</span>%
Here, we encounter an algorithm similar to the rejection method presented in Section <span class="math inline">\(\ref{Section-rejection}\)</span>.</p>
<div class="example">
<p><span id="exm:unlabeled-div-29" class="example"><strong>Example 16.18  </strong></span>To simulate a Gamma distribution with parameters $$ and $$, where $$ is not an integer, it is possible to combine a rejection method with a simulation of a Gamma distribution with integer parameters. Denoting <span class="math inline">\(a=[ \alpha ]\)</span> as the integer part of <span class="math inline">\(\alpha\)</span>, we consider the following algorithm:</p>
<hr />
<p><strong>Algorithm</strong>: xxxx</p>
<hr />
<ul>
<li><span class="math inline">\(Y\Longleftarrow Gam\left( a,a/\alpha \right) ,\)</span></li>
<li> <span class="math inline">\(X\Longleftarrow Y\)</span> with probability
<span class="math display">\[
\pi
=\big( eY\exp \left( -Y/\alpha \right) /\alpha \big) ^{\alpha
-a}.
\]</span></li>
</ul>
<hr />
<p>The Hastings-Metropolis algorithm can be written as follows, starting
from <span class="math inline">\(X_{t}\)</span>:</p>
<hr />
<p><strong>Algorithm</strong>: xxxx</p>
<hr />
<ul>
<li><span class="math inline">\(Y_{t}\Longleftarrow Gam\left( a,a/\alpha \right) ,\)</span></li>
<li> <span class="math inline">\(X_{t+1}\Longleftarrow Y_{t}\)</span> with probability
<span class="math display">\[
\pi =%
\big( Y_{t}\exp \left( \left( X_{t}-Y_{t}\right) \right) /\alpha X_{t}%
\big) ^{\alpha -a},
\]</span>
and <span class="math inline">\(X_{t+1}\Longleftarrow X_{t}\)</span> otherwise<span class="math inline">\(.\)</span></li>
</ul>
<hr />
</div>
</div>
</div>
<div id="variance-reduction" class="section level2 hasAnchor" number="16.7">
<h2><span class="header-section-number">16.7</span> Variance Reduction<a href="chap15.html#variance-reduction" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As we have seen in the introduction, the convergence rate of Monte Carlo methods is <span class="math inline">\(\sigma /\sqrt{n}\)</span> (Central Limit Theorem). To improve this speed, one can aim to “reduce the variance,” i.e., decrease the value of <span class="math inline">\(\sigma ^{2}\)</span>. Numerous methods exist, with the general idea being to use another representation - in the form of a mathematical expectation - of the quantity to be calculated.</p>
<div id="use-of-antithetic-variables" class="section level3 hasAnchor" number="16.7.1">
<h3><span class="header-section-number">16.7.1</span> Use of Antithetic Variables<a href="chap15.html#use-of-antithetic-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Suppose we want to compute $
=$ where <span class="math inline">\(U\sim\mathcal{U}ni(0,1)\)</span>. Using
<span class="math inline">\(n\)</span> independent uniform variables <span class="math inline">\(U_{1},...,U_{n}\)</span>, the standard Monte Carlo method approximates $$ as%
<span class="math display">\[\begin{equation*}
\theta \approx \frac{1}{n}\Big( h\left( U_{1}\right)
+...+h\left( U_{n}\right) \Big) =\widehat{\theta },
\end{equation*}\]</span>%
but considering the relationship
<span class="math display">\[\begin{equation}
\int_{0}^{1}h\left( x\right) dx=\frac{1}{2}\int_{0}^{1}\big(h\left( x\right) +h\left( 1-x\right) \big) dx,  \label{relation
antithetique}
\end{equation}\]</span>
we can also approximate $$ using the relation%
<span class="math display">\[\begin{equation*}
\theta \approx \frac{1}{2n}\Big( h\left( U_{1}\right) +h\left(
1-U_{1}\right) +...+h\left( U_{n}\right) +h\left( 1-U_{n}\right) \Big) =%
\widehat{\theta }_{A}.
\end{equation*}\]</span>%
If we denote $X_{i}=h( U_{i}) $ and $Y_{i}=h(
1-U_{i}) $,
then%
<span class="math display">\[\begin{equation*}
\mathbb{V}\left[ \widehat{\theta }\right] =\mathbb{V}\left[ \frac{1}{n}\left(
h\left( U_{1}\right) +...+h\left( U_{n}\right) \right) \right]
=\frac{\mathbb{V}\left[ X\right] }{n}
\end{equation*}\]</span>
while
<span class="math display">\[\begin{eqnarray*}
\mathbb{V}\left[ \widehat{\theta }_{A}\right]  &amp;=&amp;\mathbb{V}\left[
\frac{1}{2n}\left( h\left( U_{1}\right) +h\left( 1-U_{1}\right)
+...+h\left( U_{n}\right)
+h\left( 1-U_{n}\right) \right) \right]  \\
&amp;=&amp;\mathbb{V}\left[ \widehat{\theta }\right] +\frac{\mathbb{C}\left[ X_1,Y_1\right]}{n}.
\end{eqnarray*}\]</span>%
In other words, $&lt;$ if and only if,
<span class="math display">\[
\mathbb{C}\left[ h\left( U\right) ,h\left( 1-U\right) \right] &lt;0.
\]</span>
This condition is satisfied in particular if <span class="math inline">\(h\)</span> is
a monotonic function since <span class="math inline">\(U\)</span> and <span class="math inline">\(1-U\)</span> are then negatively dependent by quadrant (as we saw in Section 8.5). Therefore, if <span class="math inline">\(h\)</span> is a monotonic function, the quality of the approximation is improved.</p>
<p>In general, $=$ can be rewritten as
<span class="math display">\[
\theta
=\mathbb{E}\left[ h\left( F_X^{-1}\left( U\right) \right) \right]
=\mathbb{E}\left[ h^{\ast }\left( U\right) \right]\text{ where }h^*=h\circ F_X^{-1},
\]</span>
which allows us to reduce it to the uniform case.</p>
<div class="example">
<p><span id="exm:unlabeled-div-30" class="example"><strong>Example 16.19  </strong></span>
Consider the textbook example (in the sense that the value of $$ is known and equals <span class="math inline">\(e-1\)</span>) where we want to estimate <span class="math inline">\(\theta =\mathbb{E}[\exp U]\)</span> with <span class="math inline">\(U\sim\mathcal{U}ni(0,1)\)</span>. Since the function
$uh(u) $ is increasing, the use of antithetic variables
will reduce the variance. Note that%
<span class="math display">\[\begin{eqnarray*}
\mathbb{C}[\exp U,\exp \left( 1-U\right)] &amp;=&amp;\mathbb{E}[\exp U\exp \left( 1-U%
\right)] \\
&amp;&amp;-\mathbb{E}[\exp U]\mathbb{E}[\exp \left( 1-U\right)] \\
&amp;=&amp;e-(e-1)^{2}.
\end{eqnarray*}\]</span>%
Now, the variance of <span class="math inline">\(\exp U\)</span> is%
<span class="math display">\[\begin{equation*}
\mathbb{V}[\exp U]=\frac{e^{2}-1}{2}-(e-1)^{2},
\end{equation*}\]</span>%
so we can deduce that using a standard simulation scheme leads to%
<span class="math display">\[\begin{equation*}
\mathbb{V}\left[ \exp U\right] \approx 0.2420,
\end{equation*}\]</span>%
while using antithetic variables results in%
<span class="math display">\[\begin{eqnarray*}
\mathbb{V}\left[ \frac{\exp U+\exp \left( 1-U\right) }{2}\right] &amp;=&amp;\frac{%
\mathbb{V}[\exp U]}{2}+\frac{\mathbb{C}[\exp U,\exp \left( 1-U\right)]}{2} \\
&amp;\approx &amp;0.0039,
\end{eqnarray*}\]</span>
meaning that with the same number of simulations, the variance obtained using antithetic variables is only 1.6% of the variance obtained by a standard Monte Carlo method. The gain in convergence speed can be visualized in Figure
<span class="math inline">\(\ref{FIG-antithetic}\)</span>.</p>
</div>
<p>FIG</p>
</div>
<div id="use-of-control-variates" class="section level3 hasAnchor" number="16.7.2">
<h3><span class="header-section-number">16.7.2</span> Use of Control Variates<a href="chap15.html#use-of-control-variates" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If $$ (known) represents the mean of the control variable <span class="math inline">\(Y\)</span>, $X+c( Y-) $ is an unbiased estimator of $=%
$, and we aim to minimize its variance. It can be noted that the variance is minimized for%
<span class="math display">\[\begin{equation*}
c^{\ast }=-\frac{\mathbb{C}\left[ X,Y\right] }{\mathbb{V}\left[ Y\right]},
\end{equation*}\]</span>%
and the variance of $ _{C}=X+c^{}( Y-) $ is then%
<span class="math display">\[\begin{equation}
\mathbb{V}\left[ \widehat{\theta }_{C}\right] =\mathbb{V}[ X] -\frac{%
\mathbb{C}\left[ X,Y\right] ^{2}}{\mathbb{V}\left[ Y\right] }
\label{VARIANCE-control}
\end{equation}\]</span></p>
<div class="example">
<p><span id="exm:unlabeled-div-31" class="example"><strong>Example 16.20  </strong></span>Taking Example <span class="math inline">\(\ref{exempleantithetique}\)</span> where the goal was to estimate <span class="math inline">\(\theta =\mathbb{E}[\exp U]\)</span>, it can be noted that the use of control variables also reduces the variance.
Indeed, consider that%
<span class="math display">\[\begin{eqnarray*}
\mathbb{C}\left[ \exp U,U\right]  &amp;=&amp;\mathbb{E}[U\exp U]-\mathbb{E}[\exp U]\mathbb{E%
}[U] \\
&amp;=&amp;1-\frac{e-1}{2}.
\end{eqnarray*}\]</span>%
Equation <span class="math inline">\((\ref{VARIANCE-control})\)</span> then becomes, for
<span class="math inline">\(X=\exp U\)</span> and $Y=U
$%
<span class="math display">\[\begin{eqnarray*}
\mathbb{V}\left[ \widehat{\theta }_{C}\right]  &amp;=&amp;\mathbb{V}\left[ X\right] -\frac{%
\mathbb{C}\left[ \exp U,U\right] ^{2}}{\mathbb{V}\left[ U\right] } \\
&amp;=&amp;\frac{e^{2}-1}{2}-(e-1)^{2}-12\left( 1-\frac{e-1}{2}\right) ^{3} \\
&amp;\approx &amp;0,0039.
\end{eqnarray*}\]</span>%
This results in a variance reduction of the same order of magnitude as when using antithetic variables.</p>
</div>
</div>
<div id="use-of-conditioning" class="section level3 hasAnchor" number="16.7.3">
<h3><span class="header-section-number">16.7.3</span> Use of Conditioning<a href="chap15.html#use-of-conditioning" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Suppose we want to calculate $=[ h(X)] $, and let $Y=h( X) $. Let <span class="math inline">\(\mathbf{Z}\)</span> be a random vector, and define $V==g() $.
Given the relationship%
<span class="math display">\[\begin{equation*}
\mathbb{E}\left[ V\right] =\mathbb{E}\Big[ \mathbb{E}\left[
Y|\mathbf{Z}\right] \Big] =\mathbb{E}\left[ Y\right] \text{,}
\end{equation*}\]</span>%
we can estimate $=$ by
simulating realizations of <span class="math inline">\(V\)</span>. However, for this method to effectively reduce the variance, we need to compare the variances of <span class="math inline">\(Y\)</span> and <span class="math inline">\(V\)</span>. Using the variance decomposition formula, we see that%
<span class="math display">\[\begin{eqnarray*}
\mathbb{V}[ Y]  &amp;=&amp;\mathbb{E}\Big[ \mathbb{V}\left[ Y|\mathbf{Z}\right]
\Big]
+\mathbb{V}\Big[ \mathbb{E}\left[ Y|\mathbf{Z}\right] \Big]  \\
&amp;\geq &amp;\mathbb{V}\Big[ \mathbb{E}\left[ Y|\mathbf{Z}\right] \Big] =\mathbb{V}\left[
V\right] .
\end{eqnarray*}\]</span>%
In other words, the estimation based on simulations of <span class="math inline">\(V\)</span>
provides better results than simulations of <span class="math inline">\(Y\)</span>
(as long as the simulations are not too complex).</p>
<div class="example">
<p><span id="exm:unlabeled-div-32" class="example"><strong>Example 16.21  </strong></span>Let <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> be two independent variables, each following exponential distributions $xp(
1) $ and $xp(
1/2) $, respectively. Suppose we want to calculate $=%
$. Rewritten in terms of expectation, we want
to estimate $=$ where $Y=%
$. The standard Monte Carlo method involves simulating <span class="math inline">\(2n\)</span> independent variables,
<span class="math inline">\(A_{1},B_{1},...,A_{n},B_{n}\)</span> and then considering%
<span class="math display">\[\begin{equation*}
\widehat{\theta }=\frac{1}{n}\sum_{i=1}^{n}\mathbb{I}\left[
A_{i}+B_{i}&gt;4\right] .
\end{equation*}\]</span>%
Using conditioning, let $V=$,
then%
<span class="math display">\[\begin{eqnarray*}
\mathbb{E}\left[ Y|B=z\right]  &amp;=&amp;\Pr\left[ A+B&gt;4|B=z\right] \\
&amp;=&amp;\Pr\left[ A&gt;4-z\right]  \\
&amp;=&amp;\left\{
\begin{array}{l}
\exp \left( -z+4\right), \text{ if }0\leq z\leq 4, \\
1,\text{ if }4\leq z,%
\end{array}%
\right.
\end{eqnarray*}\]</span>%
so that%
<span class="math display">\[\begin{equation*}
V=\mathbb{E}\left[ Y|B\right] =\left\{
\begin{array}{l}
\exp \left( -B+4\right), \text{ if }0\leq B\leq 4 ,\\
1,\text{ if }4\leq B.%
\end{array}%
\right.
\end{equation*}\]</span>%
The conditioning method leads to the following estimator for $$:
<span class="math display">\[\begin{equation*}
\widehat{\theta }=\frac{1}{n}\sum_{i=1}^{n}V_{i},
\end{equation*}\]</span>%
and the algorithm for estimating $$ is then</p>
<hr />
<p><strong>Algorithm</strong>: xxxx</p>
<hr />
<ul>
<li><span class="math inline">\(i\longleftarrow 1\)</span> and <span class="math inline">\(S\longleftarrow 0\)</span></li>
<li><strong>Repeat</strong>
<ul>
<li><span class="math inline">\(B\longleftarrow -\log \left(\text{Random}\right) /2\)</span></li>
<li><strong>If</strong> <span class="math inline">\(B\leq 4\)</span> <strong>Then</strong> $V(-B+4) $</li>
<li><strong>Else</strong> <span class="math inline">\(V\longleftarrow1\)</span></li>
<li><span class="math inline">\(i\Leftarrow i+1\)</span></li>
<li><span class="math inline">\(S=S+V\)</span></li>
</ul></li>
<li><strong>Until</strong> <span class="math inline">\(i=n\)</span></li>
<li><span class="math inline">\(\theta \longleftarrow S/n\)</span></li>
</ul>
<hr />
</div>
</div>
<div id="stratified-sampling" class="section level3 hasAnchor" number="16.7.4">
<h3><span class="header-section-number">16.7.4</span> Stratified Sampling<a href="chap15.html#stratified-sampling" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let <span class="math inline">\(X\)</span> be a random variable defined on <span class="math inline">\({\mathbb{R}}\)</span>, and <span class="math inline">\(\left( \mathcal{D}_{i}\right) _{i=1,...,m}\)</span> be a partition of <span class="math inline">\({\mathbb{R}}\)</span>. Suppose we want to calculate <span class="math inline">\(\theta =\mathbb{E}\left[ h\left(X\right) \right]\)</span>. Noting that%
<span class="math display">\[\begin{eqnarray*}
\mathbb{E}\left[ h\left( X\right) \right]
&amp;=&amp;\sum_{i=1}^{m}\mathbb{E}\Big[ h\left( X\right) \mathbb{I}\left[ X\in \mathcal{D}_{i}\right]\Big]\\
&amp;=&amp;\sum_{i=1}^{m}\mathbb{E}\left[ h\left( X\right) |X\in \mathcal{D}_{i}\right]
\Pr\left[ X\in \mathcal{D}_{i}\right] ,
\end{eqnarray*}\]</span>
the idea is to estimate <span class="math inline">\(\theta\)</span> by drawing samples from different strata. We assume $p_{i}=$ is known for all <span class="math inline">\(i\)</span>. We then simulate, for <span class="math inline">\(i=1,...,m\)</span>, samples <span class="math inline">\(X_{1}^{i},...,X_{n_{i}}^{i}\)</span> according to the conditional distribution of <span class="math inline">\(X\)</span> given <span class="math inline">\(X\in \mathcal{D}_{i}\)</span>. We consider%
<span class="math display">\[
\widehat{\theta }_{S}=\sum_{i=1}^{m}p_{i}\left(\frac{1}{n_{i}}\sum_{j=1}^{n_{i}}h%
\left( X_{j}^{i}\right)\right) .
\]</span></p>
<p>The goal here is to minimize the variance of the estimator for a fixed <span class="math inline">\(n\)</span> (here <span class="math inline">\(n=n_{1}+...+n_{m}\)</span>). Therefore, we seek to solve%
<span class="math display">\[
\min_{n_{1},...,n_{m}}\mathbb{V}\left[ \widehat{\theta }_{S}\right]
=\min_{n_{1},...,n_{m}}\sum_{i=1}^{m}p_{i}^{2}\frac{\sigma
_{i}^{2}}{n_{i}},
\]</span>%
where $_{i}^{2}=$. A quick calculation yields%
<span class="math display">\[
n_{i}^{\ast }=\frac{p_{i}\sigma _{i}}{\sum_{i=1}^{n}p_{i}\sigma
_{i}},
\]</span>%
which means that <span class="math inline">\(n_{i}^{\ast }\)</span> is proportional to the probability of belonging to stratum <span class="math inline">\(i\)</span> and the intra-class standard deviation. The difficulty lies in knowing these <span class="math inline">\(p_{i}\)</span> and <span class="math inline">\(\sigma _{i}\)</span>, which can be estimated beforehand by conducting some simulations. It is also possible to consider not using the <span class="math inline">\(n_{i}^{\ast }\)</span> (i.e., optimal values), but rather using <span class="math inline">\(n_{i}\)</span> that are simply proportional to <span class="math inline">\(p_{i}\)</span> (i.e., <span class="math inline">\(n_{i}=np_{i}\)</span>), which will still reduce the variance.</p>
</div>
<div id="importance-sampling" class="section level3 hasAnchor" number="16.7.5">
<h3><span class="header-section-number">16.7.5</span> Importance Sampling<a href="chap15.html#importance-sampling" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Here, we aim to calculate $=$, which can be rewritten as%
<span class="math display">\[\begin{eqnarray*}
\theta &amp;=&amp;\mathbb{E}\left[ h\left( Z\right) \right] =\int_{{\mathbb{R}}}h\left( z\right) f_{Z}\left( z\right) dz \\
&amp;=&amp;\int_{{\mathbb{R}}}h\left( z\right) \frac{f_{Z}\left( z\right) }{f_{Z^\star}\left( z\right) }f_{Z^\star}\left( z\right) dz\\
&amp;=&amp;\mathbb{E}\left[ h^\star\left( Z^\star\right) \right] ,
\end{eqnarray*}\]</span>%
where <span class="math inline">\(Z^\star\)</span> has the density <span class="math inline">\(f_{Z^\star}\)</span>, and%
<span class="math display">\[\begin{equation*}
h^\star\left( z\right) =h\left( z\right) \frac{f_{Z}\left( z\right) }{%
f_{Z^\star}\left( z\right) }.
\end{equation*}\]</span>%
The standard Monte Carlo method involves simulating variables <span class="math inline">\(Z_{1},...,Z_{n}\)</span> with density <span class="math inline">\(f_{Z}\)</span> and then defining $X_{i}=h(
Z_{i}) $. The estimator for <span class="math inline">\(\theta\)</span> is the empirical mean of <span class="math inline">\(X_{1},...,X_{n}\)</span>. The idea here is to simulate <span class="math inline">\(Z_{1}^\ast,...,Z_{n}^\ast\)</span> with density <span class="math inline">\(f_{Z^\star}\)</span> and then define $%
Y_{i}=h^( Z_{i}^) $. The considered estimator is then the mean of the <span class="math inline">\(Y_{i}\)</span>.</p>
<p>The choice of the density <span class="math inline">\(f_{Z^\star}\)</span> is crucial here, as shown in the following example.</p>
<div class="example">
<p><span id="exm:unlabeled-div-33" class="example"><strong>Example 16.22  </strong></span>Let <span class="math inline">\(X\)</span> have a Cauchy distribution. The goal here is to estimate
<span class="math display">\[
\theta =\Pr[ X&gt;2] =\int_{2}^{\infty }\frac{dx}{\pi
\left( 1+x^{2}\right) }.
\]</span>
(the true value is close to <span class="math inline">\(0.15\)</span> - which is not a too rare event). For a standard Monte Carlo approach, the estimator is then
<span class="math display">\[
\widehat{\theta }_{MC}=\frac{1}{n}\sum_{i=1}^{n}\mathbb{I}[X_{i}&gt;2]
\]</span>%
where the variables <span class="math inline">\(X_{1},....,X_{n}\)</span> are independent and have a Cauchy distribution. Recall that by definition of the Cauchy distribution, the most efficient method to simulate such a variable is as follows:</p>
<hr />
<p><strong>Algorithm</strong>: Cauchy random variable</p>
<hr />
<ul>
<li>$Y,Zor( 0,1) $</li>
<li><span class="math inline">\(X\longleftarrow Y/Z\)</span>.</li>
</ul>
<hr />
<p>The variance of this estimator <span class="math inline">\(\widehat{\theta }_{MC}\)</span> is then <span class="math inline">\(\theta \left( 1-\theta \right) /n\approx 0.1275/n\)</span> (since <span class="math inline">\(\theta \approx 0.15\)</span>).</p>
<p>Since the distribution is symmetric (and thus <span class="math inline">\(\theta =\Pr\left[ \left| X\right| &gt;2\right] /2\)</span>), we can consider an estimator as%
<span class="math display">\[
\widehat{\theta }_{MC}^{\ast
}=\frac{1}{2n}\sum_{i=1}^{n}\mathbb{I}\Big[ \left|
X_{i}\right|&gt;2\Big] ,
\]</span>%
with a variance of <span class="math inline">\(\theta \left( 1-2\theta \right) /2n=0.0525/n\)</span>.</p>
<p>These classical Monte Carlo methods are relatively
inefficient here since many simulations are irrelevant
(since the simulations do not fall within the range
<span class="math inline">\([2,\infty )\)</span>). Improvement can be achieved by noting that $$ can be rewritten as%
<span class="math display">\[\begin{eqnarray*}
\theta  &amp;=&amp;\frac{1}{2}-\Pr[ 0\leq X\leq 2] =\frac{1}{2}%
-\int_{0}^{2}\frac{dx}{\pi \left( 1+x^{2}\right) } \\
&amp;=&amp;\frac{1}{2}-\mathbb{E}[ h\left( U\right) ] \text{ where }%
h\left( x\right) =\frac{2}{\pi \left( 1+x^{2}\right) }
\end{eqnarray*}\]</span>%
with <span class="math inline">\(U\sim \mathcal{U}ni( 0,2)\)</span>. Therefore, a natural estimator of <span class="math inline">\(\theta\)</span> is%
<span class="math display">\[
\widehat{\theta
}_{IS}=\frac{1}{2}-\frac{1}{n}\sum_{i=1}^{n}h\left( U_{i}\right) ,
\]</span>
where the variables <span class="math inline">\(U_{1},...,U_{n}\)</span> are independent and have a <span class="math inline">\(\mathcal{U}ni(0,2)\)</span> distribution.
Also, the
variance of <span class="math inline">\(\widehat{\theta }_{IS}\)</span> is given by
<span class="math display">\[
\mathbb{V}\left[ \widehat{\theta }%
_{IS}\right] =\frac{1}{n}
\Big(\mathbb{E}\left[h\left( U\right) ^{2}\right] -\left(\mathbb{E}\left[ h\left( U\right)\right] \right) ^{2}\Big)=0.0092/n.
\]</span>
In conclusion, another way
to express <span class="math inline">\(\theta\)</span> is
<span class="math display">\[
\theta =\int_{0}^{1/2}\frac{y^{-2}}{\pi \left( 1+y^{-2}\right) }dy=\mathbb{E}%
\left[ \frac{1}{4}h^\star\left( V\right) \right] ,
\]</span>
where
<span class="math display">\[
h^\star\left( y\right) =\frac{1}{2\pi \left( 1+y^{2}\right) }\text{ and }%
V\sim \mathcal{U}ni(\left[ 0,1/2\right] ).
\]</span>%
Also, consider the estimator
<span class="math display">\[
\widehat{\theta }_{IS}^\star=\frac{1}{4n}\sum_{i=1}^{n}h^\star\left(V_{i}\right)
\]</span>%
where <span class="math inline">\(V_{1},...,V_{n}\)</span> are independent and have a <span class="math inline">\(\mathcal{U}ni(0,1/2)\)</span> distribution. Then,
<span class="math display">\[
\mathbb{V}\left[ \widehat{\theta }_{IS}^\star\right] =0.00095/n\leq
\mathbb{V}\left[ \widehat{\theta }_{MC}\right] /1000.
\]</span>
Therefore, the same
precision is achieved using <span class="math inline">\(1000\)</span> times fewer
simulations. Figure <span class="math inline">\(\ref{FIG-IS-1}\)</span> shows the evolution of
the estimation with respect to <span class="math inline">\(n\)</span>.</p>
</div>
<p>FIG</p>
</div>
</div>
<div id="convergence-control-and-stopping-criteria" class="section level2 hasAnchor" number="16.8">
<h2><span class="header-section-number">16.8</span> Convergence Control and Stopping Criteria<a href="chap15.html#convergence-control-and-stopping-criteria" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We previously noted that it is possible to determine the error of the approximation made using simulations using the central limit theorem. Suppose we want to calculate $==$, and let <span class="math inline">\(\sigma ^{2}\)</span> be the variance of <span class="math inline">\(Y\)</span>. A <span class="math inline">\(95\%\)</span> confidence interval is given by
<span class="math display">\[\begin{equation}
\left[ \widehat{\theta }_{n}-1.96\frac{\widehat{\sigma }_{n}}{\sqrt{n}};%
\widehat{\theta }_{n}+1.96\frac{\widehat{\sigma
}_{n}}{\sqrt{n}}\right] , \label{confidence interval simulations}
\end{equation}\]</span>
where
<span class="math inline">\(\widehat{\theta }_{n}=\left( Y_{1}+...+Y_{n}\right) /n\)</span>
and
<span class="math display">\[
\widehat{%
\sigma }_{n}^{2}=\left( \left( Y_{1}-\widehat{\theta }_{n}\right)
^{2}+...+\left( Y_{n}-\widehat{\theta }_{n}\right) ^{2}\right)
/\left( n-1\right).
\]</span>
This means that for a fixed <span class="math inline">\(n\)</span>, we can determine the margin of error of our estimator. Conversely, we are interested in the inverse problem: given a desired level of accuracy (either in absolute error <span class="math inline">\(|\widehat{\theta }_{n}-\theta |\)</span> or in relative error <span class="math inline">\(|\widehat{\theta }_{n}-\theta |/ \theta\)</span>) and a confidence level (here we assume <span class="math inline">\(\alpha = 95\%\)</span>), how many simulations are needed?</p>
<div id="two-step-estimation" class="section level3 hasAnchor" number="16.8.1">
<h3><span class="header-section-number">16.8.1</span> Two-Step Estimation<a href="chap15.html#two-step-estimation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The confidence interval involving the variance of <span class="math inline">\(Y\)</span> allows for a two-step estimation of $$. Suppose we want to bound the relative error by <span class="math inline">\(\varepsilon\)</span>, i.e., the constraint
<span class="math display">\[
\Pr\left[ |\widehat{\theta }_{n}-\theta | / \theta \leq \varepsilon \right] =95\%
\]</span>
should be satisfied. The method is as follows:</p>
<p><span class="math inline">\((i)\)</span> Perform a small number <span class="math inline">\(n\)</span> (<span class="math inline">\(\geq 50\)</span>) of simulations to obtain estimates of $$ and <span class="math inline">\(\sigma ^{2}\)</span>, denoted <span class="math inline">\(\widehat{\theta }_{n}\)</span> and <span class="math inline">\(\widehat{\sigma }_{n}^2\)</span>, for example. Using the confidence interval <span class="math inline">\((\ref{confidence interval simulations})\)</span>, we can calculate <span class="math inline">\(n^{\ast }\)</span>
<span class="math display">\[\begin{equation*}
n^{\ast }=\frac{1.96^{2}\widehat{\sigma }_{n}^{2}}{\widehat{\theta }%
_{n}^{2}\varepsilon ^{2}}.
\end{equation*}\]</span></p>
<p><span class="math inline">\((ii)\)</span> Then perform <span class="math inline">\(n^{\ast }\)</span> simulations of variables <span class="math inline">\(Y_{i}\)</span>.</p>
</div>
<div id="sequential-approach" class="section level3 hasAnchor" number="16.8.2">
<h3><span class="header-section-number">16.8.2</span> Sequential Approach<a href="chap15.html#sequential-approach" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Suppose we are still interested in the relative error. The idea here is to perform simulations until <span class="math inline">\(1.96\widehat{\sigma }_{n}/\sqrt{n}\widehat{\theta }_{n}\)</span> is bounded by <span class="math inline">\(\varepsilon\)</span>. At each step, it is necessary to recalculate an estimate of $$, as well as an estimate of <span class="math inline">\(\sigma ^{2}\)</span>. To simplify the calculations, note that%
<span class="math display">\[\begin{equation*}
\widehat{\theta }_{n}=\widehat{\theta }_{n-1}+\frac{Y_{n}-\widehat{\theta }%
_{n-1}}{n},
\end{equation*}\]</span>%
and%
<span class="math display">\[\begin{equation*}
\widehat{\sigma }_{n}^{2}=\frac{n-2}{n-1}\widehat{\sigma
}_{n-1}^{2}+n\left( \widehat{\theta }_{n}-\widehat{\theta
}_{n-1}\right) ^{2}.
\end{equation*}\]</span></p>
</div>
</div>
<div id="bibliographical-notes-12" class="section level2 hasAnchor" number="16.9">
<h2><span class="header-section-number">16.9</span> Bibliographical Notes<a href="chap15.html#bibliographical-notes-12" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The main tools and simulation algorithms are presented in <span class="citation">(<a href="#ref-rubinstein1981simulation" role="doc-biblioref">Rubinstein and Kroese 2016</a>)</span>, <span class="citation">(<a href="#ref-ripley2009stochastic" role="doc-biblioref">Ripley 2009</a>)</span>, <span class="citation">(<a href="#ref-devroye1992nonuniform" role="doc-biblioref">Devroye 1992</a>)</span>, <span class="citation">(<a href="#ref-fishman2013monte" role="doc-biblioref">Fishman 2013</a>)</span> or <span class="citation">(<a href="#ref-gentle2003random" role="doc-biblioref">Gentle 2003</a>)</span>. For readers particularly interested in Monte Carlo methods using Markov Chains, the essential reference is <span class="citation">(<a href="#ref-robert1996methodes" role="doc-biblioref">Robert 1996</a>)</span>. Among the references in French, <span class="citation">(<a href="#ref-ycart2002simulations" role="doc-biblioref">Ycart 2002</a>)</span> provides the key points covered here.</p>
</div>
<div id="exercises-9" class="section level2 hasAnchor" number="16.10">
<h2><span class="header-section-number">16.10</span> Exercises<a href="chap15.html#exercises-9" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="exercise">
<p><span id="exr:unlabeled-div-34" class="exercise"><strong>Exercise 16.1  </strong></span>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be two independent variables with <span class="math inline">\(\mathcal{E}xp(1)\)</span> distribution.</p>
<ol style="list-style-type: decimal">
<li>Give the expression for the conditional density of <span class="math inline">\(X\)</span> given <span class="math inline">\(\{2Y&gt;(1-X^2)\}\)</span>.</li>
<li>Let <span class="math inline">\(Z\)</span> follow such a distribution, and <span class="math inline">\(T\)</span> be a variable taking values <span class="math inline">\(+1\)</span> and <span class="math inline">\(-1\)</span> with probability <span class="math inline">\(1/2\)</span>. Determine the distribution of <span class="math inline">\(TZ\)</span>.</li>
<li>Use this to derive a simulation method for the <span class="math inline">\(\mathcal{N}or(0,1)\)</span> distribution.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-35" class="exercise"><strong>Exercise 16.2  </strong></span>Consider a density function <span class="math inline">\(f\)</span> on <span class="math inline">\(\mathbb{R}\)</span>.</p>
<ol style="list-style-type: decimal">
<li>Let <span class="math inline">\((U,V)\)</span> be uniformly distributed on <span class="math inline">\(\mathcal{A}=\{(u,v)\in\mathbb{R}^2| 0\leq u\leq f(u+v)\}\)</span>. Show that <span class="math inline">\(U+V\)</span> has a density proportional to <span class="math inline">\(f\)</span>.</li>
<li>Let <span class="math inline">\((U,V)\)</span> be uniformly distributed on <span class="math inline">\(\mathcal{B}=\{(u,v)\in\mathbb{R}^2| 0\leq u\leq f(v/\sqrt{u})^{2/3}\}\)</span>. Show that <span class="math inline">\(V/\sqrt{U}\)</span> has a density proportional to <span class="math inline">\(f\)</span>.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-36" class="exercise"><strong>Exercise 16.3  </strong></span>Let <span class="math inline">\(U\sim\mathcal{U}ni[-\pi/2,\pi/2]\)</span>, and <span class="math inline">\(Z\)</span> be exponentially distributed. Show that
<span class="math display">\[
\mu + c\cdot \frac{\sin(\alpha U)}{(\cos(U))^{1/\alpha}}\left(\frac{\cos((1-\alpha)U)}{Z}\right)^{(1-\alpha)/\alpha}
\]</span>
follows an <span class="math inline">\(\alpha\)</span>-stable distribution. Deduce a method for generating stable distributions.</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-37" class="exercise"><strong>Exercise 16.4  </strong></span>Consider the following algorithm,</p>
<hr />
<p><strong>Algorithm</strong>: xxxx</p>
<hr />
<ul>
<li><strong>Repeat</strong>
<ul>
<li><span class="math inline">\(U\Longleftarrow\texttt{Random}\)</span></li>
<li><span class="math inline">\(V\Longleftarrow\texttt{Random}\)</span></li>
<li><strong>If</strong> <span class="math inline">\(U\leq 1/2\)</span> <strong>Then</strong>
<ul>
<li><span class="math inline">\(X\longleftarrow 1/(4U-1)\)</span></li>
<li><span class="math inline">\(Z\longleftarrow X^{-2}V\)</span></li>
</ul></li>
<li><strong>Else</strong>
<ul>
<li><span class="math inline">\(X\longleftarrow 4U-3\)</span></li>
<li><span class="math inline">\(Z\longleftarrow V\)</span></li>
</ul></li>
</ul></li>
<li><strong>Until</strong> <span class="math inline">\(Z \leq \min\{1-|X|/2,(1+X^2/\nu)^{-(\nu+1)/2}\}\)</span></li>
</ul>
<hr />
<p>Noting that this is a rejection algorithm, show that <span class="math inline">\(X\)</span> follows a Student’s <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(\nu\)</span> degrees of freedom.</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-38" class="exercise"><strong>Exercise 16.5  </strong></span>Let <span class="math inline">\(F\)</span> and <span class="math inline">\(G\)</span> be two distribution functions on <span class="math inline">\(\mathbb{R}^+\)</span>, with hazard rates denoted <span class="math inline">\(r_F\)</span> and <span class="math inline">\(r_G\)</span> respectively.</p>
<ol style="list-style-type: decimal">
<li>Suppose <span class="math inline">\(r_F\leq r_G\)</span>, and that the cumulative hazard rate
<span class="math display">\[
H_G(x)=\int_0^x r_G(x)dx
\]</span>
is easily invertible. Show that the following rejection algorithm can be used to simulate a random variable <span class="math inline">\(X\)</span> with hazard rate <span class="math inline">\(r_F\)</span>:</li>
</ol>
<hr />
<p><strong>Algorithm</strong>: xxxx</p>
<hr />
<ul>
<li><span class="math inline">\(S\longleftarrow0\)</span></li>
<li><strong>Repeat</strong>
<ul>
<li><span class="math inline">\(E\Longleftarrow -\log(\texttt{Random})\)</span></li>
<li><span class="math inline">\(U\Longleftarrow \texttt{Random}\)</span></li>
<li><span class="math inline">\(S\Longleftarrow S+E\)</span></li>
<li><span class="math inline">\(X\Longleftarrow H_G^{-1}(S)\)</span></li>
</ul></li>
<li><strong>Until</strong> <span class="math inline">\(U \cdot h_G(X) \leq h_F(X)\)</span></li>
</ul>
<hr />
<ol start="2" style="list-style-type: decimal">
<li>Suppose <span class="math inline">\(r_F\)</span> is decreasing, and <span class="math inline">\(r_F(0)&lt;\infty\)</span>. Show that the following rejection algorithm can be used to simulate a random variable <span class="math inline">\(X\)</span> with hazard rate <span class="math inline">\(r_F\)</span>,</li>
</ol>
<hr />
<p><strong>Algorithm</strong>: xxxx</p>
<hr />
<ul>
<li><span class="math inline">\(X\longleftarrow0\)</span></li>
<li><strong>Repeat</strong>
<ul>
<li><span class="math inline">\(B\Longleftarrow h_F(X)\)</span></li>
<li><span class="math inline">\(E\Longleftarrow -\log(\texttt{Random})\)</span></li>
<li><span class="math inline">\(U\Longleftarrow \texttt{Random}\)</span></li>
<li><span class="math inline">\(X\Longleftarrow X+E\)</span></li>
</ul></li>
<li><strong>Until</strong> <span class="math inline">\(U \cdot B \leq h_F(X)\)</span></li>
</ul>
<hr />
<ol start="3" style="list-style-type: decimal">
<li>Give the hazard rate for the <span class="math inline">\(\mathcal{P}ar(1,\alpha)\)</span> distribution, and derive a generation algorithm for this distribution.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-39" class="exercise"><strong>Exercise 16.6  </strong></span>To simulate an Ali-Mikhail-Haq copula (introduced in Exercise 8.9.16) with parameter <span class="math inline">\(\theta\)</span>, show that the following algorithm can be used:</p>
</div>

</div>
</div>
<h3>Postface<a href="postface.html#postface" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-davison1997bootstrap" class="csl-entry">
Davison, Anthony Christopher, and David Victor Hinkley. 1997. <em>Bootstrap Methods and Their Application</em>. 1. Cambridge university press.
</div>
<div id="ref-devroye1992nonuniform" class="csl-entry">
Devroye, Luc. 1992. <em>Non-Uniform Random Variate Generation</em>. Springer.
</div>
<div id="ref-efron1994introduction" class="csl-entry">
Efron, Bradley, and Robert J Tibshirani. 1994. <em>An Introduction to the Bootstrap</em>. CRC press.
</div>
<div id="ref-england1999analytic" class="csl-entry">
England, Peter, and Richard Verrall. 1999. <span>“Analytic and Bootstrap Estimates of Prediction Errors in Claims Reserving.”</span> <em>Insurance: Mathematics and Economics</em> 25 (3): 281–93.
</div>
<div id="ref-fang1990symmetric" class="csl-entry">
Fang, K. T., and K. W. Kotz S. And Ng. 1990. <em>Symmetric Multivariate and Related Distributions</em>. CRC Press.
</div>
<div id="ref-fishman2013monte" class="csl-entry">
Fishman, George. 2013. <em>Monte Carlo: Concepts, Algorithms, and Applications</em>. Springer.
</div>
<div id="ref-gentle2003random" class="csl-entry">
Gentle, James E. 2003. <em>Random Number Generation and Monte Carlo Methods</em>. Vol. 381. Springer.
</div>
<div id="ref-l1994uniform" class="csl-entry">
L’Ecuyer, Pierre. 1994. <span>“Uniform Random Number Generation.”</span> <em>Annals of Operations Research</em> 53: 77–120.
</div>
<div id="ref-ripley2009stochastic" class="csl-entry">
Ripley, Brian D. 2009. <em>Stochastic Simulation</em>. John Wiley &amp; Sons.
</div>
<div id="ref-robert1996methodes" class="csl-entry">
Robert, Christian P. 1996. <em>M<span>é</span>thodes de Monte Carlo Par Cha<span>ı̂</span>nes de Markov</em>. Economica.
</div>
<div id="ref-rosenblatt1952remarks" class="csl-entry">
Rosenblatt, Murray. 1952. <span>“Remarks on a Multivariate Transformation.”</span> <em>The Annals of Mathematical Statistics</em> 23 (3): 470–72.
</div>
<div id="ref-rubinstein1981simulation" class="csl-entry">
Rubinstein, Reuven Y, and Dirk P Kroese. 2016. <em>Simulation and the Monte Carlo Method</em>. John Wiley &amp; Sons.
</div>
<div id="ref-ycart2002simulations" class="csl-entry">
Ycart, Bernard. 2002. <span>“Simulation de Variables Aléatoires.”</span> <em>Cahiers de Math<span>é</span>matiques Appliqu<span>é</span>es, CMA</em> 11.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="chap14.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="chap16.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/16-Monte-Carlo.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
