<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Actuarial risk modeling | Non Life Insurance Mathematics</title>
  <meta name="description" content="Chapter 3 Actuarial risk modeling | Non Life Insurance Mathematics" />
  <meta name="generator" content="bookdown 0.35 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Actuarial risk modeling | Non Life Insurance Mathematics" />
  <meta property="og:type" content="book" />
  
  
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Actuarial risk modeling | Non Life Insurance Mathematics" />
  
  
  

<meta name="author" content="Arthur Charpentier and Michel Denuit" />


<meta name="date" content="2023-08-30" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="chap1.html"/>
<link rel="next" href="chap3.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Non Life Insurance Mathematics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="chap1.html"><a href="chap1.html"><i class="fa fa-check"></i><b>2</b> The risk and its contractual coverage</a>
<ul>
<li class="chapter" data-level="2.1" data-path="chap1.html"><a href="chap1.html#risk"><i class="fa fa-check"></i><b>2.1</b> Risk</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="chap1.html"><a href="chap1.html#did-you-say-risk"><i class="fa fa-check"></i><b>2.1.1</b> Did you say risk?</a></li>
<li class="chapter" data-level="2.1.2" data-path="chap1.html"><a href="chap1.html#the-reason-for-insurance-risquophobia"><i class="fa fa-check"></i><b>2.1.2</b> The reason for insurance: risquophobia</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="chap1.html"><a href="chap1.html#risk-management-methods"><i class="fa fa-check"></i><b>2.2</b> Risk management methods</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="chap1.html"><a href="chap1.html#caution-and-self-insurance"><i class="fa fa-check"></i><b>2.2.1</b> Caution and self-insurance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chap2.html"><a href="chap2.html"><i class="fa fa-check"></i><b>3</b> Actuarial risk modeling</a>
<ul>
<li class="chapter" data-level="3.1" data-path="chap2.html"><a href="chap2.html#introduction"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="chap2.html"><a href="chap2.html#probabilistic-description-of-risk"><i class="fa fa-check"></i><b>3.2</b> Probabilistic description of risk</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="chap2.html"><a href="chap2.html#events"><i class="fa fa-check"></i><b>3.2.1</b> Events</a></li>
<li class="chapter" data-level="3.2.2" data-path="chap2.html"><a href="chap2.html#elementary-events"><i class="fa fa-check"></i><b>3.2.2</b> Elementary events</a></li>
<li class="chapter" data-level="3.2.3" data-path="chap2.html"><a href="chap2.html#set-formalism"><i class="fa fa-check"></i><b>3.2.3</b> Set formalism</a></li>
<li class="chapter" data-level="3.2.4" data-path="chap2.html"><a href="chap2.html#properties-satisfied-by-the-set-of-events"><i class="fa fa-check"></i><b>3.2.4</b> Properties satisfied by the set of events</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="chap2.html"><a href="chap2.html#probability-calculation-and-lack-of-arbitrage-opportunity"><i class="fa fa-check"></i><b>3.3</b> Probability calculation and lack of arbitrage opportunity</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="chap2.html"><a href="chap2.html#the-notion-of-probability"><i class="fa fa-check"></i><b>3.3.1</b> The notion of probability</a></li>
<li class="chapter" data-level="3.3.2" data-path="chap2.html"><a href="chap2.html#risk-and-uncertainty"><i class="fa fa-check"></i><b>3.3.2</b> Risk and uncertainty</a></li>
<li class="chapter" data-level="3.3.3" data-path="chap2.html"><a href="chap2.html#probability-and-insurance-premium"><i class="fa fa-check"></i><b>3.3.3</b> Probability and insurance premium</a></li>
<li class="chapter" data-level="3.3.4" data-path="chap2.html"><a href="chap2.html#absence-of-arbitrage-opportunity"><i class="fa fa-check"></i><b>3.3.4</b> Absence of arbitrage opportunity</a></li>
<li class="chapter" data-level="3.3.5" data-path="chap2.html"><a href="chap2.html#property-of-additivity-for-incompatible-events"><i class="fa fa-check"></i><b>3.3.5</b> Property of additivity for incompatible events</a></li>
<li class="chapter" data-level="3.3.6" data-path="chap2.html"><a href="chap2.html#premiums-grow-with-risk"><i class="fa fa-check"></i><b>3.3.6</b> Premiums grow with risk</a></li>
<li class="chapter" data-level="3.3.7" data-path="chap2.html"><a href="chap2.html#fairness-property"><i class="fa fa-check"></i><b>3.3.7</b> Fairness property</a></li>
<li class="chapter" data-level="3.3.8" data-path="chap2.html"><a href="chap2.html#subadditivity-property"><i class="fa fa-check"></i><b>3.3.8</b> Subadditivity property</a></li>
<li class="chapter" data-level="3.3.9" data-path="chap2.html"><a href="chap2.html#poincaré-equality"><i class="fa fa-check"></i><b>3.3.9</b> Poincaré equality</a></li>
<li class="chapter" data-level="3.3.10" data-path="chap2.html"><a href="chap2.html#conditional-probability"><i class="fa fa-check"></i><b>3.3.10</b> Conditional probability</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="chap2.html"><a href="chap2.html#independent-events"><i class="fa fa-check"></i><b>3.4</b> Independent events</a></li>
<li class="chapter" data-level="3.5" data-path="chap2.html"><a href="chap2.html#multiplication-rule-bayes"><i class="fa fa-check"></i><b>3.5</b> Multiplication rule (Bayes)</a></li>
<li class="chapter" data-level="3.6" data-path="chap2.html"><a href="chap2.html#conditionally-independent-events"><i class="fa fa-check"></i><b>3.6</b> Conditionally independent events</a></li>
<li class="chapter" data-level="3.7" data-path="chap2.html"><a href="chap2.html#total-probability-theorem"><i class="fa fa-check"></i><b>3.7</b> Total probability theorem</a></li>
<li class="chapter" data-level="3.8" data-path="chap2.html"><a href="chap2.html#bayes-theorem"><i class="fa fa-check"></i><b>3.8</b> Bayes’ theorem</a></li>
<li class="chapter" data-level="3.9" data-path="chap2.html"><a href="chap2.html#random-variables"><i class="fa fa-check"></i><b>3.9</b> Random variables</a>
<ul>
<li class="chapter" data-level="3.9.1" data-path="chap2.html"><a href="chap2.html#definition"><i class="fa fa-check"></i><b>3.9.1</b> Definition</a></li>
<li class="chapter" data-level="3.9.2" data-path="chap2.html"><a href="chap2.html#distribution-function"><i class="fa fa-check"></i><b>3.9.2</b> Distribution function</a></li>
<li class="chapter" data-level="3.9.3" data-path="chap2.html"><a href="chap2.html#support-of-a-random-variable"><i class="fa fa-check"></i><b>3.9.3</b> Support of a random variable</a></li>
<li class="chapter" data-level="3.9.4" data-path="chap2.html"><a href="chap2.html#tail-or-survival-function"><i class="fa fa-check"></i><b>3.9.4</b> Tail (or Survival) function</a></li>
<li class="chapter" data-level="3.9.5" data-path="chap2.html"><a href="chap2.html#equality-in-distribution"><i class="fa fa-check"></i><b>3.9.5</b> Equality in distribution</a></li>
<li class="chapter" data-level="3.9.6" data-path="chap2.html"><a href="chap2.html#quantiles-and-generalized-inverses"><i class="fa fa-check"></i><b>3.9.6</b> Quantiles and generalized inverses</a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="chap2.html"><a href="chap2.html#discrete-random-variables-and-counts"><i class="fa fa-check"></i><b>3.10</b> Discrete random variables and counts</a>
<ul>
<li class="chapter" data-level="3.10.1" data-path="chap2.html"><a href="chap2.html#notion"><i class="fa fa-check"></i><b>3.10.1</b> Notion</a></li>
<li class="chapter" data-level="3.10.2" data-path="chap2.html"><a href="chap2.html#uniform-discrete-variable"><i class="fa fa-check"></i><b>3.10.2</b> Uniform discrete variable</a></li>
<li class="chapter" data-level="3.10.3" data-path="chap2.html"><a href="chap2.html#bernoulli-variables"><i class="fa fa-check"></i><b>3.10.3</b> Bernoulli variables</a></li>
<li class="chapter" data-level="3.10.4" data-path="chap2.html"><a href="chap2.html#binomial-variable"><i class="fa fa-check"></i><b>3.10.4</b> Binomial variable</a></li>
<li class="chapter" data-level="3.10.5" data-path="chap2.html"><a href="chap2.html#geometric-variable"><i class="fa fa-check"></i><b>3.10.5</b> Geometric variable</a></li>
<li class="chapter" data-level="3.10.6" data-path="chap2.html"><a href="chap2.html#negative-binomial-variable"><i class="fa fa-check"></i><b>3.10.6</b> Negative binomial variable</a></li>
<li class="chapter" data-level="3.10.7" data-path="chap2.html"><a href="chap2.html#poissons-distribution"><i class="fa fa-check"></i><b>3.10.7</b> Poisson’s distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.11" data-path="chap2.html"><a href="chap2.html#continuous-random-variables"><i class="fa fa-check"></i><b>3.11</b> Continuous random variables</a>
<ul>
<li class="chapter" data-level="3.11.1" data-path="chap2.html"><a href="chap2.html#notion-1"><i class="fa fa-check"></i><b>3.11.1</b> Notion</a></li>
<li class="chapter" data-level="3.11.2" data-path="chap2.html"><a href="chap2.html#continuous-uniform-distribution"><i class="fa fa-check"></i><b>3.11.2</b> Continuous uniform distribution</a></li>
<li class="chapter" data-level="3.11.3" data-path="chap2.html"><a href="chap2.html#beta-distribution"><i class="fa fa-check"></i><b>3.11.3</b> Beta Distribution</a></li>
<li class="chapter" data-level="3.11.4" data-path="chap2.html"><a href="chap2.html#normal-or-gaussian-distribution"><i class="fa fa-check"></i><b>3.11.4</b> Normal (or Gaussian) distribution</a></li>
<li class="chapter" data-level="3.11.5" data-path="chap2.html"><a href="chap2.html#log-normal-variable"><i class="fa fa-check"></i><b>3.11.5</b> Log-normal variable</a></li>
<li class="chapter" data-level="3.11.6" data-path="chap2.html"><a href="chap2.html#negative-exponential-distribution"><i class="fa fa-check"></i><b>3.11.6</b> (Negative) exponential distribution</a></li>
<li class="chapter" data-level="3.11.7" data-path="chap2.html"><a href="chap2.html#gamma-distribution"><i class="fa fa-check"></i><b>3.11.7</b> Gamma distribution</a></li>
<li class="chapter" data-level="3.11.8" data-path="chap2.html"><a href="chap2.html#pareto-distribution"><i class="fa fa-check"></i><b>3.11.8</b> Pareto distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.12" data-path="chap2.html"><a href="chap2.html#random-vector"><i class="fa fa-check"></i><b>3.12</b> Random vector</a>
<ul>
<li class="chapter" data-level="3.12.1" data-path="chap2.html"><a href="chap2.html#definition-1"><i class="fa fa-check"></i><b>3.12.1</b> Definition</a></li>
<li class="chapter" data-level="3.12.2" data-path="chap2.html"><a href="chap2.html#distribution-function-1"><i class="fa fa-check"></i><b>3.12.2</b> Distribution function</a></li>
<li class="chapter" data-level="3.12.3" data-path="chap2.html"><a href="chap2.html#support-of-vector-boldsymbolx"><i class="fa fa-check"></i><b>3.12.3</b> Support of vector <span class="math inline">\(\boldsymbol{X}\)</span></a></li>
<li class="chapter" data-level="3.12.4" data-path="chap2.html"><a href="chap2.html#independence"><i class="fa fa-check"></i><b>3.12.4</b> Independence</a></li>
<li class="chapter" data-level="3.12.5" data-path="chap2.html"><a href="chap2.html#gaussian-vector"><i class="fa fa-check"></i><b>3.12.5</b> Gaussian vector</a></li>
<li class="chapter" data-level="3.12.6" data-path="chap2.html"><a href="chap2.html#ellipticpart"><i class="fa fa-check"></i><b>3.12.6</b> Elliptical vectors</a></li>
<li class="chapter" data-level="3.12.7" data-path="chap2.html"><a href="chap2.html#definition-2"><i class="fa fa-check"></i><b>3.12.7</b> Definition</a></li>
<li class="chapter" data-level="3.12.8" data-path="chap2.html"><a href="chap2.html#multinomial-vector"><i class="fa fa-check"></i><b>3.12.8</b> Multinomial vector</a></li>
</ul></li>
<li class="chapter" data-level="3.13" data-path="chap2.html"><a href="chap2.html#conditional-variables"><i class="fa fa-check"></i><b>3.13</b> Conditional Variables</a>
<ul>
<li class="chapter" data-level="3.13.1" data-path="chap2.html"><a href="chap2.html#the-case-of-counting-variables"><i class="fa fa-check"></i><b>3.13.1</b> The case of counting variables</a></li>
<li class="chapter" data-level="3.13.2" data-path="chap2.html"><a href="chap2.html#the-case-of-continuous-variables"><i class="fa fa-check"></i><b>3.13.2</b> The case of continuous variables</a></li>
<li class="chapter" data-level="3.13.3" data-path="chap2.html"><a href="chap2.html#the-mixed-case-one-counting-variable-and-another-continuous"><i class="fa fa-check"></i><b>3.13.3</b> The mixed case: one counting variable and another continuous</a></li>
<li class="chapter" data-level="3.13.4" data-path="chap2.html"><a href="chap2.html#conditional-independence"><i class="fa fa-check"></i><b>3.13.4</b> Conditional independence</a></li>
</ul></li>
<li class="chapter" data-level="3.14" data-path="chap2.html"><a href="chap2.html#compound-distributions"><i class="fa fa-check"></i><b>3.14</b> Compound distributions</a>
<ul>
<li class="chapter" data-level="3.14.1" data-path="chap2.html"><a href="chap2.html#definition-4"><i class="fa fa-check"></i><b>3.14.1</b> Definition</a></li>
<li class="chapter" data-level="3.14.2" data-path="chap2.html"><a href="chap2.html#SecProdConv"><i class="fa fa-check"></i><b>3.14.2</b> Convolution product</a></li>
<li class="chapter" data-level="3.14.3" data-path="chap2.html"><a href="chap2.html#distribution-function-associated-with-a-compound-distribution"><i class="fa fa-check"></i><b>3.14.3</b> Distribution function associated with a compound distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.15" data-path="chap2.html"><a href="chap2.html#RiskTransformations"><i class="fa fa-check"></i><b>3.15</b> Transformations of risks and conventional damage clauses</a>
<ul>
<li class="chapter" data-level="3.15.1" data-path="chap2.html"><a href="chap2.html#concept"><i class="fa fa-check"></i><b>3.15.1</b> Concept</a></li>
<li class="chapter" data-level="3.15.2" data-path="chap2.html"><a href="chap2.html#DecOblig"><i class="fa fa-check"></i><b>3.15.2</b> The compulsory overdraft</a></li>
<li class="chapter" data-level="3.15.3" data-path="chap2.html"><a href="chap2.html#the-deductible"><i class="fa fa-check"></i><b>3.15.3</b> The deductible</a></li>
<li class="chapter" data-level="3.15.4" data-path="chap2.html"><a href="chap2.html#upper-limit-of-indemnity"><i class="fa fa-check"></i><b>3.15.4</b> (Upper) limit of indemnity</a></li>
<li class="chapter" data-level="3.15.5" data-path="chap2.html"><a href="chap2.html#technical-consequence-censored-data"><i class="fa fa-check"></i><b>3.15.5</b> Technical consequence: censored data</a></li>
<li class="chapter" data-level="3.15.6" data-path="chap2.html"><a href="chap2.html#poissons-distribution-and-damage-clauses"><i class="fa fa-check"></i><b>3.15.6</b> Poisson’s distribution and damage clauses</a></li>
<li class="chapter" data-level="3.15.7" data-path="chap2.html"><a href="chap2.html#perverse-effects-of-contractual-damage-clauses"><i class="fa fa-check"></i><b>3.15.7</b> Perverse effects of contractual damage clauses</a></li>
</ul></li>
<li class="chapter" data-level="3.16" data-path="chap2.html"><a href="chap2.html#exercises"><i class="fa fa-check"></i><b>3.16</b> Exercises</a></li>
<li class="chapter" data-level="3.17" data-path="chap2.html"><a href="chap2.html#bibliographical-notes"><i class="fa fa-check"></i><b>3.17</b> Bibliographical notes</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="chap3.html"><a href="chap3.html"><i class="fa fa-check"></i><b>4</b> Pure Premium</a></li>
<li class="chapter" data-level="5" data-path="chap4.html"><a href="chap4.html"><i class="fa fa-check"></i><b>5</b> Net Premium</a></li>
<li class="chapter" data-level="6" data-path="chap5.html"><a href="chap5.html"><i class="fa fa-check"></i><b>6</b> Measuring and Comparing Risks</a></li>
<li class="chapter" data-level="7" data-path="chap6.html"><a href="chap6.html"><i class="fa fa-check"></i><b>7</b> Collective Model</a></li>
<li class="chapter" data-level="8" data-path="chap7.html"><a href="chap7.html"><i class="fa fa-check"></i><b>8</b> Solvency</a></li>
<li class="chapter" data-level="9" data-path="chap8.html"><a href="chap8.html"><i class="fa fa-check"></i><b>9</b> Multiple Risks</a></li>
<li class="chapter" data-level="10" data-path="chap9.html"><a href="chap9.html"><i class="fa fa-check"></i><b>10</b> Prior Ratemaking</a></li>
<li class="chapter" data-level="11" data-path="chap10.html"><a href="chap10.html"><i class="fa fa-check"></i><b>11</b> Credibility</a></li>
<li class="chapter" data-level="12" data-path="chap11.html"><a href="chap11.html"><i class="fa fa-check"></i><b>12</b> Bonus-Malus</a></li>
<li class="chapter" data-level="13" data-path="chap12.html"><a href="chap12.html"><i class="fa fa-check"></i><b>13</b> Economics of Insurance</a></li>
<li class="chapter" data-level="14" data-path="chap13.html"><a href="chap13.html"><i class="fa fa-check"></i><b>14</b> Claims Reserving</a></li>
<li class="chapter" data-level="15" data-path="chap14.html"><a href="chap14.html"><i class="fa fa-check"></i><b>15</b> Large Risks</a></li>
<li class="chapter" data-level="16" data-path="chap15.html"><a href="chap15.html"><i class="fa fa-check"></i><b>16</b> Monte Carlo</a></li>
<li class="chapter" data-level="17" data-path="chap16.html"><a href="chap16.html"><i class="fa fa-check"></i><b>17</b> Conclusion</a></li>
<li class="chapter" data-level="" data-path="postface.html"><a href="postface.html"><i class="fa fa-check"></i>Postface</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Non Life Insurance Mathematics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chap2" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">Chapter 3</span> Actuarial risk modeling<a href="chap2.html#chap2" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="introduction" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Introduction<a href="chap2.html#introduction" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When it comes to repairing the consequences of chance, the insurance business can only be understood through the calculation of probabilities. Its origins lie in the correspondence between <a href="https://en.wikipedia.org/wiki/Blaise_Pascal">Blaise Pascal</a> and the <a href="https://en.wikipedia.org/wiki/Antoine_Gombaud">Chevalier de Méré</a>, before being formalized by the Russian school (led by <a href="https://en.wikipedia.org/wiki/Andrey_Kolmogorov">Kolmogorov</a>) during the Second World War.</p>
<p>The main aim of probability calculus is to provide a scientific method for quantifying the likelihood of certain events occurring. In this context, the notion of a random variable naturally makes its appearance. For the actuary, this may represent the cost of a claim, or the number of claims. Random variables, and the associated distribution functions describing their stochastic behavior, provide the essential tools for modeling risk transfers between policyholders and insurers. The aim of this chapter is to clarify a few points of terminology and to recall the main concepts of probability calculus that will be used in the remainder of the book.</p>
</div>
<div id="probabilistic-description-of-risk" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Probabilistic description of risk<a href="chap2.html#probabilistic-description-of-risk" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="events" class="section level3 hasAnchor" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Events<a href="chap2.html#events" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A fundamental concept for the rest of the book is that of a random event. These are events for which we cannot predict with certainty whether or not they will occur. It is to such events that the insurer’s benefits are contingent. Clearly,
- the events considered depend on the definition of the guarantees promised by the insurer.
- the description of events must be exhaustive, but without duplication.</p>
</div>
<div id="elementary-events" class="section level3 hasAnchor" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> Elementary events<a href="chap2.html#elementary-events" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s define the elementary events <span class="math inline">\(e_1,e_2,e_3,\ldots\)</span> as those satisfying
the following two properties:</p>
<ol style="list-style-type: decimal">
<li>two distinct elementary events <span class="math inline">\(e_i\)</span> and <span class="math inline">\(e_j\)</span>, <span class="math inline">\(jneq i\)</span>, are incompatible, i.e. they cannot occur simultaneously.</li>
<li>the union <span class="math inline">\(\mathcal{E}=\{e_1,e_2,\ldots\}\)</span> of all elementary events <span class="math inline">\(e_1,e_2,\ldots\)</span> corresponds to certainty, i.e. the event that always occurs.</li>
</ol>
<p>Let’s now associate with each event the set of elementary outcomes that lead to its realization. Any event <span class="math inline">\(E\)</span> can thus be expressed as the union of elementary events <span class="math inline">\(e_{i_1},\ldots,e_{i_k}\)</span>, i.e. <span class="math inline">\(E\)</span> occurs when either <span class="math inline">\(e_{i_1}\)</span>, <span class="math inline">\(e_{i_2}\)</span>, , or <span class="math inline">\(e_{i_k}\)</span> occurs. From now on, this will be noted as <span class="math inline">\(E=\{e_{i_1},\ldots,e_{i_k}\}\)</span>.</p>
</div>
<div id="set-formalism" class="section level3 hasAnchor" number="3.2.3">
<h3><span class="header-section-number">3.2.3</span> Set formalism<a href="chap2.html#set-formalism" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Having identified each event with a subset of <span class="math inline">\(\mathcal{E}\)</span>, the set formalism will play an important role in what follows. This is why, given two events <span class="math inline">\(E\)</span> and <span class="math inline">\(F\)</span>, we’ll henceforth write</p>
<ul>
<li><span class="math inline">\(\overline{E}\)</span> (which reads “not <span class="math inline">\(E\)</span>”) the event that occurs when <span class="math inline">\(E\)</span> does not occur (<span class="math inline">\(\overline{E}\)</span> is still called the <a href="https://en.wikipedia.org/wiki/Complementary_event">complementary</a> of <span class="math inline">\(E\)</span>);</li>
<li><span class="math inline">\(E\cap F\)</span> (which reads “<span class="math inline">\(E\)</span> and <span class="math inline">\(F\)</span>”) the event that occurs when <span class="math inline">\(E\)</span> and <span class="math inline">\(F\)</span> occur simultaneously;</li>
<li><span class="math inline">\(E\cup F\)</span> (which reads “<span class="math inline">\(E\)</span> or <span class="math inline">\(F\)</span>”) the event that occurs when <span class="math inline">\(E\)</span> or <span class="math inline">\(F\)</span> occurs;</li>
<li><span class="math inline">\(E\smallsetminus F\)</span> (which reads “<span class="math inline">\(E\)</span> minus <span class="math inline">\(F\)</span>”) the event that occurs when <span class="math inline">\(E\)</span> occurs, but not <span class="math inline">\(F\)</span> (so <span class="math inline">\(E\smallsetminus F=E\cap\overline{F}\)</span>);</li>
<li><span class="math inline">\(\emptyset\)</span> (which reads “impossible event”)
the event that never happens.</li>
</ul>
</div>
<div id="properties-satisfied-by-the-set-of-events" class="section level3 hasAnchor" number="3.2.4">
<h3><span class="header-section-number">3.2.4</span> Properties satisfied by the set of events<a href="chap2.html#properties-satisfied-by-the-set-of-events" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s denote <span class="math inline">\(\mathcal{A}\)</span> the set of events that condition the risk (i.e. those needed to determine the insurer’s benefits). Thus, <span class="math inline">\(\mathcal{A}\)</span> is a collection of subsets of <span class="math inline">\(\mathcal{E}\)</span>, a collection to which we impose the following three conditions (which make <span class="math inline">\(\mathcal{A}\)</span> what probabilists call a sigma-algebra or tribe):</p>
<ol style="list-style-type: decimal">
<li>the impossible event <span class="math inline">\(\emptyset\in\mathcal{A}\)</span> and the certain event <span class="math inline">\(\mathcal{E}\in\mathcal{A}\)</span>.</li>
<li>if <span class="math inline">\(E_1,E_2,\ldots\in\mathcal{A}\)</span> then <span class="math inline">\(\cup_{i\geq 1}E_i\in\mathcal{A}\)</span></li>
<li>if <span class="math inline">\(E\in\mathcal{A}\)</span> then <span class="math inline">\(\overline{E}\in\mathcal{A}\)</span>.</li>
</ol>
<p>Whatever the events <span class="math inline">\(E\)</span> and <span class="math inline">\(F\)</span> in <span class="math inline">\(\mathcal{A}\)</span>, we can check that the conditions imposed above guarantee that <span class="math inline">\(E\cap F\)</span> and <span class="math inline">\(E\smallsetminus F\)</span> also belong to <span class="math inline">\(\mathcal{A}\)</span>. More generally, whatever the sequence of events <span class="math inline">\(E_1,E_2,\ldots\)</span> in <span class="math inline">\(\mathcal{A}\)</span>, <span class="math inline">\(\cap_{i\geq 1}E_i\)</span> is also there.</p>
<p>The three conditions imposed above (i.e. the sigma-algebra structure) in fact guarantee the consistency of the reasoning, allowing us to speak of events occurring when several occur simultaneously (intersection) or when at least one of them occurs (union).</p>
</div>
</div>
<div id="probability-calculation-and-lack-of-arbitrage-opportunity" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> Probability calculation and lack of arbitrage opportunity<a href="chap2.html#probability-calculation-and-lack-of-arbitrage-opportunity" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="the-notion-of-probability" class="section level3 hasAnchor" number="3.3.1">
<h3><span class="header-section-number">3.3.1</span> The notion of probability<a href="chap2.html#the-notion-of-probability" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>How often do we hear phrases like “<em>it’s probably going to rain tomorrow</em>”, “<em>the plane will probably land 20 minutes late</em>” or “<em>there’s a good chance he’ll be able to attend this conference</em>”? Each of these sentences involves the notion of probability, even if it may seem difficult at first sight to quantify these ideas.</p>
<p>There are different ways of looking at the concept of probability. In the frequentist view, probability appears as the limiting value of the relative frequencies of occurrence of an event. The principle is relatively simple. It assumes that we are allowed to repeat a large number of times, under strictly identical conditions, an experiment in which a certain event may or may not occur. The ratio between the number of times the event has occurred and the total number of repetitions is called the relative frequency of occurrence of the event. This relative frequency tends to stabilize as the number of repetitions increases, and the limiting value is the probability of the event.</p>
<p>The shortcoming of the approach described above is, of course, its limited applicability. Many events cannot be repeated, or have not yet been repeated often enough, to produce a probability. In such cases, we can think of defining subjective probabilities: expert opinions on the likelihood of a given event.</p>
<p>Even if the notion of probability and its determination is problematic, once it has been determined, the calculation of probabilities can be used by an insurance company for risk management purposes. For example, during the first flight of the Ariane rocket, engineers calculated the probability of a successful launch on the basis of the technical file, which was used to set the insurance premium. The story goes that the risk of failure was greatly overestimated at the time.</p>
</div>
<div id="risk-and-uncertainty" class="section level3 hasAnchor" number="3.3.2">
<h3><span class="header-section-number">3.3.2</span> Risk and uncertainty<a href="chap2.html#risk-and-uncertainty" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In his 1921 book <em>Risk, Uncertainty and Profit</em>, economist Frank Knight introduced a fundamental distinction between the notions of risk and uncertainty. Risk occurs when the distribution governing the phenomenon of interest is known, but the outcome is nonetheless random. This assumes that the probability associated with the various events is known. For example, the throw of a balanced die entails a risk for the individual who would have bet 1,000 Euros on the six, but no uncertainty (since he is able to assess the probability of all the events associated with the throw of this die). On the contrary, uncertainty arises when these probabilities are at least partially unknown. If the bettor is faced with the throw of a rigged die, and does not know the chance of the various faces appearing, he is in a situation of uncertainty.</p>
</div>
<div id="probability-and-insurance-premium" class="section level3 hasAnchor" number="3.3.3">
<h3><span class="header-section-number">3.3.3</span> Probability and insurance premium<a href="chap2.html#probability-and-insurance-premium" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>From now on, we’ll assume risk, but not uncertainty. To each event <span class="math inline">\(E\)</span> of <span class="math inline">\(\mathcal{A}\)</span> we wish to associate a number, denoted <span class="math inline">\(\Pr[E]\)</span> and called the probability of <span class="math inline">\(E\)</span>. This number measures the degree of likelihood that we give <em>a priori</em> to the realization of <span class="math inline">\(E\)</span>: it is non-negative, and is greater the more likely the event is judged to be. The triplet <span class="math inline">\((\mathcal{E},\mathcal{A},\Pr)\)</span> is called a <a href="https://en.wikipedia.org/wiki/Probability_space">probabilistic space</a>, while the pair <span class="math inline">\((\mathcal{E},\mathcal{A})\)</span> is called a probabilistic space in probability theory.</p>
<p>Following Blaise Pascal’s definition, the probability <span class="math inline">\(\Pr[E]\)</span> associated with event <span class="math inline">\(E\)</span> can be thought of as the premium to be paid to receive 1 Euro in the event of <span class="math inline">\(E\)</span> occurring, provided that the insurance market meets a minimum level of rationality (i.e. it is impossible to make a profit without taking a risk). So, if <span class="math inline">\(E=\)</span> “<em>it rains in Paris on October 15</em>”, <span class="math inline">\(\Pr[E]\)</span> is the sum to be paid to receive 1 Euro if, on October 15, it rains in Paris (which may prove useful to the organizer of an outdoor event on that date). Clearly, if the policyholder no longer wants to receive 1 Euro but <span class="math inline">\(c\)</span> Euro if the event <span class="math inline">\(E\)</span> occurs, all he has to do is pay a premium <span class="math inline">\(c\Pr[E]\)</span>; <span class="math inline">\(\Pr[\cdot]\)</span> is therefore akin to a premium rate.</p>
<p>Clearly, the probability of any event is less than 1. Indeed, no rational policyholder would be prepared to pay a higher premium than the eventual indemnity paid by the insurer. So, whatever <span class="math inline">\(E\in\mathcal{A}\)</span>, <span class="math inline">\(\Pr[E]\leq 1\)</span>. We therefore seek to assign a probability to any eventuality, a number between 0 and 1, the lower the probability, the rarer the occurrence of the event.</p>
<p>From now on, we’ll assume that the insurance market is complete, i.e. that it’s possible to buy or sell a policy guaranteeing payment of 1Euro in the event of any random event occurring.</p>
</div>
<div id="absence-of-arbitrage-opportunity" class="section level3 hasAnchor" number="3.3.4">
<h3><span class="header-section-number">3.3.4</span> Absence of arbitrage opportunity<a href="chap2.html#absence-of-arbitrage-opportunity" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>It’s worth stressing here that the premiums we’re talking about transcend companies operating in a market: they exist in the absolute. In fact, a policyholder’s loss experience does not depend on which company covers him (assuming that all companies offer the same level of cover): a building is no more likely to go up in smoke if it is covered by one company rather than another. All the reasoning that follows is based on the now classic assumptions of no taxes, no transaction costs, and access to capital under identical conditions for all market players.</p>
<p>As financiers are wont to do, the arbitrage opportunity is the possibility of making a profit without risk. The idea is as follows: if the premium system adopted by market players does not satisfy a minimum of rationality, it becomes possible for certain players to carry out operations generating a definite profit, without an initial stake. Even if such a situation is not formally ruled out in practice, it would be unthinkable in a healthy market. Based on this simple principle, we’re going to establish a series of properties that probabilities must satisfy.</p>
</div>
<div id="property-of-additivity-for-incompatible-events" class="section level3 hasAnchor" number="3.3.5">
<h3><span class="header-section-number">3.3.5</span> Property of additivity for incompatible events<a href="chap2.html#property-of-additivity-for-incompatible-events" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If <span class="math inline">\(E\)</span> and <span class="math inline">\(F\)</span> are two incompatible events (i.e. they cannot occur simultaneously, henceforth <span class="math inline">\(E\cap F=\emptyset\)</span>) for which we have defined two probabilities <span class="math inline">\(\Pr[E]\)</span> and <span class="math inline">\(\Pr[F]\)</span>, i.e. we have determined the sums to be paid to receive 1 Euro in the event of these events occurring, then
<span class="math display" id="eq:AxAdd">\[\begin{equation}
\Pr[E\cup F]=\Pr[E]+\Pr[F].
\tag{3.1}
\end{equation}\]</span>
Let’s show that it can’t be otherwise. To do this, let’s see what would happen if <span class="math inline">\(\Pr[E\cup F]&gt;\Pr[E]+\Pr[F]\)</span>. In this case, the insurer issues a policy guaranteeing payment of 1Euro if either <span class="math inline">\(E\)</span> or <span class="math inline">\(F\)</span> is realized. With the premium <span class="math inline">\(\Pr[E\cup F]\)</span> collected, he takes out two policies (with a competitor), the first guaranteeing the payment of 1 Euro if <span class="math inline">\(E\)</span> occurs, and the second the same payment if <span class="math inline">\(F\)</span> occurs. This costs <span class="math inline">\(\Pr[E]+\Pr[F]\)</span> and requires no initial investment. At the end of the period, three situations may arise:</p>
<ol style="list-style-type: decimal">
<li><em>only <span class="math inline">\(E\)</span> has been realized:</em>
the insurer receives 1 Euro as indemnity and must pay 1 Euro in execution of the policy he issued. His profit therefore amounts to
<span class="math display">\[
\Pr[E\cup F]-Pr[E]-Pr[F]&gt;0.
\]</span></li>
<li><em>only <span class="math inline">\(F\)</span> has been realized:</em>
in this case, too, his profit amounts to
<span class="math display">\[
\Pr[E\cup F]-Pr[E]-Pr[F]&gt;0.
\]</span></li>
<li><em>neither <span class="math inline">\(E\)</span> nor <span class="math inline">\(F\)</span> are realized:</em>
the insurer receives nothing and pays nothing. His profit amounts to
<span class="math display">\[
\Pr[E\cup F]-\Pr[E]-\Pr[F]&gt;0.
\]</span></li>
</ol>
<p>In all cases, the insurer makes a positive profit, for an initial stake of zero, which contradicts the hypothesis of no arbitrage opportunity.</p>
<p>The reasoning is easily adapted to the case where <span class="math inline">\(\Pr[E\cup F]&lt;\Pr[E]+\Pr[F]\)</span>. It is sufficient to issue two policies, one providing for the payment of 1Euro if <span class="math inline">\(E\)</span> occurs, and the other for the same payment if <span class="math inline">\(F\)</span> occurs. The insurer then collects <span class="math inline">\(\Pr[E]+\Pr[F]\)</span>, with which he buys a policy providing 1 Euro if either <span class="math inline">\(E\)</span> or <span class="math inline">\(F\)</span> occurs (at a cost of <span class="math inline">\(\Pr[E\cup F]\)</span>). This strategy requires no initial investment, since by hypothesis <span class="math inline">\(\Pr[E\cup F]&lt;\Pr[E]+\Pr[F]\)</span>. In the three scenarios considered above, we verify that his profit amounts to <span class="math inline">\(\Pr[E]+\Pr[F]-\Pr[E\cup F]&gt;0\)</span>, which contradicts the hypothesis of no arbitrage opportunity.</p>
<p>More generally, considering a sequence of events <span class="math inline">\(E_1,E_2,E_3,\ldots\)</span> that are two by two incompatible (i.e. whatever <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>, <span class="math inline">\(E_i\)</span> and <span class="math inline">\(E_j\)</span> cannot occur simultaneously <span class="math inline">\(\Leftrightarrow\)</span> <span class="math inline">\(E_i\cap E_j=\emptyset\)</span> for any <span class="math inline">\(i\neq j\)</span>) we have:
<span class="math display" id="eq:SigmaAdd">\[\begin{equation}
\Pr\left[\cup_{k\geq 1}E_k\right]=\sum_{k\geq 1}\Pr[E_k].
\tag{3.2}
\end{equation}\]</span>
This sigma-additivity property is stronger than the additivity condition <a href="chap2.html#eq:AxAdd">(3.1)</a>. In particular, it allows us to calculate the probability of any event <span class="math inline">\(E={e_{i_1},\ldots,e_{i_k}}\)</span> as a function of the probabilities of the elementary events <span class="math inline">\(e_{i_1},\ldots,e_{i_k}\)</span> that make it up, since <a href="chap2.html#eq:SigmaAdd">(3.2)</a> guarantees that
<span class="math display">\[
\Pr[E]=\Pr[\{e_{i_1}\}]+\ldots+\Pr[\{e_{i_k}\}].
\]</span>
It is therefore sufficient to define the probability of each elementary event to deduce the probability of any event <span class="math inline">\(E\)</span>.</p>
</div>
<div id="premiums-grow-with-risk" class="section level3 hasAnchor" number="3.3.6">
<h3><span class="header-section-number">3.3.6</span> Premiums grow with risk<a href="chap2.html#premiums-grow-with-risk" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The additivity condition <a href="chap2.html#eq:AxAdd">(3.1)</a> also guarantees that premiums grow with the likelihood of insured events. To be precise, let’s consider two events <span class="math inline">\(E\)</span> and <span class="math inline">\(F\)</span>. If the occurrence of <span class="math inline">\(E\)</span> leads to the occurrence of <span class="math inline">\(F\)</span> (i.e. if <span class="math inline">\(E\)</span> has a greater chance of occurring than <span class="math inline">\(F\)</span>), it is reasonable to have <span class="math inline">\(\Pr[E]\leq\Pr[F]\)</span>. Since the insurer has a greater chance of having to pay the 1 Euro indemnity, it will demand a higher premium. From now on, we’ll note <span class="math inline">\(E\subseteq F\)</span> when the occurrence of <span class="math inline">\(F\)</span> implies that of
of <span class="math inline">\(F\)</span> implies that of <span class="math inline">\(E\)</span>. In this case,
<span class="math display">\[
F=(F\cap E)\cup(F\cap\overline{E})=E\cup(F\setminus E)
\]</span>
which expresses the fact that <span class="math inline">\(F\)</span> occurs if either <span class="math inline">\(E\)</span> or <span class="math inline">\(F\setminus E\)</span> occurs. We have thus expressed <span class="math inline">\(F\)</span> as the union of two incompatible events. The relation <a href="chap2.html#eq:AxAdd">(3.1)</a> then allows us to write
<span class="math display">\[
\Pr[F]=\Pr[E]+\Pr[F\setminus E]\geq \Pr[E].
\]</span>
so that it will always be more expensive to insure against the realization of <span class="math inline">\(F\)</span>, i.e.
<span class="math display">\[
E\subseteq F\Rightarrow \Pr[E]\leq\Pr[F].
\]</span></p>
</div>
<div id="fairness-property" class="section level3 hasAnchor" number="3.3.7">
<h3><span class="header-section-number">3.3.7</span> Fairness property<a href="chap2.html#fairness-property" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If <span class="math inline">\(E\)</span> and <span class="math inline">\(F\)</span> are two complementary events (i.e. <span class="math inline">\(F=\overline{E}\)</span>) then
<span class="math display">\[
\Pr[E]+\Pr[F]=1.
\]</span>
Indeed, having taken out a policy providing for the payment of 1 Euro if <span class="math inline">\(E\)</span> occurs, and another providing for the same payment if <span class="math inline">\(E\)</span> does not occur, we are certain to receive 1 Euro whatever happens. It would therefore be unfair to have paid more or less 1 Euro for this certain transaction. Formally, as <span class="math inline">\(E\cap F=\emptyset\)</span>, <span class="math inline">\(\Pr[E\cup F]=\Pr[E]+\Pr[F]\)</span>, we get the advertised result by noting that <span class="math inline">\(\Pr[E\cup F]=\Pr[\mathcal{E}]=1\)</span>. So, whatever the event <span class="math inline">\(E\)</span>, <span class="math inline">\(\Pr[\overline{E}]=1-\Pr[E]\)</span>.</p>
<p>Since <span class="math inline">\(\overline{\mathcal{E}}=\emptyset\)</span>, we deduce from the equity property that it costs nothing to purchase insurance coverage against an event that never occurs, i.e. <span class="math inline">\(\Pr[\emptyset]=0\)</span>. Note that this property is sometimes violated by the rates applied by insurers. This is particularly the case when the State imposes a certain degree of solidarity between policyholders. In some countries, for example, the legislator has made it compulsory for insurers to cover floods under fire policies. Quite apart from the questionable nature of such a measure (most often justified by budgetary considerations), an insured occupying an apartment on the 44th floor of a skyscraper will contribute to the cost of flood damage suffered by another insured whose building is located in a flood zone.</p>
</div>
<div id="subadditivity-property" class="section level3 hasAnchor" number="3.3.8">
<h3><span class="header-section-number">3.3.8</span> Subadditivity property<a href="chap2.html#subadditivity-property" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Whatever the events <span class="math inline">\(E\)</span> and <span class="math inline">\(F\)</span>, we have
<span class="math display">\[
\Pr[E\cup F]\leq\Pr[E]+\Pr[F].
\]</span>
Indeed, if we were to accept <span class="math inline">\(\Pr[E\cup F]&gt;\Pr[E]+\Pr[F]\)</span>, we’d be creating an arbitrage opportunity (i.e. the possibility of getting rich without risk), which cannot exist in a healthy market (the reader is invited to construct a strategy for getting rich without risk in such a case).</p>
<p>By iterating this result, we easily obtain that whatever the random events <span class="math inline">\(E_1,E_2,\ldots,E_n\)</span>, the inequality
<span class="math display">\[
\Pr[E_1\cup E_2\cup\ldots\cup E_n]\leq\sum_{i=1}^n\Pr[E_i]
\]</span>
is always satisfied.</p>
</div>
<div id="poincaré-equality" class="section level3 hasAnchor" number="3.3.9">
<h3><span class="header-section-number">3.3.9</span> Poincaré equality<a href="chap2.html#poincaré-equality" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Taking out two policies, one on <span class="math inline">\(E\cup F\)</span> and the other on <span class="math inline">\(E\cap F\)</span> always leads to the same cash flow as taking out two policies, one on <span class="math inline">\(E\)</span> and the other on <span class="math inline">\(F\)</span>. The premiums must then be equal, i.e.
<span class="math display" id="eq:PptPoincare">\[\begin{equation}
\Pr[E\cup F]+\Pr[E\cap F]=\Pr[E]+\Pr[F].
\tag{3.3}
\end{equation}\]</span></p>
<p>Note that the relationship <a href="chap2.html#eq:PptPoincare">(3.3)</a> can still be written as
<span class="math display">\[
\Pr[E\cup F]=\Pr[E]+\Pr[F]-\Pr[E\cap F].
\]</span>
This last relationship is known as <a href="https://en.wikipedia.org/wiki/Inclusion%E2%80%93exclusion_principle">Poincaré’s equality</a>.</p>
</div>
<div id="conditional-probability" class="section level3 hasAnchor" number="3.3.10">
<h3><span class="header-section-number">3.3.10</span> Conditional probability<a href="chap2.html#conditional-probability" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Insurers are often led to revise their premiums when additional information becomes available. Given the information at our disposal, materialized by the knowledge that an event <span class="math inline">\(F\)</span> has occurred, how do we re-evaluate the probability of an event <span class="math inline">\(E\)</span> occurring? This reassessment is called the conditional probability of <span class="math inline">\(E\)</span> knowing <span class="math inline">\(F\)</span>, denoted <span class="math inline">\(\Pr[E|F]\)</span>. Let’s start by noting that since <span class="math inline">\(F\)</span> has come true, it’s natural to have <span class="math inline">\(\Pr[F]&gt;0\)</span>. Under this condition, we can define the conditional probability as
<span class="math display" id="eq:DefCondExp">\[\begin{equation}
\Pr[E|F]=\frac{\Pr[E\cap F]}{\Pr[F]}\mbox{ provided that }\Pr[F]&gt;0.
\tag{3.4}
\end{equation}\]</span></p>
<p>It’s important to understand the meaning of the formula <a href="chap2.html#eq:DefCondExp">(3.4)</a>, which is actually quite intuitive. Once we know that event <span class="math inline">\(F\)</span> has occurred, only those events included in <span class="math inline">\(F\)</span> retain a chance of happening. Indeed, whatever the event <span class="math inline">\(E\)</span>, <span class="math inline">\(E\cap\overline{F}\)</span> becomes impossible (and therefore <span class="math inline">\(\Pr[E\cap\overline{F}|F]=0\)</span>) and only <span class="math inline">\(E\cap F\)</span> retains a chance of occurring. So, knowing that <span class="math inline">\(F\)</span> has come true, only the simultaneous realization of <span class="math inline">\(E\)</span> and <span class="math inline">\(F\)</span> is possible, which explains the numerator. What’s more, <span class="math inline">\(F\)</span> becomes a certain event, so we have to “normalize” the probabilities so that <span class="math inline">\(\Pr[F|F]=1\)</span>, which justifies division by <span class="math inline">\(\Pr[F]\)</span>.</p>
</div>
</div>
<div id="independent-events" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> Independent events<a href="chap2.html#independent-events" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Of course, some information does not cause the insurer to reassess the premium. This is known as <a href="https://en.wikipedia.org/wiki/Independence_(probability_theory)">independence</a>. This is expressed as follows: events <span class="math inline">\(E\)</span> and <span class="math inline">\(F\)</span> are independent when the premium claimed for the payment of 1Euro if <span class="math inline">\(E\)</span> occurs is the same whether or not we know whether <span class="math inline">\(F\)</span> has occurred, i.e..
<span class="math display">\[
\Pr[E|F]=\Pr[E|\overline{F}]\Leftrightarrow\Pr[E|F]=\Pr[E].
\]</span>
Note that by virtue of <a href="chap2.html#eq:DefCondExp">(3.4)</a> this is still equivalent to
<span class="math display">\[
\Pr[F|E]=\Pr[F],
\]</span>
or
<span class="math display">\[
\Pr[E|F]=\Pr[E]\Pr[F].
\]</span>
It is this last condition that is most often used to define the independence of two events (as it has the advantage of extending to impossible events and being symmetrical in <span class="math inline">\(E\)</span> and <span class="math inline">\(F\)</span>).</p>
<p>Let’s now extend the concept of independence to more than two events. Generally speaking, given events <span class="math inline">\(E_1,E_2,\ldots,E_n\)</span>, these are said to be independent when
<span class="math display">\[
\Pr\left[\bigcap_{i\in\mathcal{I}}E_i\right]=\prod_{i\in\mathcal{I}}\Pr[E_i]
\]</span>
whatever the subset <span class="math inline">\(\mathcal{I}\)</span> of <span class="math inline">\(\{1,\ldots,n\}\)</span>.</p>
</div>
<div id="multiplication-rule-bayes" class="section level2 hasAnchor" number="3.5">
<h2><span class="header-section-number">3.5</span> Multiplication rule (Bayes)<a href="chap2.html#multiplication-rule-bayes" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We can easily deduce from <a href="chap2.html#eq:DefCondExp">(3.4)</a> that
<span class="math display">\[
\Pr[E\cap F]=\Pr[E|F]\Pr[F].
\]</span>
This identity sometimes makes it easy to calculate <span class="math inline">\(\Pr[E\cap F]\)</span>. <a href="https://en.wikipedia.org/wiki/Chain_rule_(probability)">This rule</a> extends to any collection <span class="math inline">\(E_1,\ldots,E_n\)</span> of events such that <span class="math inline">\(\Pr[E_1\cap E_2\cap\ldots\cap E_n]&gt;0\)</span> as follows:
<span class="math display">\[
\Pr[E_1\cap E_2\cap\ldots\cap E_n]=\Pr[E_1]\Pr[E_2|E_1]\ldots\Pr[E_n|E_1\cap\ldots\cap E_{n-1}].
\]</span></p>
</div>
<div id="conditionally-independent-events" class="section level2 hasAnchor" number="3.6">
<h2><span class="header-section-number">3.6</span> Conditionally independent events<a href="chap2.html#conditionally-independent-events" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Two events <span class="math inline">\(E\)</span> and <span class="math inline">\(F\)</span> are <a href="https://en.wikipedia.org/wiki/Conditional_independence">conditionally independent</a> of a third event <span class="math inline">\(G\)</span> if
<span class="math display">\[
\Pr[E\cap F|G]=\Pr[E|G]\Pr[F|G].
\]</span>
This is again equivalent to requiring that
<span class="math display">\[
\Pr[F|E\cap G]=\Pr[F|G],
\]</span>
since
<span class="math display">\[\begin{eqnarray*}
\Pr[E\cap F|G]&amp;=&amp;\frac{\Pr[F|E\cap G]\Pr[E\cap G]}{\Pr[G]}
&amp;=&amp;\Pr[F|E\cap G]\Pr[E|G].
\end{eqnarray*}\]</span></p>
</div>
<div id="total-probability-theorem" class="section level2 hasAnchor" number="3.7">
<h2><span class="header-section-number">3.7</span> Total probability theorem<a href="chap2.html#total-probability-theorem" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Given two events <span class="math inline">\(E\)</span> and <span class="math inline">\(F\)</span> such that <span class="math inline">\(0&lt;Pr[F]&lt;1\)</span>, we can easily see that
<span class="math display">\[\begin{eqnarray*}
\Pr[E]&amp;=&amp;\Pr[E\cap F]+\Pr[E\cap\overline{F}]\\
&amp;=&amp;\Pr[E|F]\Pr[F]+\Pr[E|\overline{F}]\Pr[\overline{F}]
\end{eqnarray*}\]</span>
which deduces <span class="math inline">\(\Pr[E]\)</span> from <span class="math inline">\(\Pr[E|F]\)</span> and <span class="math inline">\(\Pr[E|\overline{F}]\)</span>.</p>
<div class="example">
<p><span id="exm:unlabeled-div-1" class="example"><strong>Example 3.1  </strong></span>Let’s denote <span class="math inline">\(E\)</span> the event “<em>the insured causes at least one claim over the year</em>” and <span class="math inline">\(F\)</span> the event “<em>the insured is a woman</em>”. If 10% of women and 15% of men cause at least one claim over the year, what is the proportion of policies causing at least one claim over the year in a portfolio comprising 2/3 men and 1/3 women? This proportion is
<span class="math display">\[\begin{eqnarray*}
\Pr[E]&amp;=&amp;\Pr[E|F]\Pr[F]+\Pr[E|\overline{F}]\Pr[\overline{F}]\\
&amp;=&amp;0.1\times \frac{1}{3}+0.15\times \frac{2}{3}=0.133.
\end{eqnarray*}\]</span></p>
</div>
<p>A natural extension of this result is known as the <a href="https://en.wikipedia.org/wiki/law_of_total_probability">total probability theorem</a> and is described below. Consider an exhaustive system of events <span class="math inline">\(\{F_1,F_2,\ldots\}\)</span>; such a system is such that any two of <span class="math inline">\(F_i\)</span> cannot occur simultaneously (i. e. <span class="math inline">\(F_i\cap F_j=\emptyset\)</span> if <span class="math inline">\(i\neq j\)</span>) and such that they cover all cases (i.e. <span class="math inline">\(\Pr[\cup_{i\geq 1}F_i]=1\)</span> with <span class="math inline">\(\Pr[F_i]&gt;0\)</span> for all <span class="math inline">\(i\)</span>). Then, whatever the event
<span class="math inline">\(E\)</span>,
<span class="math display">\[\begin{eqnarray*}
\Pr[E]&amp;=&amp;\Pr\Big[E\cap(\cup_{i\geq 1}F_i)\Big]\\
&amp;=&amp;\Pr\Big[\cup_{i\geq 1}(E\cap F_i)\Big]\\
&amp;=&amp;\sum_{i\geq 1}Pr[E\cap F_i]\\
&amp;=&amp;\sum_{i\geq 1}\Pr[E|F_i]\Pr[F_i].
\end{eqnarray*}\]</span></p>
</div>
<div id="bayes-theorem" class="section level2 hasAnchor" number="3.8">
<h2><span class="header-section-number">3.8</span> Bayes’ theorem<a href="chap2.html#bayes-theorem" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>One of the most interesting results involving conditional probabilities is undoubtedly Bayes’ theorem (named from <a href="https://en.wikipedia.org/wiki/Thomas_Bayes">Thomas Bayes</a>’s formula). Given an exhaustive system of events <span class="math inline">\(\{F_1,F_2,\ldots\}\)</span>, and <span class="math inline">\(E\)</span> any event of positive probability, the formula
<span class="math display">\[\begin{eqnarray*}
\Pr[F_i|E]&amp;=&amp;\frac{\Pr[E|F_i]\Pr[F_i]}{\Pr[E]}\\
&amp;=&amp;\frac{\Pr[E|F_i]\Pr[F_i]}
{\sum_{j\geq 1}\Pr[E|F_j]\Pr[F_j]},~i=1,2,\ldots,
\end{eqnarray*}\]</span>
is valid, where the denominator is obtained using the total probability formula.</p>
<div class="example">
<p><span id="exm:unlabeled-div-2" class="example"><strong>Example 3.2  (Fraud detection) </strong></span>To detect fraud, many insurers use specially designed computer programs. Consider an insurance company that submits its policyholders’ claims files to such a tool. If fraud does occur, the program detects it in 99% of cases. However, experience shows that the computer tool classifies as frauds 2% of files in good standing. Extrapolations to the market as a whole suggest that 1% of all claims actually involve fraud. What is the probability that a file classified as fraud by the computer program has actually given rise to this reprehensible practice? To assess this probability, let’s define the events
<span class="math display">\[\begin{eqnarray*}
E&amp;=&amp;\{\text{the computer tool detects a fraud}\}\\
F_1&amp;=&amp;\{\text{there is a fraud}\}=\overline{F}_2.
\end{eqnarray*}\]</span>
In this case, <span class="math inline">\(\Pr[F_1]=0.01\)</span>, <span class="math inline">\(\Pr[E|F_1]=0.99\)</span>, <span class="math inline">\(\Pr[E|F_2]=0.02\)</span>. The probability we’re looking for is
<span class="math display">\[\begin{eqnarray*}
\Pr[F_1|E]&amp;=&amp;\frac{\Pr[E|F_1]\Pr[F_1]}{\Pr[E|F_1]\Pr[F_1]+\Pr[E|F_2]\Pr[F_2]}\\
&amp;=&amp;\frac{0.99\times 0.01}{0.99\times 0.01+0.02\times 0.99}=\frac{1}{3}.
\end{eqnarray*}\]</span>}
Note that this value is quite low. For this reason, companies submit files for further examination by an inspector before incriminating the insured. Note that only a small proportion of files are inspected, since
<span class="math display">\[
\Pr[E]=0.99\times 0.01+0.02\times 0.99=2.97\%.
\]</span>
This last value also makes it intuitively clear why a test that seems reliable (since it detects fraud in 99% of cases when it is present) is actually wrong two times out of three: on average, the test will announce fraud in 297 cases out of 1,000, when in fact fraud will only occur in an average of 100 cases, resulting in an error of around two-thirds.
The number of cases of fraud that escape the insurer’s notice (assuming that careful inspection of the file by an employee detects any malfeasance on the part of the insured) is estimated at
<span class="math display">\[\begin{eqnarray*}
\Pr[F_1|\overline{E}]&amp;=&amp;\frac{\Pr[\overline{E}|F_1]\Pr[F_1]}{\Pr[\overline{E}|F_1]\Pr[F_1]+\Pr[\overline{E}|F_2]\Pr[F_2]}\\
&amp;=&amp;\frac{0.01\times 0.01}{0.01\times 0.01+0.98\times 0.99}=0.01\%.
\end{eqnarray*}\]</span></p>
</div>
<p>In the context of Bayes’ theorem, we often speak of probability <em>a priori</em> and probability <em>a posteriori</em> (or <em>prior</em> and <em>posterior</em>). The probabilities <span class="math inline">\(\Pr[F_1],\Pr[F_2],\ldots\)</span> are called <em>prior probabilities</em>, as they are calculated without any information. On the contrary, <span class="math inline">\(\Pr[F_i|E]\)</span> is a <em>posterior probability</em>, resulting from the revision of the probability <span class="math inline">\(\Pr[F_i]\)</span> on the basis of the information that <span class="math inline">\(E\)</span> has occurred. Thus, in the example above, there is <em>a priori</em> a one-in-a-hundred chance that the claim has resulted in fraud. <em>A posteriori</em>, i.e. knowing that the computer test has detected fraud, this probability increases to one in three.</p>
</div>
<div id="random-variables" class="section level2 hasAnchor" number="3.9">
<h2><span class="header-section-number">3.9</span> Random variables<a href="chap2.html#random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="definition" class="section level3 hasAnchor" number="3.9.1">
<h3><span class="header-section-number">3.9.1</span> Definition<a href="chap2.html#definition" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A random variable associates a number with each elementary event. It is therefore a function of <span class="math inline">\(\mathcal{E}\)</span> in <span class="math inline">\(\mathbb{R}\)</span> to which we impose certain conditions. From now on, we’ll denote random variables by capital letters: <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span>, … and their realizations by the corresponding lower-case letters: <span class="math inline">\(x\)</span>, <span class="math inline">\(y\)</span>, …</p>
<p>Formally, a <a href="https://en.wikipedia.org/wiki/Random_variable">random variable</a> is defined in relation to a probability space <span class="math inline">\((\mathcal{E},\mathcal{A},\Pr)\)</span>, although we’ll see later that actuary can often do without a precise definition of this space.</p>
<div class="definition">
<p><span id="def:DefVA" class="definition"><strong>Definition 3.1  (Random variable) </strong></span>A random variable <span class="math inline">\(X\)</span> is a function defined on the set <span class="math inline">\(\mathcal{E}\)</span> of elementary events and with values in <span class="math inline">\(\mathbb{R}\)</span>:<span class="math display">\[X: \mathcal{E}\to{\mathbb{R}};e\mapsto x=X(e)\]</span> to which we impose a technical condition (called the measurability condition): whatever the real <span class="math inline">\(x\)</span>, we want <span class="math display">\[\{X\leq x\}\equiv \{e\in\mathcal{E}|X(e)\leq x\}\in\mathcal{A}\]</span>
i.e. <span class="math inline">\(\{X\leq x\}\)</span> must be an event.</p>
</div>
<p>The measurability condition ensures that it is possible to take out a policy that pays out 1 Euro if the event “<span class="math inline">\(X\)</span> is less than or equal to <span class="math inline">\(x\)</span>” occurs.</p>
<p>There are many examples, from the most elementary where <span class="math inline">\(X\)</span> describes the face of a coin once it has landed (<span class="math inline">\(X\)</span> is 0 if you get heads and 1 if you get tails), to the more complicated case where <span class="math inline">\(X\)</span> is the number of people injured in an earthquake in the San Francisco area, or the time between two storms hitting the north of Paris. By extension, we’ll also allow <span class="math inline">\(X\)</span> to be constant.</p>
<div class="example">
<p><span id="exm:Travel" class="example"><strong>Example 3.3  (Travel) </strong></span>To fully understand the concrete meaning of this mathematical construction, let’s take a simple example. Let’s consider an insurance policy covering theft or loss of luggage during a given trip. This type of product is often offered by travel agencies to their customers. In order to avoid endless disputes over the amount of loss suffered by the traveler, the insurer pays a lump sum of 250 Euro if the luggage is lost or stolen during the trip (this also limits the risk of fraud). The only eventualities that need to be anticipated are either the theft or loss of baggage during the covered trip (in which case the insurer pays the 250 Euro lump sum), or the absence of such an event (in which case the insurer does not have to pay any benefits).</p>
<p>The following two elementary events may occur:
<span class="math display">\[\begin{eqnarray*}
e_1&amp;=&amp;\{\text{&quot;the luggage is neither stolen nor lost during the trip&quot;}\}\\
e_2&amp;=&amp;\{\text{&quot;luggage is lost or stolen during the trip&quot;}\}
\end{eqnarray*}\]</span>
Ainsi, <span class="math inline">\(\mathcal{E}=\{e_1,e_2\}\)</span> et <span class="math inline">\(\mathcal{A}=\big\{\emptyset,\{e_1\},\{e_2\},\mathcal{E}\big\}\)</span>.</p>
<p>Note that the definition of <span class="math inline">\(\mathcal{E}\)</span> is as concise as possible.
For example, we’re not interested in the specific circumstances surrounding the loss or theft of baggage, since these have no bearing on the insurer’s benefit.</p>
<p>The insurer’s financial burden <span class="math inline">\(X\)</span>, unknown at the time the policy is taken out, is therefore equal to
<span class="math display">\[
X(e)=\left\{
\begin{array}{l}
0\text{ Euro} ,\text{ if }e=e_1,\\
250\text{ Euro} ,\text{ if }e=e_2.
\end{array}
\right.
\]</span>
This function is a random variable since
<span class="math display">\[
\{X\leq x\}=\left\{
\begin{array}{l}
\emptyset,\text{ if }x&lt;0,\\
e_1,\text{ if }0\leq x &lt;250\text{ Euro} ,\\\
\mathcal{E},\text{ if }x\geq 250\text{ Euro} ,
\end{array}
\right.
\]</span>
is part of <span class="math inline">\(\mathcal{A}\)</span> whatever the value of <span class="math inline">\(x\)</span>.</p>
</div>
<p>Often, <span class="math inline">\(X(e)\)</span> represents the insurer’s total expenditure on a policy in the portfolio over a given period, when <span class="math inline">\(e\)</span> describes the observed reality. This is the variable of interest for the insurer, regardless of the type of benefits it promises. Indeed, even if the benefit takes different forms from the insured’s point of view, it always represents a financial cost for the insurer. This is all the more true as benefits in kind are usually entrusted to a third-party company, in order to better control costs. For example, the assistance package included in most automobile policies is outsourced to a specialist in this type of coverage. Some insurers even go so far as to subcontract claims handling to companies specializing in this field.</p>
</div>
<div id="distribution-function" class="section level3 hasAnchor" number="3.9.2">
<h3><span class="header-section-number">3.9.2</span> Distribution function<a href="chap2.html#distribution-function" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>It is clear that for many policies, listing the elements of <span class="math inline">\(\mathcal{E}\)</span> will prove tedious and difficult: this is due to the large number of situations that can lead to a claim, and to the uncertainty surrounding the financial consequences of such a claim. In practice, the definition of <span class="math inline">\(\mathcal{E}\)</span> is not necessary for actuarial calculations, and it is sufficient to know the distribution function of <span class="math inline">\(X\)</span> to be able to estimate the quantities the actuary needs to manage the portfolio.</p>
<p>Let’s assume that the random variable <span class="math inline">\(X\)</span> represents the amount of claims the company will have to pay out. In order to manage this risk, the actuary has information about previous claims caused by policies of the same type. This enables him to obtain a function <span class="math inline">\(F_X:{\mathbb{R}}\to[0,1]\)</span>, called the <a href="https://en.wikipedia.org/wiki/Cumulative_distribution_function">(cumulative) distribution function</a>, giving for each threshold <span class="math inline">\(x\in{\mathbb{R}}\)</span>, the probability (i.e. the “chance”) that <span class="math inline">\(X\)</span> will be less than or equal to <span class="math inline">\(x\)</span>.</p>
<div class="definition">
<p><span id="def:DefCDF" class="definition"><strong>Definition 3.2  (Cumulative Distribution Function) </strong></span>The distribution function <span class="math inline">\(F_X\)</span> associated with the random variable <span class="math inline">\(X\)</span> is defined as
<span class="math display" id="eq:Defcdf">\[\begin{equation}
F_X(x)=\Pr\Big[\{e\in\mathcal{E}|X(e)\leq x\}\Big]\equiv \Pr[X\leq x],\hspace{2mm}x\in{\mathbb{R}}.
\tag{3.5}
\end{equation}\]</span></p>
</div>
<p>Note that the measurability condition imposed in Definition <a href="chap2.html#def:DefVA">3.1</a> is very important here, as it guarantees that the event “<span class="math inline">\(X\)</span> is less than or equal to <span class="math inline">\(x\)</span>’’ is indeed an event whose probability can be measured. We can think of <span class="math inline">\(F_X(x)\)</span> as the premium to be paid to receive 1 Euro~if the event <span class="math inline">\(\{X\leq x\}\)</span> occurs.</p>
<div class="example">
<p><span id="exm:unlabeled-div-3" class="example"><strong>Example 3.4  </strong></span>The distribution function of the random variable <span class="math inline">\(X\)</span> defined in Example <a href="chap2.html#exm:Travel">3.3</a> is
<span class="math display">\[
F_X(x)=\left\{
\begin{array}{l}
\Pr[\emptyset]=0,\text{ if }x&lt;0,\\
\Pr[e_1],\text{ if }0\leq x &lt;250\text{ Euro} ,\\
\Pr[\mathcal{E}]=1,\text{ if }x\geq 250\text{ Euro} .
\end{array}
\right.
\]</span></p>
</div>
<p>All distribution functions share a series of common properties, which are listed below.</p>
<div class="proposition">
<p><span id="prp:PropCdf" class="proposition"><strong>Proposition 3.1  </strong></span>Any distribution function <span class="math inline">\(F_X\)</span> sends the real line <span class="math inline">\(\mathbb{R}\)</span>
on the unit interval <span class="math inline">\([0,1]\)</span> and</p>
<ul>
<li>is non-decreasing;</li>
<li>is continuous on the right, i.e. the identity
<span class="math display">\[
\lim_{\Delta x\to 0+}F_X(x+\Delta x)=F_X(x)
\]</span>
is valid regardless of <span class="math inline">\(x\in{\mathbb{R}}\)</span>;</li>
<li>satisfies
<span class="math display">\[
\lim_{x\to -\infty}F_X(x)=0\mbox{ and }\lim_{x\to +\infty}F_X(x)=1.
\]</span>
Any function satisfying the above conditions is the distribution function of a certain random variable <span class="math inline">\(X\)</span>.</li>
</ul>
</div>
<p>Since <span class="math inline">\(F_X\)</span> is non-decreasing, the limit
<span class="math display">\[
F_X(x-)=\lim_{\Delta x\to 0+}F_X(x-\Delta x)=\sup_{z&lt;x}F_X(z)=\Pr[X&lt;x]
\]</span>
is well defined. The function <span class="math inline">\(x\mapsto F_X(x-)\)</span> is non-decreasing and continuous on the left.</p>
<div class="remark">
<p><span id="unlabeled-div-4" class="remark"><em>Remark</em>. </span>Suppose <span class="math inline">\(X\)</span> represents the cost of a claim. Knowing <span class="math inline">\(F_X\)</span> does not mean that the actuary knows the amount of the claim. However, knowing <span class="math inline">\(F_X\)</span> provides the actuary with complete knowledge of the stochastic behavior of the random variable <span class="math inline">\(X\)</span>. Indeed, whatever the level <span class="math inline">\(x\)</span>, he knows how much it would cost to underwrite a policy providing for the payment of 1 Euro when the event <span class="math inline">\(\{X\leq x\}\)</span> occurs (i.e. when the amount of the claim is less than or equal to the threshold <span class="math inline">\(x\)</span>). Thanks to the elementary policies covering the <span class="math inline">\(\{X\leq x\}\)</span> events, the actuary is able to construct and price any more complex product.</p>
</div>
<p>It is possible to perform all actuarial calculations using <span class="math inline">\(F_X\)</span>, without explicitly defining the set <span class="math inline">\(\mathcal{E}\)</span> of elementary events and <span class="math inline">\(\mathcal{A}\)</span> of risk-conditioning events. This makes actuarial developments much easier</p>
</div>
<div id="support-of-a-random-variable" class="section level3 hasAnchor" number="3.9.3">
<h3><span class="header-section-number">3.9.3</span> Support of a random variable<a href="chap2.html#support-of-a-random-variable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The set of all possible values for a random variable is called its <a href="https://en.wikipedia.org/wiki/Support_(measure_theory)">support</a>. This notion is precisely defined as follows.</p>
<div id="DefSupp" class="defintion" name="Support">
<p>The support of a random variable <span class="math inline">\(X\)</span> with distribution function <span class="math inline">\(F_X\)</span> is the set of all points <span class="math inline">\(x\in{\mathbb{R}}\)</span> where <span class="math inline">\(F_X\)</span> is strictly increasing, i.e.
<span class="math display">\[
\text{support}(X)=\big\{x\in{\mathbb{R}}|F_X(x)&gt;F_X(x-\epsilon)\text{ for all }\epsilon&gt;0\big\}.
\]</span></p>
</div>
</div>
<div id="tail-or-survival-function" class="section level3 hasAnchor" number="3.9.4">
<h3><span class="header-section-number">3.9.4</span> Tail (or Survival) function<a href="chap2.html#tail-or-survival-function" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In addition to the distribution function, we often use its complement, called the “tail function” in non-life actuarial science (in biostatistics and life insurance, this same function is called the “survival function” when <span class="math inline">\(X\)</span> represents an individual’s lifespan). Noted <span class="math inline">\(\overline{F}_X\)</span>, it is defined as follows.</p>
<div class="definition">
<p><span id="def:unlabeled-div-5" class="definition"><strong>Definition 3.3  </strong></span>Given a random variable <span class="math inline">\(X\)</span>, the associated tail function is
<span class="math display">\[
\overline{F}_X(x)=1-F_X(x)=\Pr[X&gt;x],\hspace{2mm}x\in {\mathbb{R}};
\]</span>
<span class="math inline">\(\overline{F}_X(x)\)</span> therefore represents the probability of <span class="math inline">\(X\)</span> taking a value greater than <span class="math inline">\(x\)</span>.</p>
</div>
<p>We can see <span class="math inline">\(\overline{F}_X(x)\)</span> as the premium to be paid to receive a sum of 1 Euro if <span class="math inline">\(X\)</span> exceeds <span class="math inline">\(x\)</span>. Clearly, <span class="math inline">\(\overline{F}_X\)</span> is decreasing, since the event <span class="math inline">\(X&gt;x\)</span> is more likely than <span class="math inline">\(X&gt;x&#39;\)</span> when <span class="math inline">\(x&lt;x&#39;\)</span>, i.e. <span class="math inline">\(\{X&gt;x&#39;\}\subseteq\{X&gt;x\}\)</span>.</p>
<p>When the support of the distribution function <span class="math inline">\(F\)</span> of the claim amount is <span class="math inline">\(\mathbb{R}^+\)</span>, we generally measure the risk associated with a given distribution function by the thickness of the distribution tails (i.e. by the mass of probability distributed over the regions <span class="math inline">\((c,+\infty)\)</span>, for large values of <span class="math inline">\(c\)</span>). Thus, we speak of a thick (or heavy) tail when <span class="math inline">\(\overline{F}_X\)</span> tends only slowly towards <span class="math inline">\(0\)</span> when <span class="math inline">\(x\to +\infty\)</span>. We’ll come back to distribution tails in the chapter on extreme values.</p>
</div>
<div id="equality-in-distribution" class="section level3 hasAnchor" number="3.9.5">
<h3><span class="header-section-number">3.9.5</span> Equality in distribution<a href="chap2.html#equality-in-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Actuaries are often more interested in the distribution function of a random variable than in the random variable itself. Here, it’s essential to grasp the nuance between these two mathematical beings. The random variable <span class="math inline">\(S\)</span> may, for example, represent the total amount of claims generated by Mr. Doe over the coming year. On the other hand, let’s suppose that <span class="math inline">\(T\)</span> represents the amount of claims generated by Mrs. Doe. If these two individuals are indistinguishable for the insurer, this means that <span class="math inline">\(F_S(t)=F_T(t)\)</span> regardless of <span class="math inline">\(t\in\mathbb{R}\)</span>, which is henceforth noted as <span class="math inline">\(F_S\equiv F_T\)</span>. The company will then charge the same rate. Of course, there’s no reason why the realizations of <span class="math inline">\(S\)</span> and <span class="math inline">\(T\)</span> at the end of the period should be identical. The fact that <span class="math inline">\(S\)</span> and <span class="math inline">\(T\)</span> have the same distribution function does not mean that <span class="math inline">\(S=T\)</span>.</p>
<p>For an actuary who knows neither Mr. So-and-so nor Mrs. So-and-so, the only interest is in the common distribution function of <span class="math inline">\(S\)</span> and <span class="math inline">\(T\)</span>. From now on, we’ll write <span class="math inline">\(S=_{\text{distribution}}T\)</span> to express the fact that <span class="math inline">\(F_S\equiv F_T\)</span>. This means that <span class="math inline">\(S\)</span> and <span class="math inline">\(T\)</span> have the same distribution (or are identically distributed).</p>
</div>
<div id="quantiles-and-generalized-inverses" class="section level3 hasAnchor" number="3.9.6">
<h3><span class="header-section-number">3.9.6</span> Quantiles and generalized inverses<a href="chap2.html#quantiles-and-generalized-inverses" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="continuous-left-hand-or-right-hand-versions-of-a-monotone-function" class="section level4 hasAnchor" number="3.9.6.1">
<h4><span class="header-section-number">3.9.6.1</span> Continuous left-hand or right-hand versions of a monotone function<a href="chap2.html#continuous-left-hand-or-right-hand-versions-of-a-monotone-function" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Let be a monotone function <span class="math inline">\(g\)</span>. Such a function can only have a countable finite or infinite number of discontinuities.</p>
<div class="definition">
<p><span id="def:unlabeled-div-6" class="definition"><strong>Definition 3.4  </strong></span>The left-continuous version <span class="math inline">\(g_-\)</span> of <span class="math inline">\(g\)</span> and the right-continuous version <span class="math inline">\(g_+\)</span> of <span class="math inline">\(g\)</span> are defined as follows:
<span class="math display">\[
g _{-}(x)=\lim_{\Delta x\to 0+}g (x-\Delta x)\mbox{ and }
g _{+}(x)=\lim_{\Delta x\to 0+}g (x+\Delta x).
\]</span></p>
</div>
<p>It’s easy to check that <span class="math inline">\(g_-\)</span> is indeed continuous on the left, while <span class="math inline">\(g_+\)</span> is continuous on the right. Moreover,
<span class="math display">\[
g\mbox{ is continuous on the left in }x\Leftrightarrow
g _{-}(x)=g (x)
\]</span>
and
<span class="math display">\[
g\mbox{ is continuous on the right in }x\Leftrightarrow
g _{+}(x)=g (x).
\]</span></p>
</div>
<div id="generalized-inverse-of-a-non-decreasing-function" class="section level4 hasAnchor" number="3.9.6.2">
<h4><span class="header-section-number">3.9.6.2</span> Generalized inverse of a non-decreasing function<a href="chap2.html#generalized-inverse-of-a-non-decreasing-function" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>There are essentially two possible inverses for a non-decreasing function, introduced in the following definition.</p>
<div class="definition">
<p><span id="def:DeVylderInverses" class="definition"><strong>Definition 3.5  </strong></span>Given a non-decreasing function <span class="math inline">\(g\)</span>, the <a href="https://en.wikipedia.org/wiki/Inverse_function">inverses</a> <span class="math inline">\(g ^{-1}\)</span> and <span class="math inline">\(g ^{-1\bullet }\)</span> of <span class="math inline">\(g\)</span> are defined as follows:
<span class="math display">\[\begin{eqnarray*}
g ^{-1}(y)&amp;=&amp;\inf \left\{ x\in {\mathbb{R}}\mid y\leq g (x)\right\} \\
&amp;=&amp;\sup \left\{
x\in {\mathbb{R}}\mid y&gt;g (x)\right\}\\
g ^{-1\bullet }(y)&amp;=&amp;\inf \left\{ x\in {\mathbb{R}}\mid y&lt;g (x)\right\} \\
&amp;=&amp;\sup \left\{
x\in {\mathbb{R}}\mid y\geq g (x)\right\} ,
\end{eqnarray*}\]</span>
with the convention <span class="math inline">\(\inf\{\emptyset\}=+\infty\)</span> and <span class="math inline">\(\sup\{\emptyset\} =-\infty\)</span>.</p>
</div>
<p>We can check that <span class="math inline">\(g^{-1}\)</span> and <span class="math inline">\(g^{-1\bullet }\)</span> are both non-decreasing, and that <span class="math inline">\(g ^{-1}\)</span> is continuous on the left while <span class="math inline">\(g ^{-1\bullet }\)</span> is continuous on the right.</p>
<p>The following result will come in very handy later on.</p>
<div class="proposition">
<p><span id="prp:DhDevGoovLemma" class="proposition"><strong>Proposition 3.2  </strong></span>Let <span class="math inline">\(g\)</span> be non-decreasing. Whatever the real numbers <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, the following inequalities hold:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(g ^{-1}(y)\leq x\Leftrightarrow y\leq g _{+}(x)\)</span>,</li>
<li><span class="math inline">\(x\leq g^{-1\bullet }(y)\Leftrightarrow g _{-}(x)\leq y\)</span>.</li>
</ol>
</div>
<div class="proof">
<p><span id="unlabeled-div-7" class="proof"><em>Proof</em>. </span>We establish only (1); the reasoning leading to (2) is similar. The implication
<span class="math display">\[
g ^{-1}(y)\leq x\Rightarrow y\leq g _{+}(x)
\]</span>
is established when the contrapositive
<span class="math display">\[
y&gt;g _{+}(x)\Rightarrow x&lt;g ^{-1}(y)
\]</span>
is proved. Suppose <span class="math inline">\(y&gt;g _{+}(x)\)</span>. Then there exists a <span class="math inline">\(\epsilon &gt;0\)</span> such that <span class="math inline">\(y&gt;g (x+\epsilon )\)</span>. If we return to the definition of <span class="math inline">\(g^{-1}(y)\)</span> in terms of supremum, we find <span class="math inline">\(x+\epsilon \leq g ^{-1}(y)\)</span>, which implies that <span class="math inline">\(x&lt;g ^{-1}(y)\)</span>. Now let’s establish the “<span class="math inline">\(\Leftarrow\)</span>” part of (1). If <span class="math inline">\(y\leq g _{+}(x)\)</span> then we can write <span class="math inline">\(y\leq g(x+\epsilon)\)</span> for all <span class="math inline">\(\epsilon &gt;0\)</span>. From the definition of <span class="math inline">\(g^{-1}(y)\)</span> in terms of infimum, we are able to conclude that <span class="math inline">\(g^{-1}(y)\leq x+\epsilon\)</span> for all <span class="math inline">\(\epsilon &gt;0\)</span>. Passing to the limit for <span class="math inline">\(\epsilon \downarrow 0\)</span>, we obtain <span class="math inline">\(g^{-1}(y)\leq x\)</span>.</p>
</div>
</div>
<div id="generalized-inverse-of-a-non-increasing-function" class="section level4 hasAnchor" number="3.9.6.3">
<h4><span class="header-section-number">3.9.6.3</span> Generalized inverse of a non-increasing function<a href="chap2.html#generalized-inverse-of-a-non-increasing-function" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Let’s now turn our attention to non-increasing functions. To avoid any ambiguity, we’ll assume that <span class="math inline">\(g\)</span> is not constant.</p>
<div class="definition">
<p><span id="def:DefInvDec" class="definition"><strong>Definition 3.6  </strong></span>Let <span class="math inline">\(g\)</span> be a non-increasing (and non-constant) function. The inverses <span class="math inline">\(g ^{-1}\)</span> and <span class="math inline">\(g ^{-1\bullet}\)</span> of <span class="math inline">\(g\)</span> are defined as follows:
<span class="math display">\[\begin{eqnarray*}
g ^{-1}(y) &amp;=&amp;\inf \left\{ x\mid g (x)\leq y\right\} \\
&amp;=&amp;\sup \left\{
x\mid y&lt;g (x)\right\} , \\
g ^{-1\bullet }(y) &amp;=&amp;\inf \left\{ x\mid g (x)&lt;y\right\}\\
&amp;=&amp;\sup \left\{
x\mid y\leq g (x)\right\},
\end{eqnarray*}\]</span>
with the convention <span class="math inline">\(\inf\{\emptyset\}=+\infty\)</span> and <span class="math inline">\(\sup\{\emptyset\} =-\infty\)</span>.</p>
</div>
<p>It’s easy to check that <span class="math inline">\(g ^{-1}\)</span> and <span class="math inline">\(g ^{-1\bullet }\)</span> are both non-increasing, that <span class="math inline">\(g ^{-1}\)</span> is continuous on the right and <span class="math inline">\(g^{-1\bullet }\)</span> is continuous on the left. Furthermore, <span class="math inline">\(g ^{-1}(y)=g ^{-1\bullet }(y)\)</span> if, and only if, <span class="math inline">\(g^{-1}\)</span> is continuous in <span class="math inline">\(y\)</span>.</p>
<p>The following result is given without proof, the latter being in every respect similar to that of Property <a href="chap2.html#prp:DhDevGoovLemma">3.2</a>.</p>
<div class="proposition">
<p><span id="prp:DhDevGoovaLemmabis" class="proposition"><strong>Proposition 3.3  </strong></span>Let <span class="math inline">\(g\)</span> be non-increasing (and non-constant). Whatever the real numbers <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, the following equivalences apply:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(g ^{-1}(y)\leq x\Leftrightarrow y\geq g _{+}(x)\)</span>,</li>
<li><span class="math inline">\(x\leq g ^{-1\bullet }(y)\Leftrightarrow g _{-}(x)\geq y\)</span>.</li>
</ol>
</div>
</div>
<div id="quantile-functions" class="section level4 hasAnchor" number="3.9.6.4">
<h4><span class="header-section-number">3.9.6.4</span> Quantile functions<a href="chap2.html#quantile-functions" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>A concept widely used in the following is the <a href="https://en.wikipedia.org/wiki/Quantile">quantile</a>. This is a threshold that will only be exceeded (by the claims burden, for example) in a fixed proportion of cases. The function that maps this threshold to the proportion in question is called a quantile function and is defined as follows (in accordance with Definition <a href="chap2.html#def:DeVylderInverses">3.5</a>.</p>
<div class="definition">
<p><span id="def:DefQuantile" class="definition"><strong>Definition 3.7  </strong></span>The quantile of order <span class="math inline">\(p\)</span> of the random variable <span class="math inline">\(X\)</span>, denoted <span class="math inline">\(q_p\)</span>, is defined as follows:
<span class="math display">\[
q_p=F_X^{-1}(p)=\inf\big\{x\in {\mathbb{R}}|F_X(x)\geq p\big\},~p\in [0,1].
\]</span>
The function <span class="math inline">\(F_X^{-1}\)</span> defined in this way is called the quantile function associated with the distribution function <span class="math inline">\(F_X\)</span>.</p>
</div>
<p>Some quantiles have special names (depending on the value of <span class="math inline">\(p\)</span>). Thus, when <span class="math inline">\(p=\frac{1}{2}\)</span> we speak of median, when <span class="math inline">\(p=\frac{1}{4}\)</span> of first quartile, when <span class="math inline">\(p=\frac{3}{4}\)</span> of third quartile, when <span class="math inline">\(p=\frac{k}{10}\)</span> of <span class="math inline">\(k\)</span>th decile, <span class="math inline">\(k=1,\ldots,9\)</span>, and when <span class="math inline">\(p=\frac{k}{100}\)</span> of <span class="math inline">\(k\)</span>th percentile, <span class="math inline">\(k=1,\ldots,99\)</span>.</p>
<p>Although the quantile function is conventionally defined in accordance with Definition <a href="chap2.html#def:DefQuantile">3.7</a>, the inverse is also used
<span class="math display">\[
F_X^{-1\bullet}(p)=\inf\{x\in{\mathbb{R}}|F_X(x)&gt;p\}.
\]</span></p>
<p>The results of Property <a href="chap2.html#prp:DhDevGoovLemma">3.2</a> can of course be applied to distribution functions. From (i) we derive
<span class="math display" id="eq:GenerationF0">\[\begin{equation}
\tag{3.6}
F_{X}^{-1}(p)\leq x\Leftrightarrow p\leq F_X(x).
\end{equation}\]</span>
Similarly, (ii) provides
<span class="math display" id="eq:GenerationF1">\[\begin{equation}
\tag{3.7}
x\leq F_{X}^{-1\bullet }(p)\Leftrightarrow F_X(x-)=\Pr\left[ X&lt;x\right] \leq p.
\end{equation}\]</span></p>
<p>Clearly
<span class="math display">\[
F_X^{-1\bullet}(0)=\inf\{x\in {\mathbb{R}}|F_X(x)&gt;0\}
\]</span>
is the minimum value taken by <span class="math inline">\(X\)</span> (possibly equal to <span class="math inline">\(-\infty\)</span>, but most often equal to 0 in our framework) and
<span class="math display">\[
F_X^{-1}(1)=\sup\{x\in {\mathbb{R}}|F_X(x)&lt;1\}
\]</span>
is the maximum value taken by <span class="math inline">\(X\)</span>, which can be equal to <span class="math inline">\(+\infty\)</span>. The support of <span class="math inline">\(X\)</span> is therefore included in the interval <span class="math inline">\([F_X^{-1\bullet}(0),F_X^{-1}(1)]\)</span>.</p>
<div class="remark">
<p><span id="unlabeled-div-8" class="remark"><em>Remark</em>. </span><span class="math inline">\(F_X^{-1}(1)\)</span> is sometimes referred to as the Maximum Possible Claim (MPC). Clearly, whatever the insurance policy, the SMP is finite. However, choosing the SMP is sometimes difficult, especially in lines where the insurer provides unlimited cover. This is why SMP<span class="math inline">\(=+\infty\)</span> is often used in lines where the SMP is very high and difficult to assess, to ensure that the insurer’s commitments are not undervalued. This leads the actuary to use distributions whose support is <span class="math inline">\(\mathbb{R}^+\)</span> to model the cost of claims.</p>
</div>
</div>
<div id="properties-of-quantile-functions" class="section level4 hasAnchor" number="3.9.6.5">
<h4><span class="header-section-number">3.9.6.5</span> Properties of quantile functions<a href="chap2.html#properties-of-quantile-functions" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Note that <span class="math inline">\(F_X^{-1}\)</span> is non-decreasing. Consider <span class="math inline">\(p_1\geq p_2\in [0,1]\)</span>. The inclusion
<span class="math display">\[
\{x\in{\mathbb{R}}|F_X(x)\geq p_2\}\subseteq
\{x\in{\mathbb{R}}|F_X(x)\geq p_1\}
\]</span>
guarantees that <span class="math inline">\(F_X^{-1}(p_1)\geq F_X^{-1}(p_2)\)</span>.</p>
<p>Finally, note that the identity <span class="math inline">\(F_X^{-1}(F_X(x))=x\)</span> is generally false, unless <span class="math inline">\(F_X^{-1}\)</span> is continuous at <span class="math inline">\(F_X(x)\)</span>. This means that the identity <span class="math inline">\(F_X^{-1}(F_X(x))=x\)</span> holds for all values of <span class="math inline">\(x\)</span> that do not correspond to a step in <span class="math inline">\(F_X\)</span>. On the other hand, it is always true that <span class="math inline">\(F_X(F_X^{-1}(p))=p\)</span>.</p>
</div>
<div id="inverse-of-tail-function" class="section level4 hasAnchor" number="3.9.6.6">
<h4><span class="header-section-number">3.9.6.6</span> Inverse of tail function<a href="chap2.html#inverse-of-tail-function" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We can easily define the inverse of the tail function from Definition <a href="chap2.html#def:DefInvDec">3.6</a>, since this is a non-increasing function. It’s easy to see that the inverses of the distribution function <span class="math inline">\(F_X\)</span> and the corresponding tail function <span class="math inline">\(\overline{F}_X\)</span> are linked by the relations
<span class="math display" id="eq:DefInvQueue">\[\begin{equation}
\tag{3.8}
F_{X}^{-1}(p)=\overline{F}_{X}^{-1}(1-p)
\mbox{ and }
F_{X}^{-1\bullet }(p)=\overline{F}_{X}^{-1\bullet }(1-p),
\end{equation}\]</span>
whatever <span class="math inline">\(p\in[0,1]\)</span>.</p>
</div>
</div>
</div>
<div id="discrete-random-variables-and-counts" class="section level2 hasAnchor" number="3.10">
<h2><span class="header-section-number">3.10</span> Discrete random variables and counts<a href="chap2.html#discrete-random-variables-and-counts" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="notion" class="section level3 hasAnchor" number="3.10.1">
<h3><span class="header-section-number">3.10.1</span> Notion<a href="chap2.html#notion" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As their name suggests, these variables count a number of events, such as the number of claims made in a given period. Henceforth, we’ll denote count variables by mid-alphabet capital letters: <span class="math inline">\(I\)</span>, <span class="math inline">\(J\)</span>, <span class="math inline">\(K\)</span>, <span class="math inline">\(L\)</span>, <span class="math inline">\(N\)</span>, <span class="math inline">\(M\)</span>, Their support is therefore (contained in) <span class="math inline">\({\mathbb{N}}=\{0,1,2,\ldots}\)</span>.</p>
<p>A counting random variable <span class="math inline">\(N\)</span> is characterized by the sequence of probabilities <span class="math inline">\(\p_k,\hspace{2mm}k\in{\mathbb{N}}\}\)</span> associated with the different integers, i.e. <span class="math inline">\(p_k=\Pr[N=k]\)</span>. The distribution function of <span class="math inline">\(N\)</span> is “stepped”, with jumps in amplitude <span class="math inline">\(p_k\)</span> occurring at integers <span class="math inline">\(k\)</span>, i.e.
<span class="math display">\[
F_N(x)=\Pr[N\leq x]=\sum_{k=0}^{\lfloor x\rfloor}p_k,\hspace{2mm}x\in{\mathbb{R}},
\]</span>
where <span class="math inline">\(\lfloor x\rfloor\)</span> denotes the integer part of the real <span class="math inline">\(x\)</span>. Clearly, <span class="math inline">\(F_N(x)=0\)</span> if <span class="math inline">\(x&lt;0\)</span>.</p>
<div class="remark">
<p><span id="unlabeled-div-9" class="remark"><em>Remark</em>. </span>More generally, we speak of a discrete variable when it takes its values from a set <span class="math inline">\(\{a_0,a_1,a_2,\ldots}\)</span>, which is finite or can be put in bijection with <span class="math inline">\({\mathbb{N}}\)</span>. The results concerning counting variables are easily extended to the discrete case by substituting <span class="math inline">\(a_k\)</span> for the integer <span class="math inline">\(k\)</span>.</p>
</div>
<p>Among counting variables, we’ll mainly use those whose probability distributions are presented in the following paragraphs. Many of them are associated with what probabilists call a Bernoulli scheme or a binomial lattice. This involves repeating a random experiment a number of times, each time observing whether a specified event occurs. The experiment must be reproduced under identical conditions, and previous results must not influence the eventual occurrence of the event of interest.</p>
</div>
<div id="uniform-discrete-variable" class="section level3 hasAnchor" number="3.10.2">
<h3><span class="header-section-number">3.10.2</span> Uniform discrete variable<a href="chap2.html#uniform-discrete-variable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A random variable <span class="math inline">\(N\)</span> is said to have a <a href="https://en.wikipedia.org/wiki/Discrete_uniform_distribution">discrete uniform</a> distribution on <span class="math inline">\(\{0,1,\ldots,n\}\)</span>, henceforth denoted <span class="math inline">\(N\sim\mathcal{DU}ni(n)\)</span>, when
<span class="math display">\[
\Pr[N=k]=\frac{1}{n+1}\text{ for }k=0,1,\ldots,n.
\]</span>
The probability masses assigned to the integers <span class="math inline">\(0,1,\ldots,n\)</span> are therefore all identical, hence the term uniform: no value of the support <span class="math inline">\(\{0,1,\ldots,n\}\)</span> is more probable than the others. Clearly, the distribution function associated with this random variable is given by
<span class="math display">\[
F_N(x)=\left\{
\begin{array}{l}
0,\text{ if }x&lt;0,\\
\displaystyle\frac{\lfloor x\rfloor +1}{n+1},\text{ if }0\leq x&lt;n,\\
1,\text{ if }x\geq n.
\end{array}
\right.
\]</span>
It is therefore a function whose graph makes jumps of amplitude <span class="math inline">\(\frac{1}{n+1}\)</span> at each integer <span class="math inline">\(0,1,\ldots,n\)</span>.</p>
</div>
<div id="bernoulli-variables" class="section level3 hasAnchor" number="3.10.3">
<h3><span class="header-section-number">3.10.3</span> Bernoulli variables<a href="chap2.html#bernoulli-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A random variable <span class="math inline">\(N\)</span> is called a Bernoulli variable with parameter <span class="math inline">\(0&lt;q&lt;1\)</span>, or said to obey this distribution, which is henceforth noted as <span class="math inline">\(N\sim\mathcal{B}er(q)\)</span>, when
<span class="math display">\[
\Pr[N=1]=q=1-\Pr[N=0].
\]</span>
A Bernoulli variable indicates whether the event of interest was realized when the experiment was repeated in a Bernoulli scheme (such a scheme assumes that a random experiment is performed and that we are interested in the realization of a given event on this occasion).</p>
<div class="example">
<p><span id="exm:unlabeled-div-10" class="example"><strong>Example 3.5  </strong></span>In actuarial science, Bernoulli’s distribution is traditionally associated with the indicator of a random event. The indicator is equal to 1 when the event is realized (and thus indicates the realization of this event). Such events are “the policy has produced at least one claim during the reference period” or “the insured died during the year”, for example.</p>
</div>
</div>
<div id="binomial-variable" class="section level3 hasAnchor" number="3.10.4">
<h3><span class="header-section-number">3.10.4</span> Binomial variable<a href="chap2.html#binomial-variable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A random variable <span class="math inline">\(N\)</span> is said to be <a href="https://en.wikipedia.org/wiki/Binomial_distribution">Binomial</a> with exponent <span class="math inline">\(m\)</span> and parameter <span class="math inline">\(q\)</span>, <span class="math inline">\(m\in{\mathbb{N}}\)</span> and <span class="math inline">\(0&lt;q&lt;1\)</span>, or to obey this distribution, which will henceforth be noted as <span class="math inline">\(N\sim\mathcal{B}in(m,q)\)</span>, when <span class="math inline">\(N\)</span> takes its values in <span class="math inline">\(\{0,1,\ldots,m\}\)</span> and
<span class="math display">\[
\Pr[N=k]=\left(
\begin{array}{c}
m \\
k
\end{array}
\right)q^k(1-q)^{m-k},\hspace{2mm}k=0,1,\ldots,m,
\]</span>
where the notation <span class="math inline">\((:)\)</span> denotes Newton’s binomial coefficient (sometimes also denoted <span class="math inline">\(C_m^k\)</span>), defined by
<span class="math display">\[
\left(
\begin{array}{c}
m \\
k
\end{array}
\right)=\frac{m!}{k!(m-k)!}.
\]</span>
By extension, we sometimes refer to a random variable obeying this distribution as <span class="math inline">\(\mathcal{B}in(m,q)\)</span>.</p>
<div class="remark">
<p><span id="unlabeled-div-11" class="remark"><em>Remark</em>. </span>Note that when <span class="math inline">\(m=1\)</span>, we have for <span class="math inline">\(k\in\{0,1\}\)</span>,
<span class="math display">\[
\Pr[N=k]=q^k(1-q)^{1-k}=\left\{
\begin{array}{l}
q,\text{ if }k=1,\\
1-q,\text{ if }k=0,
\end{array}
\right.
\]</span>
and we find <a href="https://en.wikipedia.org/wiki/Jacob_Bernoulli">Bernoulli</a>’s distribution, which thus appears as a special case of the Binomial, i.e. <span class="math inline">\(\mathcal{B}in(1,q)=\mathcal{B}er(q)\)</span>.</p>
</div>
<p>The binomial distribution is classically associated with the number of successes in a Bernoulli scheme: if a random experiment is repeated <span class="math inline">\(m\)</span> independently and under identical conditions, the number of repetitions in which a certain event <span class="math inline">\(E\)</span> occurs has the distribution <span class="math inline">\(\mathcal{B}in(m,\Pr[E])\)</span>. Indeed, given <span class="math inline">\(q=\Pr[E]\)</span>, a glance at the shape of the binomial probability is enough to convince us that the above interpretation is correct: <span class="math inline">\(q^k\)</span> guarantees that <span class="math inline">\(E\)</span> has been realized <span class="math inline">\(k\)</span> times, <span class="math inline">\((1-q)^{m-k}\)</span> ensures that <span class="math inline">\(E\)</span> has not been realized more often (the other <span class="math inline">\(m-k\)</span> repetitions not having resulted in the realization of <span class="math inline">\(E\)</span>) while the binomial coefficient expresses that the order in which the realizations occurred is unimportant (since we’re only interested in the number of times <span class="math inline">\(E\)</span> has been realized).</p>
<div class="example">
<p><span id="exm:unlabeled-div-12" class="example"><strong>Example 3.6  </strong></span>The binomial distribution lends itself well to modeling the number of claims affecting the portfolio in forms of insurance where at most one claim per period is possible for each policy (such as life insurance, cancellation insurance or repatriation insurance taken out for a specified trip, for example). If <span class="math inline">\(q\)</span> is the probability that one of the <span class="math inline">\(m\)</span> policies in the portfolio will give rise to a claim, and if these <span class="math inline">\(m\)</span> policies are identical and do not influence each other, the number <span class="math inline">\(N\)</span> of claims will have the distribution <span class="math inline">\(\mathcal{B}in(m,q)\)</span>.</p>
</div>
</div>
<div id="geometric-variable" class="section level3 hasAnchor" number="3.10.5">
<h3><span class="header-section-number">3.10.5</span> Geometric variable<a href="chap2.html#geometric-variable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The counting variable <span class="math inline">\(N\)</span> is said to be <a href="https://en.wikipedia.org/wiki/Geometric_distribution">geometric</a> with parameter <span class="math inline">\(q\)</span>, <span class="math inline">\(0&lt;q&lt;1\)</span>, or to obey this distribution, which will henceforth be noted as <span class="math inline">\(N\sim\mathcal{G}eo(q)\)</span> when
<span class="math display">\[
\Pr[N=k]=q(1-q)^k,\hspace{2mm}k\in{\mathbb{N}}.
\]</span>
If we consider a binomial scheme and speak of success when an event <span class="math inline">\(E\)</span> occurs during one of the repetitions of the random experiment (and of failure in the opposite case), the distribution <span class="math inline">\(\mathcal{G}eo(q)\)</span> can be seen as that of the number of failures preceding the first success. When <span class="math inline">\(N=k\)</span>, it will therefore have taken <span class="math inline">\(k+1\)</span> repetitions of the experiment to obtain a first success.</p>
<div class="example">
<p><span id="exm:unlabeled-div-13" class="example"><strong>Example 3.7  </strong></span>Consider a company inspector checking claims files for fraud. If <span class="math inline">\(q\)</span> denotes the proportion of files in which fraud has occurred, the probability that he will check <span class="math inline">\(k\)</span> of them before coming across a first file in which fraud has occurred is <span class="math inline">\(q(1-q)^k\)</span>.</p>
</div>
</div>
<div id="negative-binomial-variable" class="section level3 hasAnchor" number="3.10.6">
<h3><span class="header-section-number">3.10.6</span> Negative binomial variable<a href="chap2.html#negative-binomial-variable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A random variable <span class="math inline">\(N\)</span> is said to have a <a href="https://en.wikipedia.org/wiki/Negative_binomial_distribution">negative binomial</a> distribution with parameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(q\)</span>, <span class="math inline">\(\alpha&gt;0\)</span> and <span class="math inline">\(0&lt;q&lt;1\)</span>, or is said to obey this distribution, which will henceforth be noted as <span class="math inline">\(N\sim\mathcal{NB}in(\alpha,q)\)</span>, when <span class="math inline">\(N\)</span> takes its values in <span class="math inline">\({\mathbb{N}}\)</span> and
<span class="math display">\[
\Pr[N=k]=\left(
\begin{array}{c}
\alpha+k-1 \\
k
\end{array}
\right)q^\alpha(1-q)^k,\hspace{f2mm}k\in{\mathbb{N}}.
\]</span>
When <span class="math inline">\(\alpha\)</span> is an integer, <span class="math inline">\(\Pr[N=k]\)</span> uses the classical binomial coefficient. However, this definition must be extended to the case where <span class="math inline">\(\alpha\)</span>
is non-integer. This extension calls on the <a href="https://en.wikipedia.org/wiki/Gamma_function">gamma function</a>, denoted <span class="math inline">\(\Gamma(\cdot)\)</span>, and defined by
<span class="math display">\[
\Gamma(t)=\int_{x\in{\mathbb{R}}^+}x^{t-1}\exp(-x)dx,\hspace{2mm}t\in{\mathbb{R}}^+.
\]</span>
A simple integration by parts shows that equality
<span class="math display">\[
\Gamma(t)=(t-1)\Gamma(t-1)
\]</span>
is valid whatever <span class="math inline">\(t&gt;1\)</span>. This is an interpolation of the factorial function, since
<span class="math display">\[
\Gamma(n+1)=n!\mbox{ for }n\in{\mathbb{N}}.
\]</span>
The generalized binomial coefficient involved in the probability associated with the distribution
<span class="math inline">\(\mathcal{NB}in(\alpha,q)\)</span> must therefore be understood as follows:
<span class="math display">\[
\left(
\begin{array}{c}
\alpha+k-1 \\
k
\end{array}
\right)=\frac{\Gamma(\alpha+k)}{\Gamma(k+1)\Gamma(\alpha)}=
\frac{\Gamma(\alpha+k)}{k!\Gamma(\alpha)}.
\]</span></p>
<p>When <span class="math inline">\(\alpha\)</span> is an integer, the distribution <span class="math inline">\(\mathcal{NB}in(\alpha,q)\)</span> is still called Pascal’s distribution. This distribution has a clear interpretation in the context of a binomial scheme. As above, let’s suppose we’re interested in the eventual realization of an event <span class="math inline">\(E\)</span>, and call such a realization a success (with <span class="math inline">\(q=\Pr[E]\)</span>). When <span class="math inline">\(\alpha\)</span> is an integer, the variable <span class="math inline">\(\mathcal{NB}in(\alpha,q)\)</span> is simply the number of failures required to obtain <span class="math inline">\(\alpha\)</span> success. Indeed, the form of the negative binomial probability indicates that in total <span class="math inline">\(\alpha\)</span> successes (factor <span class="math inline">\(q^\alpha\)</span>) and <span class="math inline">\(k\)</span> failures (factor <span class="math inline">\((1-q)^k\)</span>) have been obtained, while the binomial coefficient reflects the fact that the order of appearance of these successes does not matter, except for the last result, which must be a success (we can therefore freely place the <span class="math inline">\(k\)</span> failures among the <span class="math inline">\(\alpha+k-1\)</span> successive realizations of the random experiment).</p>
<div class="example">
<p><span id="exm:unlabeled-div-14" class="example"><strong>Example 3.8  </strong></span>Let’s return for a moment to our inspector who decides to check claims files until he discovers <span class="math inline">\(\alpha\)</span> fraud. Knowing that the proportion of files resulting in fraud is <span class="math inline">\(q\)</span>, <span class="math inline">\(\Pr[N=k]\)</span> is the probability that the inspector will have had to check <span class="math inline">\(k\)</span> files in good standing before discovering the <span class="math inline">\(\alpha\)</span> fraudulent files. In total, our man will have checked <span class="math inline">\(k+\alpha\)</span> files.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-15" class="remark"><em>Remark</em>. </span>In particular, when <span class="math inline">\(\alpha=1\)</span>, we find the geometric distribution, i.e. <span class="math inline">\(\mathcal{NB}in(1,q)=\mathcal{G}eo(q)\)</span>.</p>
</div>
</div>
<div id="poissons-distribution" class="section level3 hasAnchor" number="3.10.7">
<h3><span class="header-section-number">3.10.7</span> Poisson’s distribution<a href="chap2.html#poissons-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Poisson’s distribution was obtained as a limit of the binomial distribution by Poisson as early as 1837. Among the first empirical applications of this probability distribution was the study by Ladislaus Bortkiewicz in 1898, who used Poisson’s distribution to model the annual number of soldiers killed by horse kicks in the Prussian army. Also known as the distribution of rare events, Poisson’s distribution was originally associated with counting accidents or breakdowns.</p>
<p><a href="https://en.wikipedia.org/wiki/Poisson_distribution">Poisson’s distribution</a> was introduced as an approximation to the binomial distribution when <span class="math inline">\(m\)</span> was very large and <span class="math inline">\(q\)</span> very small. Consider a random variable <span class="math inline">\(N_m\sim\mathcal{B}in(m,\frac{\lambda}{m})\)</span>. We have
<span class="math display">\[
\Pr[N_m=0]=\left(1-\frac{\lambda}{m}\right)^m\to\exp(-\lambda),\text{ if }m\to +\infty.
\]</span>
In addition
<span class="math display">\[
\frac{\Pr[N_m=k+1]}{\Pr[N_m=k]}=\frac{\frac{m-k}{k+1}\frac{\lambda}{m}}{1-\frac{\lambda}{m}}\to\frac{\lambda}{k+1},
\text{ if }m\to +\infty
\]</span>
so that
<span class="math display">\[
\lim_{m\to+\infty}\Pr[N_m=k]=\exp(-\lambda)\frac{\lambda^k}{k!}.
\]</span>
The probability appearing in the right-hand side of this last equation is that defining Poisson’s distribution. More precisely, when
<span class="math display">\[
\Pr[N=k]=\exp(-\lambda)\frac{\lambda^k}{k!},
\hspace{2mm}k\in{\mathbb{N}},
\]</span>
the counting variable <span class="math inline">\(N\)</span> is said to have a Poisson distribution with parameter
which will henceforth be denoted <span class="math inline">\(N\sim\mathcal{P}oi(\lambda)\)</span>.
By extension, we’ll also use the term <span class="math inline">\(\mathcal{P}oi(\lambda)\)</span> to designate a random variable obeying this distribution. Poisson’s distribution can thus be seen as that of the number of successes in a Bernoulli scheme, when the number of repetitions is very large (<span class="math inline">\(mq\to +\infty\)</span>) and the probability of success negligible (<span class="math inline">\(q\to 0\)</span>) so that <span class="math inline">\(mq\to \lambda&gt;0\)</span>.</p>
</div>
</div>
<div id="continuous-random-variables" class="section level2 hasAnchor" number="3.11">
<h2><span class="header-section-number">3.11</span> Continuous random variables<a href="chap2.html#continuous-random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="notion-1" class="section level3 hasAnchor" number="3.11.1">
<h3><span class="header-section-number">3.11.1</span> Notion<a href="chap2.html#notion-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the context of this book, a random variable <span class="math inline">\(X\)</span> is said to be continuous when its distribution function <span class="math inline">\(F_X\)</span> admits the representation
<span class="math display" id="eq:DefAbsCont">\[\begin{equation}
F_X(x)=\int_{y\leq x}f_X(y)dy,~x\in {\mathbb{R}},
\tag{3.9}
\end{equation}\]</span>
for an integrable function <span class="math inline">\(f_X:{\mathbb{R}}\to{\mathbb{R}}^+\)</span> called the <a href="https://en.wikipedia.org/wiki/Probability_density_function">probability density</a> of <span class="math inline">\(X\)</span>. A continuous random variable therefore has a continuous distribution function <span class="math inline">\(F_X\)</span>.</p>
<p>The function <span class="math inline">\(f_X\)</span> used in the representation <a href="chap2.html#eq:DefAbsCont">(3.9)</a> has a concrete meaning: if we plot the graph of <span class="math inline">\(f_X\)</span>, the area of the surface bounded in the plane by the graph of <span class="math inline">\(f_X\)</span>, the x-axis and the verticals at <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> (<span class="math inline">\(a&lt;b\)</span>) is the probability that <span class="math inline">\(X\)</span> takes a value in the interval <span class="math inline">\((a,b]\)</span>, i.e..
<span class="math display">\[
\Pr[a&lt;X\leq b]=F_X(b)-F_X(a)=\int_{x=a}^bf_X(x)dx.
\]</span>
Figure <a href="chap2.html#fig:FigChapI5">3.1</a> illustrates the meaning of probability density. By stretching the length <span class="math inline">\(b-a\)</span> of the interval towards 0, we can clearly see that
<span class="math display">\[
\lim_{b\to a}\Pr[a&lt;X\leq b]=\Pr[X=a]=0
\]</span>
whatever the real <span class="math inline">\(a\)</span>; a continuous random variable therefore has no mass points (the prerogative of discrete variables).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:FigChapI5"></span>
<img src="bookdown-demo_files/figure-html/FigChapI5-1.png" alt="Graph of the density function $f_X$ corresponding to a continuous variable $X$" width="768" />
<p class="caption">
Figure 3.1: Graph of the density function <span class="math inline">\(f_X\)</span> corresponding to a continuous variable <span class="math inline">\(X\)</span>
</p>
</div>
<p>We can easily deduce from the characteristics that all distribution functions must possess that the density <span class="math inline">\(f_X:{\mathbb{R}}\to{\mathbb{R}}^+\)</span> must satisfy the condition
<span class="math display">\[
\int_{y\in{\mathbb{R}}}f_X(y)dy=1.
\]</span>
Clearly, <a href="chap2.html#eq:DefAbsCont">(3.9)</a> guarantees that
<span class="math display">\[\begin{eqnarray*}
f_X(x)&amp;=&amp;\frac{d}{dx}F_X(x)\\
&amp;=&amp;\lim_{\Delta x\to 0}\frac{F_X(x+\Delta x)-F_X(x)}{\Delta x}
\\
&amp;=&amp;\lim_{\Delta x\to 0}\frac{Pr[x&lt;X\leq x+\Delta x]}{\Delta x},
\end{eqnarray*}\]</span>}
so that the approximation
<span class="math display">\[
\Pr[x&lt;X\leq x+\Delta x]\approx f_X(x)\Delta x
\]</span>
is valid as long as <span class="math inline">\(\Delta x\)</span> is sufficiently small. So, although <span class="math inline">\(f_X(x)\)</span> is not a probability (and there’s no guarantee that <span class="math inline">\(f_X(x)\leq 1\)</span>), the areas where <span class="math inline">\(f_X\)</span> takes on very large values correspond to probable realizations of <span class="math inline">\(X\)</span>, while the areas where <span class="math inline">\(f_X\)</span> takes on small values correspond to unlikely realizations of <span class="math inline">\(X\)</span>. If <span class="math inline">\(f_X=0\)</span> on an interval <span class="math inline">\(I\)</span>, then the variable <span class="math inline">\(X\)</span> cannot take any value in <span class="math inline">\(I\)</span> (such areas correspond to a plateau for the graph of <span class="math inline">\(F_X\)</span>).</p>
<div class="remark">
<p><span id="unlabeled-div-16" class="remark"><em>Remark</em>. </span>It’s worth bearing in mind that continuous random variables are only approximations for easy calculations. Indeed, claims are expressed in euros, or even cents of a euro, but a value of 123.547324euro would be of little interest (as we could only pay out 123.55euro). In reality, then, all the random variables that make up the actuary’s daily routine are discrete. Those modeling the cost of claims take on so many possible values (in this case <span class="math inline">\(\frac{k}{100}\)</span>, <span class="math inline">\(k\in{\mathbb{N}}\)</span>) that it’s more convenient to consider that all positive real values are possible. This brings us to the notion of continuous distribution and probability density described above.</p>
</div>
<p>In the remainder of this book, we shall use the following continuous distributions.</p>
</div>
<div id="continuous-uniform-distribution" class="section level3 hasAnchor" number="3.11.2">
<h3><span class="header-section-number">3.11.2</span> Continuous uniform distribution<a href="chap2.html#continuous-uniform-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The uniform (continuous) distribution appeared very early in statistics to model the result of pure chance, or phenomena about which no information was available. The first traces of uniform distribution date back to Bayes in 1763 and Laplace in 1872.</p>
<p>A random variable <span class="math inline">\(X\)</span> is said to have a uniform distribution over the interval <span class="math inline">\([0,1]\)</span> or to obey this distribution, which will henceforth be noted as <span class="math inline">\(X\sim\mathcal{U}ni(0,1)\)</span>, when <span class="math inline">\(X\)</span> takes its values in this interval and admits the distribution function
<span class="math display">\[
F_X(x)=\left\{
\begin{array}{l}
0,\mbox{ if }x&lt;0,
\\
x,\mbox{ if }0\leq x&lt;1,
\\
1,\mbox{ if }x\geq 1.
\end{array}
\right.
\]</span>
The corresponding probability density is always equal to 1 on the unit interval, and zero elsewhere, i.e.
<span class="math display">\[
f_X(x)=\left\{
\begin{array}{l}
1,\mbox{ if }0\leq x\leq 1,
\\
0,\mbox{ otherwise}
\end{array}
\right.
\]</span></p>
<p>The uniform distribution plays a special role in probability theory, because of the following result.</p>
<div class="proposition">
<p><span id="prp:NumberAl" class="proposition"><strong>Proposition 3.4  </strong></span></p>
<ol style="list-style-type: decimal">
<li>If the distribution function <span class="math inline">\(F_X\)</span> of <span class="math inline">\(X\)</span> is continuous then <span class="math inline">\(F_X(X)\sim\mathcal{U}ni[0,1]\)</span>.</li>
<li>Let <span class="math inline">\(U\sim\mathcal{U}ni[0,1]\)</span> and let <span class="math inline">\(X\)</span> be any random variable. The random variable <span class="math inline">\(F_X^{-1}(U)\)</span> has the same distribution as <span class="math inline">\(X\)</span>.</li>
</ol>
</div>
<div class="proof">
<p><span id="unlabeled-div-17" class="proof"><em>Proof</em>. </span></p>
<ol style="list-style-type: decimal">
<li>Whatever <span class="math inline">\(t\)</span> is in <span class="math inline">\((0,1)\)</span>, we have by Equation <a href="chap2.html#eq:GenerationF0">(3.6)</a>
<span class="math display">\[\begin{eqnarray*}
\Pr[F_X(X)\geq t]&amp;=&amp;\Pr[X\geq F_X^{-1}(t)]\\
&amp;=&amp;1-F_X\big(F_X^{-1}(t)\big)=1-t,
\end{eqnarray*}\]</span>}
which shows that <span class="math inline">\(F_X(X)\)</span> has a <span class="math inline">\(\mathcal{U}ni[0,1]\)</span> distribution.</li>
<li>Whatever <span class="math inline">\(x\in {\mathbb{R}}\)</span>, we have by Equation <a href="chap2.html#eq:GenerationF0">(3.6)</a>
<span class="math display">\[\begin{eqnarray*}
\Pr[F_X^{-1}(U)\leq x] &amp; = &amp; \Pr[F_X(x)\geq U]=F_X(x),
\end{eqnarray*}\]</span>
which completes the proof.</li>
</ol>
</div>
<div class="remark">
<p><span id="unlabeled-div-18" class="remark"><em>Remark</em>. </span>Point (2) of Proposition <a href="#prp:NombrAl"><strong>??</strong></a> is central to simulation. This approach, which we’ll describe in more detail later, involves recreating randomness by means of a computer, and enables us to solve problems for which an analytical solution is out of reach. The principle is simple to implement: if the computer is able to generate a sequence of numbers <span class="math inline">\(u_1,u_2,\ldots\)</span> that is indistinguishable from a sequence of realizations of random variables <span class="math inline">\(\mathcal{U}ni[0,1]\)</span>, this sequence can easily be transformed into realizations of random variables with distribution function <span class="math inline">\(F_X\)</span> by applying the transformation <span class="math inline">\(F_X^{-1}\)</span> to them.</p>
</div>
<p>More generally, a <a href="https://en.wikipedia.org/wiki/Continuous_uniform_distribution">uniform distribution</a> over the interval <span class="math inline">\([a,b]\)</span> is one whose probability density is constant over this interval and zero outside it. More precisely, a random variable is said to have a uniform distribution on the interval <span class="math inline">\([a,b]\)</span>, or to obey this distribution, which will henceforth be noted as <span class="math inline">\(X\sim\mathcal{U}ni[a,b]\)</span>, when the support of <span class="math inline">\(X\)</span> is the interval in question and the density of <span class="math inline">\(X\)</span> is given by
<span class="math display">\[
f_X(x)=\left\{
\begin{array}{l}
\displaystyle\frac{1}{b-a},\text{ if }a\leq x\leq b,\\
0,\text{ otherwise}
\end{array}
\right.
\]</span>
Given a random variable <span class="math inline">\(U\sim\mathcal{U}ni[0,1]\)</span>, we can easily see that
<span class="math display">\[
\frac{X-a}{b-a}=_{\text{distribution}}U\Leftrightarrow X=_{\text{distribution}}a+(b-a)U.
\]</span></p>
</div>
<div id="beta-distribution" class="section level3 hasAnchor" number="3.11.3">
<h3><span class="header-section-number">3.11.3</span> Beta Distribution<a href="chap2.html#beta-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Like the uniform distribution, the <a href="https://en.wikipedia.org/wiki/Beta_distribution">Beta distribution</a> has an interval as its support, but does not assign to each interval contained in the support a probability mass proportional to its length. More precisely, a random variable <span class="math inline">\(X\)</span> is said to have a Beta distribution with parameters <span class="math inline">\(\alpha&gt;0\)</span> and <span class="math inline">\(\beta&gt;0\)</span>, which will henceforth be denoted <span class="math inline">\(X\sim\mathcal{B}et(\alpha,\beta)\)</span>, when <span class="math inline">\(X\)</span> takes its values in the interval <span class="math inline">\([0,1]\)</span> and admits the density
<span class="math display">\[
f_X(x)=\begin{cases}
\displaystyle\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha-1}(1-x)^{\beta-1},\text{ if }0\leq x\leq 1,\\
0,\text{ otherwise}
\end{cases}
\]</span></p>
<p>From <span class="math inline">\(X\)</span>, we can easily construct a random variable <span class="math inline">\(Z\)</span> that takes its values in the interval <span class="math inline">\([a,b]\)</span> using the formula <span class="math inline">\(Z=a+(b-a)X\)</span>. It’s easy to check that the density of <span class="math inline">\(Z\)</span> is given by
<span class="math display">\[
f_Z(x)=
\begin{cases}
\displaystyle\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)(b-a)^{\alpha+\beta-1}}(z-a)^{\alpha-1}(b-z)^{\beta-1},\text{ if }a \leq z\leq b,\\
0,\text{ otherwise}
\end{cases}
\]</span></p>
<p>Figure <a href="chap2.html#fig:densbeta">3.2</a> shows the graph of probability densities associated with the <span class="math inline">\(\mathcal{B}et(\alpha,\beta)\)</span> distributions, for different parameter values. It can be seen that the density can take on very different shapes depending on the parameter values.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:densbeta"></span>
<img src="bookdown-demo_files/figure-html/densbeta-1.png" alt="Densities of Beta ditributions with different values for $\alpha$ and $\beta$" width="768" />
<p class="caption">
Figure 3.2: Densities of Beta ditributions with different values for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>
</p>
</div>
</div>
<div id="normal-or-gaussian-distribution" class="section level3 hasAnchor" number="3.11.4">
<h3><span class="header-section-number">3.11.4</span> Normal (or Gaussian) distribution<a href="chap2.html#normal-or-gaussian-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The standard <a href="https://en.wikipedia.org/wiki/Normal_distribution">normal distribution</a> is obtained from the following reasoning (linking the normal distribution to the theory of observation errors). The idea is to define a probability density expressing that the corresponding random variable takes small values around 0, symmetrically distributed with respect to the origin, and that large values are less likely the further away from the origin you are. We quickly come to postulate a density proportional to <span class="math inline">\(\exp(-x^2)\)</span>, which we then standardize so that the integral over <span class="math inline">\({\mathbb{R}}\)</span> gives 1.</p>
<p>The normal distribution is undoubtedly one of the best-known (if not the best-known) in statistics. It first appeared as an approximation to the binomial distribution. In the early 19th century, its central role in theory was demonstrated by <a href="https://en.wikipedia.org/wiki/Pierre-Simon_Laplace">Laplace</a> and <a href="https://en.wikipedia.org/wiki/Carl_Friedrich_Gauss">Gauss</a>. In astronomy, as in many other branches of the exact sciences, it plays a fundamental role in modeling measurement and observation errors.</p>
<p>A random variable <span class="math inline">\(X\)</span> is said to have a Normal distribution with parameters <span class="math inline">\(\mu\in {\mathbb{R}}\)</span> and <span class="math inline">\(\sigma&gt;0\)</span>, or to obey this distribution, which is henceforth noted as <span class="math inline">\(X\sim\mathcal{N}or(\mu,\sigma^2)\)</span>, when <span class="math inline">\(X\)</span> takes its values in <span class="math inline">\({\mathbb{R}}\)</span> and admits the distribution function
<span class="math display">\[
x\mapsto \Pr[X\leq x]=\Phi\left(\frac{x-\mu}{\sigma}\right),
\]</span>
where <span class="math inline">\(\Phi(\cdot)\)</span> is the distribution function associated with the <span class="math inline">\(\mathcal{N}or(0,1)\)</span> distribution given by
<span class="math display">\[
\Phi(x)=\frac{1}{\sqrt{2\pi}}\int_{y=-\infty}^x\exp(-y^2/2)dy,
\hspace{2mm}x\in {\mathbb{R}}.
\]</span></p>
<p>We can easily see that if <span class="math inline">\(X\sim\mathcal{N}or(\mu,\sigma^2)\)</span> and if <span class="math inline">\(Z\sim\mathcal{N}or(0,1)\)</span> then <span class="math inline">\(X=_{\text{distributio }}\mu+\sigma Z\)</span>. This explains the central role played by the <span class="math inline">\(\mathcal{N}or(0,1)\)</span> distribution and its <span class="math inline">\(\Phi(\cdot)\)</span> distribution function, known as the standard normal distribution, or centered reduced normal distribution.</p>
<p>We sometimes refer to a random variable obeying this distribution as <span class="math inline">\(\mathcal{N}or(\mu,\sigma^2)\)</span>. The probability density associated with <span class="math inline">\(\Phi\)</span> is denoted <span class="math inline">\(\phi\)</span> and is equal to
<span class="math display">\[
\frac{d}{dx}\Phi(x)=\phi(x)=\frac{1}{\sqrt{2\pi}}\exp(-\frac{x^2}{2}).
\]</span>
Figure <a href="chap2.html#fig:densnorm">3.3</a> shows the graph of probability densities associated with <span class="math inline">\(\mathcal{N}or(10,1)\)</span>, <span class="math inline">\(\mathcal{N}or(10,2)\)</span> and <span class="math inline">\(\mathcal{N}or(10,3)\)</span>. The densities are clearly symmetrical with respect to $=$10 and the parameter <span class="math inline">\(\sigma^2\)</span> controls the dispersion of possible values around <span class="math inline">\(\mu\)</span>: as <span class="math inline">\(\sigma^2\)</span> increases, larger or smaller values relative to <span class="math inline">\(\mu\)</span> become more likely. Generally speaking, the <span class="math inline">\(\phi\)</span> density of the <span class="math inline">\(\mathcal{N}or(0,1)\)</span> distribution is unimodal (maximum at <span class="math inline">\(\mu\)</span>). It has two inflection points, in <span class="math inline">\(\mu\pm\sigma\)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:densnorm"></span>
<img src="bookdown-demo_files/figure-html/densnorm-1.png" alt="Densities of Gaussian distributions with parameters $\mu=10$ and different values for $\sigma^2$" width="768" />
<p class="caption">
Figure 3.3: Densities of Gaussian distributions with parameters <span class="math inline">\(\mu=10\)</span> and different values for <span class="math inline">\(\sigma^2\)</span>
</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-19" class="remark"><em>Remark</em>. </span>In recent decades, the limitations of classical Gaussian statistics have come to light, and the symmetry of the normal distribution has been severely criticized in many disciplines, including actuarial science. Nevertheless, most asymptotic results in statistics are based on the normal distribution, which is why this tool is so important.</p>
</div>
</div>
<div id="log-normal-variable" class="section level3 hasAnchor" number="3.11.5">
<h3><span class="header-section-number">3.11.5</span> Log-normal variable<a href="chap2.html#log-normal-variable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As we have seen above, the normal distribution, because of its symmetry and the probability weight it gives to negative values, is not the ideal candidate for modeling claims costs. A simple solution to the problem would be to transform the variable to be modeled so that it better suits the actuary’s needs. Quite naturally, we can imagine using an exponential transformation, which leads to the <a href="https://en.wikipedia.org/wiki/Log-normal_distribution">log-normal distribution</a>. More precisely, if <span class="math inline">\(Y\sim\mathcal{N}or(\mu,\sigma^2)\)</span> and we define <span class="math inline">\(X=\exp Y\)</span>, for <span class="math inline">\(x&gt;0\)</span> we get,
<span class="math display">\[\begin{eqnarray*}
\Pr[X\leq x] &amp;=&amp; \Pr[\exp Y\leq x]\\\\
&amp; = &amp; \Pr[Y\leq\ln x]\\\
&amp; = &amp; \Phi\left(\frac{\ln x-\mu}{\sigma}\right).
\end{eqnarray*}\]</span>t().
\end{eqnarray*}</p>
<p>This leads naturally to the following definition. A random variable <span class="math inline">\(X\)</span> is said to have a Log-Normal distribution with parameters <span class="math inline">\(\mu\in{\mathbb{R}}\)</span> and <span class="math inline">\(\sigma^2&gt;0\)</span>, which will henceforth be denoted <span class="math inline">\(X\sim\mathcal{LN}or(\mu,\sigma^2)\)</span>, when <span class="math inline">\(X\)</span> takes its values in <span class="math inline">\({\mathbb{R}}^+\)</span> and admits the distribution function
<span class="math display">\[
F_X(x)=\left\{
\begin{array}{l}
\Phi\left(\frac{\ln(x)-\mu}
{\sigma}\right),\mbox{ if }x&gt;0,
\\
0,\mbox{ otherwise}.
\end{array}
\right.
\]</span></p>
<p>Lognormal distribution can be traced back to Francis Galton, who in 1879 introduced it as the limit distribution of a product of positive random variables. It was subsequently widely used in economics and finance.</p>
<p>The probability density of <span class="math inline">\(X\)</span> is given by
<span class="math display">\[
f_X(x)=\left\{
\begin{array}{l}
\frac{1}{x\sigma\sqrt{2\pi}}\exp\left(-\frac{1}{2}\left(\frac{\ln(x)-\mu}
{\sigma}\right)^2\right),\mbox{ if }x&gt;0,
\\
0,\mbox{ otherwise}.
\end{array}
\right.
\]</span>
The quantile function is
<span class="math display">\[
F_X^{-1}(p)=\exp\big(\mu+\sigma\Phi^{-1}(p)\big).
\]</span>
Figure <a href="chap2.html#fig:denslognorm">3.4</a> shows the probability density associated with the distribution <span class="math inline">\(\mathcal{LN}or(\mu,\sigma^2)\)</span> for <span class="math inline">\(\mu=0\)</span> and <span class="math inline">\(\sigma=0.5\)</span>, 1 and 1.5. The probability density is unimodal and the mode corresponds to <span class="math inline">\(\exp(\mu-\sigma^2)\)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:denslognorm"></span>
<img src="bookdown-demo_files/figure-html/denslognorm-1.png" alt="Densities of log-normal distributions with parameters $\mu=0$ and different values for $\sigma^2$." width="768" />
<p class="caption">
Figure 3.4: Densities of log-normal distributions with parameters <span class="math inline">\(\mu=0\)</span> and different values for <span class="math inline">\(\sigma^2\)</span>.
</p>
</div>
</div>
<div id="negative-exponential-distribution" class="section level3 hasAnchor" number="3.11.6">
<h3><span class="header-section-number">3.11.6</span> (Negative) exponential distribution<a href="chap2.html#negative-exponential-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A random variable <span class="math inline">\(X\)</span> is said to have a <a href="https://en.wikipedia.org/wiki/Exponential_distribution">(negative) exponential distribution</a> (The term “negative exponential” is used to avoid any risk of confusion with the exponential family, a monument of modern statistics, which will be used in the chapter on generalized linear models) of parameter <span class="math inline">\(\theta&gt;0\)</span>, or said to obey this distribution, which will henceforth be noted as <span class="math inline">\(X\sim\mathcal{E}xp(\theta)\)</span>, when <span class="math inline">\(X\)</span> takes its values in <span class="math inline">\(\mathbb{R}^+\)</span> and admits the probability density
<span class="math display">\[
f_X(x)=
\begin{cases}
\theta\exp(-\theta x),\mbox{ if }x&gt;0, \\
0,\mbox{ otherwise}.
\end{cases}
\]</span></p>
<p>The distribution function associated with the distribution <span class="math inline">\(\mathcal{E}xp(\theta)\)</span> is given by
<span class="math display">\[
F_X(x)=
\begin{cases}
1-\exp(-\theta x),\mbox{ if }x&gt;0, \\
0,\mbox{ otherwise}.
\end{cases}
\]</span>
The quantile function is
<span class="math display">\[
F_X^{-1}(p)=-\frac{1}{\theta}\ln(1-p).
\]</span></p>
<p>By extension, we sometimes refer to a random variable obeying this distribution as <span class="math inline">\(\mathcal{E}xp(\theta)\)</span>. This is undoubtedly the most popular probability distribution in actuarial science. It has many interesting mathematical properties, which explains why actuaries are so fond of it. It should be borne in mind, however, that it reflects claims amounts that are relatively harmless for the company, with a thin distribution tail, since <span class="math inline">\({overline{F}_X(x)=\exp(-\theta x)\)</span> exhibits exponential decay over <span class="math inline">\({\mathbb{R}}^+\)</span>.</p>
<p>Figure <a href="chap2.html#fig:expo">3.5</a> shows the probability density associated with the <span class="math inline">\(\mathcal{E}xp(\theta)\)</span> distribution for different values of <span class="math inline">\(\theta\)</span>. The probability density is unimodal and the mode is 0.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:expo"></span>
<img src="bookdown-demo_files/figure-html/expo-1.png" alt="Densities of exponential distributions with different parameters $\theta$" width="768" />
<p class="caption">
Figure 3.5: Densities of exponential distributions with different parameters <span class="math inline">\(\theta\)</span>
</p>
</div>
</div>
<div id="gamma-distribution" class="section level3 hasAnchor" number="3.11.7">
<h3><span class="header-section-number">3.11.7</span> Gamma distribution<a href="chap2.html#gamma-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The Gamma distribution was obtained by Laplace as early as 1836. Bienaymé demonstrated a special case (the chi-square or chi-squared distribution) in 1838 in relation to the multinomial distribution. A random variable <span class="math inline">\(X\)</span> is said to have a Gamma distribution with parameters <span class="math inline">\(\alpha&gt;0\)</span> and <span class="math inline">\(\tau&gt;0\)</span>, or to obey this distribution, which will henceforth be noted as <span class="math inline">\(X\sim\mathcal{G}am(\alpha,\tau)\)</span>,
when <span class="math inline">\(X\)</span> has the probability density
<span class="math display">\[
f_X(x)=
\begin{cases}
\displaystyle\frac{x^{\alpha-1}\tau^\alpha\exp(-x\tau)}{\Gamma(\alpha)},\mbox{ if }x\geq 0,\\
0,\mbox{ otherwise}
\end{cases}
\]</span>
We sometimes refer to a random variable obeying this distribution as <span class="math inline">\(\mathcal{G}am(\alpha,\tau)\)</span>.</p>
<p>The tail function is written as
<span class="math display">\[
\overline{F}_X(x)=1-\Gamma(\alpha,\tau x),
\]</span>
where <span class="math inline">\(\Gamma(\cdot,\cdot)\)</span> is the incomplete Gamma function, defined by
<span class="math display">\[
\Gamma(t,\xi)=\frac{1}{\Gamma(t)}\int_{x=0}^\xi x^{t-1}\exp(-x)dx,\hspace{2mm}\xi,t\in{\mathbb{R}}^+.
\]</span></p>
<p>Figure <a href="chap2.html#fig:densgamma1">3.6</a> shows the graph of <span class="math inline">\(f_X\)</span> for different values of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\tau\)</span>. The mode is <span class="math inline">\((\alpha-1)/\tau\)</span> when <span class="math inline">\(\alpha\geq 1\)</span>, and 0 otherwise. In addition,
<span class="math display">\[
\lim_{x\to 0}f_X(x)=+\infty,\text{ when }\alpha&lt;1,
\]</span>
and
<span class="math display">\[
\lim_{x\to 0}f_X(x)=0,\text{ when }\alpha=1.
\]</span></p>
<p>We speak of a standard gamma distribution when <span class="math inline">\(\tau=1\)</span>; in this case,
<span class="math display">\[
f_X(x)=\frac{x^{\alpha-1}\exp(-x)}{\Gamma(\alpha)},\hspace{2mm}x\in{\mathbb{R}}^+.
\]</span>
If <span class="math inline">\(X\)</span> has a gamma distribution with parameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\tau\)</span>, then <span class="math inline">\(\tau X\)</span> has a standard gamma distribution with parameter <span class="math inline">\(\alpha\)</span>.</p>
<div class="remark">
<p><span id="unlabeled-div-20" class="remark"><em>Remark</em>. </span>An interesting special case of the Gamma distribution is the chi-square (or chi-two) distribution with <span class="math inline">\(n\)</span> degrees of freedom. This is the gamma distribution with parameters <span class="math inline">\(\alpha=n/2\)</span> and <span class="math inline">\(\tau=1/2\)</span>.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-21" class="remark"><em>Remark</em>. </span>When <span class="math inline">\(\alpha=1\)</span>, we find the negative exponential distribution, i.e. <span class="math inline">\(\mathcal{G}am(1,\tau)=\mathcal{E}xp(\tau)\)</span>.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-22" class="remark"><em>Remark</em>. </span>When <span class="math inline">\(\alpha\)</span> is a positive integer, this is sometimes referred to as an Erlang distribution. The distribution function is then given by
<span class="math display">\[
F_X(x)=1-\sum_{j=0}^{\alpha-1}\exp(-x\tau)
\frac{(x\tau)^j}{j!},\hspace{2mm}x\geq 0.
\]</span></p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:densgamma1"></span>
<img src="bookdown-demo_files/figure-html/densgamma1-1.png" alt="Densities of gamma distributions with parameters $\tau=1/4$ (on top), 1/2 (in the middle) and 1 (belows) for differnt values for $\alpha$" width="768" />
<p class="caption">
Figure 3.6: Densities of gamma distributions with parameters <span class="math inline">\(\tau=1/4\)</span> (on top), 1/2 (in the middle) and 1 (belows) for differnt values for <span class="math inline">\(\alpha\)</span>
</p>
</div>
</div>
<div id="pareto-distribution" class="section level3 hasAnchor" number="3.11.8">
<h3><span class="header-section-number">3.11.8</span> Pareto distribution<a href="chap2.html#pareto-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This distribution takes its name from an Italian-born economics professor, Vilfredo Pareto, who introduced it in 1897 to model the distribution of income in a population. It can be obtained by transforming a random variable with a negative exponential distribution. Since the negative exponential distribution models claims that are not very dangerous for the company, we could consider that the amount of the claim <span class="math inline">\(X\)</span> has the same distribution as <span class="math inline">\(\exp(Y)\)</span>, where <span class="math inline">\(Y\sim\mathcal{E}xp(\alpha)\)</span>.
<span class="math display">\[\begin{eqnarray*}
\Pr[X\leq x] &amp; = &amp; \Pr[\exp(Y)\leq x]\\
&amp; = &amp; \Pr[Y\leq \ln x] \\
&amp;=&amp; \left\{
  \begin{array}{l}
  0,\mbox{ if }x\in]0,1],\\
  1-x^{-\alpha},\mbox{ if }x&gt;1.
\end{array}
\right.
\end{eqnarray*}\]</span>
This model represents a much less favorable situation for the insurer, since the tail function <a href="https://en.wikipedia.org/wiki/Polynomial_delay">decreases polynomially</a> (and no longer exponentially) towards 0. To obtain Pareto’s distribution, simply consider the distribution of the random variable
<span class="math display">\[
X=\theta\{\exp(Y)-1\},\text{ where }Y\sim\mathcal{E}xp(\alpha),
\]</span>
whose survival function is
<span class="math display">\[\begin{eqnarray*}
\Pr[X&gt;x]&amp;=&amp;\Pr[Y&gt;\ln(1+(x/\theta))]\\
&amp;=&amp;\left(1+\frac{x}{\theta}\right)^{-\alpha}\\
&amp;=&amp;\left(\frac{\theta}{x+\theta}\right)^\alpha.
\end{eqnarray*}\]</span>
Thus, a random variable <span class="math inline">\(X\)</span> is said to have a <a href="https://en.wikipedia.org/wiki/Pareto_distribution">Pareto distribution</a> with parameters <span class="math inline">\(\alpha&gt;0\)</span> and <span class="math inline">\(\theta&gt;0\)</span>, which will henceforth be denoted <span class="math inline">\(X\sim\mathcal{P}ar(\alpha,\theta)\)</span>, when <span class="math inline">\(X\)</span> admits the distribution function
<span class="math display">\[
F_X(x)=\left\{
\begin{array}{l}
1-\left(\displaystyle\frac{\theta}{x+\theta}\right)^\alpha,
\mbox{ if }x\geq 0,
\\
0,\mbox{ otherwise}
\end{array}
\right.
\]</span>
Pareto’s distribution defined in this way is sometimes called <a href="https://en.wikipedia.org/wiki/Lomax_distribution">Lomax’s distribution</a> (or Type II Pareto’s distribution). From now on, we’ll sometimes refer to a random variable obeying this distribution as <span class="math inline">\(\mathcal{P}ar(\alpha,\theta)\)</span>. The quantile function is
<span class="math display">\[
F_X^{-1}(p)=\theta\big((1-p)^{-1/\alpha}-1\big).
\]</span></p>
<p>Pareto’s distribution is considered to be one of the most dangerous for the insurer (since the probability of the claim amount exceeding <span class="math inline">\(x\)</span> decreases polynomially with the amount <span class="math inline">\(x\)</span>, whereas the decrease is exponential with the other models introduced above). This explains its frequent use in reinsurance. The probability density associated with the Pareto distribution is given by
<span class="math display">\[
f_X(x)=\left\{
\begin{array}{l}
\frac{\alpha\theta^\alpha}{(x+\theta)^{\alpha+1}},
\mbox{ if }x\geq 0,
\\
0,\mbox{ otherwise,}
\end{array}
\right.
\]</span>
where <span class="math inline">\(\alpha&gt;0\)</span>, <span class="math inline">\(\theta&gt;0\)</span>; <span class="math inline">\(f_X\)</span> is clearly unimodal (the unique mode is in 0). See Figure <a href="chap2.html#fig:DensPareto">3.7</a> for a graph of <span class="math inline">\(f_{alpha,\theta}\)</span> and different parameter values.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:DensPareto"></span>
<img src="bookdown-demo_files/figure-html/DensPareto-1.png" alt="Densities of Pareto distributions with $\theta=1$ and different values for $\alpha$" width="768" />
<p class="caption">
Figure 3.7: Densities of Pareto distributions with <span class="math inline">\(\theta=1\)</span> and different values for <span class="math inline">\(\alpha\)</span>
</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-23" class="remark"><em>Remark</em>. </span>Later on, we’ll come across a slightly different version of Pareto’s distribution, known as the generalized Pareto distribution, which appears naturally in the study of the behavior of the maximum of <span class="math inline">\(n\)</span> random variables, when <span class="math inline">\(n\to +\infty\)</span>.</p>
</div>
</div>
</div>
<div id="random-vector" class="section level2 hasAnchor" number="3.12">
<h2><span class="header-section-number">3.12</span> Random vector<a href="chap2.html#random-vector" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="definition-1" class="section level3 hasAnchor" number="3.12.1">
<h3><span class="header-section-number">3.12.1</span> Definition<a href="chap2.html#definition-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A random vector <span class="math inline">\(\boldsymbol{X}\)</span> is the result of the union of <span class="math inline">\(n\)</span> <a href="https://en.wikipedia.org/wiki/Multivariate_random_variable">random variables</a> <span class="math inline">\(X_1,X_2,\ldots,X_n\)</span> defined on the same probability space <span class="math inline">\((\mathcal{E},\mathcal{A},\Pr)\)</span>. In the following, we will always consider <span class="math inline">\(\boldsymbol{X}\)</span> as a column vector:
<span class="math display">\[
\boldsymbol{X}=\left(\begin{array}{c}
X_1\\
X_2\\
\vdots\\
X_n
\end{array}
\right)
=(X_1,X_2,\ldots,X_n)^\top,
\]</span>
where the superscript “<span class="math inline">\(\top\)</span>” indicates the transposition.</p>
</div>
<div id="distribution-function-1" class="section level3 hasAnchor" number="3.12.2">
<h3><span class="header-section-number">3.12.2</span> Distribution function<a href="chap2.html#distribution-function-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="general-definition" class="section level4 hasAnchor" number="3.12.2.1">
<h4><span class="header-section-number">3.12.2.1</span> General definition<a href="chap2.html#general-definition" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>As with random variables, the stochastic behavior of a vector <span class="math inline">\(\boldsymbol{X}\)</span> is fully described by its joint distribution function <span class="math inline">\(F_{\boldsymbol{X}}\)</span>, defined as follows.</p>
<div class="definition">
<p><span id="def:unlabeled-div-24" class="definition"><strong>Definition 3.8  </strong></span>The distribution function <span class="math inline">\(F_{\boldsymbol{X}}\)</span> associated with the random vector <span class="math inline">\(\boldsymbol{X}\)</span> is defined as follows:
<span class="math display">\[\begin{eqnarray*}
F_{\boldsymbol{X}}(\boldsymbol{x})&amp;=&amp;
\Pr\big[\{e\in\mathcal{E}|X_1(e)\leq x_1,X_2(e)\leq x_2,\ldots,X_n(e)\leq x_n\}\big]\\
&amp;=&amp;\Pr[X_1\leq x_1,X_2\leq x_2,\ldots,X_n\leq x_n],\boldsymbol{x}\in{\mathbb{R}}^n.
\end{eqnarray*}\]</span></p>
</div>
<p>In the light of this definition, <span class="math inline">\(F_{\boldsymbol{X}}(\boldsymbol{x})\)</span> is therefore the probability that, simultaneously, each of the <span class="math inline">\(X_i\)</span> components of the <span class="math inline">\(\boldsymbol{X}\)</span> vector is lower than the corresponding <span class="math inline">\(x_i\)</span> level. It is therefore the premium to be paid to receive a 1payment if the variables <span class="math inline">\(X_1,\ldots,X_n\)</span> are simultaneously below the thresholds <span class="math inline">\(x_1,\ldots,x_n\)</span>.</p>
<p>From now on, we’ll often use vector notation. For example, the event <span class="math inline">\(\{\boldsymbol{X}\leq \boldsymbol{x}\}\)</span> is equivalent to the event <span class="math inline">\(\{X_1\leq x_1,\ldots,X_n\leq x_n\}\)</span>.</p>
</div>
<div id="in-dimension-2" class="section level4 hasAnchor" number="3.12.2.2">
<h4><span class="header-section-number">3.12.2.2</span> In dimension 2<a href="chap2.html#in-dimension-2" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We saw in Property <a href="chap2.html#prp:PropCdf">3.1</a> what conditions a univariate distribution function must satisfy. This result can be generalized to the <span class="math inline">\(2\)</span> dimension as follows.</p>
<div class="proposition">
<p><span id="prp:DefCDFBiv" class="proposition"><strong>Proposition 3.5  </strong></span><span class="math inline">\(F_{\boldsymbol{X}}\)</span> is the distribution function of a <span class="math inline">\(\boldsymbol{X}\)</span> pair if, and only if, <span class="math inline">\(F_{\boldsymbol{X}}\)</span></p>
<ul>
<li>is non-decreasing</li>
<li>is continuous on the right</li>
<li>satisfies
<span class="math display">\[\begin{eqnarray*}
\lim_{t\to -\infty}F_{\boldsymbol{X}}(x_1,t)&amp;=&amp;0,\text{ whatever }x_1\in{\mathbb{R}},\\
\lim_{t\to -\infty}F_{\boldsymbol{X}}(t,x_2)&amp;=&amp;0,\text{ whatever }x_2\in{\mathbb{R}},\\
\lim_{x_1,x_2\to +\infty}F_{\boldsymbol{X}}(x_1,x_2)&amp;=&amp;1,
\end{eqnarray*}\]</span>
as well as
<span class="math display" id="eq:increasing">\[\begin{eqnarray}
&amp;&amp;F_{\boldsymbol{X}}(x_1+\delta,x_2+\epsilon)-F_{\boldsymbol{X}}(x_1+\delta,x_2)\nonumber\\
&amp;&amp;-F_{\boldsymbol{X}}(x_1,x_2+\epsilon)+F_{\boldsymbol{X}}(x_1,x_2)\geq 0
\tag{3.10}
\end{eqnarray}\]</span>
whatever <span class="math inline">\(\boldsymbol{x}\in{\mathbb{R}}^2\)</span> and <span class="math inline">\(\epsilon\)</span>, <span class="math inline">\(\delta&gt;0\)</span>.</li>
</ul>
</div>
<div class="remark">
<p><span id="unlabeled-div-25" class="remark"><em>Remark</em>. </span>The condition <a href="chap2.html#eq:increasing">(3.10)</a> is still equivalent, in the case where <span class="math inline">\(F_{\boldsymbol{X}}\)</span> is twice derivable, to
<span class="math display">\[
\frac{\partial ^{2}}{\partial x_1\partial x_2}F_{\boldsymbol{X}}(\boldsymbol{x})\geq 0.
\]</span></p>
</div>
<p>In general, <a href="chap2.html#eq:increasing">(3.10)</a> guarantees that the probability of <span class="math inline">\(\boldsymbol{X}\)</span> taking a value in the rectangle of vertices <span class="math inline">\((x_1,x_2)\)</span>, <span class="math inline">\((x_1+\delta,x_2)\)</span>, <span class="math inline">\((x_1,x_2+\epsilon)\)</span> and <span class="math inline">\((x_1+\delta,x_2+\epsilon)\)</span> is always positive. In the rest of this book, we’ll refer to the same type of constraint as the <a href="chap2.html#eq:increasing">(3.10)</a> inequality as <a href="https://en.wikipedia.org/wiki/Supermodular_function">supermodularity</a>.</p>
</div>
<div id="any-dimension" class="section level4 hasAnchor" number="3.12.2.3">
<h4><span class="header-section-number">3.12.2.3</span> Any dimension<a href="chap2.html#any-dimension" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Let’s now look at the properties required of any distribution function in any dimension. This is an immediate generalization of Property <a href="chap2.html#prp:DefCDFBiv">3.5</a>, except for the <a href="chap2.html#eq:increasing">(3.10)</a> condition, which becomes a little more complicated.</p>
<div class="proposition">
<p><span id="prp:Multcdf" class="proposition"><strong>Proposition 3.6  </strong></span><span class="math inline">\(F_{\boldsymbol{X}}\)</span> is the distribution function of a <span class="math inline">\(n\)</span>-dimensional random vector <span class="math inline">\(F_{\boldsymbol{X}}\)</span> if, and only if, <span class="math inline">\(F_{\boldsymbol{X}}\)</span></p>
<ul>
<li>is non-decreasing;</li>
<li>is continuous on the right;</li>
<li>satisfies
<span class="math display">\[\begin{eqnarray*}
\lim_{x_j\to -\infty}F_{\boldsymbol{X}}(x_1,x_2,\ldots,x_n)&amp;=&amp;0,
\mbox{ for }j=1,2,\ldots,n,\\
\lim_{x_1,x_2,\ldots,x_n\to +\infty}F_{\boldsymbol{X}}(x_1,x_2,\ldots,x_n)&amp;=&amp;1,
\end{eqnarray*}\]</span>
and, whatever <span class="math inline">\((\alpha_1,\alpha_2,\ldots,\alpha_n),(\beta_1,\beta_2,\ldots,\beta_n)\in {\mathbb{R}}^n\)</span>, with <span class="math inline">\(\alpha_i\leq \beta_i\)</span> for <span class="math inline">\(i=1,2,\ldots,n\)</span>, defining:
<span class="math display">\[\begin{eqnarray*}
\Delta_{\alpha_i,\beta_i}F_{\boldsymbol{X}}(\boldsymbol{x})
&amp;=&amp;F_{\boldsymbol{X}}(x_1,\ldots,x_{i-1},\beta_i,x_{i+1},\ldots, x_n)\\
&amp; &amp;-F_{\boldsymbol{X}}(x_1,\ldots,x_{i-1},\alpha_i,x_{i+1},\ldots, x_n),
\end{eqnarray*}\]</span>
<span class="math display">\[
\Delta_{\alpha_1,\beta_1}\Delta_{\alpha_2,\beta_2}\ldots\Delta_{\alpha_n,\beta_n}F_{\boldsymbol{X}}(\boldsymbol{x})\geq 0.
\]</span></li>
</ul>
</div>
<div class="remark">
<p><span id="unlabeled-div-26" class="remark"><em>Remark</em>. </span>As in dimension 2, the inequality in Property <a href="chap2.html#prp:Multcdf">3.6</a> guarantees that:
<span class="math display">\[
\Pr[\boldsymbol{\alpha}\leq\boldsymbol{X}\leq\boldsymbol{\beta}]\geq 0\mbox{ for all }
\boldsymbol{\alpha}\leq\boldsymbol{\beta}\in {\mathbb{R}}^n.
\]</span>
When <span class="math inline">\(F_{\boldsymbol{X}}\)</span> is sufficiently regular, the inequality in Property <a href="chap2.html#prp:Multcdf">3.6</a> is still equivalent to
<span class="math display">\[
\frac{\partial^n}{\partial x_1\partial x_2\ldots\partial x_n}
F_{\boldsymbol{X}}(\boldsymbol{x})=f_{\boldsymbol{X}}(\boldsymbol{x})\geq 0\mbox{ on } {\mathbb{R}}^n,
\]</span>
where <span class="math inline">\(f_{\boldsymbol{X}}\)</span> is the joint probability density of the vector <span class="math inline">\(\boldsymbol{X}\)</span>. We then have
<span class="math display">\[
F_{\boldsymbol{X}}(\boldsymbol{x})=\int_{-\infty}^{x_1}\ldots \int_{-\infty}^{x_n}f_{\boldsymbol{X}}(\boldsymbol{y})dy_1\ldots dy_n.
\]</span></p>
</div>
</div>
<div id="marginal-distribution-functions" class="section level4 hasAnchor" number="3.12.2.4">
<h4><span class="header-section-number">3.12.2.4</span> Marginal distribution functions<a href="chap2.html#marginal-distribution-functions" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The behavior of each <span class="math inline">\(X_j\)</span> component is described by the corresponding <span class="math inline">\(F_{X_j}\)</span> distribution function. The functions <span class="math inline">\(F_{X_1}\)</span>, <span class="math inline">\(F_{X_2}\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(F_{X_n}\)</span> are called marginal distribution functions. Clearly,
<span class="math display">\[
\lim_{x_i\to +\infty\text{ for all }i\neq j}F_{\boldsymbol{X}}(\boldsymbol{x})=F_{X_j}(x_j).
\]</span></p>
</div>
</div>
<div id="support-of-vector-boldsymbolx" class="section level3 hasAnchor" number="3.12.3">
<h3><span class="header-section-number">3.12.3</span> Support of vector <span class="math inline">\(\boldsymbol{X}\)</span><a href="chap2.html#support-of-vector-boldsymbolx" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The support <span class="math inline">\(\mathcal{S}_{\boldsymbol{X}}\)</span> of the random vector <span class="math inline">\(\boldsymbol{X}\)</span> is defined as the subset of <span class="math inline">\({\mathbb{R}}^n\)</span> containing the points <span class="math inline">\(\boldsymbol{x}\)</span> where <span class="math inline">\(F_{\boldsymbol{X}}\)</span> is strictly increasing. <span class="math inline">\(\mathcal{S}_{\boldsymbol{X}}\)</span> is included in the hyper-rectangle
<span class="math display">\[
\Big[F_{X_1}^{-1\bullet}(0),F_{X_1}^{-1}(1)\Big]\times
\Big[F_{X_2}^{-1\bullet}(0),F_{X_2}^{-1}(1)\Big]\times
\ldots
\Big[F_{X_n}^{-1\bullet}(0),F_{X_n}^{-1}(1)\Big]
\]</span>
where <span class="math inline">\(F_{X_i}\)</span>, <span class="math inline">\(i=1,2,\ldots,n\)</span>, are the marginals of <span class="math inline">\(F_{\boldsymbol{X}}\)</span>.</p>
</div>
<div id="independence" class="section level3 hasAnchor" number="3.12.4">
<h3><span class="header-section-number">3.12.4</span> Independence<a href="chap2.html#independence" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Often, the actuary will be confronted with several random variables <span class="math inline">\(X_1,X_2,X_3,\ldots\)</span> (representing, for example, the insurer’s expense for different policies in the portfolio). It is then important to know whether the value taken by one of these variables could influence those taken by the others.</p>
<div class="example">
<p><span id="exm:unlabeled-div-27" class="example"><strong>Example 3.9  </strong></span>If we consider the insurance of buildings against earthquakes, and if <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> represent two risks of the same characteristics located in the same geographical area, we would be tempted to say that if <span class="math inline">\(X_1\)</span> takes on a large value (meaning that the first building has suffered significant damage from the earthquake), <span class="math inline">\(X_2\)</span> should also take on a high value. Conversely, a small value of <span class="math inline">\(X_1\)</span> could mean that <span class="math inline">\(X_2\)</span> is also likely to be quite small (since a small value of <span class="math inline">\(X_1\)</span> makes a low-magnitude earthquake more likely). In such a situation, the risks <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are said to be positively dependent.</p>
</div>
<p>Another classic situation is when the value of <span class="math inline">\(X_1\)</span> has no influence on that of <span class="math inline">\(X_2\)</span>.</p>
<div class="example">
<p><span id="exm:unlabeled-div-28" class="example"><strong>Example 3.10  </strong></span>Consider, for example, two fire hazards located in different regions. It’s hard to see how the consequences of a fire affecting one of the buildings could provide information about a hypothetical loss affecting the other.</p>
</div>
<p>Intuitively, random variables <span class="math inline">\(X_1,X_2,\ldots,X_n\)</span> are mutually independent if the value of any one of them does not influence the others.</p>
<div class="definition">
<p><span id="def:DefInd" class="definition"><strong>Definition 3.9  </strong></span>Formally, random variables <span class="math inline">\(X_1,\ldots,X_n\)</span> are <a href="https://en.wikipedia.org/wiki/Independence_(probability_theory)">independent</a> when
<span class="math display" id="eq:EgInd">\[\begin{eqnarray}
&amp;&amp;\Pr[X_i\leq x_i\text{ for }i\in\mathcal{I}|X_j\leq x_j\text{ for }j\in\mathcal{J}]\nonumber\}
&amp;=&amp;\Pr[X_i\leq x_i\text{ for }i\in\mathcal{I}]
\tag{3.11}
\end{eqnarray}\]</span>
whatever the reals <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_j\)</span>, and the subsets <span class="math inline">\(\mathcal{I}\)</span> and <span class="math inline">\(\mathcal{J}\)</span> disjoint from <span class="math inline">\(\{1,\ldots,n\}\)</span>.</p>
</div>
<p>This reflects the idea that knowing that <span class="math inline">\(X_j\)</span>, <span class="math inline">\(j\in\mathcal{J}\)</span>, are small (i.e. below the <span class="math inline">\(x_j\)</span> thresholds) tells us nothing about the probability that <span class="math inline">\(X_i\)</span>, <span class="math inline">\(i\in\mathcal{I}\)</span>, are also small (i.e. below the <span class="math inline">\(x_i\)</span> thresholds). More generally, <a href="chap2.html#eq:EgInd">(3.11)</a> guarantees that the occurrence of any event involving <span class="math inline">\(X_j\)</span>, <span class="math inline">\(j\in\mathcal{J}\)</span>, has no influence on the probability of any event involving <span class="math inline">\(X_i\)</span>, <span class="math inline">\(i\in\mathcal{I}\)</span>, occurring.</p>
<p>The only way equality <a href="chap2.html#eq:EgInd">(3.11)</a> can be satisfied is if the joint distribution function
of the vector <span class="math inline">\(\boldsymbol{X}\)</span> factorizes into the product of its marginals.</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-29" class="proposition"><strong>Proposition 3.7  </strong></span>The random variables <span class="math inline">\(X_1,X_2,\ldots,X_n\)</span> are independent if, and only if, the factorization
<span class="math display">\[
F_{\boldsymbol{X}}(\boldsymbol{x})=\prod_{i=1}^nF_{X_i}(x_i)\mbox{ holds for all }\boldsymbol{x}\in {\mathbb{R}}^n.
\]</span></p>
</div>
<p>Independence can also be verified using probability densities.</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-30" class="proposition"><strong>Proposition 3.8  </strong></span></p>
<ol style="list-style-type: decimal">
<li>If <span class="math inline">\(X_i\)</span> are continuous, the random variables <span class="math inline">\(X_1,\ldots,X_n\)</span> are independent if, and only if, their joint probability density factorizes, i.e.
<span class="math display">\[
f_{\boldsymbol{X}}(\boldsymbol{x})
%=\frac{\partial^n}{\partial x_1\ldots\partial x_n}F_{\Xvec}(\xvec)
=\prod_{i=1}^nf_{X_i}(x_i).
\]</span></li>
<li>If <span class="math inline">\(X_i\)</span> are count variables, so are the joint probabilities, i.e.
<span class="math display">\[
\Pr[X_1=k_1,\ldots,X_n=k_n]=\prod_{i=1}^n\Pr[X_i=k_i].
\]</span></li>
</ol>
</div>
</div>
<div id="gaussian-vector" class="section level3 hasAnchor" number="3.12.5">
<h3><span class="header-section-number">3.12.5</span> Gaussian vector<a href="chap2.html#gaussian-vector" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A random variable with a <span class="math inline">\(\mathcal{N}or(\mu,\sigma^2)\)</span> distribution has a density that can be expressed as
<span class="math display">\[
f_X(x) = \frac{1}{\sigma\sqrt{2\pi}} \exp \left\{ -\frac{1}{2} Q_1(x;\mu,\sigma^2) \right\}, \quad x \in {\mathbb{R}},
\]</span>
with
<span class="math display">\[
Q_1(x;\mu,\sigma^2) = \frac{1}{\sigma^2} (x-\mu)^2 = (x-\mu)(\sigma^2)^{-1}(x-\mu)
\]</span>
where <span class="math inline">\(\mu \in {\mathbb{R}}\)</span> and <span class="math inline">\(\sigma^2 &gt; 0\)</span>. Let’s extend this definition to the bivariate case. Given a pair of real numbers
<span class="math display">\[
{\boldsymbol{\mu}} = \left( \begin{array}{c} \mu_1\\\mu_2 \end{array} \right)
\]</span>
and a matrix
<span class="math display">\[
{\boldsymbol{\Sigma}} = \left(
\begin{array}{cc} \sigma_1^2 &amp; \sigma_{12} \\ \sigma_{12} &amp; \sigma_2^2
\end{array} \right)
\]</span>
such that <span class="math inline">\(\sigma_i^2 &gt; 0\)</span>, <span class="math inline">\(i=1,2\)</span>, <span class="math inline">\(|\sigma_{12}|&lt;\sigma_1\sigma_2\)</span>. From now on, we’ll denote <span class="math inline">\(|\boldsymbol{\Sigma}|\)</span> the determinant of the <span class="math inline">\(\boldsymbol{\Sigma}\)</span> matrix. A random pair <span class="math inline">\({\boldsymbol{X}} = (X_1,X_2)\)</span> is said to have a <a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution">normal distribution</a>, which will henceforth be denoted <span class="math inline">\({\boldsymbol{X}}sim\mathcal{N}or(\boldsymbol{\mu},\boldsymbol{\Sigma})\)</span>, if it admits a density of the form
<span class="math display">\[
f_{\boldsymbol{X}}({\boldsymbol{x}}) = \frac{1}{2\pi|{\boldsymbol{\Sigma}}|^{1/2}}\exp\left[ -\frac{1}{2} Q_2({\boldsymbol{x}};{\boldsymbol{\mu}},{\boldsymbol{\Sigma}})\right],~{\boldsymbol{x}}\in {\mathbb{R}}^2
\]</span>
where
<span class="math display">\[
      Q_2({\boldsymbol{x}};{\boldsymbol{\mu}},{\boldsymbol{\Sigma}}) = ({\boldsymbol{x}}-{\boldsymbol{\mu}})^{t}{\boldsymbol{\Sigma}}^{-1}({\boldsymbol{x}}-{\boldsymbol{\mu}})
      \]</span>
Figure <a href="chap2.html#fig:pdfNorm2">3.8</a> shows different graphs of the bivariate probability density associated with the normal distribution for <span class="math inline">\(\boldsymbol{\mu}=(0,0)^\top\)</span>, <span class="math inline">\(\sigma_1^2=\sigma_2^2=1\)</span> and different values of <span class="math inline">\(\sigma_{12}\)</span>.</p>
<div class="remark">
<p><span id="unlabeled-div-31" class="remark"><em>Remark</em>. </span>In some cases, the <span class="math inline">\(\boldsymbol{\Sigma}\)</span> matrix is not invertible, and we have to adapt the definition of the <span class="math inline">\(\mathcal{N}or(\boldsymbol{\mu},\boldsymbol{\Sigma})\)</span> distribution (since inversion in <span class="math inline">\(Q_2\)</span> is no longer allowed). More precisely, <span class="math inline">\({\boldsymbol{X}}\)</span> is said to have a singular normal distribution (singular because the <span class="math inline">\(\boldsymbol{\Sigma}\)</span> matrix is singular, i.e. non-invertible) if there are real numbers <span class="math inline">\(\sigma_1\)</span>, <span class="math inline">\(\sigma_2\)</span>, <span class="math inline">\(\mu_1\)</span> and <span class="math inline">\(\mu_2\)</span> such that <span class="math inline">\({\boldsymbol{X}}=_{\text{distribution}}(\sigma_1 Z + \mu_1, \sigma_2 Z + \mu_2)\)</span>, where <span class="math inline">\(Z\sim\mathcal{N}or(0,1)\)</span>.</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pdfNorm2"></span>
<img src="bookdown-demo_files/figure-html/pdfNorm2-1.png" alt="Density of the bivariate Gaussian distribution with parameters  $\muvec=(0,0)^\top$, $\sigma_1^2=\sigma_2^2=1$ and different values for $\sigma_{12}$" width="1152" />
<p class="caption">
Figure 3.8: Density of the bivariate Gaussian distribution with parameters <span class="math inline">\(\boldsymbol{\mu}=(0,0)^\top\)</span>, <span class="math inline">\(\sigma_1^2=\sigma_2^2=1\)</span> and different values for <span class="math inline">\(\sigma_{12}\)</span>
</p>
</div>
<p>The definitions given above are easily extended to any dimension (by substituting a <span class="math inline">\(n\)</span>-dimensional vector for the <span class="math inline">\(\boldsymbol{\mu}\)</span> pair and a symmetrical <a href="https://en.wikipedia.org/wiki/Nonnegative_matrix">positive</a>-<a href="https://en.wikipedia.org/wiki/Definite_matrix">definite</a> <span class="math inline">\(n\times n\)</span>-square matrix for the <span class="math inline">\(2\times 2\)</span> <span class="math inline">\(\boldsymbol{\Sigma}\)</span> matrix, and dividing by <span class="math inline">\((2\pi)^{n/2}\)</span>).</p>
<p>The following property indicates that the parameter <span class="math inline">\(\sigma_{12}\)</span> is the one controlling the possible dependency between <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>.</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-32" class="proposition"><strong>Proposition 3.9  </strong></span>When <span class="math inline">\(\sigma_{12}=0\)</span>, the components of <span class="math inline">\(\boldsymbol{X}\sim\mathcal{N}or(\boldsymbol{\mu},\boldsymbol{\Sigma})\)</span> are independent.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-33" class="proof"><em>Proof</em>. </span>When <span class="math inline">\(\sigma_{12}=0\)</span>, <span class="math inline">\(Q_2\)</span> equals
<span class="math display">\[\begin{eqnarray*}
Q_2({\boldsymbol{x}};{\boldsymbol{\mu}},{\boldsymbol{\Sigma}}) &amp;=&amp; ({\boldsymbol{x}}-{\boldsymbol{\mu}})^{t}\frac{1}{\sigma_1^2\sigma_2^2}
\left(\begin{array}{cc}\sigma_2^2&amp;0\\0&amp;\sigma_1^2\end{array}\right)({\boldsymbol{x}}-{\boldsymbol{\mu}})\\
&amp;=&amp;\frac{1}{\sigma_1^2}(x_1-\mu_1)^2+\frac{1}{\sigma_2^2}(x_2-\mu_2)^2.
\end{eqnarray*}\]</span>}
Therefore, the joint density
<span class="math display">\[\begin{eqnarray*}
f_{\boldsymbol{X}}(\boldsymbol{x})&amp;=&amp;\frac{1}{\sigma_1\sqrt{2\pi}} \exp \left\{ -\frac{1}{2} Q_1(x_1;\mu_1,\sigma_1^2) \right\}
\\
&amp;&amp;\times\frac{1}{\sigma_2\sqrt{2\pi}} \exp \left\{ -\frac{1}{2} Q_1(x_2;\mu_2,\sigma_2^2) \right\}
\end{eqnarray*}\]</span>
factorizes into the product of the densities associated with <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-34" class="remark"><em>Remark</em>. </span>The generally accepted definition of the multivariate Gaussian distribution is as follows. A vector <span class="math inline">\(\boldsymbol{X}=(X_1,\ldots,X_n)\)</span> is said to be Gaussian when any linear combination <span class="math inline">\(\sum_{j=1}^n\alpha_jX_j\)</span> of its components is of univariate normal distribution (with the convention of <span class="math inline">\(\mathcal{N}or(\mu,0)\)</span> designating a unit probability mass placed in <span class="math inline">\(\mu\)</span>). In particular, each component <span class="math inline">\(X_1,\ldots,X_n\)</span> has a normal distribution.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-35" class="remark"><em>Remark</em>. </span>Note that for a vector to be Gaussian, it is not sufficient for its marginals to be normal. Indeed, if we consider the pair <span class="math inline">\((X_1,X_2)\)</span> such that <span class="math inline">\(X_1\sim\mathcal{N}or(0,1)\)</span> and
<span class="math display">\[
X_2=\begin{cases}
X_1,\text{ if }|X_1|\leq 1\\
-X_1,\text{ otherwise},
\end{cases}
\]</span>
we have <span class="math inline">\(X_2\sim\mathcal{N}or(0,1)\)</span>, but <span class="math inline">\((X_1,X_2)\)</span> is not a Gaussian vector since the line <span class="math inline">\(x_1=-x_2\)</span> is assigned a probability mass
<span class="math display">\[
0&lt;\Pr[X_1+X_2=0]&lt;1,
\]</span>
which means that <span class="math inline">\(X_1+X_2\)</span> cannot have a normal distribution.</p>
</div>
<p>The <span class="math inline">\(\mathcal{N}or(\boldsymbol{\mu},\boldsymbol{\Sigma})\)</span> distribution can be obtained from the standard (or centered-reduced) normal distribution <span class="math inline">\(\mathcal{N}or(\boldsymbol{0},\text{Id})\)</span> (i.e. the distribution of a vector whose components are independent and have the same <span class="math inline">\(\mathcal{N}or(0,1)\)</span> distribution) using the following result.</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-36" class="proposition"><strong>Proposition 3.10  </strong></span>Let <span class="math inline">\(\boldsymbol{X}\sim\mathcal{N}or(\boldsymbol{\mu},\boldsymbol{\Sigma})\)</span> and <span class="math inline">\(\boldsymbol{Z}\sim\mathcal{N}or(\boldsymbol{0},\text{Id})\)</span>. By defining the matrix <span class="math inline">\(\boldsymbol{T}\)</span> such that <span class="math inline">\(\boldsymbol{T}^\top\boldsymbol{T}=\boldsymbol{\Sigma}\)</span>, we have <span class="math inline">\(\boldsymbol{X}=_{\text{distribution}}\boldsymbol{\mu}+\boldsymbol{T}^\top\boldsymbol{Z}\)</span>.</p>
</div>
</div>
<div id="ellipticpart" class="section level3 hasAnchor" number="3.12.6">
<h3><span class="header-section-number">3.12.6</span> Elliptical vectors<a href="chap2.html#ellipticpart" class="anchor-section" aria-label="Anchor link to header"></a></h3>
</div>
<div id="definition-2" class="section level3 hasAnchor" number="3.12.7">
<h3><span class="header-section-number">3.12.7</span> Definition<a href="chap2.html#definition-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Elliptical distributions were introduced by Kelker in 1970. Spherical distributions are generalizations of the distribution $or( ,) $ and <a href="https://en.wikipedia.org/wiki/Elliptical_distribution">elliptical distributions</a> are generalizations of the distribution $or( ,) $.</p>
<div class="definition">
<p><span id="def:unlabeled-div-37" class="definition"><strong>Definition 3.10  </strong></span>A random vector <span class="math inline">\(\boldsymbol{X}\)</span> of <span class="math inline">\({\mathbb{R}}^{n}\)</span> is said to have an elliptical distribution of generator <span class="math inline">\(g\)</span> and parameters <span class="math inline">\(\boldsymbol{\mu}\)</span> and <span class="math inline">\(\boldsymbol{\Sigma}\)</span>, which will henceforth be denoted <span class="math inline">\(\boldsymbol{X}\sim\mathcal{E}ll(g,\boldsymbol{\mu},\boldsymbol{\Sigma})\)</span> when it admits the density
<span class="math display" id="eq:ellipticaldensity">\[\begin{equation}
f_{\boldsymbol{X}}\left( \boldsymbol{x}\right) =\frac{c_{n}}{{\left| \boldsymbol{\Sigma}\right| }^{1/2}}
g\left( \frac{1}{2}\left( \boldsymbol{x}-\boldsymbol{\mu}\right) ^\top\boldsymbol{\Sigma}^{-1} \left(
\boldsymbol{x}-\boldsymbol{\mu}\right) \right) ,
\tag{3.12}
\end{equation}\]</span>
where the normalization constant <span class="math inline">\(c_{n}\)</span> is given by%.
<span class="math display">\[\begin{equation*}
c_{n}=\frac{\Gamma \left( n/2\right) }{\left( 2\pi \right)
^{n/2}} \int_{0}^{\infty }t^{n/2-1}g\left( t\right)dt .
\end{equation*}\]</span></p>
</div>
<div class="remark">
<p><span id="unlabeled-div-38" class="remark"><em>Remark</em>. </span>If we take <span class="math inline">\(g\)</span> to be the exponential function, we see that the multivariate normal distribution <span class="math inline">\(\mathcal{N}or(\boldsymbol{\mu},\boldsymbol{\Sigma})\)</span> is found.</p>
</div>
<div id="marginal-behavior" class="section level4 hasAnchor" number="3.12.7.1">
<h4><span class="header-section-number">3.12.7.1</span> Marginal behavior<a href="chap2.html#marginal-behavior" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The marginals of elliptical distributions are themselves elliptical, i.e.<br />
<span class="math display">\[
\boldsymbol{X}\sim\mathcal{E}ll(g,\boldsymbol{\mu},\boldsymbol{\Sigma})\Rightarrow
X_k\sim\mathcal{E}ll(g,\mu_k,\sigma_k^2)\text{ for }k=1,\ldots,n.
\]</span>
Moreover, whatever the matrix <span class="math inline">\(\boldsymbol{B}\)</span> and the vector <span class="math inline">\(\boldsymbol{b}\)</span>,
<span class="math display">\[
\boldsymbol{X}\sim\mathcal{E}ll(g,\boldsymbol{\mu},\boldsymbol{\Sigma})\Rightarrow
\boldsymbol{X}\sim\mathcal{E}ll(g,\boldsymbol{b}+\boldsymbol{B}\boldsymbol{\mu},
\boldsymbol{B}\boldsymbol{\Sigma}\boldsymbol{B}^\top).
\]</span></p>
</div>
<div id="multivariate-student-ts-distribution" class="section level4 hasAnchor" number="3.12.7.2">
<h4><span class="header-section-number">3.12.7.2</span> Multivariate Student <span class="math inline">\(t\)</span>’s distribution<a href="chap2.html#multivariate-student-ts-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The multivariate Student’s distribution is obtained with the generator in dimension <span class="math inline">\(n\)</span>,
<span class="math display">\[\begin{equation*}
g\left( t\right) =\left( 1+\frac{t}{k_{p}}\right) ^{-p},
\end{equation*}\]</span>
where parameter <span class="math inline">\(p\)</span> is greater than <span class="math inline">\(n/2\)</span>, and <span class="math inline">\(k_{p}\)</span> is a normalization constant.</p>
<p>The density of <span class="math inline">\(\boldsymbol{x}\)</span> is then given by
<span class="math display">\[\begin{equation*}
f_{\boldsymbol{X}}\left( \boldsymbol{x}\right) =\frac{c_{n}}{{\left| \boldsymbol{\Sigma}
\right| }^{1/2}}\left[ 1+\frac{\left( \boldsymbol{x}-\boldsymbol{\mu}\right) ^\top\boldsymbol{\Sigma}
\left( \boldsymbol{x}-\boldsymbol{\mu}\right) }{\Gamma \left( p-n/2\right) }\right]
^{-p}\text{ for }\boldsymbol{x}\in {\mathbb{R}}^{n}.
\end{equation*}\]</span></p>
<p>The multivariate Student’s distribution can also be obtained using the
the following construction, in terms of random variables.</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-39" class="proposition"><strong>Proposition 3.11  </strong></span>Let $or(,) $ and <span class="math inline">\(S\)</span> have mutually independent chi-square distributions with <span class="math inline">\(m\)</span> degrees of freedom. The vector <span class="math inline">\(\boldsymbol{Y}=\sqrt{m}\boldsymbol{Z}/S\)</span> obeys multivariate Student’s distribution, with <span class="math inline">\(m\)</span> degrees of freedom. This can be generalized by considering <span class="math inline">\(\boldsymbol{Y}^{\ast }=\boldsymbol{\mu}+\boldsymbol{T}^\top\boldsymbol{Y}\)</span>, where <span class="math inline">\(\boldsymbol{T}^\top\boldsymbol{T}=\boldsymbol{\Sigma}\)</span>.</p>
</div>
</div>
<div id="multivariate-cauchys-distribution" class="section level4 hasAnchor" number="3.12.7.3">
<h4><span class="header-section-number">3.12.7.3</span> Multivariate Cauchy’s distribution<a href="chap2.html#multivariate-cauchys-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The multivariate <a href="https://en.wikipedia.org/wiki/Cauchy_distribution">Cauchy distribution</a> has the density
<span class="math display">\[\begin{equation*}
f_{vec}\left( \boldsymbol{x}\right) =\frac{\Gamma \left( \left( n+1\right)
/2\right) }{\pi ^{\left( n+1\right) /2}{\left| \boldsymbol{\Sigma}
\right| }^{1/2}}\Big( 1+\left( \boldsymbol{x}-\boldsymbol{\mu}\right) ^\top\boldsymbol{\Sigma}\left(
\boldsymbol{x}-\boldsymbol{\mu}\right) \Big) ^{-\left( n+1\right) /2}\text{ for }\boldsymbol{x}\in {\mathbb{R}}^{n}.
\end{equation*}\]</span></p>
</div>
<div id="multivariate-logistic-distribution" class="section level4 hasAnchor" number="3.12.7.4">
<h4><span class="header-section-number">3.12.7.4</span> Multivariate logistic distribution<a href="chap2.html#multivariate-logistic-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The multivariate logistic distribution admits the generator
<span class="math display">\[
g\left( t\right) =\frac{\exp(-t)}{\big(1+\exp(-t)\big) ^{2}}.
\]</span>
The density is then
<span class="math display">\[
f_{\boldsymbol{x}}\left(\boldsymbol{x}\right) =\frac{c_{n}}{{\left| \boldsymbol{\Sigma}\right| }^{1/2}}\frac{\exp\left( -\frac{1}{2}\left(\boldsymbol{x}-\boldsymbol{\mu}\right)^\top\Sigma^{-1}\left(\boldsymbol{x}-\boldsymbol{\mu}\right)\right)}{\big( 1+\exp\left(-\frac{1}{2}\left( \boldsymbol{x}-\boldsymbol{\mu}\right)^\top\boldsymbol{\Sigma}^{-1}\left(\boldsymbol{x}-\boldsymbol{\mu}\right)\right) \big)^{2}}\text{ for }\boldsymbol{x}\in{\mathbb{R}}^{n}.
\]</span></p>
</div>
<div id="multivariate-exponential-power-distribution" class="section level4 hasAnchor" number="3.12.7.5">
<h4><span class="header-section-number">3.12.7.5</span> Multivariate exponential power distribution<a href="chap2.html#multivariate-exponential-power-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The multivariate exponential power distribution admits the generator
<span class="math display">\[
g\left(t\right) =\exp \left( -rt^{s}\right),
\]</span>
where <span class="math inline">\(r\)</span> and <span class="math inline">\(s\)</span> are strictly positive parameters. The density is then given by
<span class="math display">\[\begin{equation*}
f_{\boldsymbol{X}}\left( \boldsymbol{x}\right) =\frac{c_{n}}{{\left| \boldsymbol{\Sigma}
\right| }^{1/2}}\exp \left( -\frac{r}{2}\big( \left( \boldsymbol{x}-\boldsymbol{\mu}\right)
^\top\boldsymbol{\Sigma}^{-1}\left( \boldsymbol{x}-\boldsymbol{\mu}\right) \big) ^{s}\right) \text{ for }\boldsymbol{x}\in {\mathbb{R}}^{n},
\end{equation*}\]</span>
where the normalization constant <span class="math inline">\(c_{n}\)</span> is here
<span class="math display">\[\begin{equation*}
c_{n}=\frac{s\Gamma \left( n/2\right) }{\left( 2\pi \right)
^{n/2}\Gamma \left( n/\left( 2s\right) \right) }r^{n/\left(
2s\right) },
\end{equation*}\]</span>
with the special case of the Gaussian vector when <span class="math inline">\(s=r=1\)</span>, and the special case of Laplace’s distribution when <span class="math inline">\(s=1/2\)</span> and <span class="math inline">\(r=\sqrt{2}\)</span>.</p>
</div>
</div>
<div id="multinomial-vector" class="section level3 hasAnchor" number="3.12.8">
<h3><span class="header-section-number">3.12.8</span> Multinomial vector<a href="chap2.html#multinomial-vector" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="definition-3" class="section level4 hasAnchor" number="3.12.8.1">
<h4><span class="header-section-number">3.12.8.1</span> Definition<a href="chap2.html#definition-3" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The <a href="https://en.wikipedia.org/wiki/Multinomial_distribution">multinomial distribution</a> can be thought of as that of a vector <span class="math inline">\(\boldsymbol{N}\)</span> with binomial distribution components, but dependent on each other. With probabilities <span class="math inline">\(p_1,p_2,\ldots,p_n\)</span> such that <span class="math inline">\(p_1+\ldots+p_n=1\)</span>, <span class="math inline">\(\boldsymbol{N}\)</span> has a multinomial distribution with exponent <span class="math inline">\(m\)</span> and parameters <span class="math inline">\(p_1,\ldots,p_m\)</span>, which will henceforth be denoted <span class="math inline">\(\boldsymbol{N}\sim\mathcal{M}ult(m,p_1,\ldots,p_n)\)</span>,
when
<span class="math display">\[
\Pr[N_1=k_1,\ldots,N_n=k_n]=\frac{m!}{k_1!k_2!\ldots k_n!}p_1^{k_1}p_2^{k_2}\ldots p_n^{k_n},
\]</span>
for integers <span class="math inline">\(k_1,\ldots,k_n\)</span> such that <span class="math inline">\(k_1+\ldots+k_n=m\)</span>.</p>
</div>
<div id="marginal-distributions" class="section level4 hasAnchor" number="3.12.8.2">
<h4><span class="header-section-number">3.12.8.2</span> Marginal distributions<a href="chap2.html#marginal-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>When <span class="math inline">\(m=2\)</span>, we see that <span class="math inline">\(\mathcal{M}ult(m,p_1,p_2)\)</span> is the distribution of the pair <span class="math inline">\((N,m-N)\)</span> where <span class="math inline">\(N\sim\mathcal{B}in(m,p_1)\)</span>. In general, each component <span class="math inline">\(N_i\)</span> of a vector <span class="math inline">\((N_1,\ldots,N_n)\)</span> with distribution <span class="math inline">\(\mathcal{M}ult(m,p_1,\ldots,p_n)\)</span> has distribution <span class="math inline">\(\mathcal{B}in(m,p_i)\)</span>. The components of this vector are strongly dependent, as they are linked by the relation
<span class="math display">\[
N_1+\ldots+N_n=m.
\]</span></p>
</div>
</div>
</div>
<div id="conditional-variables" class="section level2 hasAnchor" number="3.13">
<h2><span class="header-section-number">3.13</span> Conditional Variables<a href="chap2.html#conditional-variables" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="the-case-of-counting-variables" class="section level3 hasAnchor" number="3.13.1">
<h3><span class="header-section-number">3.13.1</span> The case of counting variables<a href="chap2.html#the-case-of-counting-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s start with the simple case and consider the pair of counting random variables <span class="math inline">\((N_1,N_2)\)</span>. The distribution of the pair <span class="math inline">\((N_1,N_2)\)</span> is described by the joint probabilities <span class="math inline">\(\Pr[N_1=n_1,N_2=n_2]\)</span>, for <span class="math inline">\(n_1,n_2\)</span>.
The marginal distributions of <span class="math inline">\(N_1\)</span> and <span class="math inline">\(N_2\)</span> are obtained by summing over the possible values of the other component of the pair, i.e.
<span class="math display">\[
\Pr[N_1=n_1]=\sum_{j\in{\mathbb{N}}}\Pr[N_1=n_1,N_2=j],\hspace{2mm}n_1\in{\mathbb{N}},
\]</span>
and
<span class="math display">\[
\Pr[N_2=n_2]=\sum_{j\in{\mathbb{N}}}\Pr[N_1=j,N_2=n_2],\hspace{2mm}n_2\in{\mathbb{N}}.
\]</span></p>
<p>Since the event <span class="math inline">\(\{N_1=n_1\}\)</span> is assigned a positive probability mass (when <span class="math inline">\(n_1\)</span> belongs to the support of <span class="math inline">\(N_1\)</span>), the conditional distribution of <span class="math inline">\(N_2\)</span> knowing <span class="math inline">\(N_1=n_1\)</span>, <span class="math inline">\(n_1\in{\mathbb{N}}\)</span>, is simply given by
<span class="math display">\[
\Pr[N_2=n_2|N_1=n_1]=\frac{\Pr[N_1=n_1,N_2=n_2]}{\Pr[N_1=n_1]},\hspace{2mm}n_2\in{\mathbb{N}},
\]</span>
by virtue of the definition <a href="chap2.html#eq:DefCondExp">(3.4)</a> of conditional probability.</p>
</div>
<div id="the-case-of-continuous-variables" class="section level3 hasAnchor" number="3.13.2">
<h3><span class="header-section-number">3.13.2</span> The case of continuous variables<a href="chap2.html#the-case-of-continuous-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>With a pair <span class="math inline">\((X_1,X_2)\)</span> of continuous random variables, it’s more difficult to define <span class="math inline">\(\Pr[X_1=x_1|X_2=x_2]\)</span> because <span class="math inline">\(\Pr[X_2=x_2]=0\)</span> and the elementary definition <a href="chap2.html#eq:DefCondExp">(3.4)</a> doesn’t apply. In order to define the conditional distribution of <span class="math inline">\(X_1\)</span> knowing <span class="math inline">\(X_2=x_2\)</span>, we can proceed by crossing the limit as described below. First of all, whatever <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\in{\mathbb{R}}\)</span> and <span class="math inline">\(h&gt;0\)</span>,
<span class="math display">\[
\Pr[X_1\leq x_1|x_2\leq X_2\leq x_2+h]=\frac{\int_{\xi_2=x_2}^{x_2+h}\int_{\xi_1=-\infty}^{x_1}f_{\boldsymbol{X}}(\xi_1,\xi_2)d\xi_1d\xi_2}
{\int_{\xi_2=x_2}^{x_2+h}f_{X_2}(\xi_2)d\xi_2}.
\]</span>
By dividing the numerator and denominator by <span class="math inline">\(h\)</span> and then stretching <span class="math inline">\(h\to 0\)</span>, we see that the right-hand side tends towards
<span class="math display">\[
F_{1|2}(x_1|x_2)=\frac{\int_{\xi_1=-\infty}^{x_1}f_{\boldsymbol{X}}(\xi_1,x_2)d\xi_1}
{f_{X_2}(x_2)}\equiv\Pr[X_1\leq x_1|X_2=x_2].
\]</span>
We can check that at a fixed <span class="math inline">\(x_2\)</span>, <span class="math inline">\(F_{1|2}(\cdot|x_2)\)</span> is a distribution function with density
<span class="math display" id="eq:DensCond">\[\begin{equation}
\tag{3.13}
f_{1|2}(x_1|x_2)=\frac{f_{\boldsymbol{X}}(x_1,x_2)}{f_2(x_2)}.
\end{equation}\]</span>
We call <span class="math inline">\(f_{1|2}(\cdot|x_2)\)</span> the density of <span class="math inline">\(X_1\)</span> conditional on <span class="math inline">\(X_2=x_2\)</span>, and <span class="math inline">\(F_{1|2}(\cdot|x_2)\)</span> the distribution function of <span class="math inline">\(X_1\)</span> conditional on <span class="math inline">\(X_2=x_2\)</span>.</p>
<div class="example">
<p><span id="exm:unlabeled-div-40" class="example"><strong>Example 3.11  </strong></span>Let’s consider <span class="math inline">\(\boldsymbol{X}\sim\mathcal{N}or(\boldsymbol{\mu},\boldsymbol{\Sigma})\)</span> and let <span class="math inline">\(r = \frac{\sigma_{12}}{\sigma_1\sigma_2}\)</span>. Since <span class="math inline">\(|{\boldsymbol{\Sigma}}|=\sigma_1^2\sigma_2^2(1-r^2)\)</span>, the inverse of <span class="math inline">\({\boldsymbol{\Sigma}}\)</span> exists if, and only if, <span class="math inline">\(|r|&lt;1\)</span>. In the latter case, a direct calculation yields
<span class="math display">\[
{\boldsymbol{\Sigma}}^{-1} = \frac{1}{\sigma_1^2\sigma_2^2(1-r^2)}
   \left( \begin{array}{cc} \sigma_2^2 &amp; -r\sigma_1\sigma_2 \ -r\sigma_1\sigma_2 &amp; \sigma_1^2 \end{array} \right).
   \]</span>
For <span class="math inline">\(|r|&lt;1\)</span>, we can write
<span class="math display">\[\begin{eqnarray*}
f_{{\boldsymbol{X}}}({\boldsymbol{x}}) &amp;=&amp; \frac{1}{2\pi\sigma_1\sigma_2\sqrt{1-r^2}}
  \exp\biggl\{ -\frac{1}{2(1-r^2)}\times\\
&amp; &amp; \biggl\{ \biggl( \frac{x_1-\mu_1}{\sigma_1} \biggr)^2-2r\biggl( \frac{x_1-\mu_1}{\sigma_1} \biggr) \biggl(\frac{x_2-\mu_2}{\sigma_2} \biggr)+\biggl( \frac{x_2-\mu_2}{\sigma_2} \biggr)^2 \biggr\}.\biggr\}.
\end{eqnarray*}\]</span>
From the identity
<span class="math display">\[\begin{equation*}\begin{split}
&amp;\left( \frac{x_1-\mu_1}{\sigma_1} \right)^2 - 2 r \left( \frac{x_1-\mu_1}{\sigma_1} \right)
  \left( \frac{x_2-\mu_2}{\sigma_2} \right) + \left( \frac{x_2-\mu_2}{\sigma_2} \right)^2 \\
&amp;\qquad\qquad = \quad (1-r^2) \left( \frac{x_2-\mu_2}{\sigma_2} \right)^2 +
  \left\{ \frac{x_1-\mu_1}{\sigma_1} -r \frac{x_2-\mu_2}{\sigma_2} \right\}^2,
\end{split}\end{equation*}\]</span>
we get <span class="math inline">\(f_{{\boldsymbol{X}}}({\boldsymbol{x}}) = f_2(x_2) f_{1|2}(x_1|x_2)\)</span> with
<span class="math display">\[
f_2(x_2) = \frac{1}{\sigma_2\sqrt{2\pi}} \exp \left\{ -\frac{1}{2\sigma^2} (x_2-\mu_2)^2 \right\}
\]</span>
and
<span class="math display">\[\begin{eqnarray*}
f_{1|2}(x_1|x_2) &amp;=&amp; \frac{1}{\sqrt{2\pi}\sigma_1\sqrt{1-r^2}} \exp\biggl\{ -\frac{1}{2\sigma_1^2(1-r^2)} \times \\
&amp; &amp; \qquad \biggl(x_1 - (\mu_1+r\frac{\sigma_1}{\sigma_2}(x_2-\mu_2)) \biggr)^2 \biggr\}.
\end{eqnarray*}\]</span>
When <span class="math inline">\(|r|&lt;1\)</span>, the conditional distribution of <span class="math inline">\(X_1\)</span> knowing <span class="math inline">\(X_2=x_2\)</span> is therefore
<span class="math display">\[
\mathcal{N}or\left(\mu_1+r\frac{\sigma_1}{\sigma_2}(x_2-\mu_2),\sigma_1^2 (1-r^2)\right).
\]</span></p>
</div>
</div>
<div id="the-mixed-case-one-counting-variable-and-another-continuous" class="section level3 hasAnchor" number="3.13.3">
<h3><span class="header-section-number">3.13.3</span> The mixed case: one counting variable and another continuous<a href="chap2.html#the-mixed-case-one-counting-variable-and-another-continuous" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s consider the random pair <span class="math inline">\((X,N)\)</span> where <span class="math inline">\(X\)</span> is continuous and <span class="math inline">\(N\)</span> is a counting variable. We can imagine, for example, that <span class="math inline">\(X\)</span> represents the cost of claims and <span class="math inline">\(N\)</span> their number. The joint probability density of <span class="math inline">\((X,N)\)</span> is defined by
<span class="math display">\[
f(x,n)=\lim_{\Delta x\to 0}\frac{Pr[x&lt;X\leq x+\Delta x,N=n]}{\Delta x}.
\]</span>
We have of course
<span class="math display">\[
\sum_{n\in{\mathbb{N}}}\int_{x\in{\mathbb{R}}^+}f(x,n)dx=1.
\]</span>
The marginal distributions of <span class="math inline">\(X\)</span> and <span class="math inline">\(N\)</span> are respectively described by the density of <span class="math inline">\(X\)</span>.
<span class="math display">\[
f_1(x)=\sum_{n\in{\mathbb{N}}}f(x,n)
\]</span>
and the probability distribution of <span class="math inline">\(N\)</span>
<span class="math display">\[
f_2(n)=\int_{x\in{\mathbb{R}}^+}f(x,n)dx.
\]</span>
The conditional distributions are then described by the density of <span class="math inline">\(X\)</span> knowing <span class="math inline">\(N=n\)</span>.
<span class="math display">\[
f_{1|2}(x|n)=\frac{f(x,n)}{f_2(n)}
\]</span>
and by the distribution of probability of <span class="math inline">\(N\)</span> knowing <span class="math inline">\(X=x\)</span>$
<span class="math display">\[
f_{2|1}(n|x)=\frac{f(x,n)}{f_1(x)}.
\]</span></p>
<div class="example">
<p><span id="exm:unlabeled-div-41" class="example"><strong>Example 3.12  (Poisson mixture) </strong></span>Suppose that the joint distribution of the pair <span class="math inline">\((X,N)\)</span> is described by
<span class="math display">\[
f(x,n)=\exp(-x)\frac{x^n}{n!}g(x),\hspace{2mm}x\in{\mathbb{R}}^+,\hspace{2mm}n\in{\mathbb{N}}.
\]</span>
Clearly, the marginals are given by
<span class="math display">\[
f_1(x)=\sum_{n\in{\mathbb{N}}}f(x,n)=g(x)
\]</span>
for <span class="math inline">\(X\)</span> and by
<span class="math display">\[
f_2(n)=\int_{x\in{\mathbb{R}}^+}f(x,n)dx=\mathbb{E}[\exp(-X)X^n/n!]
\]</span>
for <span class="math inline">\(N\)</span>. The conditional density of <span class="math inline">\(N\)</span> knowing <span class="math inline">\(X=x\)</span> is
<span class="math display">\[
f_{2|1}(n|x)=\exp(-x)\frac{x^n}{n!}
\]</span>
so that conditional on <span class="math inline">\(X=x\)</span>, <span class="math inline">\(N\sim\mathcal{P}oi(x)\)</span>. The conditional density of <span class="math inline">\(X\)</span> knowing <span class="math inline">\(N=n\)</span> is written as
<span class="math display">\[
f_{1|2}(x|n)=\frac{\exp(-x)x^ng(x)}{\int_{\xi\in{\mathbb{R}}^+}\exp(-\xi)\xi^ng(\xi)d\xi}.
\]</span></p>
</div>
</div>
<div id="conditional-independence" class="section level3 hasAnchor" number="3.13.4">
<h3><span class="header-section-number">3.13.4</span> Conditional independence<a href="chap2.html#conditional-independence" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We will also study the case of <a href="https://en.wikipedia.org/wiki/Conditional_independence">conditionally independent</a> variables,</p>
<div class="definition">
<p><span id="def:unlabeled-div-42" class="definition"><strong>Definition 3.11  </strong></span>Let be a random vector <span class="math inline">\(\boldsymbol{\Theta}\)</span>. The variables <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are conditionally independent of <span class="math inline">\(\boldsymbol{\Theta}\)</span> if the following condition is satisfied:
<span class="math display">\[\begin{equation*}
\Pr[X_1\leq x_1,X_2\leq x_2|\boldsymbol{\Theta}=\boldsymbol{\theta}]
=\Pr[X_1\leq x_1|\boldsymbol{\Theta}= \boldsymbol{\theta}]\Pr[X_2\leq x_2|\boldsymbol{\Theta}= \boldsymbol{\theta}].
\end{equation*}\]</span></p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-43" class="example"><strong>Example 3.13  </strong></span>Suppose there exists a random variable <span class="math inline">\(\Theta\)</span> such that, conditional on <span class="math inline">\(\Theta=\theta\)</span>, the random variables <span class="math inline">\(N_1\)</span> and <span class="math inline">\(N_2\)</span> are independent and have respective distributions <span class="math inline">\(\mathcal{P}oi(\lambda_1\theta)\)</span> and
oi(_2)$. We then have
<span class="math display">\[\begin{eqnarray*}
&amp;&amp;\Pr[N_1=n_1,N_2=n_2]\\
&amp;=&amp;\int_{\theta\in{\mathbb{R}}^+}\Pr[N_1=n_1,N_2=n_2|\Theta=\theta]f_\Theta(\theta)d\theta\\
&amp;=&amp;\int_{\theta\in{\mathbb{R}}^+}\Pr[N_1=n_1|\Theta=\theta]\Pr[N_2=n_2|\Theta=\theta]f_\Theta(\theta)d\theta\\
&amp;=&amp;\int_{\theta\in{\mathbb{R}}^+}\exp(-\lambda_1\theta-\lambda_2\theta)\frac{(\lambda_1\theta)^{n_1}(\lambda_2\theta)^{n_2}}
{n_1!n_2!}f_\Theta(\theta)d\theta.
\end{eqnarray*}\]</span>
If <span class="math inline">\(\Theta\sim\mathcal{G}am(a,a)\)</span>, we get
<span class="math display">\[\begin{eqnarray*}
\Pr[N_1=n]&amp;=&amp;\frac{\lambda_1^na^a}{n!\Gamma(a)}\int_{\theta\in{\mathbb{R}}^+}\theta^{a+n-1}\exp(-\theta(a+\lambda_1))d\theta\\
&amp;=&amp;\frac{\lambda_1^na^a}{n!\Gamma(a)}(a+\lambda_1)^{-a-n}\Gamma(a+n)\\
&amp;=&amp;\frac{\Gamma(a+n)}{n!\Gamma(a)}\left(\frac{\lambda_1}{a+\lambda_1}\right)^n\left(\frac{a}{a+\lambda_1}\right)^a
\end{eqnarray*}\]</span>
so that <span class="math inline">\(N_1\sim\mathcal{NB}in(a,a/(a+\lambda_1))\)</span>. Similarly, <span class="math inline">\(N_2\sim\mathcal{NB}in(a,a/(a+\lambda_2))\)</span>. The distribution of the pair <span class="math inline">\(\boldsymbol{N}\)</span> is called the bivariate Negative Binomial distribution.</p>
</div>
</div>
</div>
<div id="compound-distributions" class="section level2 hasAnchor" number="3.14">
<h2><span class="header-section-number">3.14</span> Compound distributions<a href="chap2.html#compound-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="definition-4" class="section level3 hasAnchor" number="3.14.1">
<h3><span class="header-section-number">3.14.1</span> Definition<a href="chap2.html#definition-4" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>These probabilistic models “compose” a counting distribution (for the number of claims) with a continuous distribution (for their respective costs) to describe the total cost that an insured, a class of insureds or a portfolio generates for the insurer. If <span class="math inline">\(N\)</span> denotes the number of claims incurred during a given period, and <span class="math inline">\(X_i\)</span>, <span class="math inline">\(i=1,2,\ldots\)</span>, the amounts of these claims, then the company’s total financial burden <span class="math inline">\(S\)</span> for the period under consideration is written as follows:
<span class="math display" id="eq:LoiComp">\[\begin{equation}
\tag{3.14}
S=\sum_{i=1}^NX_i
\end{equation}\]</span>
with the convention that <span class="math inline">\(S=0\)</span> when <span class="math inline">\(N=0\)</span>. The random variables <span class="math inline">\(X_i\)</span>, <span class="math inline">\(i=1,2,\ldots\)</span> are assumed to be independent and identically distributed, and <span class="math inline">\(N\)</span> is assumed to be independent of <span class="math inline">\(X_i\)</span>.</p>
</div>
<div id="SecProdConv" class="section level3 hasAnchor" number="3.14.2">
<h3><span class="header-section-number">3.14.2</span> Convolution product<a href="chap2.html#SecProdConv" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Given two independent random variables, it is often interesting for the actuary to consider their sum. Given two independent counting random variables <span class="math inline">\(N_1\)</span> and <span class="math inline">\(N_2\)</span>, the sum <span class="math inline">\(N_1+N_2\)</span> has the distribution function
<span class="math display">\[
\Pr[N_1+N_2\leq k]=\sum_{j=0}^k\Pr[N_1\leq k-j]\Pr[N_2=j],
\]</span>
which again gives
<span class="math display">\[
\Pr[N_1+N_2=k]=\sum_{j=0}^k\Pr[N_1=k-j]\Pr[N_2=j].
\]</span></p>
<div class="example">
<p><span id="exm:unlabeled-div-44" class="example"><strong>Example 3.14  (Convolution of Poisson distributions) </strong></span>Suppose <span class="math inline">\(N_1\sim\mathcal{P}oi(\lambda_1)\)</span> and <span class="math inline">\(N_2\sim\mathcal{P}oi(\lambda_2)\)</span> are independent.
In this case,
<span class="math display">\[\begin{eqnarray*}
\Pr[N_1+N_2=k]&amp;=&amp;\sum_{j=0}^k\exp(-\lambda_1)\frac{\lambda_1^{k-j}}{(k-j)!}\exp(-\lambda_2)\frac{\lambda_2^j}{j!}\\
&amp;=&amp;\exp(-\lambda_1-\lambda_2)\frac{1}{k!}\underbrace{\sum_{j=0}^k\left(
\begin{array}{c}
k \\
j
\end{array}
\right)\lambda_1^{k-j}\lambda_2^j}_{=(\lambda_1+\lambda_2)^k}
\end{eqnarray*}\]</span>
Ainsi, <span class="math inline">\(N_1+N_2\sim\mathcal{P}oi(\lambda_1+\lambda_2)\)</span>. The Poisson distribution is said to be stable by convolution.</p>
</div>
<p>The above example allows us to reconcile the multinomial and Poisson distributions.</p>
<div class="example">
<p><span id="exm:unlabeled-div-45" class="example"><strong>Example 3.15  </strong></span>Let be the independent random variables <span class="math inline">\(N_1\sim\mathcal{P}oi(\lambda_1),\ldots,N_n\sim\mathcal{P}oi(\lambda_n)\)</span>. Conditional on <span class="math inline">\(N_1+\ldots+N_n=m\)</span>,
<span class="math display">\[
(N_1,\ldots,N_n)\sim \mathcal{M}ult(m,\lambda_1/\lambda_\bullet,\ldots,\lambda_n/\lambda_\bullet)
\]</span>
where <span class="math inline">\(\lambda_\bullet=\sum_j\lambda_j\)</span>.</p>
<p>Whatever the integers <span class="math inline">\(k_1,\ldots,k_n\)</span>, equality
<span class="math display">\[
\Pr[N_1=k_1,\ldots,N_n=k_n|N_1+\ldots+N_n=k_1+\ldots+k_n]
\]</span>
<span class="math display">\[
=
\frac{\prod_{j=1}^n\exp(-\lambda_j)\frac{\lambda_j^{k_j}}{k_j!}}
{\exp\left(-\sum_{j=1}^n\lambda_j\right)\frac{\left(\sum_{j=1}^n\lambda_j\right)^{\sum_{j=1}^nk_j}}
{\left(\sum_{j=1}^nk_j\right)!}}
\]</span>
proves the announced result.</p>
</div>
<p>The following example deals with a form of bivariate Poisson distribution.</p>
<div class="example">
<p><span id="exm:PoissonCommonShock" class="example"><strong>Example 3.16  </strong></span>Consider the random pair <span class="math inline">\((N_1,N_2)\)</span> where
<span class="math display">\[
N_1=L_1+M\text{ and }N_2=L_2+M;
\]</span>
M$ represents a “common shock” simultaneously affecting <span class="math inline">\(N_1\)</span> and <span class="math inline">\(N_2\)</span>.
The random variables <span class="math inline">\(L_1\)</span>, <span class="math inline">\(L_2\)</span> and <span class="math inline">\(M\)</span> are assumed to have independent distributions <span class="math inline">\(L_1\sim\mathcal{P}oi(\lambda_1)\)</span>, <span class="math inline">\(L_2\sim\mathcal{P}oi(\lambda_2)\)</span> and <span class="math inline">\(M\sim\mathcal{P}oi(\mu)\)</span>. The marginal distributions of the pair <span class="math inline">\((N_1,N_2)\)</span> are clearly <span class="math inline">\(\mathcal{P}oi(\lambda_1+\mu)\)</span> and <span class="math inline">\(\mathcal{P}oi(\lambda_2+\mu)\)</span>, respectively.</p>
<p>The joint distribution of <span class="math inline">\((N_1,N_2)\)</span> is obtained as follows
<span class="math display">\[\begin{eqnarray*}
&amp; &amp; \Pr[N_1=n_1,N_2=n_2]\\}
&amp;=&amp;\sum_{k=0}^{+\infty}\Pr[N_1=n_1,N_2=n_2|M=k]\Pr[M=k]\\
&amp;=&amp;\sum_{k=0}^{\min\{n_1,n_2\}}\Pr[L_1=n_1-k]\Pr[L_2=n_2-k]\Pr[M=k]\\
&amp;=&amp;\exp(-\lambda_1-\lambda_2-\mu)\sum_{k=0}^{\min\{n_1,n_2\}}\frac{\mu^k}{k!}
\frac{\lambda_1^{n_1-k}}{(n_1-k)!}\frac{\lambda_2^{n_2-k}}{(n_2-k)!}.
\end{eqnarray*}\]</span>}
We can then calculate the conditional distribution of <span class="math inline">\(N_2\)</span> knowing <span class="math inline">\(N_1=n_1\)</span>:
<span class="math display">\[\begin{eqnarray*}
&amp; &amp; \Pr[N_2=n_2|N_1=n_1]\\\
&amp;=&amp;\exp(-\lambda_1-\lambda_2-\mu)\sum_{k=0}^{\min\{n_1,n_2\}}\frac{\mu^k}{k!}
\frac{\lambda_1^{n_1-k}}{(n_1-k)!}\frac{\lambda_2^{n_2-k}}{(n_2-k)!}\\
&amp; &amp; \hspace{10mm}\times\left\{
\exp(-\lambda_1-\mu)\frac{(\lambda_1+\mu)^{n_1}}{n_1!}\right\}^{-1}\\
&amp;=&amp;\exp(-\lambda_2)\sum_{k=0}^{\min\{n_1,n_2\}}\left(
\begin{array}{c}
n_1 \\
k
\end{array}
\right)\frac{\mu^k\lambda_1^{n_1-k}\lambda_2^{n_2-k}}
{(n_2-k)!(\lambda_1+\mu)^{n_1}}.
\end{eqnarray*}\]</span>da_1+)^{n_1}}.
\end{eqnarray*}</p>
</div>
<p>In general, the <a href="https://en.wikipedia.org/wiki/Convolution">convolution product</a> (henceforth referred to as “<span class="math inline">\(\star\)</span>”) of distribution functions provides the distribution function of a sum of independent random variables admitting these distribution functions.</p>
<div class="definition">
<p><span id="def:unlabeled-div-46" class="definition"><strong>Definition 3.12  </strong></span>Given two distribution functions <span class="math inline">\(F_1\)</span> and
<span class="math inline">\(F_2\)</span>, the convolution product <span class="math inline">\(F_1star F_2\)</span> is defined as follows:
<span class="math display">\[
F_1\star F_2(x)=\int_{y\in\mathbb{R}}F_1(x-y)dF_2(y),\hspace{2mm}x\in \mathbb{R}.
\]</span></p>
</div>
<p>In the context of this book, the distribution functions considered will most often have a support contained in <span class="math inline">\({\mathbb{R}}^+\)</span>. In this case, the convolution product <span class="math inline">\(F_1star F_2\)</span> is given by
<span class="math display">\[
F_1\star F_2(x)=\int_{y=0}^xF_1(x-y)dF_2(y),\hspace{2mm}x\geq 0.
\]</span></p>
<p>If we denote <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> as independent random variables with distribution functions <span class="math inline">\(F_1\)</span> and <span class="math inline">\(F_2\)</span>, it is clear that
<span class="math display">\[\begin{eqnarray*}
F_1star F_2(x)&amp;=&amp;\Pr[X_1+X_2\leq x].
\end{eqnarray*}\]</span></p>
<p>If <span class="math inline">\(F_1\)</span> and <span class="math inline">\(F_2\)</span> admit the probability densities <span class="math inline">\(f_1\)</span> and <span class="math inline">\(f_2\)</span>, the density of <span class="math inline">\(X_1+X_2\)</span>, denoted <span class="math inline">\(f_1star f_2\)</span>, is given by
<span class="math display">\[
f_1\star f_2(x)=\frac{d}{dx}F_1\star F_2(x)=
\int_{y\in\mathbb{R}}f_1(x-y)f_2(y)dy,\hspace{2mm}x\in\mathbb{R}.
\]</span>
When the supports of <span class="math inline">\(F_1\)</span> and <span class="math inline">\(F_2\)</span> are contained in <span class="math inline">\(\mathbb{R}^+\)</span>, this can be rewritten as follows
<span class="math display">\[
f_1\star f_2(x)=\frac{d}{dx}F_1\star F_2(x)=
\int_{y=0}^xf_1(x-y)f_2(y)dy,\hspace{2mm}x\geq 0.
\]</span></p>
<div class="example">
<p><span id="exm:unlabeled-div-47" class="example"><strong>Example 3.17  (Convolution of Gamma distributions) </strong></span>Consider <span class="math inline">\(X_1\sim\mathcal{G}am(\alpha_1,\tau)\)</span> and <span class="math inline">\(X_2\sim\mathcal{G}am(\alpha_2,\tau)\)</span>, mutually independent.
The density of the sum <span class="math inline">\(X_1+X_2\)</span> is given by
<span class="math display">\[
\frac{1}{\Gamma(\alpha_1)\Gamma(\alpha_2)}\int_0^x(x-y)^{\alpha_1-1}\tau^{\alpha_1+\alpha_2}y^{\alpha_2-1}\exp(-x\tau)dy
\]</span>
<span class="math display">\[
=\exp(-x\tau)\tau^{\alpha_1+\alpha_2}\frac{x^{\alpha_1+\alpha_2-1}}{\Gamma(\alpha_1+\alpha_2)}.
\]</span>
We find that <span class="math inline">\(X_1+X_2\sim\mathcal{G}am(\alpha_1+\alpha_2,\tau)\)</span>, so the family of Gamma distributions is stable by convolution.</p>
</div>
<p>We’ll come back to the problem of determining the distribution function of a sum <span class="math inline">\(X_1+X_2\)</span> of dependent random variables in Chapter <a href="chap8.html#chap8">9</a>.</p>
</div>
<div id="distribution-function-associated-with-a-compound-distribution" class="section level3 hasAnchor" number="3.14.3">
<h3><span class="header-section-number">3.14.3</span> Distribution function associated with a compound distribution<a href="chap2.html#distribution-function-associated-with-a-compound-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If we denote <span class="math inline">\(F_X\)</span> the joint distribution function of <span class="math inline">\(X_i\)</span>, then the distribution function <span class="math inline">\(F_S\)</span> of <span class="math inline">\(S\)</span> defined in <a href="#eq:Compounddistribution">(<strong>??</strong>)</a> is given by
<span class="math display">\[\begin{eqnarray*}
F_S(s)&amp;=&amp;\Pr\left[\sum_{i=1}^NX_i\leq s\right]\\
&amp;=&amp;\sum_{k=0}^{+\infty}\Pr\left[\sum_{i=1}^NX_i\leq s,N=k\right]\\
&amp;=&amp;\sum_{k=0}^{+\infty}\Pr\left[\sum_{i=1}^kX_i\leq s\right]\Pr[N=k]\\
&amp;=&amp;\sum_{k=0}^{+\infty}\Pr[N=k]F_X^{\star(k)}(s),\hspace{2mm}s\in {\mathbb{R}}^+,
\end{eqnarray*}\]</span>
where the sequence <span class="math inline">\(\{F_X^{\star(k)},\hspace{2mm}k \in {\mathbb{N}}\}\)</span> of convolution powers of <span class="math inline">\(F_X\)</span> satisfies the recurrence scheme
<span class="math display">\[\begin{eqnarray*}
F_X^{\star(k)}(x)&amp;=&amp;F_X^{\star(k-1)}\star F_X(x)\\
&amp;=&amp;\int_{y=0}^xF_X^{\star(k-1)}(x-y)dF_X(y),\hspace{2mm}
x\in {\mathbb{R}}^+,\hspace{2mm} k=1,2,\ldots,
\end{eqnarray*}\]</span>
starting from <span class="math inline">\(F_X^{\star(1)}=F_X\)</span> and with the convention
<span class="math display">\[
F_X^{\star(0)}(x)=\left\{
\begin{array}{l}
0,\mbox{ if }x&lt;0,\}
1,\mbox{ otherwise}.
\end{array}
\right.
\]</span></p>
<div class="example">
<p><span id="exm:unlabeled-div-48" class="example"><strong>Example 3.18  (Compound binomial distribution) </strong></span>If <span class="math inline">\(N\sim\mathcal{B}in(n,q)\)</span> then we’re talking about a compound binomial distribution, which is written <span class="math inline">\(S\sim\mathcal{CB}in(n,q,F_X)\)</span>. In this case,
<span class="math display">\[
F_S(s)=\sum_{k=0}^n\left(
\begin{array}{c}
n \\
k
\end{array}
\right)q^k(1-q)^{n-k}F_X^{\star(k)}(s).
\]</span></p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-49" class="example"><strong>Example 3.19  (Compound Poisson distribution) </strong></span>If <span class="math inline">\(N\sim\mathcal{P}oi(\lambda)\)</span>, we speak of a <a href="https://en.wikipedia.org/wiki/Compound_Poisson_distribution">compound Poisson</a> distribution, and we’ll note <span class="math inline">\(S\sim\mathcal{CP}oi(\lambda,F_X)\)</span>. In this case,
<span class="math display">\[
F_S(s)=\sum_{k=0}^{+\infty}\exp(-\lambda)\frac{\lambda^k}{k!}F_X^{\star(k)}(s).
\]</span></p>
</div>
</div>
</div>
<div id="RiskTransformations" class="section level2 hasAnchor" number="3.15">
<h2><span class="header-section-number">3.15</span> Transformations of risks and conventional damage clauses<a href="chap2.html#RiskTransformations" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="concept" class="section level3 hasAnchor" number="3.15.1">
<h3><span class="header-section-number">3.15.1</span> Concept<a href="chap2.html#concept" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In practice, the partial guarantee provided by the insurer means that a random variable <span class="math inline">\(S\)</span>, representing the financial loss threatening the insured’s assets, is replaced by a random variable <span class="math inline">\(g(S)\)</span>, deemed more advantageous by the insured. The insurance contract therefore does not eliminate the risk from the insured’s assets, but transforms it to make it acceptable.</p>
<p>The aim of this section is to examine the impact of standard clauses limiting the insurer’s intervention.</p>
</div>
<div id="DecOblig" class="section level3 hasAnchor" number="3.15.2">
<h3><span class="header-section-number">3.15.2</span> The compulsory overdraft<a href="chap2.html#DecOblig" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When a compulsory overdraft is stipulated in the general terms and conditions, the insurer leaves part of the claims amount to be paid by the insured. Mandatory overdraft is a doctrinal concept that is not used in regulatory texts (which only speak of deductible, the latter term actually covering another contractual clause relating to damages, discussed below).</p>
<p>When a policy provides for a compulsory overdraft of <span class="math inline">\(\delta\)</span> and the amount of the claim is <span class="math inline">\(X\)</span>, the amount <span class="math inline">\(Y\)</span> of the indemnity paid by the insurer is equal to
<span class="math display">\[
Y=\left\{
\begin{array}{l}
0,\mbox{ if }X\leq \delta,
\\
X-\delta,\mbox{ if }X&gt;\delta.
\end{array}
\right.
\]</span>
Note that mandatory overdraft affects</p>
<ul>
<li>the number of claims, since claims for amounts less than the mandatory overdraft are not reported to the insurer;</li>
<li>the amount of claims, since claims in excess of the compulsory overdraft will be deducted from the compulsory overdraft.</li>
</ul>
<p>The distribution function of <span class="math inline">\(Y\)</span> is obtained as follows: there is a positive probability that <span class="math inline">\(Y\)</span> is zero, given by
<span class="math display">\[
\Pr[Y=0]=\Pr[X\leq\delta]=F_X(\delta).
\]</span>
Then, for <span class="math inline">\(y&gt;0\)</span>,
<span class="math display">\[
F_Y(y)=\Pr[Y\leq y]=\Pr[X\leq \delta+y]=F_X(\delta+y).
\]</span>
The distribution function of <span class="math inline">\(Z\)</span> holds for <span class="math inline">\(z&gt;0\)</span>.
<span class="math display">\[\begin{eqnarray*}
F_Z(z)&amp;=&amp;\Pr[Z\leq z|Y&gt;0]=\Pr[Y\leq z|Y&gt;0]\\
&amp;=&amp;\frac{\Pr[0&lt;Y\leq z]}{\Pr[Y&gt;0]}=\frac{\Pr[\delta&lt;X\leq z+\delta]}{1-F_X(\delta)}\\
&amp;=&amp;\frac{F_X(z+\delta)-F_X(\delta)}{1-F_X(\delta)}
\end{eqnarray*}\]</span>
i.e.
<span class="math display">\[
F_Z(z)=\left\{
\begin{array}{l}
0,\mbox{ if }z\leq 0,\\
\displaystyle\frac{F_X(z+\delta)-F_X(\delta)}{1-F_X(\delta)},\mbox{ if }z&gt;0.
\end{array}
\right.
\]</span></p>
</div>
<div id="the-deductible" class="section level3 hasAnchor" number="3.15.3">
<h3><span class="header-section-number">3.15.3</span> The deductible<a href="chap2.html#the-deductible" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Where a deductible is stipulated in the general conditions, the insurer covers the full amount of the loss if it exceeds the <a href="https://en.wikipedia.org/wiki/Deductible">deductible</a>. On the other hand, claims for amounts below the deductible are not eligible for compensation. The deductible therefore influences the number of claims compensated (and therefore reported to the company), but does not affect the amount of claims exceeding the deductible.</p>
<p>In formal terms, the deductible entitles the claimant to full reimbursement, provided that the amount <span class="math inline">\(X\)</span> of the claim exceeds a specified amount <span class="math inline">\(\kappa\)</span>. In this case, the amount <span class="math inline">\(Y\)</span> of the indemnity to which the insured who has suffered a loss in the amount <span class="math inline">\(X\)</span> is entitled is given by
<span class="math display">\[
Y=\left\{
\begin{array}{l}
0,\mbox{ if }X\leq \kappa,
\\
X,\mbox{ if }X&gt;\kappa.
\end{array}
\right.
\]</span>
Therefore,
<span class="math display">\[
\Pr[Y=0]=\Pr[X\leq\kappa]=F_X(\kappa).
\]</span>
Then, for <span class="math inline">\(y&gt;0\)</span>,
<span class="math display">\[
F_Y(y)=\left\{
\begin{array}{l}
F_X(\kappa),\mbox{ if }y\leq\kappa\\
F_X(y),\mbox{ if }y&gt;\kappa.
\end{array}
\right.
\]</span>
The company’s effective payment <span class="math inline">\(Z\)</span> admits the distribution function
<span class="math display">\[\begin{eqnarray*}
F_Z(z)&amp;=&amp;\Pr[Y\leq z|Y&gt;0]=\frac{\Pr[0&lt;Y\leq z]}{\Pr[Y&gt;0]}
&amp;=&amp;\left\{
\begin{array}{l}
0,\mbox{ if }z\leq \kappa,\\
\displaystyle\frac{F_X(z)-F_X(\kappa)}{1-F_X(\kappa)},\mbox{ if }z&gt;\kappa.
\end{array}
\right.
\end{eqnarray*}\]</span>
Most of the time, the statistics available concern <span class="math inline">\(Z\)</span> payments and not the actual cost of <span class="math inline">\(X\)</span> claims. This is because companies record the amounts paid out under their contracts, and often do not keep track of the actual cost of claims.</p>
</div>
<div id="upper-limit-of-indemnity" class="section level3 hasAnchor" number="3.15.4">
<h3><span class="header-section-number">3.15.4</span> (Upper) limit of indemnity<a href="chap2.html#upper-limit-of-indemnity" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Many policies provide for an upper limit to the amount of compensation the insurer may be required to pay under the contract. If we note <span class="math inline">\(\omega\)</span> the indemnity ceiling, the sum <span class="math inline">\(Y\)</span> that the insured will receive following a loss of <span class="math inline">\(X\)</span> will be
<span class="math display">\[
Y=\min\{X,\omega\}=\left\{
\begin{array}{l}
X,\mbox{ if }X\leq \omega,
\\
\omega,\mbox{ if }X&gt;\omega.
\end{array}
\right.
\]</span>
Hence, the distribution function <span class="math inline">\(F_Y\)</span> of <span class="math inline">\(Y\)</span> is expressed as follows:
<span class="math display">\[
F_Y(y)=\left\{
\begin{array}{l}
F_X(y),\mbox{ if }y&lt;\omega,\\
1,\mbox{ if }y\geq\omega.
\end{array}
\right.
\]</span>
A mass point therefore appears in <span class="math inline">\(\omega\)</span> where all claims above the <span class="math inline">\(\omega\)</span> compensation ceiling are concentrated.</p>
</div>
<div id="technical-consequence-censored-data" class="section level3 hasAnchor" number="3.15.5">
<h3><span class="header-section-number">3.15.5</span> Technical consequence: censored data<a href="chap2.html#technical-consequence-censored-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The contractual clauses examined above have important consequences for the actuary. Indeed, the actuary is only concerned with the amount <span class="math inline">\(X\)</span> of the loss suffered by the insured, whereas he or she often only has access to the amounts <span class="math inline">\(Z\)</span> paid by the insurer as compensation. Knowledge of the amounts paid by an insurance company that has introduced clauses limiting its intervention in the terms and conditions of its policies is not equivalent to knowledge of the claim amounts themselves. This is a phenomenon of censorship which the actuary must take into account when considering a modification to the general policy conditions.</p>
</div>
<div id="poissons-distribution-and-damage-clauses" class="section level3 hasAnchor" number="3.15.6">
<h3><span class="header-section-number">3.15.6</span> Poisson’s distribution and damage clauses<a href="chap2.html#poissons-distribution-and-damage-clauses" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Poisson’s distribution has the following property, which makes it very easy to use when the policy provides for a deductible or compulsory overdraft.</p>
<div class="proposition">
<p><span id="prp:PropPoisson" class="proposition"><strong>Proposition 3.12  </strong></span>Let <span class="math inline">\(N\sim\mathcal{P}oi(\lambda)\)</span> be the number of claims affecting an insurance company during a reference period. Suppose that these claims fall into <span class="math inline">\(m\)</span> categories, with probabilities <span class="math inline">\(q_1,q_2, \ldots,q_m\)</span>, respectively (according to a multinomial scheme), and that <span class="math inline">\(N_i\)</span> denotes the number of claims of type <span class="math inline">\(i\)</span>, <span class="math inline">\(i=1,2,\ldots,m\)</span>. The random variables <span class="math inline">\(N_1,N_2, \ldots,N_m\)</span> are independent Poisson random variables with respective parameters <span class="math inline">\(\lambda q_1\)</span>, <span class="math inline">\(\lambda q_2\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(\lambda q_m\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-50" class="proof"><em>Proof</em>. </span>First, note that conditional on <span class="math inline">\(N=n\)</span>, <span class="math inline">\(N_i\sim\mathcal{B}in(n,q_i)\)</span>.
This allows us to write
<span class="math display">\[\begin{eqnarray*}
\Pr[N_i=k]&amp;=&amp;\sum_{n=k}^{+\infty}\Pr[N_i=k|N=n]\Pr[N=n]\\
&amp;=&amp;\sum_{n=k}^{+\infty}\left(
\begin{array}{c}
n \\
k
\end{array}
\right)q_i^k(1-q_i)^{n-k}\exp(-\lambda)\frac{\lambda^n}{n!}\\
&amp;=&amp;\exp(-\lambda)\frac{(\lambda q_i)^k}{k!}\underbrace{\sum_{n=k}^{+\infty}\frac{1}{(n-k)!}
(1-q_i)^{n-k}\lambda^{n-k}}_{=\exp(\lambda(1-q_i))}\\
&amp;=&amp;\exp(-\lambda q_i)\frac{(\lambda q_i)^k}{k!},
\end{eqnarray*}\]</span>
which shows that <span class="math inline">\(N_i\sim\mathcal{P}oi(\lambda q_i)\)</span>. To show the independence of <span class="math inline">\(N_i\)</span>, simply write
<span class="math display">\[\begin{eqnarray*}
&amp;&amp;\Pr[N_1=k_1,\ldots,N_m=k_m]\\
&amp;=&amp;
\Pr[N_1=k_1,\ldots,N_m=k_m|N=k_1+\ldots+k_m]\\
&amp; &amp; \exp(-\lambda)\frac{\lambda^{k_1+\ldots+k_m}}{(k_1+\ldots+k_m)!}\\
&amp;=&amp;\frac{(k_1+\ldots+k_m)!}{k_1!\ldots k_m!}(q_1)^{k_1}\ldots(q_m)^{k_m}\\
&amp;&amp;\exp(-\lambda)\frac{\lambda^{k_1+\ldots+k_m}}{(k_1+\ldots+k_m)!}\\
&amp;=&amp;\prod_{j=1}^m\exp(-\lambda q_j)\frac{(\lambda q_j)^{k_j}}{k_j!}.
\end{eqnarray*}\]</span></p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-51" class="example"><strong>Example 3.20  </strong></span>Suppose an insurance company markets policies with a deductible (or compulsory overdraft) of <span class="math inline">\(\kappa\)</span>. This gives rise to two types of claims: those under <span class="math inline">\(\kappa\)</span> (which are not reported to the insurer) and those over <span class="math inline">\(\kappa\)</span> (which are reported to and paid by the insurer). If <span class="math inline">\(N\sim\mathcal{P}oi(\lambda)\)</span> is the number of claims relating to a policy, and if there is independence between the number and cost of claims, which are themselves independent and have the same distribution, the number <span class="math inline">\(N_\kappa\)</span> of claims declared per policy always has a Poisson distribution, with mean <span class="math inline">\(\lambda q\)</span> where <span class="math inline">\(q\)</span> is the probability that the amount of a claim exceeds <span class="math inline">\(\kappa\)</span>.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-52" class="remark"><em>Remark</em>. </span>In the case, often encountered in practice (possibly after discretization) where the claim amounts <span class="math inline">\(X_1,X_2,X_3,\ldots\)</span> take only a finite number of distinct values <span class="math inline">\(\{v_1,v_2,\ldots,v_p\}\)</span> let’s say, the Property <a href="chap2.html#prp:PropPoisson">3.12</a> provides the following representation of <span class="math inline">\(S\sim\mathcal{CP}oi(\lambda,F)\)</span>:
<span class="math display">\[
S=v_1N_1+v_2N_2+\ldots+v_pN_p,
\]</span>
where the random variables <span class="math inline">\(N_1,N_2,\ldots,N_p\)</span> are independent and have Poisson distributions with respective parameters <span class="math inline">\(\lambda\Pr[X=v_1],\lambda\Pr[X=v_2],\ldots,\lambda\Pr[X=v_p]\)</span>.</p>
</div>
</div>
<div id="perverse-effects-of-contractual-damage-clauses" class="section level3 hasAnchor" number="3.15.7">
<h3><span class="header-section-number">3.15.7</span> Perverse effects of contractual damage clauses<a href="chap2.html#perverse-effects-of-contractual-damage-clauses" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Finally, it should be noted that the contractual clauses examined in this section sometimes have an influence on the amounts of claims reported to companies, or even on the reporting of claims itself. For example, when a policy provides for a compulsory overdraft of <span class="math inline">\(\delta\)</span>, this will have the effect of reducing claims for amounts only slightly in excess of <span class="math inline">\(\delta\)</span>; the insured often prefers to pay this out of his or her own pocket, rather than recovering only a few euros at the cost of administrative hassle and possibly higher future premiums (when a bonus-malus type mechanism is provided for in the general conditions). On the other hand, when a deductible is included, the declared amount of the claim is sometimes “inflated”, so as to exceed the amount of the deductible and thus entitle the claimant to compensation. As a result, it is not uncommon to observe an abnormal concentration of claim amounts slightly in excess of the deductible.</p>
<p>What’s more, compulsory deductibles and overdrafts are difficult to apply in liability insurance. In motor liability, for example, the insurer of the at-fault driver will have to compensate the victim in full for his loss (the deductible clause not being enforceable against the injured third party), and will then have to recover the deductible or compulsory overdraft from his policyholder. As recovery of this sum is often extremely complicated, liability insurers rarely use this type of clause.</p>
<p>Mandatory deductibles and overdrafts, on the other hand, are much appreciated when the insurer can deduct them from the indemnity paid to the insured (as in vehicle damage or fire insurance, for example).</p>
</div>
</div>
<div id="exercises" class="section level2 hasAnchor" number="3.16">
<h2><span class="header-section-number">3.16</span> Exercises<a href="chap2.html#exercises" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="exercise">
<p><span id="exr:unlabeled-div-53" class="exercise"><strong>Exercise 3.1  (Pareto and log-exponential distributions) </strong></span>If <span class="math inline">\(X\sim\mathcal{P}ar(\alpha,\theta)\)</span>, show that
<span class="math display">\[
\ln\left(1+\frac{X}{\theta}\right)\sim\mathcal{E}xp(\alpha).
\]</span></p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-54" class="exercise"><strong>Exercise 3.2  (Bivariate logistic distribution) </strong></span>The univariate logistic distribution has the following distribution function
<span class="math display">\[
F_{X}\left( x\right) =\frac{1}{1+\exp(-x)}
\]</span>
for <span class="math inline">\(x\in{\mathbb{R}}\)</span>, and satifies <span class="math inline">\(f_{X}\left(x\right)=F_{X}\left( x\right)\left(1-F_{X}\left( x\right) \right)\)</span>.
A bivariate version of this distribution introduced by Gumbel has the joint distribution function
<span class="math display">\[\begin{equation*}
F_{\boldsymbol{x}}\left( x_1,x_2\right) =\frac{1}{1+\exp(-x_1)+\exp(-x_2)}\text{ for }\boldsymbol{x}\in {\mathbb{R}}^2.
\end{equation*}\]</span>
Show that</p>
<ol style="list-style-type: decimal">
<li>the probability that <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are of opposite sign is <span class="math inline">\(1/3\)</span>.</li>
<li>the conditional density of <span class="math inline">\(X_1\)</span> knowing <span class="math inline">\(X_2=x_2\)</span> is
<span class="math display">\[\begin{equation*}
f_{1|2}\left( x_1|x_2\right) =\frac{2\exp(-x_1)\big( 1-\exp(-x_2)\big)^{2}}
{\big( 1+\exp(-x_1)+\exp(-x_2)\big) ^{3}}\text{.}
\end{equation*}\]</span></li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-55" class="exercise"><strong>Exercise 3.3  (Convolution of uniform distributions) </strong></span>Let <span class="math inline">\(U_1,U_2,\ldots\)</span> be a sequence of independent random variables of distribution <span class="math inline">\(\mathcal{U}ni(0,1)\)</span>.</p>
<ol style="list-style-type: decimal">
<li>What is the distribution function of <span class="math inline">\(X_1+X_2\)</span>?</li>
<li>Show that
<span class="math display">\[
\Pr[X_1+\ldots+X_n\leq x]=\frac{1}{n!}\sum_{k=0}^{n-1}(-1)^k\left(
\begin{array}{c}
n \\
k
\end{array}
\right)(x-k)_+^n,\hspace{2mm}0\leq x\leq n,
\]</span>
where <span class="math inline">\((x-k)_+=0\)</span> if <span class="math inline">\(x&lt;k\)</span> and <span class="math inline">\(x-k\)</span> otherwise.</li>
<li>draw from point 2 that the probability density of <span class="math inline">\(X_1+\ldots+X_n\)</span> is given for <span class="math inline">\(x\in[0,n]\)</span> by
<span class="math display">\[
\frac{1}{(n-1)!}\sum_{k=0}^{j}(-1)^k\left(
\begin{array}{c}
n \\
k
\end{array}
\right)(x-k)^{n-1}
\]</span>
for <span class="math inline">\(j\leq x\leq j+1\)</span> and <span class="math inline">\(j=0,1,\ldots,n-1\)</span>.</li>
<li>draw from point 3 that the density of <span class="math inline">\(\overline{X}^{(n)}=\frac{1}{n}\sum_{i=1}^nX_i\)</span> is given for <span class="math inline">\(0\leq x\leq 1\)</span> by
<span class="math display">\[
\frac{n^n}{(n-1)!}\sum_{k=0}^{[xn]}(-1)^k\left(
\begin{array}{c}
n \\
k
\end{array}
\right)\left(x-\frac{k}{n}\right)^{n-1}.
\]</span></li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:BernPois" class="exercise"><strong>Exercise 3.4  (Distance in total variation) </strong></span>One way of assessing the similarity between the probability distributions of two random counting variables <span class="math inline">\(M\)</span> and <span class="math inline">\(N\)</span> is to calculate the distance between them. Numerous definitions of distances between probability distributions exist, including the distance in total variation, denoted <span class="math inline">\(d_{TV}\)</span>, defined by
<span class="math display">\[
d_{TV}(M,N)=\sum_{k=0}^{+\infty}|\Pr[M=k]-\Pr[N=k]|.
\]</span>
Let <span class="math inline">\(I\sim\mathcal{B}er(q)\)</span> and <span class="math inline">\(N\sim\mathcal{P}oi(\lambda)\)</span> with <span class="math inline">\(\lambda\leq q\)</span>. Show that
<span class="math display">\[
d_{TV}(N,I)=2\{q-\lambda\exp(-\lambda)\}
\]</span>
and that this distance is minimal for <span class="math inline">\(\lambda=q\)</span>.</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-56" class="exercise"><strong>Exercise 3.5  (Proportional compulsory overdraft) </strong></span>A proportional compulsory overdraft leaves a portion of <span class="math inline">\(100\alpha\%\)</span> of the claim amount to be borne by the insured, <span class="math inline">\(0\leq\alpha\leq1\)</span>. This type of clause is often used when the insured’s behavior can influence the total amount of the claim. This is the case, for example, with hospitalization insurance: some companies leave 20% of total medical expenses to be paid by the insured, to encourage him/her to keep hospital stays to a strict minimum. What is the distribution function of the amount paid by the company under such a contract?</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-57" class="exercise"><strong>Exercise 3.6  </strong></span>Assume that the claims costs <span class="math inline">\(C_1,C_2,\ldots\)</span> are independent of the distribution <span class="math inline">\(\mathcal{E}xp(\alpha)\)</span> and let <span class="math inline">\(S_{n}=C_{1}+...+C_{n}\)</span>.</p>
<ol style="list-style-type: decimal">
<li>Show that the distribution functions of <span class="math inline">\(S_n\)</span> and <span class="math inline">\(S_{n+1}\)</span> are related by
<span class="math display">\[\begin{eqnarray*}
&amp;&amp;F_{S_{n}+1}(x)\\
&amp;=&amp;\int_{0}^{+\infty}F_{S_n}(x-t)dF(t)\\
&amp;=&amp;\int_{0}^{+\infty }\left(1-\sum_{j=0}^{n-1}\frac{\left[\alpha( x-t)\right]^j}{j!}\exp\left(-\alpha(x-t)\right)\alpha\exp\left(-\alpha t\right)\right)dt
\end{eqnarray*}\]</span></li>
<li>Deduce that
<span class="math display">\[\begin{equation*}
F_{S_{n}}\left( x\right) =1-\sum_{j=0}^{n-1}\frac{\left[ \alpha x\right] ^{j}
}{j!}\exp \left( -\alpha x\right).
\end{equation*}\]</span></li>
<li>Noting $(j)=$, show that the distribution function of <span class="math inline">\(S=\sum_{k=1}^NC_k\)</span> is written as
<span class="math display">\[\begin{equation*}
F_S\left( x\right) =1-\exp \left( -\alpha x\right) \sum_{j=0}^{\infty }
\overline{p}\left( j\right) \frac{\left[ \alpha x\right] ^{j}}{j!}.
\end{equation*}\]</span></li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-58" class="exercise"><strong>Exercise 3.7  (Marshall and Olkin's bivariate exponential distribution) </strong></span>Consider independent random variables <span class="math inline">\(Y_1\)</span>, <span class="math inline">\(Y_2\)</span> and <span class="math inline">\(Z\)</span>.
Let <span class="math inline">\(H_1\)</span> and <span class="math inline">\(H_2\)</span> be the distribution functions of <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span>,
and assume <span class="math inline">\(Z\sim\mathcal{E}xp(\theta)\)</span>. Now let’s define
<span class="math display">\[\begin{cases} X_1 = \min\{Y_1,Z\}, \\ X_2 = \min\{Y_2,Z\}. \end{cases}\]</span></p>
<ol style="list-style-type: decimal">
<li>Show that the distribution functions of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are given by
<span class="math display">\[ F_{X_j}(x_j) = 1-\exp(-\theta x_j) {\overline{H}}_j(x_j), \]</span>
for <span class="math inline">\(j=1,2\)</span>.</li>
<li>Show that the joint distribution function of <span class="math inline">\(\boldsymbol{X}\)</span> is given by
<span class="math display" id="eq:BivExpMo">\[
F_{\boldsymbol{X}}({\boldsymbol{x}})=F_{X_1}(x_1)+F_{X_2}(x_2)-1+\exp(\theta\max\{x_1,x_2\}) {\overline{F}}_{X_1}(x_1){\overline{F}}_{X_2}(x_2).
\tag{3.15}
\]</span></li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:SecBivPar" class="exercise"><strong>Exercise 3.8  (Bivariate Pareto distribution) </strong></span>Assume that conditional on <span class="math inline">\(\Theta\)</span>, the random variables <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent
and have the distribution <span class="math inline">\(\mathcal{E}xp(\theta)\)</span>. If <span class="math inline">\(\Theta\sim\mathcal{G}am(\alpha,\lambda)\)</span>, show that
<span class="math display" id="eq:CdfBivPar">\[
F_{{\boldsymbol{X}}}({\boldsymbol{x}})=F_{X_1}(x_1) + F_{X_2}(x_2) - 1 + \left\{
\left( {\overline{F}}_{X_1}(x_1) \right)^{-1/\alpha} \left({\overline{F}}_{X_2}(x_2) \right)^{-1/\alpha}-1\right\}^{-\alpha}.\tag{3.16}
\]</span></p>
</div>
</div>
<div id="bibliographical-notes" class="section level2 hasAnchor" number="3.17">
<h2><span class="header-section-number">3.17</span> Bibliographical notes<a href="chap2.html#bibliographical-notes" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The two volumes of <span class="citation">(<a href="#ref-feller1950introduction" role="doc-biblioref">Feller 1950</a>)</span> are still first-rate works on probability calculus. <span class="citation">(<a href="#ref-chow2003probability" role="doc-biblioref">Chow and Teicher 2003</a>)</span> provides a good technical introduction to the subject.</p>
<p>For more details on the parametric models presented in this chapter, the reader may wish to consult <span class="citation">(<a href="#ref-johnson1992univariate" role="doc-biblioref">Johnson, Kemp, and Kotz 2005</a>)</span> for univariate counting models, and <span class="citation">(<a href="#ref-johnson1995discrete" role="doc-biblioref">Johnson, Kotz, and Balakrishnan 1995a</a>)</span> for multivariate counting models. For continuous models, please refer to <span class="citation">(<a href="#ref-johnson1995continuous" role="doc-biblioref">Johnson, Kotz, and Balakrishnan 1995b</a>)</span> for univariate distributions, and <span class="citation">(<a href="#ref-johnson1996continuous" role="doc-biblioref">Johnson, Kotz, and Balakrishnan 1996</a>)</span> for multivariate continuous distributions. For a more actuarial presentation of these parametric models, see <span class="citation">(<a href="#ref-klugman1998loss" role="doc-biblioref">Klugman, Panjer, and Willmot 1998</a>)</span>. Classic works on multivariate distributions include <span class="citation">(<a href="#ref-mardia1970families" role="doc-biblioref">Mardia 1970</a>)</span>, <span class="citation">(<a href="#ref-hutchinson1991engineering" role="doc-biblioref">Hutchinson and Lai 1991</a>)</span> and <span class="citation">(<a href="#ref-joe1997multivariate" role="doc-biblioref">Joe 1997</a>)</span>.</p>

</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-chow2003probability" class="csl-entry">
Chow, Yuan Shih, and Henry Teicher. 2003. <em>Probability Theory: Independence, Interchangeability, Martingales</em>. Springer.
</div>
<div id="ref-feller1950introduction" class="csl-entry">
Feller, William. 1950. <em>An Introduction to Probability Theory and Its Applications</em>. John Wiley &amp; Sons.
</div>
<div id="ref-hutchinson1991engineering" class="csl-entry">
Hutchinson, TP, and Chin Diew Lai. 1991. <em>The Engineering Statistician’s Guide to Continuous Bivariate Distributions</em>. Rumsby Scientific Publisher.
</div>
<div id="ref-joe1997multivariate" class="csl-entry">
Joe, Harry. 1997. <em>Multivariate Models and Multivariate Dependence Concepts</em>. CRC press.
</div>
<div id="ref-johnson1992univariate" class="csl-entry">
Johnson, Norman L, Adrienne W Kemp, and Samuel Kotz. 2005. <em>Univariate Discrete Distributions</em>. Vol. 444. John Wiley &amp; Sons.
</div>
<div id="ref-johnson1995discrete" class="csl-entry">
Johnson, Norman L, Samuel Kotz, and Narayanaswamy Balakrishnan. 1995a. <em>Continuous Multivariate Distributions</em>. John wiley &amp; sons.
</div>
<div id="ref-johnson1995continuous" class="csl-entry">
———. 1995b. <em>Continuous Univariate Distributions</em>. John wiley &amp; sons.
</div>
<div id="ref-johnson1996continuous" class="csl-entry">
———. 1996. <em>Continuous Multivariate Distributions</em>. John wiley &amp; sons.
</div>
<div id="ref-klugman1998loss" class="csl-entry">
Klugman, Stuart A, Harry H Panjer, and Gordon E Willmot. 1998. <em>Loss Models: From Data to Decisions</em>. John Wiley &amp; Sons.
</div>
<div id="ref-mardia1970families" class="csl-entry">
Mardia, Kantilal Varichand. 1970. <em>Families of Bivariate Distributions</em>. Giffin.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="chap1.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="chap3.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/03-Modeling.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
